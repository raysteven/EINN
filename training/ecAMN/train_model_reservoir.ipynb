{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74aacc47",
   "metadata": {
    "id": "725c9c6d",
    "tags": []
   },
   "source": [
    "# Install conda on your Colab environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9004b54",
   "metadata": {},
   "source": [
    "Ignore this first cell if you are running the notebook in a local environment.\n",
    "\n",
    "One can still run it locally but it will have no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba604c53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40951,
     "status": "ok",
     "timestamp": 1664526150927,
     "user": {
      "displayName": "Leon Faure",
      "userId": "06483081905364613855"
     },
     "user_tz": -120
    },
    "id": "c4f08880",
    "outputId": "eaae29a9-4739-4b0f-a2a7-56bfa89f0bf0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell first - it will install a conda distribution (mamba)\n",
    "# on your Drive then restart the kernel automatically \n",
    "# (don't worry about the crashing/restarting kernel messages)\n",
    "# It HAS to be runned FIRST everytime you use the notebook in colab\n",
    "\n",
    "import os\n",
    "import sys\n",
    "RunningInCOLAB  = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "    condacolab.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a763d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up your Colab or local environment\n",
    "# Then import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1f4cc",
   "metadata": {},
   "source": [
    "Run this cell in both cases of use (local or Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3795193b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117260,
     "status": "ok",
     "timestamp": 1664526767265,
     "user": {
      "displayName": "Leon Faure",
      "userId": "06483081905364613855"
     },
     "user_tz": -120
    },
    "id": "BYwheAEcr-ME",
    "outputId": "8ba41a54-6751-4c00-ed1a-938db78cafb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yeast9_medium.ipynb', '.git', 'Build_Model_AMN_Ray.ipynb', 'Create_Medium.ipynb', 'files', 'environment_amn_RS2.yml', 'Build_Dataset_parallel.ipynb', 'boxplot_train_vs_val_loss.png', 'Build_Model_MM.ipynb', 'Build_Model_AMN_KO.ipynb', 'LICENSE', 'Dataset_model', 'Build_Dataset.ipynb', 'environment_amn_RS4.yml', 'Build_Model_AMN.ipynb', 'violin_train_vs_val_loss.png', 'Dataset_experimental', 'Tutorial.ipynb', 'growth_rate_plot.html', 'Build_Model_AMN_KO_Ray.ipynb', 'Build_Model_RC (Copy).ipynb', 'Build_Dataset_Ray2.ipynb', 'environment_amn_RS.yml', 'environment_amn_light.yml', 'Build_Dataset_KO_Ray.ipynb', 'Result', 'README.md', 'val_loss_folds_per_seed.png', 'violin_val_loss_all_folds.png', 'Build_Experimental.ipynb', 'Build_Model_RC.ipynb', 'Dataset_input', 'environment_amn.yml', 'Build_Dataset_KO.ipynb', 'Build_Dataset_3.ipynb', 'Figures', 'Library', 'Build_Model_ANN_Dense.ipynb', 'amn_dep_manual.bat', 'Reservoir', 'Build_Dataset_2.ipynb', 'Duplicate_Model.ipynb', 'environment_amn_RS3.yml', '.gitignore', 'landscapeviz', 'hist_val_loss.png', 'Figures.ipynb', 'Analyze_Various_Data.ipynb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 20:07:39.280225: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-06-19 20:07:39.280248: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: raylinux\n",
      "2025-06-19 20:07:39.280254: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: raylinux\n",
      "2025-06-19 20:07:39.280349: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 550.144.3\n",
      "2025-06-19 20:07:39.280367: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 550.144.3\n",
      "2025-06-19 20:07:39.280372: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 550.144.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "RunningInCOLAB  = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    \n",
    "    # Check everything is fine with conda in Colab\n",
    "    import condacolab\n",
    "    condacolab.check()\n",
    "    \n",
    "    # Mount your drive environment in the colab runtime\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    \n",
    "    # Change this variable to your path on Google Drive to which the repo has been cloned\n",
    "    # If you followed the colab notebook 'repo_cloning.ipynb', nothing to change here\n",
    "    repo_path_in_drive = '/content/drive/My Drive/Github/amn_release/'\n",
    "    # Change directory to your repo cloned in your drive\n",
    "    DIRECTORY = repo_path_in_drive\n",
    "    os.chdir(repo_path_in_drive)\n",
    "    # Copy the environment given in the environment_amn_light.yml\n",
    "    !mamba env update -n base -f environment_amn_light.yml\n",
    "    \n",
    "    # This is one of the few Colab-compatible font\n",
    "    font = 'Liberation Sans'\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # In this case the local root of the repo is our working directory\n",
    "    DIRECTORY = './'\n",
    "    font = 'arial'\n",
    "\n",
    "# printing the working directory files. One can check you see the same folders and files as in the git webpage.\n",
    "print(os.listdir(DIRECTORY))\n",
    "\n",
    "from Library.Build_Model import *\n",
    "\n",
    "# We declare this function here and not in the\n",
    "# function-storing python file to modify it easily\n",
    "# as it can change the printouts of the methods\n",
    "def printout(filename, Stats, model, time): \n",
    "    # printing Stats\n",
    "    print('Stats for %s CPU-time %.4f' % (filename, time))\n",
    "    print('R2 = %.4f (+/- %.4f) Constraint = %.4f (+/- %.4f)' % \\\n",
    "          (Stats.train_objective[0], Stats.train_objective[1],\n",
    "           Stats.train_loss[0], Stats.train_loss[1]))\n",
    "    print('Q2 = %.4f (+/- %.4f) Constraint = %.4f (+/- %.4f)' % \\\n",
    "          (Stats.test_objective[0], Stats.test_objective[1],\n",
    "           Stats.test_loss[0], Stats.test_loss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97945dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_training_history(history_dict, directory, trainname):\n",
    "    \"\"\"\n",
    "    Save the training history to a CSV file with an 'epoch' column.\n",
    "\n",
    "    Parameters:\n",
    "        history_dict (dict): The `history.history` dictionary containing training metrics.\n",
    "        directory (str): The directory where the history file should be saved.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the saved file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)  # Create directory if it doesn't exist\n",
    "\n",
    "    file_path = os.path.join(directory, f\"{trainname}_training_history.csv\")\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    history_df = pd.DataFrame(history_dict)\n",
    "    \n",
    "    # Add the 'epoch' column as the first column\n",
    "    history_df.insert(0, \"epochs\", range(1, len(history_df) + 1))\n",
    "    \n",
    "    # Save to a CSV file\n",
    "    history_df.to_csv(file_path, index=False)\n",
    "    \n",
    "    print(f\"Training history saved to {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def save_metrics_to_excel(metrics, output_file, kfold=False):\n",
    "    \"\"\"\n",
    "    Saves the metrics data to an Excel file with multiple sheets.\n",
    "\n",
    "    Parameters:\n",
    "        metrics (dict): The dictionary containing metrics data.\n",
    "        output_file (str): The path of the Excel file to save.\n",
    "    \"\"\"\n",
    "    if kfold==True:\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            for seed, seed_data in metrics.items():\n",
    "                # Create a DataFrame for each fold and concatenate them\n",
    "                fold_dfs = []\n",
    "                for fold_index, fold_content in seed_data['folds'].items():\n",
    "                    df = pd.DataFrame(fold_content)\n",
    "                    df['fold'] = fold_index  # Add fold information to the DataFrame\n",
    "                    fold_dfs.append(df)\n",
    "\n",
    "                # Combine all folds into a single DataFrame\n",
    "                combined_df = pd.concat(fold_dfs, ignore_index=True)\n",
    "\n",
    "                # Write the DataFrame to a sheet named after the seed\n",
    "                sheet_name = f\"Seed_{seed}\"\n",
    "                combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            for seed, seed_data in metrics.items():\n",
    "                df = pd.DataFrame(seed_data)\n",
    "                # Write the DataFrame to a sheet named after the seed\n",
    "                sheet_name = f\"Seed_{seed}\"\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def histories_to_metrics(histories, kfold=False):\n",
    "    \n",
    "    if kfold == True:\n",
    "        metrics = {}\n",
    "        for seed_index in range(len(histories)):\n",
    "            fold = {}\n",
    "            fold['folds'] = {}\n",
    "            for fold_index in range(len(histories[seed_index])):\n",
    "                fold_content = histories[seed_index][fold_index].history\n",
    "                try:\n",
    "                    fold_content['train_loss'] = fold_content.pop('loss')\n",
    "                    fold_content['train_acc'] = fold_content.pop('my_r2')\n",
    "                    fold_content['val_acc'] = fold_content.pop('val_my_r2')\n",
    "                except:\n",
    "                    pass\n",
    "                fold_content['epochs'] = list(range(1, len(histories[seed_index][fold_index].history['train_loss']) + 1))\n",
    "                fold['folds'][fold_index+1] = fold_content\n",
    "            metrics[seed_index+1] = fold\n",
    "        return metrics\n",
    "    else:\n",
    "        metrics = {}\n",
    "        for seed_index in range(len(histories)):\n",
    "            fold_content = histories[seed_index].history\n",
    "            try:\n",
    "                fold_content['train_loss'] = fold_content.pop('loss')\n",
    "                fold_content['train_acc'] = fold_content.pop('my_r2')\n",
    "                fold_content['val_acc'] = fold_content.pop('val_my_r2')\n",
    "            except:\n",
    "                pass\n",
    "            fold_content['epochs'] = list(range(1, len(histories[seed_index].history['train_loss']) + 1))\n",
    "            metrics[seed_index+1] = fold_content\n",
    "        return metrics\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_metrics(metrics, k_fold=False, log_scale=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes training, validation, and test loss and accuracy, and optionally saves the plots.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics (dict): A dictionary containing loss and accuracy data for multiple seeds. Each key should represent a seed number (e.g., 0, 1, 2), and each value should be another dictionary containing:\n",
    "        - 'epochs': A list or array of epoch numbers\n",
    "        - 'folds': (Optional) A dictionary where each key is the fold number (e.g., 'fold_1') and each value\n",
    "          is another dictionary with the same structure as the non-k-fold metrics.\n",
    "        - 'train_loss': A list or array of training loss values (if not using k-fold)\n",
    "        - 'val_loss': A list or array of validation loss values (if not using k-fold)\n",
    "        - 'test_loss': (Optional) A list or array of test loss values (if not using k-fold)\n",
    "        - 'train_acc': A list or array of training accuracy values (if not using k-fold)\n",
    "        - 'val_acc': A list or array of validation accuracy values (if not using k-fold)\n",
    "        - 'test_acc': (Optional) A list or array of test accuracy values (if not using k-fold)\n",
    "    - k_fold (bool): Whether the data contains k-fold information.\n",
    "    - log_scale (bool): Whether to use a logarithmic (base 10) scale for the y-axis.\n",
    "    - save_path (str): Path to save the generated plots. A timestamped folder will be created within this path.\n",
    "    \"\"\"\n",
    "    def apply_log_scale(ax):\n",
    "        if log_scale:\n",
    "            ax.set_yscale('log')\n",
    "            ax.set_ylabel(f\"Value (Log Scale)\")\n",
    "\n",
    "    def save_plot(fig, filename):\n",
    "        if save_path:\n",
    "            full_path = os.path.join(output_folder, filename)\n",
    "            fig.savefig(full_path, bbox_inches='tight')\n",
    "\n",
    "    def plot_individual(df, metric, title_suffix, filename_suffix):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.lineplot(\n",
    "            data=df[df['Metric'] == metric],\n",
    "            x='Epoch', y='Value', hue='Set', style='Set', markers=False, dashes=False, ax=ax\n",
    "        )\n",
    "        ax.set_title(f'{metric} Over Epochs {title_suffix}')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric)\n",
    "        apply_log_scale(ax)\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            save_plot(fig, f\"{metric.lower()}_{filename_suffix}.png\")\n",
    "        plt.show()\n",
    "\n",
    "    # Create the output folder\n",
    "    if save_path:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_folder = os.path.join(save_path, timestamp)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Combine all seeds into a single DataFrame\n",
    "    all_seeds_data = []\n",
    "\n",
    "    for seed, seed_metrics in metrics.items():\n",
    "        if k_fold:\n",
    "            for fold, fold_metrics in seed_metrics['folds'].items():\n",
    "                epochs = fold_metrics['epochs']\n",
    "                for epoch, train_loss, val_loss, train_acc, val_acc in zip(\n",
    "                    epochs, fold_metrics['train_loss'], fold_metrics['val_loss'], fold_metrics['train_acc'], fold_metrics['val_acc']):\n",
    "                    all_seeds_data.append([seed, fold, epoch, 'Training', 'Loss', train_loss])\n",
    "                    all_seeds_data.append([seed, fold, epoch, 'Validation', 'Loss', val_loss])\n",
    "                    all_seeds_data.append([seed, fold, epoch, 'Training', 'Accuracy', train_acc])\n",
    "                    all_seeds_data.append([seed, fold, epoch, 'Validation', 'Accuracy', val_acc])\n",
    "\n",
    "                if 'test_loss' in fold_metrics and 'test_acc' in fold_metrics:\n",
    "                    for epoch, test_loss, test_acc in zip(epochs, fold_metrics['test_loss'], fold_metrics['test_acc']):\n",
    "                        all_seeds_data.append([seed, fold, epoch, 'Test', 'Loss', test_loss])\n",
    "                        all_seeds_data.append([seed, fold, epoch, 'Test', 'Accuracy', test_acc])\n",
    "        else:\n",
    "            epochs = seed_metrics['epochs']\n",
    "            for epoch, train_loss, val_loss, train_acc, val_acc in zip(\n",
    "                epochs, seed_metrics['train_loss'], seed_metrics['val_loss'], seed_metrics['train_acc'], seed_metrics['val_acc']):\n",
    "                all_seeds_data.append([seed, None, epoch, 'Training', 'Loss', train_loss])\n",
    "                all_seeds_data.append([seed, None, epoch, 'Validation', 'Loss', val_loss])\n",
    "                all_seeds_data.append([seed, None, epoch, 'Training', 'Accuracy', train_acc])\n",
    "                all_seeds_data.append([seed, None, epoch, 'Validation', 'Accuracy', val_acc])\n",
    "\n",
    "            if 'test_loss' in seed_metrics and 'test_acc' in seed_metrics:\n",
    "                for epoch, test_loss, test_acc in zip(epochs, seed_metrics['test_loss'], seed_metrics['test_acc']):\n",
    "                    all_seeds_data.append([seed, None, epoch, 'Test', 'Loss', test_loss])\n",
    "                    all_seeds_data.append([seed, None, epoch, 'Test', 'Accuracy', test_acc])\n",
    "\n",
    "    df_all_seeds = pd.DataFrame(all_seeds_data, columns=['Seed', 'Fold', 'Epoch', 'Set', 'Metric', 'Value'])\n",
    "\n",
    "    # Average across seeds and folds\n",
    "    avg_data = df_all_seeds.groupby(['Epoch', 'Set', 'Metric'])['Value'].mean().reset_index()\n",
    "\n",
    "    # Plot averaged metrics\n",
    "    sns.set_theme(style='whitegrid')\n",
    "\n",
    "    # Combined plot for Loss and Accuracy (Average across seeds and folds)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6), sharex=True)\n",
    "\n",
    "    # Loss Plot\n",
    "    sns.lineplot(\n",
    "        data=avg_data[avg_data['Metric'] == 'Loss'],\n",
    "        x='Epoch', y='Value', hue='Set', markers=True, dashes=False, ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title('Average Loss Over Epochs (All Seeds and Folds)')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    apply_log_scale(axes[0])\n",
    "\n",
    "    # Accuracy Plot\n",
    "    sns.lineplot(\n",
    "        data=avg_data[avg_data['Metric'] == 'Accuracy'],\n",
    "        x='Epoch', y='Value', hue='Set', markers=True, dashes=False, ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title('Average Accuracy Over Epochs (All Seeds and Folds)')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    apply_log_scale(axes[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        save_plot(fig, \"average_metrics_all_seeds.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Individual plots for averaged metrics\n",
    "    plot_individual(avg_data, 'Loss', '(All Seeds and Folds)', 'average_loss_all_seeds')\n",
    "    plot_individual(avg_data, 'Accuracy', '(All Seeds and Folds)', 'average_accuracy_all_seeds')\n",
    "\n",
    "    # Individual seed and fold plots\n",
    "    for seed in df_all_seeds['Seed'].unique():\n",
    "        df_seed = df_all_seeds[df_all_seeds['Seed'] == seed]\n",
    "\n",
    "        if k_fold:\n",
    "            for fold in df_seed['Fold'].dropna().unique():\n",
    "                df_fold = df_seed[df_seed['Fold'] == fold]\n",
    "\n",
    "                # Combined plot for Loss and Accuracy (Single Seed and Fold)\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(15, 6), sharex=True)\n",
    "\n",
    "                # Loss Plot\n",
    "                sns.lineplot(\n",
    "                    data=df_fold[df_fold['Metric'] == 'Loss'],\n",
    "                    x='Epoch', y='Value', hue='Set', markers=True, dashes=False, ax=axes[0]\n",
    "                )\n",
    "                axes[0].set_title(f'Loss Over Epochs (Seed {seed}, Fold {fold})')\n",
    "                axes[0].set_xlabel('Epoch')\n",
    "                axes[0].set_ylabel('Loss')\n",
    "                apply_log_scale(axes[0])\n",
    "\n",
    "                # Accuracy Plot\n",
    "                sns.lineplot(\n",
    "                    data=df_fold[df_fold['Metric'] == 'Accuracy'],\n",
    "                    x='Epoch', y='Value', hue='Set', markers=True, dashes=False, ax=axes[1]\n",
    "                )\n",
    "                axes[1].set_title(f'Accuracy Over Epochs (Seed {seed}, Fold {fold})')\n",
    "                axes[1].set_xlabel('Epoch')\n",
    "                axes[1].set_ylabel('Accuracy')\n",
    "                apply_log_scale(axes[1])\n",
    "\n",
    "                plt.tight_layout()\n",
    "                if save_path:\n",
    "                    save_plot(fig, f\"metrics_seed_{seed}_fold_{fold}.png\")\n",
    "                plt.show()\n",
    "\n",
    "                # Individual plots\n",
    "                plot_individual(df_fold, 'Loss', f'(Seed {seed}, Fold {fold})', f\"loss_seed_{seed}_fold_{fold}\")\n",
    "                plot_individual(df_fold, 'Accuracy', f'(Seed {seed}, Fold {fold})', f\"accuracy_seed_{seed}_fold_{fold}\")\n",
    "\n",
    "        else:\n",
    "            # Combined plot for Loss and Accuracy (Single Seed)\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6), sharex=True)\n",
    "\n",
    "            # Loss Plot\n",
    "            sns.lineplot(\n",
    "                data=df_seed[df_seed['Metric'] == 'Loss'],\n",
    "                x='Epoch', y='Value', hue='Set', markers=True, dashes=False, ax=axes[0]\n",
    "            )\n",
    "            axes[0].set_title(f'Loss Over Epochs (Seed {seed})')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            apply_log_scale(axes[0])\n",
    "\n",
    "            # Accuracy Plot\n",
    "            sns.lineplot(\n",
    "                data=df_seed[df_seed['Metric'] == 'Accuracy'],\n",
    "                x='Epoch', y='Value', hue='Set', markers=True, dashes=False, ax=axes[1]\n",
    "            )\n",
    "            axes[1].set_title(f'Accuracy Over Epochs (Seed {seed})')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "            apply_log_scale(axes[1])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            if save_path:\n",
    "                save_plot(fig, f\"metrics_seed_{seed}.png\")\n",
    "            plt.show()\n",
    "\n",
    "            # Individual plots\n",
    "            plot_individual(df_seed, 'Loss', f'(Seed {seed})', f\"loss_seed_{seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6b9a0",
   "metadata": {
    "id": "3onjn3E-S9ZT",
    "tags": []
   },
   "source": [
    "# Reservoir computing with experimental data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d136e69-6632-4583-9124-08bf2d9e6edf",
   "metadata": {},
   "source": [
    "This cell loads a reservoir (an AMN trained on *in silico* data with its parameters fixed), then learns on an experimental dataset.\n",
    "\n",
    "The creation of both *in silico* and experimental datasets is done separately, in `Build_Dataset.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1842c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Set parameter LicenseID to value 2656971\n",
      "Academic license - for non-commercial use only - expires 2026-04-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:50:55.458059: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reactions:  1186 1186\n",
      "number of metabolites:  2084\n",
      "filtered measurements size:  1\n",
      "RC reservoir file: ./Reservoir/iML1515_ec6_UB_AMN_QP\n",
      "RC model type: RC\n",
      "RC scaler: 0.0\n",
      "RC model input dim: 38\n",
      "RC model output dim: 1\n",
      "RC model medium bound: UB\n",
      "training set size (110, 38) (110, 1)\n",
      "reservoir S, Pin, Pout matrices (2084, 1186) (38, 1186) (1, 1186)\n",
      "RC training epochs: 2000\n",
      "RC training regression: True\n",
      "RC training learn rate: 0.0001\n",
      "RC training dropout: 0.25\n",
      "RC training batch size: 5\n",
      "RC training validation iter: 0\n",
      "RC training xfold: 0\n",
      "RC training early stopping: False\n",
      "--------prior network --------\n",
      "training file: None\n",
      "model type: ANN_Dense\n",
      "model scaler: 0.0\n",
      "model input dim: 10\n",
      "model output dim: 10\n",
      "model medium bound: \n",
      "timestep: 0\n",
      "no training set provided\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "--------reservoir network-----\n",
      "training file: ./Dataset_model/iML1515_ec6_UB\n",
      "model type: AMN_QP\n",
      "model scaler: 7.95\n",
      "model input dim: 38\n",
      "model output dim: 2376\n",
      "model medium bound: UB\n",
      "timestep: 4\n",
      "training set size (11000, 38) (11000, 1)\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "gradient learn rate: 0.001\n",
      "gradient decay rate: 0.9\n",
      "training epochs: 20\n",
      "training regression: True\n",
      "training learn rate: 0.001\n",
      "training dropout: 0.25\n",
      "training batch size: 100\n",
      "training validation iter: 0\n",
      "training xfold: 5\n",
      "training early stopping: False\n",
      "AMN scaler 0.0\n",
      "RC input shape (110, 38) (110, 1)\n",
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "Physical devices cannot be modified after being initialized\n",
      "----------------------------------- RC\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 10 relu True\n",
      "Prior inputs and outputs (None, 10) (None, 10)\n",
      "Res inputs added to Prior_outputs 28\n",
      "Res inputs (final) (None, 38)\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 1186 relu False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:52:35.984459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMN output shapes for PoutV, SV, PinV, Vpos, V, outputs (None, 1) (None, 1) (None, 1) (None, 1) (None, 1186) (None, 2376)\n",
      "=======================\n",
      "PoutV: (None, 1)\n",
      "SV: (None, 1)\n",
      "PinV: (None, 1)\n",
      "Vpos: (None, 1)\n",
      "V: (None, 1186)\n",
      "V0: KerasTensor(type_spec=TensorSpec(shape=(None, 1186), dtype=tf.float32, name=None), name='tf.__operators__.add_1/AddV2:0', description=\"created by layer 'tf.__operators__.add_1'\")\n",
      "Vin: KerasTensor(type_spec=TensorSpec(shape=(None, 38), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=\"created by layer 'tf.math.truediv'\")\n",
      "Vout: tf.Tensor([], shape=(0, 0), dtype=float32)\n",
      "Res_outputs-------------------- (None, 2376)\n",
      "SV, PinV, Vpos, V-------------- (None, 1) (None, 1) (None, 1) (None, 1186)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 10)           0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 500)          5500        ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 28)           0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           5010        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 38)           0           ['lambda[0][0]',                 \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan (TFOpLam  (None, 38)          0           ['input_1[0][0]',                \n",
      " bda)                                                             'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 38)           0           ['concatenate[0][0]',            \n",
      "                                                                  'tf.math.divide_no_nan[0][0]']  \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   (None, 38)           0           ['tf.math.multiply[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 500)          19500       ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 500)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1186)         594186      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLambda)  (None, 1186)         0           ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 1186)         0           ['tf.linalg.matmul[0][0]',       \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.nn.relu (TFOpLambda)        (None, 1186)         0           ['tf.math.subtract[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_1 (TFOpL  (None, 1186)        0           ['tf.nn.relu[0][0]',             \n",
      " ambda)                                                           'tf.nn.relu[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLambda  (None, 1186)        0           ['tf.math.divide_no_nan_1[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 1186)        0           ['tf.math.divide_no_nan_1[0][0]',\n",
      " )                                                                'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 1186)        0           ['tf.math.subtract_1[0][0]',     \n",
      " )                                                                'tf.linalg.matmul[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 1186)        0           ['dense_3[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_1[0][0]',     \n",
      " da)                                                              'tf.math.multiply_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 1186)        0           ['tf.math.multiply_3[0][0]',     \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_3 (TFOpLambda  (None, 38)          0           ['tf.__operators__.add_1[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLambda  (None, 38)          0           ['tf.linalg.matmul_3[0][0]',     \n",
      " )                                                                'tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " tf.nn.relu_1 (TFOpLambda)      (None, 38)           0           ['tf.math.subtract_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.negative (TFOpLambda)  (None, 1186)         0           ['tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_1 (TFOpLambda  (None, 2084)        0           ['tf.__operators__.add_1[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_2 (TFOpL  (None, 38)          0           ['tf.nn.relu_1[0][0]',           \n",
      " ambda)                                                           'tf.nn.relu_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.relu_2 (TFOpLambda)      (None, 1186)         0           ['tf.math.negative[0][0]']       \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_2 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 38)          0           ['tf.nn.relu_1[0][0]',           \n",
      " )                                                                'tf.math.divide_no_nan_2[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_3 (TFOpL  (None, 1186)        0           ['tf.nn.relu_2[0][0]',           \n",
      " ambda)                                                           'tf.nn.relu_2[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.truediv_2 (TFOpLambda)  (None, 1186)        0           ['tf.linalg.matmul_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_4 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_5[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.negative_1 (TFOpLambda  (None, 1186)        0           ['tf.math.divide_no_nan_3[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_3 (TFOpLambda)  (None, 1186)        0           ['tf.math.truediv_2[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.truediv_5 (TFOpLambda)  (None, 1186)        0           ['tf.linalg.matmul_4[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 1186)        0           ['tf.nn.relu_2[0][0]',           \n",
      " )                                                                'tf.math.negative_1[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 1186)        0           ['tf.math.truediv_3[0][0]',      \n",
      " mbda)                                                            'tf.math.truediv_5[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.truediv_7 (TFOpLambda)  (None, 1186)        0           ['tf.math.multiply_6[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 1186)        0           ['tf.__operators__.add_2[0][0]', \n",
      " mbda)                                                            'tf.math.truediv_7[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 1186)        0           ['tf.__operators__.add_1[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 1186)        0           ['tf.__operators__.add_3[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_4[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_7[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_3 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_8[0][0]',     \n",
      " )                                                                'tf.math.multiply_9[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 1186)        0           ['tf.__operators__.add_1[0][0]', \n",
      " mbda)                                                            'tf.math.subtract_3[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_7 (TFOpLambda  (None, 38)          0           ['tf.__operators__.add_4[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_4 (TFOpLambda  (None, 38)          0           ['tf.linalg.matmul_7[0][0]',     \n",
      " )                                                                'tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " tf.nn.relu_3 (TFOpLambda)      (None, 38)           0           ['tf.math.subtract_4[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.negative_2 (TFOpLambda  (None, 1186)        0           ['tf.__operators__.add_4[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_5 (TFOpLambda  (None, 2084)        0           ['tf.__operators__.add_4[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_5 (TFOpL  (None, 38)          0           ['tf.nn.relu_3[0][0]',           \n",
      " ambda)                                                           'tf.nn.relu_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.relu_4 (TFOpLambda)      (None, 1186)         0           ['tf.math.negative_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_6 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_5[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_3[0][0]',           \n",
      " a)                                                               'tf.math.divide_no_nan_5[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_6 (TFOpL  (None, 1186)        0           ['tf.nn.relu_4[0][0]',           \n",
      " ambda)                                                           'tf.nn.relu_4[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.truediv_9 (TFOpLambda)  (None, 1186)        0           ['tf.linalg.matmul_6[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_8 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_10[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.negative_3 (TFOpLambda  (None, 1186)        0           ['tf.math.divide_no_nan_6[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_10 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_9[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_12 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_8[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_4[0][0]',           \n",
      " a)                                                               'tf.math.negative_3[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 1186)        0           ['tf.math.truediv_10[0][0]',     \n",
      " mbda)                                                            'tf.math.truediv_12[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_14 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_11[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 1186)        0           ['tf.__operators__.add_5[0][0]', \n",
      " mbda)                                                            'tf.math.truediv_14[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_12 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_6[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_13 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_3[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_14 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_12[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_5 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_13[0][0]',    \n",
      " )                                                                'tf.math.multiply_14[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 1186)        0           ['tf.__operators__.add_4[0][0]', \n",
      " mbda)                                                            'tf.math.subtract_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_11 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_7[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_6 (TFOpLambda  (None, 38)          0           ['tf.linalg.matmul_11[0][0]',    \n",
      " )                                                                'tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " tf.nn.relu_5 (TFOpLambda)      (None, 38)           0           ['tf.math.subtract_6[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.negative_4 (TFOpLambda  (None, 1186)        0           ['tf.__operators__.add_7[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_9 (TFOpLambda  (None, 2084)        0           ['tf.__operators__.add_7[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_8 (TFOpL  (None, 38)          0           ['tf.nn.relu_5[0][0]',           \n",
      " ambda)                                                           'tf.nn.relu_5[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.relu_6 (TFOpLambda)      (None, 1186)         0           ['tf.math.negative_4[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_10 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_9[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_15 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_5[0][0]',           \n",
      " a)                                                               'tf.math.divide_no_nan_8[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_9 (TFOpL  (None, 1186)        0           ['tf.nn.relu_6[0][0]',           \n",
      " ambda)                                                           'tf.nn.relu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.truediv_16 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_10[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_12 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_15[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_5 (TFOpLambda  (None, 1186)        0           ['tf.math.divide_no_nan_9[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_17 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_16[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_19 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_12[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_16 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_6[0][0]',           \n",
      " a)                                                               'tf.math.negative_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 1186)        0           ['tf.math.truediv_17[0][0]',     \n",
      " mbda)                                                            'tf.math.truediv_19[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_21 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_16[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 1186)        0           ['tf.__operators__.add_8[0][0]', \n",
      " mbda)                                                            'tf.math.truediv_21[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_17 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_9[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_18 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_5[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_19 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_17[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_7 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_18[0][0]',    \n",
      " )                                                                'tf.math.multiply_19[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_7[0][0]', \n",
      " ambda)                                                           'tf.math.subtract_7[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_15 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_10[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_8 (TFOpLambda  (None, 38)          0           ['tf.linalg.matmul_15[0][0]',    \n",
      " )                                                                'tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " tf.nn.relu_7 (TFOpLambda)      (None, 38)           0           ['tf.math.subtract_8[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.negative_6 (TFOpLambda  (None, 1186)        0           ['tf.__operators__.add_10[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_13 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_10[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_11 (TFOp  (None, 38)          0           ['tf.nn.relu_7[0][0]',           \n",
      " Lambda)                                                          'tf.nn.relu_7[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.relu_8 (TFOpLambda)      (None, 1186)         0           ['tf.math.negative_6[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_14 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_13[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_20 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_7[0][0]',           \n",
      " a)                                                               'tf.math.divide_no_nan_11[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_12 (TFOp  (None, 1186)        0           ['tf.nn.relu_8[0][0]',           \n",
      " Lambda)                                                          'tf.nn.relu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.truediv_23 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_14[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_16 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_20[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_7 (TFOpLambda  (None, 1186)        0           ['tf.math.divide_no_nan_12[0][0]'\n",
      " )                                                               ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_24 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_23[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_26 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_16[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_21 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_8[0][0]',           \n",
      " a)                                                               'tf.math.negative_7[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 1186)        0           ['tf.math.truediv_24[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_26[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_28 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_21[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_11[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_28[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_22 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_12[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_23 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_7[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_24 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_22[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_9 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_23[0][0]',    \n",
      " )                                                                'tf.math.multiply_24[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_10[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_9[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_19 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_13[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_10 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_19[0][0]',    \n",
      " a)                                                               'tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " tf.math.negative_8 (TFOpLambda  (None, 1186)        0           ['tf.__operators__.add_13[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_18 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_13[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.nn.relu_9 (TFOpLambda)      (None, 38)           0           ['tf.math.subtract_10[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_10 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_8[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_12 (TFOpLamb  (None, 1)           0           ['tf.linalg.matmul_18[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_13 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_9[0][0]']           \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_14 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_10[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_17 (TFOpLambd  (None, 1)           0           ['tf.__operators__.add_13[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_29 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_12[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_30 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_13[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_31 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_14[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2376)         0           ['tf.linalg.matmul_17[0][0]',    \n",
      "                                                                  'tf.math.truediv_29[0][0]',     \n",
      "                                                                  'tf.math.truediv_30[0][0]',     \n",
      "                                                                  'tf.math.truediv_31[0][0]',     \n",
      "                                                                  'tf.__operators__.add_13[0][0]',\n",
      "                                                                  'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1)            0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 1)            0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 1)            0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 1)            0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 1186)         0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 1228)         0           ['lambda_2[0][0]',               \n",
      "                                                                  'lambda_3[0][0]',               \n",
      "                                                                  'lambda_4[0][0]',               \n",
      "                                                                  'lambda_5[0][0]',               \n",
      "                                                                  'lambda_6[0][0]',               \n",
      "                                                                  'tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 624,196\n",
      "Trainable params: 10,510\n",
      "Non-trainable params: 613,686\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "nbr parameters: 624196\n",
      "Epoch 1/2000\n",
      "22/22 [==============================] - 3s 55ms/step - loss: 0.0407 - my_r2: -11.2603 - val_loss: 0.0391 - val_my_r2: -10.9757\n",
      "Epoch 2/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0379 - my_r2: -11.6477 - val_loss: 0.0350 - val_my_r2: -9.7314\n",
      "Epoch 3/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0336 - my_r2: -10.0172 - val_loss: 0.0307 - val_my_r2: -8.3908\n",
      "Epoch 4/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0301 - my_r2: -8.7457 - val_loss: 0.0265 - val_my_r2: -7.0978\n",
      "Epoch 5/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0252 - my_r2: -5.1751 - val_loss: 0.0229 - val_my_r2: -6.0138\n",
      "Epoch 6/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0202 - my_r2: -3.5935 - val_loss: 0.0195 - val_my_r2: -4.9590\n",
      "Epoch 7/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0188 - my_r2: -4.8481 - val_loss: 0.0163 - val_my_r2: -4.0080\n",
      "Epoch 8/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0144 - my_r2: -2.1252 - val_loss: 0.0131 - val_my_r2: -3.0109\n",
      "Epoch 9/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0119 - my_r2: -1.5880 - val_loss: 0.0107 - val_my_r2: -2.2933\n",
      "Epoch 10/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0102 - my_r2: -4.8883 - val_loss: 0.0087 - val_my_r2: -1.6923\n",
      "Epoch 11/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0082 - my_r2: -1.1218 - val_loss: 0.0073 - val_my_r2: -1.2642\n",
      "Epoch 12/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0071 - my_r2: -0.4228 - val_loss: 0.0063 - val_my_r2: -0.9444\n",
      "Epoch 13/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0059 - my_r2: -0.4102 - val_loss: 0.0055 - val_my_r2: -0.7031\n",
      "Epoch 14/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0053 - my_r2: -0.6095 - val_loss: 0.0049 - val_my_r2: -0.5086\n",
      "Epoch 15/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0052 - my_r2: -0.4822 - val_loss: 0.0044 - val_my_r2: -0.3542\n",
      "Epoch 16/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0044 - my_r2: 0.0570 - val_loss: 0.0040 - val_my_r2: -0.2341\n",
      "Epoch 17/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0037 - my_r2: 0.1314 - val_loss: 0.0037 - val_my_r2: -0.1442\n",
      "Epoch 18/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0040 - my_r2: -0.0094 - val_loss: 0.0035 - val_my_r2: -0.0749\n",
      "Epoch 19/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0036 - my_r2: -0.1342 - val_loss: 0.0032 - val_my_r2: 1.2653e-04\n",
      "Epoch 20/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0037 - my_r2: 0.1176 - val_loss: 0.0030 - val_my_r2: 0.0529\n",
      "Epoch 21/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0035 - my_r2: -0.0865 - val_loss: 0.0029 - val_my_r2: 0.1073\n",
      "Epoch 22/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0031 - my_r2: 0.2270 - val_loss: 0.0027 - val_my_r2: 0.1456\n",
      "Epoch 23/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0027 - my_r2: 0.3103 - val_loss: 0.0026 - val_my_r2: 0.1802\n",
      "Epoch 24/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0028 - my_r2: 0.2589 - val_loss: 0.0025 - val_my_r2: 0.2161\n",
      "Epoch 25/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0028 - my_r2: 0.4399 - val_loss: 0.0024 - val_my_r2: 0.2415\n",
      "Epoch 26/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0023 - my_r2: 0.2130 - val_loss: 0.0023 - val_my_r2: 0.2701\n",
      "Epoch 27/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0024 - my_r2: 0.0196 - val_loss: 0.0022 - val_my_r2: 0.2932\n",
      "Epoch 28/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0024 - my_r2: 0.2774 - val_loss: 0.0021 - val_my_r2: 0.3183\n",
      "Epoch 29/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0024 - my_r2: 0.2496 - val_loss: 0.0021 - val_my_r2: 0.3406\n",
      "Epoch 30/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0022 - my_r2: 0.3365 - val_loss: 0.0020 - val_my_r2: 0.3637\n",
      "Epoch 31/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0023 - my_r2: 0.4524 - val_loss: 0.0019 - val_my_r2: 0.3832\n",
      "Epoch 32/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0021 - my_r2: 0.3057 - val_loss: 0.0019 - val_my_r2: 0.4015\n",
      "Epoch 33/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0023 - my_r2: 0.5404 - val_loss: 0.0018 - val_my_r2: 0.4182\n",
      "Epoch 34/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0019 - my_r2: 0.4954 - val_loss: 0.0018 - val_my_r2: 0.4335\n",
      "Epoch 35/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0022 - my_r2: 0.2729 - val_loss: 0.0017 - val_my_r2: 0.4478\n",
      "Epoch 36/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0018 - my_r2: 0.4499 - val_loss: 0.0017 - val_my_r2: 0.4643\n",
      "Epoch 37/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0018 - my_r2: 0.5802 - val_loss: 0.0016 - val_my_r2: 0.4780\n",
      "Epoch 38/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0021 - my_r2: 0.4687 - val_loss: 0.0016 - val_my_r2: 0.4910\n",
      "Epoch 39/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0018 - my_r2: 0.5714 - val_loss: 0.0015 - val_my_r2: 0.5006\n",
      "Epoch 40/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0019 - my_r2: 0.5824 - val_loss: 0.0015 - val_my_r2: 0.5134\n",
      "Epoch 41/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0018 - my_r2: 0.5582 - val_loss: 0.0014 - val_my_r2: 0.5234\n",
      "Epoch 42/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0017 - my_r2: 0.6139 - val_loss: 0.0014 - val_my_r2: 0.5325\n",
      "Epoch 43/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0017 - my_r2: 0.4709 - val_loss: 0.0014 - val_my_r2: 0.5439\n",
      "Epoch 44/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0019 - my_r2: 0.5282 - val_loss: 0.0014 - val_my_r2: 0.5529\n",
      "Epoch 45/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0015 - my_r2: 0.5798 - val_loss: 0.0013 - val_my_r2: 0.5633\n",
      "Epoch 46/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0015 - my_r2: 0.4441 - val_loss: 0.0013 - val_my_r2: 0.5706\n",
      "Epoch 47/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0013 - my_r2: 0.6113 - val_loss: 0.0013 - val_my_r2: 0.5775\n",
      "Epoch 48/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0014 - my_r2: 0.3444 - val_loss: 0.0013 - val_my_r2: 0.5862\n",
      "Epoch 49/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0015 - my_r2: 0.5926 - val_loss: 0.0012 - val_my_r2: 0.5920\n",
      "Epoch 50/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0016 - my_r2: 0.5208 - val_loss: 0.0012 - val_my_r2: 0.5984\n",
      "Epoch 51/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0014 - my_r2: 0.3833 - val_loss: 0.0012 - val_my_r2: 0.6062\n",
      "Epoch 52/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0016 - my_r2: 0.5714 - val_loss: 0.0012 - val_my_r2: 0.6147\n",
      "Epoch 53/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0014 - my_r2: 0.4850 - val_loss: 0.0011 - val_my_r2: 0.6207\n",
      "Epoch 54/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0015 - my_r2: 0.4226 - val_loss: 0.0011 - val_my_r2: 0.6268\n",
      "Epoch 55/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0016 - my_r2: 0.0907 - val_loss: 0.0011 - val_my_r2: 0.6325\n",
      "Epoch 56/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0015 - my_r2: 0.6728 - val_loss: 0.0011 - val_my_r2: 0.6361\n",
      "Epoch 57/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0013 - my_r2: 0.6386 - val_loss: 0.0011 - val_my_r2: 0.6416\n",
      "Epoch 58/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0012 - my_r2: 0.6737 - val_loss: 0.0011 - val_my_r2: 0.6476\n",
      "Epoch 59/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0015 - my_r2: 0.5593 - val_loss: 0.0010 - val_my_r2: 0.6547\n",
      "Epoch 60/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0013 - my_r2: 0.6358 - val_loss: 0.0010 - val_my_r2: 0.6594\n",
      "Epoch 61/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0013 - my_r2: 0.6708 - val_loss: 0.0010 - val_my_r2: 0.6628\n",
      "Epoch 62/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0012 - my_r2: 0.7106 - val_loss: 9.8944e-04 - val_my_r2: 0.6671\n",
      "Epoch 63/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0011 - my_r2: 0.7284 - val_loss: 9.7662e-04 - val_my_r2: 0.6718\n",
      "Epoch 64/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0012 - my_r2: 0.5028 - val_loss: 9.6493e-04 - val_my_r2: 0.6764\n",
      "Epoch 65/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0012 - my_r2: 0.6600 - val_loss: 9.5428e-04 - val_my_r2: 0.6788\n",
      "Epoch 66/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0013 - my_r2: 0.6714 - val_loss: 9.3880e-04 - val_my_r2: 0.6822\n",
      "Epoch 67/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0012 - my_r2: 0.5416 - val_loss: 9.2503e-04 - val_my_r2: 0.6876\n",
      "Epoch 68/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0010 - my_r2: 0.7991 - val_loss: 9.1212e-04 - val_my_r2: 0.6915\n",
      "Epoch 69/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0012 - my_r2: 0.7357 - val_loss: 8.9767e-04 - val_my_r2: 0.6946\n",
      "Epoch 70/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0013 - my_r2: 0.5752 - val_loss: 8.8451e-04 - val_my_r2: 0.7009\n",
      "Epoch 71/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0011 - my_r2: 0.7458 - val_loss: 8.7428e-04 - val_my_r2: 0.7036\n",
      "Epoch 72/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.0012 - my_r2: 0.5411 - val_loss: 8.6483e-04 - val_my_r2: 0.7080\n",
      "Epoch 73/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.0012 - my_r2: 0.6365 - val_loss: 8.5567e-04 - val_my_r2: 0.7122\n",
      "Epoch 74/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 9.9378e-04 - my_r2: 0.7386 - val_loss: 8.4381e-04 - val_my_r2: 0.7136\n",
      "Epoch 75/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 9.8052e-04 - my_r2: 0.7554 - val_loss: 8.3476e-04 - val_my_r2: 0.7174\n",
      "Epoch 76/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 9.7162e-04 - my_r2: 0.7109 - val_loss: 8.2522e-04 - val_my_r2: 0.7217\n",
      "Epoch 77/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0011 - my_r2: 0.7359 - val_loss: 8.1718e-04 - val_my_r2: 0.7258\n",
      "Epoch 78/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0011 - my_r2: 0.6054 - val_loss: 8.0579e-04 - val_my_r2: 0.7287\n",
      "Epoch 79/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 0.0010 - my_r2: 0.7367 - val_loss: 7.9642e-04 - val_my_r2: 0.7314\n",
      "Epoch 80/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.0010 - my_r2: 0.3989 - val_loss: 7.8553e-04 - val_my_r2: 0.7335\n",
      "Epoch 81/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0012 - my_r2: 0.7139 - val_loss: 7.7706e-04 - val_my_r2: 0.7367\n",
      "Epoch 82/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.3862e-04 - my_r2: 0.4789 - val_loss: 7.6868e-04 - val_my_r2: 0.7382\n",
      "Epoch 83/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 0.0012 - my_r2: 0.7460 - val_loss: 7.6177e-04 - val_my_r2: 0.7417\n",
      "Epoch 84/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0013 - my_r2: 0.6706 - val_loss: 7.5054e-04 - val_my_r2: 0.7445\n",
      "Epoch 85/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 9.8852e-04 - my_r2: 0.6733 - val_loss: 7.4121e-04 - val_my_r2: 0.7479\n",
      "Epoch 86/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 9.8339e-04 - my_r2: 0.7917 - val_loss: 7.3221e-04 - val_my_r2: 0.7505\n",
      "Epoch 87/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 9.5631e-04 - my_r2: 0.7260 - val_loss: 7.2229e-04 - val_my_r2: 0.7531\n",
      "Epoch 88/2000\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 9.4151e-04 - my_r2: 0.6717 - val_loss: 7.1556e-04 - val_my_r2: 0.7564\n",
      "Epoch 89/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 9.8922e-04 - my_r2: 0.6674 - val_loss: 7.0837e-04 - val_my_r2: 0.7585\n",
      "Epoch 90/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 9.7766e-04 - my_r2: 0.7101 - val_loss: 7.0179e-04 - val_my_r2: 0.7620\n",
      "Epoch 91/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 9.8903e-04 - my_r2: 0.7422 - val_loss: 6.9496e-04 - val_my_r2: 0.7644\n",
      "Epoch 92/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 9.3739e-04 - my_r2: 0.6966 - val_loss: 6.8730e-04 - val_my_r2: 0.7668\n",
      "Epoch 93/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 9.7382e-04 - my_r2: 0.7757 - val_loss: 6.7854e-04 - val_my_r2: 0.7690\n",
      "Epoch 94/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0010 - my_r2: 0.6502 - val_loss: 6.6908e-04 - val_my_r2: 0.7710\n",
      "Epoch 95/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 9.4952e-04 - my_r2: 0.5785 - val_loss: 6.6208e-04 - val_my_r2: 0.7750\n",
      "Epoch 96/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 8.4280e-04 - my_r2: 0.8123 - val_loss: 6.5582e-04 - val_my_r2: 0.7786\n",
      "Epoch 97/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 7.2012e-04 - my_r2: 0.8387 - val_loss: 6.4854e-04 - val_my_r2: 0.7799\n",
      "Epoch 98/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 8.4850e-04 - my_r2: 0.6832 - val_loss: 6.4192e-04 - val_my_r2: 0.7815\n",
      "Epoch 99/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 8.0981e-04 - my_r2: 0.6787 - val_loss: 6.3765e-04 - val_my_r2: 0.7837\n",
      "Epoch 100/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 8.5497e-04 - my_r2: 0.7899 - val_loss: 6.3231e-04 - val_my_r2: 0.7847\n",
      "Epoch 101/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 8.6712e-04 - my_r2: 0.7065 - val_loss: 6.2478e-04 - val_my_r2: 0.7844\n",
      "Epoch 102/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0010 - my_r2: 0.7382 - val_loss: 6.1854e-04 - val_my_r2: 0.7873\n",
      "Epoch 103/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 8.5785e-04 - my_r2: 0.6566 - val_loss: 6.1492e-04 - val_my_r2: 0.7849\n",
      "Epoch 104/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0011 - my_r2: 0.5331 - val_loss: 6.0594e-04 - val_my_r2: 0.7902\n",
      "Epoch 105/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 8.1409e-04 - my_r2: 0.7729 - val_loss: 6.0034e-04 - val_my_r2: 0.7917\n",
      "Epoch 106/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 8.6104e-04 - my_r2: 0.7707 - val_loss: 5.9442e-04 - val_my_r2: 0.7940\n",
      "Epoch 107/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 8.9569e-04 - my_r2: 0.6578 - val_loss: 5.9149e-04 - val_my_r2: 0.7966\n",
      "Epoch 108/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 9.4917e-04 - my_r2: 0.7248 - val_loss: 5.8527e-04 - val_my_r2: 0.7992\n",
      "Epoch 109/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 8.9880e-04 - my_r2: 0.7296 - val_loss: 5.7817e-04 - val_my_r2: 0.7992\n",
      "Epoch 110/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 8.3057e-04 - my_r2: 0.6416 - val_loss: 5.7305e-04 - val_my_r2: 0.8026\n",
      "Epoch 111/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 9.2113e-04 - my_r2: 0.7865 - val_loss: 5.6734e-04 - val_my_r2: 0.8025\n",
      "Epoch 112/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 9.5668e-04 - my_r2: 0.3757 - val_loss: 5.6172e-04 - val_my_r2: 0.8047\n",
      "Epoch 113/2000\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 8.2794e-04 - my_r2: 0.6665 - val_loss: 5.5650e-04 - val_my_r2: 0.8073\n",
      "Epoch 114/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.6449e-04 - my_r2: 0.6940 - val_loss: 5.5237e-04 - val_my_r2: 0.8103\n",
      "Epoch 115/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 9.2908e-04 - my_r2: 0.7635 - val_loss: 5.4484e-04 - val_my_r2: 0.8120\n",
      "Epoch 116/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.9683e-04 - my_r2: 0.8521 - val_loss: 5.4106e-04 - val_my_r2: 0.8129\n",
      "Epoch 117/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 8.3976e-04 - my_r2: 0.8321 - val_loss: 5.3798e-04 - val_my_r2: 0.8155\n",
      "Epoch 118/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 7.6332e-04 - my_r2: 0.7439 - val_loss: 5.3349e-04 - val_my_r2: 0.8176\n",
      "Epoch 119/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.4231e-04 - my_r2: 0.7922 - val_loss: 5.2750e-04 - val_my_r2: 0.8177\n",
      "Epoch 120/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.9029e-04 - my_r2: 0.8177 - val_loss: 5.2359e-04 - val_my_r2: 0.8172\n",
      "Epoch 121/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.9760e-04 - my_r2: 0.7287 - val_loss: 5.2218e-04 - val_my_r2: 0.8175\n",
      "Epoch 122/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 8.7743e-04 - my_r2: 0.7881 - val_loss: 5.1647e-04 - val_my_r2: 0.8193\n",
      "Epoch 123/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 6.6532e-04 - my_r2: 0.7854 - val_loss: 5.1259e-04 - val_my_r2: 0.8209\n",
      "Epoch 124/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.7053e-04 - my_r2: 0.8009 - val_loss: 5.0645e-04 - val_my_r2: 0.8239\n",
      "Epoch 125/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 8.3399e-04 - my_r2: 0.8234 - val_loss: 5.0199e-04 - val_my_r2: 0.8250\n",
      "Epoch 126/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 7.5155e-04 - my_r2: 0.8172 - val_loss: 5.0310e-04 - val_my_r2: 0.8241\n",
      "Epoch 127/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 7.3137e-04 - my_r2: 0.7082 - val_loss: 4.9935e-04 - val_my_r2: 0.8267\n",
      "Epoch 128/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 8.3093e-04 - my_r2: 0.8061 - val_loss: 4.9211e-04 - val_my_r2: 0.8284\n",
      "Epoch 129/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.9299e-04 - my_r2: 0.8158 - val_loss: 4.8597e-04 - val_my_r2: 0.8288\n",
      "Epoch 130/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.9968e-04 - my_r2: 0.8058 - val_loss: 4.8277e-04 - val_my_r2: 0.8300\n",
      "Epoch 131/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 6.6181e-04 - my_r2: 0.8398 - val_loss: 4.8065e-04 - val_my_r2: 0.8307\n",
      "Epoch 132/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.3491e-04 - my_r2: 0.7833 - val_loss: 4.7806e-04 - val_my_r2: 0.8325\n",
      "Epoch 133/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 7.2835e-04 - my_r2: 0.8581 - val_loss: 4.7106e-04 - val_my_r2: 0.8337\n",
      "Epoch 134/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 8.6047e-04 - my_r2: 0.7299 - val_loss: 4.6623e-04 - val_my_r2: 0.8350\n",
      "Epoch 135/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.9951e-04 - my_r2: 0.8306 - val_loss: 4.6383e-04 - val_my_r2: 0.8369\n",
      "Epoch 136/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.2723e-04 - my_r2: 0.8271 - val_loss: 4.6018e-04 - val_my_r2: 0.8380\n",
      "Epoch 137/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.6961e-04 - my_r2: 0.7895 - val_loss: 4.5490e-04 - val_my_r2: 0.8387\n",
      "Epoch 138/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 6.9902e-04 - my_r2: 0.8530 - val_loss: 4.5063e-04 - val_my_r2: 0.8400\n",
      "Epoch 139/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.0794e-04 - my_r2: 0.8507 - val_loss: 4.4802e-04 - val_my_r2: 0.8408\n",
      "Epoch 140/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.6935e-04 - my_r2: 0.7518 - val_loss: 4.4410e-04 - val_my_r2: 0.8414\n",
      "Epoch 141/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 8.2599e-04 - my_r2: 0.7434 - val_loss: 4.4044e-04 - val_my_r2: 0.8418\n",
      "Epoch 142/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.8095e-04 - my_r2: 0.8658 - val_loss: 4.3711e-04 - val_my_r2: 0.8432\n",
      "Epoch 143/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.1278e-04 - my_r2: 0.7867 - val_loss: 4.3384e-04 - val_my_r2: 0.8444\n",
      "Epoch 144/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 7.1869e-04 - my_r2: 0.7908 - val_loss: 4.3097e-04 - val_my_r2: 0.8469\n",
      "Epoch 145/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 8.3534e-04 - my_r2: 0.8495 - val_loss: 4.2782e-04 - val_my_r2: 0.8475\n",
      "Epoch 146/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 6.9432e-04 - my_r2: 0.7422 - val_loss: 4.2619e-04 - val_my_r2: 0.8478\n",
      "Epoch 147/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.9506e-04 - my_r2: 0.8050 - val_loss: 4.2194e-04 - val_my_r2: 0.8490\n",
      "Epoch 148/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.5202e-04 - my_r2: 0.7755 - val_loss: 4.2167e-04 - val_my_r2: 0.8503\n",
      "Epoch 149/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.8258e-04 - my_r2: 0.7005 - val_loss: 4.1926e-04 - val_my_r2: 0.8498\n",
      "Epoch 150/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.8446e-04 - my_r2: 0.7557 - val_loss: 4.1609e-04 - val_my_r2: 0.8505\n",
      "Epoch 151/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 7.0646e-04 - my_r2: 0.8543 - val_loss: 4.1518e-04 - val_my_r2: 0.8526\n",
      "Epoch 152/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.4022e-04 - my_r2: 0.8262 - val_loss: 4.0946e-04 - val_my_r2: 0.8534\n",
      "Epoch 153/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.8941e-04 - my_r2: 0.8520 - val_loss: 4.0813e-04 - val_my_r2: 0.8555\n",
      "Epoch 154/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.1564e-04 - my_r2: 0.8031 - val_loss: 4.0634e-04 - val_my_r2: 0.8574\n",
      "Epoch 155/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.7219e-04 - my_r2: 0.8198 - val_loss: 3.9810e-04 - val_my_r2: 0.8596\n",
      "Epoch 156/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 6.4806e-04 - my_r2: 0.8382 - val_loss: 3.9588e-04 - val_my_r2: 0.8594\n",
      "Epoch 157/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 7.5638e-04 - my_r2: 0.7987 - val_loss: 3.9264e-04 - val_my_r2: 0.8609\n",
      "Epoch 158/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.0865e-04 - my_r2: 0.7642 - val_loss: 3.9583e-04 - val_my_r2: 0.8624\n",
      "Epoch 159/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 7.1027e-04 - my_r2: 0.8269 - val_loss: 3.9235e-04 - val_my_r2: 0.8633\n",
      "Epoch 160/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.9489e-04 - my_r2: 0.8634 - val_loss: 3.9094e-04 - val_my_r2: 0.8652\n",
      "Epoch 161/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.3951e-04 - my_r2: 0.8089 - val_loss: 3.9061e-04 - val_my_r2: 0.8652\n",
      "Epoch 162/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 7.9027e-04 - my_r2: 0.7615 - val_loss: 3.8373e-04 - val_my_r2: 0.8664\n",
      "Epoch 163/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.9791e-04 - my_r2: 0.8790 - val_loss: 3.7957e-04 - val_my_r2: 0.8667\n",
      "Epoch 164/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.6054e-04 - my_r2: 0.8255 - val_loss: 3.7712e-04 - val_my_r2: 0.8679\n",
      "Epoch 165/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 6.1730e-04 - my_r2: 0.8512 - val_loss: 3.7491e-04 - val_my_r2: 0.8694\n",
      "Epoch 166/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.9039e-04 - my_r2: 0.8580 - val_loss: 3.7132e-04 - val_my_r2: 0.8692\n",
      "Epoch 167/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 7.7087e-04 - my_r2: 0.6735 - val_loss: 3.7107e-04 - val_my_r2: 0.8710\n",
      "Epoch 168/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.9190e-04 - my_r2: 0.7665 - val_loss: 3.6737e-04 - val_my_r2: 0.8716\n",
      "Epoch 169/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.5838e-04 - my_r2: 0.8341 - val_loss: 3.6758e-04 - val_my_r2: 0.8720\n",
      "Epoch 170/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 6.2684e-04 - my_r2: 0.7208 - val_loss: 3.6392e-04 - val_my_r2: 0.8736\n",
      "Epoch 171/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.1365e-04 - my_r2: 0.8654 - val_loss: 3.6337e-04 - val_my_r2: 0.8739\n",
      "Epoch 172/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.2878e-04 - my_r2: 0.8278 - val_loss: 3.6236e-04 - val_my_r2: 0.8737\n",
      "Epoch 173/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.5636e-04 - my_r2: 0.8158 - val_loss: 3.5795e-04 - val_my_r2: 0.8761\n",
      "Epoch 174/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 6.2642e-04 - my_r2: 0.5204 - val_loss: 3.5465e-04 - val_my_r2: 0.8765\n",
      "Epoch 175/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.3754e-04 - my_r2: 0.8897 - val_loss: 3.5037e-04 - val_my_r2: 0.8773\n",
      "Epoch 176/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.9586e-04 - my_r2: 0.8271 - val_loss: 3.4786e-04 - val_my_r2: 0.8773\n",
      "Epoch 177/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.3460e-04 - my_r2: 0.6409 - val_loss: 3.4492e-04 - val_my_r2: 0.8784\n",
      "Epoch 178/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.1389e-04 - my_r2: 0.7460 - val_loss: 3.4274e-04 - val_my_r2: 0.8777\n",
      "Epoch 179/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.5206e-04 - my_r2: 0.8108 - val_loss: 3.3918e-04 - val_my_r2: 0.8790\n",
      "Epoch 180/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 6.2013e-04 - my_r2: 0.8806 - val_loss: 3.3732e-04 - val_my_r2: 0.8795\n",
      "Epoch 181/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 5.1897e-04 - my_r2: 0.8874 - val_loss: 3.3430e-04 - val_my_r2: 0.8811\n",
      "Epoch 182/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 5.6533e-04 - my_r2: 0.8215 - val_loss: 3.3229e-04 - val_my_r2: 0.8819\n",
      "Epoch 183/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 6.3175e-04 - my_r2: 0.7856 - val_loss: 3.3096e-04 - val_my_r2: 0.8831\n",
      "Epoch 184/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 6.1576e-04 - my_r2: 0.8372 - val_loss: 3.2940e-04 - val_my_r2: 0.8832\n",
      "Epoch 185/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 6.9678e-04 - my_r2: 0.8105 - val_loss: 3.2688e-04 - val_my_r2: 0.8844\n",
      "Epoch 186/2000\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 6.0880e-04 - my_r2: 0.8164 - val_loss: 3.2384e-04 - val_my_r2: 0.8833\n",
      "Epoch 187/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 5.3993e-04 - my_r2: 0.8887 - val_loss: 3.2242e-04 - val_my_r2: 0.8834\n",
      "Epoch 188/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 5.4845e-04 - my_r2: 0.8871 - val_loss: 3.2023e-04 - val_my_r2: 0.8864\n",
      "Epoch 189/2000\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 5.4089e-04 - my_r2: 0.8824 - val_loss: 3.1768e-04 - val_my_r2: 0.8869\n",
      "Epoch 190/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 5.9769e-04 - my_r2: 0.8724 - val_loss: 3.1781e-04 - val_my_r2: 0.8867\n",
      "Epoch 191/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 5.7051e-04 - my_r2: 0.8200 - val_loss: 3.1725e-04 - val_my_r2: 0.8878\n",
      "Epoch 192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5392e-04 - my_r2: 0.8998 - val_loss: 3.1437e-04 - val_my_r2: 0.8901\n",
      "Epoch 193/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 6.1554e-04 - my_r2: 0.7684 - val_loss: 3.2751e-04 - val_my_r2: 0.8859\n",
      "Epoch 194/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.0418e-04 - my_r2: 0.8681 - val_loss: 3.2318e-04 - val_my_r2: 0.8869\n",
      "Epoch 195/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.4907e-04 - my_r2: 0.8791 - val_loss: 3.1061e-04 - val_my_r2: 0.8896\n",
      "Epoch 196/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.9394e-04 - my_r2: 0.8784 - val_loss: 3.0464e-04 - val_my_r2: 0.8902\n",
      "Epoch 197/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.6582e-04 - my_r2: 0.8999 - val_loss: 3.0573e-04 - val_my_r2: 0.8882\n",
      "Epoch 198/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 6.1306e-04 - my_r2: 0.8177 - val_loss: 3.0107e-04 - val_my_r2: 0.8912\n",
      "Epoch 199/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.2136e-04 - my_r2: 0.8339 - val_loss: 3.0027e-04 - val_my_r2: 0.8913\n",
      "Epoch 200/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.4638e-04 - my_r2: 0.8613 - val_loss: 3.0099e-04 - val_my_r2: 0.8915\n",
      "Epoch 201/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.0490e-04 - my_r2: 0.9021 - val_loss: 3.0044e-04 - val_my_r2: 0.8932\n",
      "Epoch 202/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.4675e-04 - my_r2: 0.7922 - val_loss: 3.0242e-04 - val_my_r2: 0.8931\n",
      "Epoch 203/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.5428e-04 - my_r2: 0.8872 - val_loss: 2.9879e-04 - val_my_r2: 0.8942\n",
      "Epoch 204/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.7606e-04 - my_r2: 0.7851 - val_loss: 2.9449e-04 - val_my_r2: 0.8961\n",
      "Epoch 205/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.5576e-04 - my_r2: 0.8855 - val_loss: 2.9315e-04 - val_my_r2: 0.8964\n",
      "Epoch 206/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.1333e-04 - my_r2: 0.8738 - val_loss: 2.8826e-04 - val_my_r2: 0.8970\n",
      "Epoch 207/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.6364e-04 - my_r2: 0.8790 - val_loss: 2.8508e-04 - val_my_r2: 0.8973\n",
      "Epoch 208/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.5185e-04 - my_r2: 0.6287 - val_loss: 2.8406e-04 - val_my_r2: 0.8974\n",
      "Epoch 209/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.8638e-04 - my_r2: 0.8394 - val_loss: 2.8160e-04 - val_my_r2: 0.8986\n",
      "Epoch 210/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.6490e-04 - my_r2: 0.5320 - val_loss: 2.8122e-04 - val_my_r2: 0.9000\n",
      "Epoch 211/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.3710e-04 - my_r2: 0.7908 - val_loss: 2.7753e-04 - val_my_r2: 0.9016\n",
      "Epoch 212/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 6.0122e-04 - my_r2: 0.0435 - val_loss: 2.7507e-04 - val_my_r2: 0.9034\n",
      "Epoch 213/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 5.5917e-04 - my_r2: 0.8353 - val_loss: 2.7489e-04 - val_my_r2: 0.9035\n",
      "Epoch 214/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.0888e-04 - my_r2: 0.8554 - val_loss: 2.7196e-04 - val_my_r2: 0.9045\n",
      "Epoch 215/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.2148e-04 - my_r2: 0.8828 - val_loss: 2.7349e-04 - val_my_r2: 0.9021\n",
      "Epoch 216/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2822e-04 - my_r2: 0.8703 - val_loss: 2.7283e-04 - val_my_r2: 0.9031\n",
      "Epoch 217/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.9954e-04 - my_r2: 0.8555 - val_loss: 2.6966e-04 - val_my_r2: 0.9043\n",
      "Epoch 218/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.7485e-04 - my_r2: 0.8667 - val_loss: 2.6517e-04 - val_my_r2: 0.9063\n",
      "Epoch 219/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.4463e-04 - my_r2: 0.8607 - val_loss: 2.6305e-04 - val_my_r2: 0.9069\n",
      "Epoch 220/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.8765e-04 - my_r2: 0.8834 - val_loss: 2.6179e-04 - val_my_r2: 0.9064\n",
      "Epoch 221/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.4703e-04 - my_r2: 0.8425 - val_loss: 2.6201e-04 - val_my_r2: 0.9055\n",
      "Epoch 222/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.0424e-04 - my_r2: 0.8869 - val_loss: 2.5979e-04 - val_my_r2: 0.9068\n",
      "Epoch 223/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.7268e-04 - my_r2: 0.8633 - val_loss: 2.5926e-04 - val_my_r2: 0.9069\n",
      "Epoch 224/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.4340e-04 - my_r2: 0.8484 - val_loss: 2.5833e-04 - val_my_r2: 0.9081\n",
      "Epoch 225/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.7440e-04 - my_r2: 0.8780 - val_loss: 2.5756e-04 - val_my_r2: 0.9086\n",
      "Epoch 226/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 6.1107e-04 - my_r2: 0.8627 - val_loss: 2.5559e-04 - val_my_r2: 0.9089\n",
      "Epoch 227/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.1999e-04 - my_r2: 0.8711 - val_loss: 2.5260e-04 - val_my_r2: 0.9103\n",
      "Epoch 228/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.1937e-04 - my_r2: 0.8767 - val_loss: 2.5418e-04 - val_my_r2: 0.9096\n",
      "Epoch 229/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.5734e-04 - my_r2: 0.8575 - val_loss: 2.5126e-04 - val_my_r2: 0.9106\n",
      "Epoch 230/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.3344e-04 - my_r2: 0.8594 - val_loss: 2.5018e-04 - val_my_r2: 0.9107\n",
      "Epoch 231/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.7866e-04 - my_r2: 0.7986 - val_loss: 2.4885e-04 - val_my_r2: 0.9111\n",
      "Epoch 232/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.7852e-04 - my_r2: 0.8950 - val_loss: 2.4852e-04 - val_my_r2: 0.9116\n",
      "Epoch 233/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.3648e-04 - my_r2: 0.8513 - val_loss: 2.4359e-04 - val_my_r2: 0.9145\n",
      "Epoch 234/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.7431e-04 - my_r2: 0.8126 - val_loss: 2.4402e-04 - val_my_r2: 0.9155\n",
      "Epoch 235/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.8136e-04 - my_r2: 0.8294 - val_loss: 2.5060e-04 - val_my_r2: 0.9140\n",
      "Epoch 236/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.2543e-04 - my_r2: 0.8907 - val_loss: 2.4081e-04 - val_my_r2: 0.9167\n",
      "Epoch 237/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.8612e-04 - my_r2: 0.7853 - val_loss: 2.4011e-04 - val_my_r2: 0.9162\n",
      "Epoch 238/2000\n",
      "22/22 [==============================] - 1s 46ms/step - loss: 5.4767e-04 - my_r2: 0.8578 - val_loss: 2.3829e-04 - val_my_r2: 0.9168\n",
      "Epoch 239/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.2324e-04 - my_r2: 0.6720 - val_loss: 2.3467e-04 - val_my_r2: 0.9170\n",
      "Epoch 240/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.0400e-04 - my_r2: 0.8807 - val_loss: 2.3350e-04 - val_my_r2: 0.9177\n",
      "Epoch 241/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.2932e-04 - my_r2: 0.8698 - val_loss: 2.3268e-04 - val_my_r2: 0.9171\n",
      "Epoch 242/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.4217e-04 - my_r2: 0.8903 - val_loss: 2.3141e-04 - val_my_r2: 0.9185\n",
      "Epoch 243/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.1105e-04 - my_r2: 0.8678 - val_loss: 2.3889e-04 - val_my_r2: 0.9188\n",
      "Epoch 244/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.1137e-04 - my_r2: 0.7990 - val_loss: 2.2983e-04 - val_my_r2: 0.9214\n",
      "Epoch 245/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.9738e-04 - my_r2: 0.8674 - val_loss: 2.2729e-04 - val_my_r2: 0.9221\n",
      "Epoch 246/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2111e-04 - my_r2: 0.8849 - val_loss: 2.2437e-04 - val_my_r2: 0.9227\n",
      "Epoch 247/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.3481e-04 - my_r2: 0.6631 - val_loss: 2.2432e-04 - val_my_r2: 0.9233\n",
      "Epoch 248/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.9982e-04 - my_r2: 0.8752 - val_loss: 2.2119e-04 - val_my_r2: 0.9225\n",
      "Epoch 249/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.3932e-04 - my_r2: 0.8194 - val_loss: 2.2303e-04 - val_my_r2: 0.9226\n",
      "Epoch 250/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.3252e-04 - my_r2: -0.2977 - val_loss: 2.2891e-04 - val_my_r2: 0.9207\n",
      "Epoch 251/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.0140e-04 - my_r2: 0.8502 - val_loss: 2.2086e-04 - val_my_r2: 0.9219\n",
      "Epoch 252/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 4.4583e-04 - my_r2: 0.9015 - val_loss: 2.1939e-04 - val_my_r2: 0.9226\n",
      "Epoch 253/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.4327e-04 - my_r2: 0.8659 - val_loss: 2.1751e-04 - val_my_r2: 0.9226\n",
      "Epoch 254/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.2110e-04 - my_r2: 0.8615 - val_loss: 2.1619e-04 - val_my_r2: 0.9226\n",
      "Epoch 255/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.2366e-04 - my_r2: 0.8676 - val_loss: 2.1367e-04 - val_my_r2: 0.9249\n",
      "Epoch 256/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.5698e-04 - my_r2: 0.8819 - val_loss: 2.1441e-04 - val_my_r2: 0.9244\n",
      "Epoch 257/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.3899e-04 - my_r2: 0.8691 - val_loss: 2.1932e-04 - val_my_r2: 0.9236\n",
      "Epoch 258/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.8085e-04 - my_r2: 0.8290 - val_loss: 2.1343e-04 - val_my_r2: 0.9283\n",
      "Epoch 259/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.2428e-04 - my_r2: 0.8386 - val_loss: 2.0898e-04 - val_my_r2: 0.9286\n",
      "Epoch 260/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2299e-04 - my_r2: 0.8914 - val_loss: 2.0660e-04 - val_my_r2: 0.9294\n",
      "Epoch 261/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.8066e-04 - my_r2: 0.8804 - val_loss: 2.0732e-04 - val_my_r2: 0.9288\n",
      "Epoch 262/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.0226e-04 - my_r2: 0.8650 - val_loss: 2.0618e-04 - val_my_r2: 0.9287\n",
      "Epoch 263/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.0433e-04 - my_r2: 0.8402 - val_loss: 2.0389e-04 - val_my_r2: 0.9288\n",
      "Epoch 264/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.0737e-04 - my_r2: 0.8674 - val_loss: 2.0496e-04 - val_my_r2: 0.9275\n",
      "Epoch 265/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.3125e-04 - my_r2: 0.8431 - val_loss: 2.0211e-04 - val_my_r2: 0.9301\n",
      "Epoch 266/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 5.2356e-04 - my_r2: 0.8857 - val_loss: 2.0477e-04 - val_my_r2: 0.9313\n",
      "Epoch 267/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.9782e-04 - my_r2: 0.8229 - val_loss: 2.0149e-04 - val_my_r2: 0.9318\n",
      "Epoch 268/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.4633e-04 - my_r2: 0.8293 - val_loss: 1.9752e-04 - val_my_r2: 0.9321\n",
      "Epoch 269/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2202e-04 - my_r2: 0.8992 - val_loss: 1.9883e-04 - val_my_r2: 0.9314\n",
      "Epoch 270/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.5535e-04 - my_r2: 0.9104 - val_loss: 1.9609e-04 - val_my_r2: 0.9321\n",
      "Epoch 271/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.5709e-04 - my_r2: 0.8843 - val_loss: 1.9283e-04 - val_my_r2: 0.9323\n",
      "Epoch 272/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.5004e-04 - my_r2: 0.8903 - val_loss: 1.9382e-04 - val_my_r2: 0.9327\n",
      "Epoch 273/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.6997e-04 - my_r2: 0.8987 - val_loss: 1.9361e-04 - val_my_r2: 0.9334\n",
      "Epoch 274/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.3232e-04 - my_r2: 0.8858 - val_loss: 1.9184e-04 - val_my_r2: 0.9344\n",
      "Epoch 275/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 6.0470e-04 - my_r2: 0.8249 - val_loss: 1.9121e-04 - val_my_r2: 0.9342\n",
      "Epoch 276/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.0194e-04 - my_r2: 0.9020 - val_loss: 1.9177e-04 - val_my_r2: 0.9335\n",
      "Epoch 277/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2855e-04 - my_r2: 0.8949 - val_loss: 1.8894e-04 - val_my_r2: 0.9349\n",
      "Epoch 278/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.3241e-04 - my_r2: 0.8655 - val_loss: 1.9088e-04 - val_my_r2: 0.9339\n",
      "Epoch 279/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.2060e-04 - my_r2: 0.8688 - val_loss: 1.9098e-04 - val_my_r2: 0.9342\n",
      "Epoch 280/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.4523e-04 - my_r2: 0.8630 - val_loss: 1.9185e-04 - val_my_r2: 0.9348\n",
      "Epoch 281/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.8164e-04 - my_r2: 0.8491 - val_loss: 1.8527e-04 - val_my_r2: 0.9363\n",
      "Epoch 282/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.7725e-04 - my_r2: 0.9001 - val_loss: 1.8580e-04 - val_my_r2: 0.9369\n",
      "Epoch 283/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.2073e-04 - my_r2: 0.8999 - val_loss: 1.8639e-04 - val_my_r2: 0.9367\n",
      "Epoch 284/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.5398e-04 - my_r2: 0.9109 - val_loss: 1.8257e-04 - val_my_r2: 0.9385\n",
      "Epoch 285/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 5.4908e-04 - my_r2: 0.8767 - val_loss: 1.8462e-04 - val_my_r2: 0.9394\n",
      "Epoch 286/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.9921e-04 - my_r2: 0.8965 - val_loss: 1.8350e-04 - val_my_r2: 0.9399\n",
      "Epoch 287/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.4859e-04 - my_r2: 0.8632 - val_loss: 1.8173e-04 - val_my_r2: 0.9389\n",
      "Epoch 288/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.6993e-04 - my_r2: 0.8384 - val_loss: 1.8249e-04 - val_my_r2: 0.9392\n",
      "Epoch 289/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 5.5585e-04 - my_r2: 0.8555 - val_loss: 1.8010e-04 - val_my_r2: 0.9393\n",
      "Epoch 290/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.8685e-04 - my_r2: 0.8902 - val_loss: 1.7752e-04 - val_my_r2: 0.9392\n",
      "Epoch 291/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 3.9388e-04 - my_r2: 0.9032 - val_loss: 1.7390e-04 - val_my_r2: 0.9392\n",
      "Epoch 292/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 4.3570e-04 - my_r2: 0.8926 - val_loss: 1.7260e-04 - val_my_r2: 0.9399\n",
      "Epoch 293/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 3.8279e-04 - my_r2: 0.9069 - val_loss: 1.7194e-04 - val_my_r2: 0.9393\n",
      "Epoch 294/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 4.8056e-04 - my_r2: 0.9127 - val_loss: 1.7228e-04 - val_my_r2: 0.9385\n",
      "Epoch 295/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 5.1721e-04 - my_r2: 0.8244 - val_loss: 1.7138e-04 - val_my_r2: 0.9380\n",
      "Epoch 296/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 4.1691e-04 - my_r2: 0.8838 - val_loss: 1.7174e-04 - val_my_r2: 0.9374\n",
      "Epoch 297/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 4.5696e-04 - my_r2: 0.8821 - val_loss: 1.7660e-04 - val_my_r2: 0.9340\n",
      "Epoch 298/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 4.2150e-04 - my_r2: 0.9095 - val_loss: 1.7741e-04 - val_my_r2: 0.9327\n",
      "Epoch 299/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 3.9807e-04 - my_r2: 0.8942 - val_loss: 1.6976e-04 - val_my_r2: 0.9369\n",
      "Epoch 300/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 4.6617e-04 - my_r2: 0.8523 - val_loss: 1.6957e-04 - val_my_r2: 0.9396\n",
      "Epoch 301/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 5.0713e-04 - my_r2: 0.8284 - val_loss: 1.6899e-04 - val_my_r2: 0.9404\n",
      "Epoch 302/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9998e-04 - my_r2: 0.8865 - val_loss: 1.6509e-04 - val_my_r2: 0.9416\n",
      "Epoch 303/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.4063e-04 - my_r2: 0.8655 - val_loss: 1.6773e-04 - val_my_r2: 0.9412\n",
      "Epoch 304/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.3083e-04 - my_r2: 0.9053 - val_loss: 1.6774e-04 - val_my_r2: 0.9414\n",
      "Epoch 305/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.0645e-04 - my_r2: 0.8071 - val_loss: 1.6779e-04 - val_my_r2: 0.9402\n",
      "Epoch 306/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2466e-04 - my_r2: 0.8669 - val_loss: 1.6752e-04 - val_my_r2: 0.9403\n",
      "Epoch 307/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.8141e-04 - my_r2: 0.8380 - val_loss: 1.5904e-04 - val_my_r2: 0.9438\n",
      "Epoch 308/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 4.2893e-04 - my_r2: 0.7536 - val_loss: 1.5932e-04 - val_my_r2: 0.9435\n",
      "Epoch 309/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.9364e-04 - my_r2: 0.8307 - val_loss: 1.5628e-04 - val_my_r2: 0.9428\n",
      "Epoch 310/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.4645e-04 - my_r2: 0.9151 - val_loss: 1.5652e-04 - val_my_r2: 0.9426\n",
      "Epoch 311/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.6124e-04 - my_r2: 0.9146 - val_loss: 1.5849e-04 - val_my_r2: 0.9411\n",
      "Epoch 312/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.1389e-04 - my_r2: 0.8847 - val_loss: 1.5699e-04 - val_my_r2: 0.9418\n",
      "Epoch 313/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 4.3800e-04 - my_r2: 0.8829 - val_loss: 1.5764e-04 - val_my_r2: 0.9412\n",
      "Epoch 314/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.9559e-04 - my_r2: 0.8891 - val_loss: 1.5840e-04 - val_my_r2: 0.9413\n",
      "Epoch 315/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.1471e-04 - my_r2: 0.9379 - val_loss: 1.5723e-04 - val_my_r2: 0.9420\n",
      "Epoch 316/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.5608e-04 - my_r2: 0.8827 - val_loss: 1.5726e-04 - val_my_r2: 0.9427\n",
      "Epoch 317/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.4399e-04 - my_r2: 0.9209 - val_loss: 1.5136e-04 - val_my_r2: 0.9448\n",
      "Epoch 318/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 4.1649e-04 - my_r2: 0.8953 - val_loss: 1.5200e-04 - val_my_r2: 0.9458\n",
      "Epoch 319/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.9147e-04 - my_r2: 0.8991 - val_loss: 1.5294e-04 - val_my_r2: 0.9451\n",
      "Epoch 320/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.1933e-04 - my_r2: 0.7196 - val_loss: 1.4929e-04 - val_my_r2: 0.9465\n",
      "Epoch 321/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.5407e-04 - my_r2: 0.9019 - val_loss: 1.4780e-04 - val_my_r2: 0.9478\n",
      "Epoch 322/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.1554e-04 - my_r2: 0.8764 - val_loss: 1.5132e-04 - val_my_r2: 0.9481\n",
      "Epoch 323/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 4.5847e-04 - my_r2: 0.8836 - val_loss: 1.5090e-04 - val_my_r2: 0.9485\n",
      "Epoch 324/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.7791e-04 - my_r2: 0.9114 - val_loss: 1.5851e-04 - val_my_r2: 0.9462\n",
      "Epoch 325/2000\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 4.1046e-04 - my_r2: 0.9120 - val_loss: 1.5808e-04 - val_my_r2: 0.9465\n",
      "Epoch 326/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.4773e-04 - my_r2: 0.8436 - val_loss: 1.5764e-04 - val_my_r2: 0.9473\n",
      "Epoch 327/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.5762e-04 - my_r2: 0.9050 - val_loss: 1.5246e-04 - val_my_r2: 0.9474\n",
      "Epoch 328/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.9056e-04 - my_r2: 0.8847 - val_loss: 1.4681e-04 - val_my_r2: 0.9494\n",
      "Epoch 329/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.0805e-04 - my_r2: 0.8644 - val_loss: 1.4387e-04 - val_my_r2: 0.9508\n",
      "Epoch 330/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.6731e-04 - my_r2: 0.8906 - val_loss: 1.4303e-04 - val_my_r2: 0.9512\n",
      "Epoch 331/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.6189e-04 - my_r2: 0.8964 - val_loss: 1.4416e-04 - val_my_r2: 0.9476\n",
      "Epoch 332/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.9061e-04 - my_r2: 0.8667 - val_loss: 1.4414e-04 - val_my_r2: 0.9473\n",
      "Epoch 333/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.7192e-04 - my_r2: 0.8861 - val_loss: 1.3923e-04 - val_my_r2: 0.9501\n",
      "Epoch 334/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.5583e-04 - my_r2: 0.9167 - val_loss: 1.3516e-04 - val_my_r2: 0.9521\n",
      "Epoch 335/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.8906e-04 - my_r2: 0.8892 - val_loss: 1.3463e-04 - val_my_r2: 0.9518\n",
      "Epoch 336/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.4254e-04 - my_r2: 0.9287 - val_loss: 1.3452e-04 - val_my_r2: 0.9509\n",
      "Epoch 337/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.5615e-04 - my_r2: 0.8533 - val_loss: 1.3610e-04 - val_my_r2: 0.9511\n",
      "Epoch 338/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.8226e-04 - my_r2: 0.8864 - val_loss: 1.3577e-04 - val_my_r2: 0.9515\n",
      "Epoch 339/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.7731e-04 - my_r2: 0.9184 - val_loss: 1.3427e-04 - val_my_r2: 0.9511\n",
      "Epoch 340/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.6214e-04 - my_r2: 0.8505 - val_loss: 1.3364e-04 - val_my_r2: 0.9519\n",
      "Epoch 341/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.7549e-04 - my_r2: 0.8955 - val_loss: 1.3542e-04 - val_my_r2: 0.9526\n",
      "Epoch 342/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.3064e-04 - my_r2: 0.9202 - val_loss: 1.3242e-04 - val_my_r2: 0.9529\n",
      "Epoch 343/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.1123e-04 - my_r2: 0.8895 - val_loss: 1.3014e-04 - val_my_r2: 0.9537\n",
      "Epoch 344/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.6953e-04 - my_r2: 0.8733 - val_loss: 1.2830e-04 - val_my_r2: 0.9542\n",
      "Epoch 345/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2024e-04 - my_r2: 0.9125 - val_loss: 1.2929e-04 - val_my_r2: 0.9534\n",
      "Epoch 346/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.8276e-04 - my_r2: 0.9115 - val_loss: 1.2990e-04 - val_my_r2: 0.9541\n",
      "Epoch 347/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 4.2331e-04 - my_r2: 0.8523 - val_loss: 1.2773e-04 - val_my_r2: 0.9552\n",
      "Epoch 348/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.2394e-04 - my_r2: 0.9120 - val_loss: 1.2631e-04 - val_my_r2: 0.9550\n",
      "Epoch 349/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.7292e-04 - my_r2: 0.8465 - val_loss: 1.2475e-04 - val_my_r2: 0.9548\n",
      "Epoch 350/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.6295e-04 - my_r2: 0.8907 - val_loss: 1.2534e-04 - val_my_r2: 0.9533\n",
      "Epoch 351/2000\n",
      "22/22 [==============================] - 1s 58ms/step - loss: 4.4040e-04 - my_r2: 0.8844 - val_loss: 1.2402e-04 - val_my_r2: 0.9540\n",
      "Epoch 352/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.7372e-04 - my_r2: 0.9134 - val_loss: 1.2386e-04 - val_my_r2: 0.9541\n",
      "Epoch 353/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.9398e-04 - my_r2: 0.8964 - val_loss: 1.2207e-04 - val_my_r2: 0.9557\n",
      "Epoch 354/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.2841e-04 - my_r2: 0.8933 - val_loss: 1.2358e-04 - val_my_r2: 0.9549\n",
      "Epoch 355/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.8755e-04 - my_r2: 0.9107 - val_loss: 1.2114e-04 - val_my_r2: 0.9566\n",
      "Epoch 356/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.4183e-04 - my_r2: 0.8343 - val_loss: 1.2064e-04 - val_my_r2: 0.9567\n",
      "Epoch 357/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2654e-04 - my_r2: 0.8967 - val_loss: 1.1995e-04 - val_my_r2: 0.9564\n",
      "Epoch 358/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.6020e-04 - my_r2: 0.8731 - val_loss: 1.1810e-04 - val_my_r2: 0.9576\n",
      "Epoch 359/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.2212e-04 - my_r2: 0.8356 - val_loss: 1.1861e-04 - val_my_r2: 0.9575\n",
      "Epoch 360/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.3222e-04 - my_r2: 0.9203 - val_loss: 1.1892e-04 - val_my_r2: 0.9567\n",
      "Epoch 361/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.9283e-04 - my_r2: 0.8882 - val_loss: 1.2741e-04 - val_my_r2: 0.9548\n",
      "Epoch 362/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.0808e-04 - my_r2: 0.9064 - val_loss: 1.1903e-04 - val_my_r2: 0.9565\n",
      "Epoch 363/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.9423e-04 - my_r2: 0.8969 - val_loss: 1.1862e-04 - val_my_r2: 0.9566\n",
      "Epoch 364/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.2884e-04 - my_r2: 0.8803 - val_loss: 1.2028e-04 - val_my_r2: 0.9569\n",
      "Epoch 365/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.2079e-04 - my_r2: 0.9183 - val_loss: 1.2092e-04 - val_my_r2: 0.9568\n",
      "Epoch 366/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.2447e-04 - my_r2: 0.9021 - val_loss: 1.2040e-04 - val_my_r2: 0.9561\n",
      "Epoch 367/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.3339e-04 - my_r2: 0.8752 - val_loss: 1.1847e-04 - val_my_r2: 0.9558\n",
      "Epoch 368/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.9258e-04 - my_r2: 0.8979 - val_loss: 1.1605e-04 - val_my_r2: 0.9571\n",
      "Epoch 369/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.1627e-04 - my_r2: 0.9119 - val_loss: 1.1571e-04 - val_my_r2: 0.9574\n",
      "Epoch 370/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.0521e-04 - my_r2: 0.8812 - val_loss: 1.1548e-04 - val_my_r2: 0.9570\n",
      "Epoch 371/2000\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 3.4207e-04 - my_r2: 0.8441 - val_loss: 1.2046e-04 - val_my_r2: 0.9566\n",
      "Epoch 372/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.7260e-04 - my_r2: 0.8494 - val_loss: 1.1551e-04 - val_my_r2: 0.9578\n",
      "Epoch 373/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.8129e-04 - my_r2: 0.9001 - val_loss: 1.1255e-04 - val_my_r2: 0.9583\n",
      "Epoch 374/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.4091e-04 - my_r2: 0.9164 - val_loss: 1.1180e-04 - val_my_r2: 0.9590\n",
      "Epoch 375/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.8903e-04 - my_r2: 0.9042 - val_loss: 1.1037e-04 - val_my_r2: 0.9596\n",
      "Epoch 376/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 4.2998e-04 - my_r2: 0.8411 - val_loss: 1.1053e-04 - val_my_r2: 0.9609\n",
      "Epoch 377/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.6775e-04 - my_r2: 0.9072 - val_loss: 1.1150e-04 - val_my_r2: 0.9609\n",
      "Epoch 378/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.7415e-04 - my_r2: 0.8809 - val_loss: 1.0875e-04 - val_my_r2: 0.9615\n",
      "Epoch 379/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.6868e-04 - my_r2: 0.9009 - val_loss: 1.1618e-04 - val_my_r2: 0.9602\n",
      "Epoch 380/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.9833e-04 - my_r2: 0.8930 - val_loss: 1.0948e-04 - val_my_r2: 0.9623\n",
      "Epoch 381/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.9116e-04 - my_r2: 0.8833 - val_loss: 1.0928e-04 - val_my_r2: 0.9619\n",
      "Epoch 382/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.7097e-04 - my_r2: 0.8079 - val_loss: 1.0836e-04 - val_my_r2: 0.9615\n",
      "Epoch 383/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.5685e-04 - my_r2: 0.8709 - val_loss: 1.0838e-04 - val_my_r2: 0.9615\n",
      "Epoch 384/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.5320e-04 - my_r2: 0.9225 - val_loss: 1.0837e-04 - val_my_r2: 0.9624\n",
      "Epoch 385/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 3.5241e-04 - my_r2: 0.9124 - val_loss: 1.0654e-04 - val_my_r2: 0.9629\n",
      "Epoch 386/2000\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 3.4679e-04 - my_r2: 0.9096 - val_loss: 1.0722e-04 - val_my_r2: 0.9632\n",
      "Epoch 387/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.8671e-04 - my_r2: 0.8428 - val_loss: 1.1398e-04 - val_my_r2: 0.9624\n",
      "Epoch 388/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.2861e-04 - my_r2: 0.8884 - val_loss: 1.2194e-04 - val_my_r2: 0.9601\n",
      "Epoch 389/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.1436e-04 - my_r2: 0.8815 - val_loss: 1.0218e-04 - val_my_r2: 0.9647\n",
      "Epoch 390/2000\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 2.9571e-04 - my_r2: 0.9026 - val_loss: 1.0485e-04 - val_my_r2: 0.9636\n",
      "Epoch 391/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.5207e-04 - my_r2: 0.8890 - val_loss: 1.0109e-04 - val_my_r2: 0.9640\n",
      "Epoch 392/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 4.8911e-04 - my_r2: 0.8254 - val_loss: 1.0267e-04 - val_my_r2: 0.9639\n",
      "Epoch 393/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 2.8633e-04 - my_r2: 0.9036 - val_loss: 1.0114e-04 - val_my_r2: 0.9644\n",
      "Epoch 394/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.3225e-04 - my_r2: 0.9200 - val_loss: 1.0436e-04 - val_my_r2: 0.9642\n",
      "Epoch 395/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.7502e-04 - my_r2: 0.9243 - val_loss: 1.0461e-04 - val_my_r2: 0.9641\n",
      "Epoch 396/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.1805e-04 - my_r2: 0.9182 - val_loss: 1.0779e-04 - val_my_r2: 0.9635\n",
      "Epoch 397/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 3.3977e-04 - my_r2: 0.9255 - val_loss: 1.0800e-04 - val_my_r2: 0.9633\n",
      "Epoch 398/2000\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 3.4119e-04 - my_r2: 0.8849 - val_loss: 1.0940e-04 - val_my_r2: 0.9618\n",
      "Epoch 399/2000\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 4.4085e-04 - my_r2: 0.9017 - val_loss: 1.0439e-04 - val_my_r2: 0.9630\n",
      "Epoch 400/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.5825e-04 - my_r2: 0.9240 - val_loss: 9.7036e-05 - val_my_r2: 0.9655\n",
      "Epoch 401/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 3.4270e-04 - my_r2: 0.9199 - val_loss: 9.7798e-05 - val_my_r2: 0.9644\n",
      "Epoch 402/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 3.6380e-04 - my_r2: 0.9102 - val_loss: 9.5930e-05 - val_my_r2: 0.9648\n",
      "Epoch 403/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.2371e-04 - my_r2: 0.9097 - val_loss: 9.6400e-05 - val_my_r2: 0.9650\n",
      "Epoch 404/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.5339e-04 - my_r2: 0.8316 - val_loss: 9.7098e-05 - val_my_r2: 0.9647\n",
      "Epoch 405/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.3793e-04 - my_r2: 0.9128 - val_loss: 9.6278e-05 - val_my_r2: 0.9658\n",
      "Epoch 406/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.4532e-04 - my_r2: 0.9220 - val_loss: 9.2840e-05 - val_my_r2: 0.9672\n",
      "Epoch 407/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 3.9089e-04 - my_r2: 0.7816 - val_loss: 9.4916e-05 - val_my_r2: 0.9672\n",
      "Epoch 408/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 3.4545e-04 - my_r2: 0.9154 - val_loss: 9.3417e-05 - val_my_r2: 0.9676\n",
      "Epoch 409/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 3.7195e-04 - my_r2: 0.9009 - val_loss: 9.3802e-05 - val_my_r2: 0.9672\n",
      "Epoch 410/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.4264e-04 - my_r2: 0.8904 - val_loss: 9.0958e-05 - val_my_r2: 0.9679\n",
      "Epoch 411/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 4.1397e-04 - my_r2: 0.9078 - val_loss: 9.4395e-05 - val_my_r2: 0.9667\n",
      "Epoch 412/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 3.8064e-04 - my_r2: 0.8961 - val_loss: 9.2541e-05 - val_my_r2: 0.9666\n",
      "Epoch 413/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.0171e-04 - my_r2: 0.9283 - val_loss: 9.2592e-05 - val_my_r2: 0.9660\n",
      "Epoch 414/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 2.9922e-04 - my_r2: 0.9243 - val_loss: 9.2488e-05 - val_my_r2: 0.9658\n",
      "Epoch 415/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 4.2367e-04 - my_r2: 0.9185 - val_loss: 9.1218e-05 - val_my_r2: 0.9676\n",
      "Epoch 416/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 3.1266e-04 - my_r2: 0.8839 - val_loss: 9.0736e-05 - val_my_r2: 0.9678\n",
      "Epoch 417/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 4.0855e-04 - my_r2: 0.9189 - val_loss: 8.7215e-05 - val_my_r2: 0.9689\n",
      "Epoch 418/2000\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 3.4146e-04 - my_r2: 0.8854 - val_loss: 8.7471e-05 - val_my_r2: 0.9686\n",
      "Epoch 419/2000\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 3.2575e-04 - my_r2: 0.8891 - val_loss: 8.9308e-05 - val_my_r2: 0.9682\n",
      "Epoch 420/2000\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 4.1916e-04 - my_r2: 0.8933 - val_loss: 9.6455e-05 - val_my_r2: 0.9658\n",
      "Epoch 421/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 3.4578e-04 - my_r2: 0.9244 - val_loss: 9.9333e-05 - val_my_r2: 0.9654\n",
      "Epoch 422/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7606e-04 - my_r2: 0.8967 - val_loss: 9.0215e-05 - val_my_r2: 0.9677\n",
      "Epoch 423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0141e-04 - my_r2: 0.9191 - val_loss: 9.1625e-05 - val_my_r2: 0.9666\n",
      "Epoch 424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9048e-04 - my_r2: 0.9330 - val_loss: 9.3217e-05 - val_my_r2: 0.9656\n",
      "Epoch 425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2472e-04 - my_r2: 0.8839 - val_loss: 9.3917e-05 - val_my_r2: 0.9649\n",
      "Epoch 426/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7996e-04 - my_r2: 0.9161 - val_loss: 9.3236e-05 - val_my_r2: 0.9653\n",
      "Epoch 427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5333e-04 - my_r2: 0.8751 - val_loss: 9.0328e-05 - val_my_r2: 0.9667\n",
      "Epoch 428/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8095e-04 - my_r2: 0.9383 - val_loss: 8.9756e-05 - val_my_r2: 0.9674\n",
      "Epoch 429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3047e-04 - my_r2: 0.7973 - val_loss: 8.5098e-05 - val_my_r2: 0.9693\n",
      "Epoch 430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4128e-04 - my_r2: 0.9235 - val_loss: 8.4139e-05 - val_my_r2: 0.9696\n",
      "Epoch 431/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0413e-04 - my_r2: 0.9010 - val_loss: 8.6363e-05 - val_my_r2: 0.9685\n",
      "Epoch 432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6384e-04 - my_r2: 0.8564 - val_loss: 8.2308e-05 - val_my_r2: 0.9696\n",
      "Epoch 433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6009e-04 - my_r2: 0.9323 - val_loss: 8.2261e-05 - val_my_r2: 0.9702\n",
      "Epoch 434/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1101e-04 - my_r2: 0.9055 - val_loss: 8.5292e-05 - val_my_r2: 0.9701\n",
      "Epoch 435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0970e-04 - my_r2: 0.9092 - val_loss: 8.3544e-05 - val_my_r2: 0.9706\n",
      "Epoch 436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1559e-04 - my_r2: 0.9263 - val_loss: 8.7013e-05 - val_my_r2: 0.9674\n",
      "Epoch 437/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3179e-04 - my_r2: 0.9237 - val_loss: 8.4832e-05 - val_my_r2: 0.9685\n",
      "Epoch 438/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9154e-04 - my_r2: 0.9297 - val_loss: 8.2572e-05 - val_my_r2: 0.9702\n",
      "Epoch 439/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6276e-04 - my_r2: 0.9085 - val_loss: 8.5066e-05 - val_my_r2: 0.9695\n",
      "Epoch 440/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8135e-04 - my_r2: 0.8649 - val_loss: 8.2189e-05 - val_my_r2: 0.9699\n",
      "Epoch 441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5141e-04 - my_r2: 0.9189 - val_loss: 8.3693e-05 - val_my_r2: 0.9695\n",
      "Epoch 442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7679e-04 - my_r2: 0.9141 - val_loss: 8.4151e-05 - val_my_r2: 0.9697\n",
      "Epoch 443/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4820e-04 - my_r2: 0.9190 - val_loss: 8.2924e-05 - val_my_r2: 0.9700\n",
      "Epoch 444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9138e-04 - my_r2: 0.8516 - val_loss: 8.2364e-05 - val_my_r2: 0.9696\n",
      "Epoch 445/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0839e-04 - my_r2: 0.9295 - val_loss: 8.5128e-05 - val_my_r2: 0.9695\n",
      "Epoch 446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1971e-04 - my_r2: 0.9177 - val_loss: 8.4698e-05 - val_my_r2: 0.9691\n",
      "Epoch 447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0793e-04 - my_r2: 0.9234 - val_loss: 8.1615e-05 - val_my_r2: 0.9707\n",
      "Epoch 448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8147e-04 - my_r2: 0.8972 - val_loss: 7.8205e-05 - val_my_r2: 0.9719\n",
      "Epoch 449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7950e-04 - my_r2: 0.8846 - val_loss: 8.3739e-05 - val_my_r2: 0.9689\n",
      "Epoch 450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3516e-04 - my_r2: 0.8722 - val_loss: 8.8327e-05 - val_my_r2: 0.9672\n",
      "Epoch 451/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7281e-04 - my_r2: 0.9044 - val_loss: 7.7772e-05 - val_my_r2: 0.9724\n",
      "Epoch 452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2436e-04 - my_r2: 0.9208 - val_loss: 7.9978e-05 - val_my_r2: 0.9711\n",
      "Epoch 453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5029e-04 - my_r2: 0.9328 - val_loss: 8.1884e-05 - val_my_r2: 0.9707\n",
      "Epoch 454/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0607e-04 - my_r2: 0.9280 - val_loss: 7.8255e-05 - val_my_r2: 0.9715\n",
      "Epoch 455/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0289e-04 - my_r2: 0.9090 - val_loss: 7.5403e-05 - val_my_r2: 0.9726\n",
      "Epoch 456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7599e-04 - my_r2: 0.8978 - val_loss: 7.6580e-05 - val_my_r2: 0.9720\n",
      "Epoch 457/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6323e-04 - my_r2: 0.9233 - val_loss: 7.9107e-05 - val_my_r2: 0.9722\n",
      "Epoch 458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4298e-04 - my_r2: 0.9274 - val_loss: 7.7948e-05 - val_my_r2: 0.9723\n",
      "Epoch 459/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6876e-04 - my_r2: 0.9202 - val_loss: 7.9690e-05 - val_my_r2: 0.9709\n",
      "Epoch 460/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8164e-04 - my_r2: 0.8633 - val_loss: 8.1786e-05 - val_my_r2: 0.9700\n",
      "Epoch 461/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7849e-04 - my_r2: 0.9089 - val_loss: 8.0278e-05 - val_my_r2: 0.9706\n",
      "Epoch 462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0913e-04 - my_r2: 0.9402 - val_loss: 7.5109e-05 - val_my_r2: 0.9732\n",
      "Epoch 463/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0560e-04 - my_r2: 0.9383 - val_loss: 7.7073e-05 - val_my_r2: 0.9735\n",
      "Epoch 464/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7024e-04 - my_r2: 0.8906 - val_loss: 7.3469e-05 - val_my_r2: 0.9735\n",
      "Epoch 465/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3563e-04 - my_r2: 0.9047 - val_loss: 7.0369e-05 - val_my_r2: 0.9741\n",
      "Epoch 466/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1797e-04 - my_r2: 0.9063 - val_loss: 7.1062e-05 - val_my_r2: 0.9743\n",
      "Epoch 467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7762e-04 - my_r2: 0.9282 - val_loss: 7.4428e-05 - val_my_r2: 0.9731\n",
      "Epoch 468/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4775e-04 - my_r2: 0.9228 - val_loss: 7.3010e-05 - val_my_r2: 0.9749\n",
      "Epoch 469/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4536e-04 - my_r2: 0.8894 - val_loss: 7.5228e-05 - val_my_r2: 0.9742\n",
      "Epoch 470/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1593e-04 - my_r2: 0.8966 - val_loss: 7.2669e-05 - val_my_r2: 0.9745\n",
      "Epoch 471/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9816e-04 - my_r2: 0.8434 - val_loss: 7.6799e-05 - val_my_r2: 0.9743\n",
      "Epoch 472/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6352e-04 - my_r2: 0.9221 - val_loss: 7.1075e-05 - val_my_r2: 0.9755\n",
      "Epoch 473/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8198e-04 - my_r2: 0.9014 - val_loss: 7.3192e-05 - val_my_r2: 0.9744\n",
      "Epoch 474/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2981e-04 - my_r2: 0.9107 - val_loss: 8.1634e-05 - val_my_r2: 0.9709\n",
      "Epoch 475/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7769e-04 - my_r2: 0.9170 - val_loss: 7.7479e-05 - val_my_r2: 0.9717\n",
      "Epoch 476/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3413e-04 - my_r2: 0.8883 - val_loss: 7.5445e-05 - val_my_r2: 0.9721\n",
      "Epoch 477/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9019e-04 - my_r2: 0.8882 - val_loss: 6.8597e-05 - val_my_r2: 0.9743\n",
      "Epoch 478/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7985e-04 - my_r2: 0.9273 - val_loss: 7.6109e-05 - val_my_r2: 0.9727\n",
      "Epoch 479/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9217e-04 - my_r2: 0.8031 - val_loss: 8.0769e-05 - val_my_r2: 0.9723\n",
      "Epoch 480/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8535e-04 - my_r2: 0.9177 - val_loss: 6.6952e-05 - val_my_r2: 0.9759\n",
      "Epoch 481/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6718e-04 - my_r2: 0.8292 - val_loss: 6.7353e-05 - val_my_r2: 0.9760\n",
      "Epoch 482/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9781e-04 - my_r2: 0.9411 - val_loss: 6.8859e-05 - val_my_r2: 0.9750\n",
      "Epoch 483/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0489e-04 - my_r2: 0.9091 - val_loss: 7.5026e-05 - val_my_r2: 0.9729\n",
      "Epoch 484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2650e-04 - my_r2: 0.9178 - val_loss: 7.3022e-05 - val_my_r2: 0.9737\n",
      "Epoch 485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2241e-04 - my_r2: 0.8894 - val_loss: 6.7893e-05 - val_my_r2: 0.9747\n",
      "Epoch 486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6171e-04 - my_r2: 0.8754 - val_loss: 6.6672e-05 - val_my_r2: 0.9757\n",
      "Epoch 487/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7474e-04 - my_r2: 0.8969 - val_loss: 6.6123e-05 - val_my_r2: 0.9763\n",
      "Epoch 488/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1420e-04 - my_r2: 0.9352 - val_loss: 6.7595e-05 - val_my_r2: 0.9761\n",
      "Epoch 489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5787e-04 - my_r2: 0.8865 - val_loss: 6.4389e-05 - val_my_r2: 0.9771\n",
      "Epoch 490/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2966e-04 - my_r2: 0.8913 - val_loss: 6.2712e-05 - val_my_r2: 0.9777\n",
      "Epoch 491/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7716e-04 - my_r2: 0.9385 - val_loss: 6.3632e-05 - val_my_r2: 0.9773\n",
      "Epoch 492/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4678e-04 - my_r2: 0.8847 - val_loss: 6.2829e-05 - val_my_r2: 0.9784\n",
      "Epoch 493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8690e-04 - my_r2: 0.9294 - val_loss: 6.2793e-05 - val_my_r2: 0.9777\n",
      "Epoch 494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4535e-04 - my_r2: 0.8882 - val_loss: 6.1446e-05 - val_my_r2: 0.9777\n",
      "Epoch 495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5070e-04 - my_r2: 0.9309 - val_loss: 7.5383e-05 - val_my_r2: 0.9715\n",
      "Epoch 496/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4407e-04 - my_r2: 0.9159 - val_loss: 6.3411e-05 - val_my_r2: 0.9768\n",
      "Epoch 497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0831e-04 - my_r2: 0.9227 - val_loss: 6.1863e-05 - val_my_r2: 0.9778\n",
      "Epoch 498/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2031e-04 - my_r2: 0.8923 - val_loss: 6.2119e-05 - val_my_r2: 0.9782\n",
      "Epoch 499/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0958e-04 - my_r2: 0.9161 - val_loss: 6.3615e-05 - val_my_r2: 0.9773\n",
      "Epoch 500/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0212e-04 - my_r2: 0.8207 - val_loss: 6.1004e-05 - val_my_r2: 0.9777\n",
      "Epoch 501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2314e-04 - my_r2: 0.8995 - val_loss: 6.4048e-05 - val_my_r2: 0.9767\n",
      "Epoch 502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5351e-04 - my_r2: 0.8884 - val_loss: 6.7232e-05 - val_my_r2: 0.9765\n",
      "Epoch 503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9925e-04 - my_r2: 0.9027 - val_loss: 7.1290e-05 - val_my_r2: 0.9758\n",
      "Epoch 504/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3774e-04 - my_r2: 0.8197 - val_loss: 6.0956e-05 - val_my_r2: 0.9784\n",
      "Epoch 505/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0423e-04 - my_r2: 0.9292 - val_loss: 6.0584e-05 - val_my_r2: 0.9783\n",
      "Epoch 506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2611e-04 - my_r2: 0.8254 - val_loss: 5.9302e-05 - val_my_r2: 0.9779\n",
      "Epoch 507/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1060e-04 - my_r2: 0.8980 - val_loss: 6.0541e-05 - val_my_r2: 0.9780\n",
      "Epoch 508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1144e-04 - my_r2: 0.9238 - val_loss: 6.0402e-05 - val_my_r2: 0.9793\n",
      "Epoch 509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9009e-04 - my_r2: 0.9112 - val_loss: 6.3322e-05 - val_my_r2: 0.9784\n",
      "Epoch 510/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6190e-04 - my_r2: 0.9276 - val_loss: 5.8633e-05 - val_my_r2: 0.9795\n",
      "Epoch 511/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7973e-04 - my_r2: 0.8687 - val_loss: 5.7218e-05 - val_my_r2: 0.9798\n",
      "Epoch 512/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5087e-04 - my_r2: 0.9381 - val_loss: 5.7351e-05 - val_my_r2: 0.9798\n",
      "Epoch 513/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7500e-04 - my_r2: 0.9384 - val_loss: 5.5314e-05 - val_my_r2: 0.9802\n",
      "Epoch 514/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8635e-04 - my_r2: 0.9107 - val_loss: 5.9165e-05 - val_my_r2: 0.9795\n",
      "Epoch 515/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3253e-04 - my_r2: 0.9159 - val_loss: 6.2364e-05 - val_my_r2: 0.9777\n",
      "Epoch 516/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6717e-04 - my_r2: 0.9203 - val_loss: 6.1286e-05 - val_my_r2: 0.9777\n",
      "Epoch 517/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8519e-04 - my_r2: 0.9095 - val_loss: 5.9089e-05 - val_my_r2: 0.9787\n",
      "Epoch 518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8868e-04 - my_r2: 0.9120 - val_loss: 5.5238e-05 - val_my_r2: 0.9799\n",
      "Epoch 519/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2308e-04 - my_r2: 0.8702 - val_loss: 6.6361e-05 - val_my_r2: 0.9778\n",
      "Epoch 520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3101e-04 - my_r2: 0.8966 - val_loss: 5.6451e-05 - val_my_r2: 0.9803\n",
      "Epoch 521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7771e-04 - my_r2: 0.8578 - val_loss: 5.6508e-05 - val_my_r2: 0.9802\n",
      "Epoch 522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7798e-04 - my_r2: 0.9147 - val_loss: 6.1332e-05 - val_my_r2: 0.9790\n",
      "Epoch 523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2976e-04 - my_r2: 0.9223 - val_loss: 6.6772e-05 - val_my_r2: 0.9768\n",
      "Epoch 524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5547e-04 - my_r2: 0.8722 - val_loss: 5.7763e-05 - val_my_r2: 0.9793\n",
      "Epoch 525/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9990e-04 - my_r2: 0.9297 - val_loss: 5.6683e-05 - val_my_r2: 0.9796\n",
      "Epoch 526/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8478e-04 - my_r2: 0.9337 - val_loss: 5.7839e-05 - val_my_r2: 0.9790\n",
      "Epoch 527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8616e-04 - my_r2: 0.9193 - val_loss: 5.3873e-05 - val_my_r2: 0.9808\n",
      "Epoch 528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7842e-04 - my_r2: 0.9297 - val_loss: 5.6808e-05 - val_my_r2: 0.9799\n",
      "Epoch 529/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7242e-04 - my_r2: 0.9236 - val_loss: 5.2221e-05 - val_my_r2: 0.9806\n",
      "Epoch 530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4041e-04 - my_r2: 0.8740 - val_loss: 5.3282e-05 - val_my_r2: 0.9803\n",
      "Epoch 531/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3451e-04 - my_r2: 0.9275 - val_loss: 5.4901e-05 - val_my_r2: 0.9799\n",
      "Epoch 532/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0728e-04 - my_r2: 0.9188 - val_loss: 6.0193e-05 - val_my_r2: 0.9770\n",
      "Epoch 533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3661e-04 - my_r2: 0.8893 - val_loss: 5.7800e-05 - val_my_r2: 0.9770\n",
      "Epoch 534/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3698e-04 - my_r2: 0.8979 - val_loss: 5.4590e-05 - val_my_r2: 0.9790\n",
      "Epoch 535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7999e-04 - my_r2: 0.9386 - val_loss: 5.8964e-05 - val_my_r2: 0.9787\n",
      "Epoch 536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3170e-04 - my_r2: 0.9224 - val_loss: 5.6808e-05 - val_my_r2: 0.9787\n",
      "Epoch 537/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3121e-04 - my_r2: 0.8999 - val_loss: 5.1539e-05 - val_my_r2: 0.9801\n",
      "Epoch 538/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1510e-04 - my_r2: 0.9285 - val_loss: 5.4514e-05 - val_my_r2: 0.9789\n",
      "Epoch 539/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6936e-04 - my_r2: 0.9065 - val_loss: 5.1486e-05 - val_my_r2: 0.9809\n",
      "Epoch 540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4565e-04 - my_r2: 0.9515 - val_loss: 5.1405e-05 - val_my_r2: 0.9806\n",
      "Epoch 541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3127e-04 - my_r2: 0.9232 - val_loss: 5.3045e-05 - val_my_r2: 0.9792\n",
      "Epoch 542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7754e-04 - my_r2: 0.8781 - val_loss: 5.4483e-05 - val_my_r2: 0.9801\n",
      "Epoch 543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1030e-04 - my_r2: 0.9219 - val_loss: 5.2900e-05 - val_my_r2: 0.9814\n",
      "Epoch 544/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2203e-04 - my_r2: 0.8928 - val_loss: 5.3776e-05 - val_my_r2: 0.9809\n",
      "Epoch 545/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8854e-04 - my_r2: 0.9350 - val_loss: 5.4555e-05 - val_my_r2: 0.9803\n",
      "Epoch 546/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1033e-04 - my_r2: 0.9061 - val_loss: 5.4783e-05 - val_my_r2: 0.9799\n",
      "Epoch 547/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1766e-04 - my_r2: 0.8784 - val_loss: 4.9081e-05 - val_my_r2: 0.9819\n",
      "Epoch 548/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9300e-04 - my_r2: 0.8166 - val_loss: 5.0236e-05 - val_my_r2: 0.9810\n",
      "Epoch 549/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3625e-04 - my_r2: 0.9327 - val_loss: 5.7487e-05 - val_my_r2: 0.9779\n",
      "Epoch 550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4405e-04 - my_r2: 0.9174 - val_loss: 5.0124e-05 - val_my_r2: 0.9811\n",
      "Epoch 551/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0195e-04 - my_r2: 0.9098 - val_loss: 4.9357e-05 - val_my_r2: 0.9810\n",
      "Epoch 552/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4859e-04 - my_r2: 0.9055 - val_loss: 5.1157e-05 - val_my_r2: 0.9803\n",
      "Epoch 553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0760e-04 - my_r2: 0.9387 - val_loss: 5.0841e-05 - val_my_r2: 0.9813\n",
      "Epoch 554/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3371e-04 - my_r2: 0.9081 - val_loss: 5.0025e-05 - val_my_r2: 0.9818\n",
      "Epoch 555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8521e-04 - my_r2: 0.9310 - val_loss: 5.3108e-05 - val_my_r2: 0.9806\n",
      "Epoch 556/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9296e-04 - my_r2: 0.9177 - val_loss: 5.2371e-05 - val_my_r2: 0.9802\n",
      "Epoch 557/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9598e-04 - my_r2: 0.9304 - val_loss: 4.7155e-05 - val_my_r2: 0.9824\n",
      "Epoch 558/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3712e-04 - my_r2: 0.9018 - val_loss: 5.0850e-05 - val_my_r2: 0.9816\n",
      "Epoch 559/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6243e-04 - my_r2: 0.9203 - val_loss: 5.3502e-05 - val_my_r2: 0.9806\n",
      "Epoch 560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1444e-04 - my_r2: 0.8949 - val_loss: 4.8033e-05 - val_my_r2: 0.9815\n",
      "Epoch 561/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7328e-04 - my_r2: 0.8865 - val_loss: 4.8915e-05 - val_my_r2: 0.9801\n",
      "Epoch 562/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7080e-04 - my_r2: 0.9355 - val_loss: 4.7074e-05 - val_my_r2: 0.9810\n",
      "Epoch 563/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4760e-04 - my_r2: 0.9266 - val_loss: 4.6735e-05 - val_my_r2: 0.9829\n",
      "Epoch 564/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1403e-04 - my_r2: 0.9249 - val_loss: 5.5013e-05 - val_my_r2: 0.9812\n",
      "Epoch 565/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6225e-04 - my_r2: 0.9131 - val_loss: 4.7137e-05 - val_my_r2: 0.9828\n",
      "Epoch 566/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5984e-04 - my_r2: 0.9056 - val_loss: 4.9329e-05 - val_my_r2: 0.9814\n",
      "Epoch 567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2085e-04 - my_r2: 0.9024 - val_loss: 4.7382e-05 - val_my_r2: 0.9822\n",
      "Epoch 568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6458e-04 - my_r2: 0.9108 - val_loss: 4.7923e-05 - val_my_r2: 0.9821\n",
      "Epoch 569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2233e-04 - my_r2: 0.9267 - val_loss: 4.7406e-05 - val_my_r2: 0.9819\n",
      "Epoch 570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9198e-04 - my_r2: 0.9223 - val_loss: 4.5433e-05 - val_my_r2: 0.9829\n",
      "Epoch 571/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5418e-04 - my_r2: 0.9018 - val_loss: 4.4674e-05 - val_my_r2: 0.9827\n",
      "Epoch 572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1676e-04 - my_r2: 0.9289 - val_loss: 4.7215e-05 - val_my_r2: 0.9817\n",
      "Epoch 573/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8960e-04 - my_r2: 0.9294 - val_loss: 4.4801e-05 - val_my_r2: 0.9828\n",
      "Epoch 574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2761e-04 - my_r2: 0.9221 - val_loss: 4.4227e-05 - val_my_r2: 0.9827\n",
      "Epoch 575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1248e-04 - my_r2: 0.9237 - val_loss: 5.0494e-05 - val_my_r2: 0.9802\n",
      "Epoch 576/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3694e-04 - my_r2: 0.9036 - val_loss: 4.2648e-05 - val_my_r2: 0.9831\n",
      "Epoch 577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6228e-04 - my_r2: 0.8695 - val_loss: 4.4727e-05 - val_my_r2: 0.9823\n",
      "Epoch 578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8409e-04 - my_r2: 0.9206 - val_loss: 4.6445e-05 - val_my_r2: 0.9813\n",
      "Epoch 579/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7066e-04 - my_r2: 0.8718 - val_loss: 4.2503e-05 - val_my_r2: 0.9831\n",
      "Epoch 580/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5058e-04 - my_r2: 0.9379 - val_loss: 4.5105e-05 - val_my_r2: 0.9824\n",
      "Epoch 581/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7326e-04 - my_r2: 0.8880 - val_loss: 4.3494e-05 - val_my_r2: 0.9829\n",
      "Epoch 582/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1824e-04 - my_r2: 0.9350 - val_loss: 4.1708e-05 - val_my_r2: 0.9838\n",
      "Epoch 583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0662e-04 - my_r2: 0.8991 - val_loss: 4.7557e-05 - val_my_r2: 0.9824\n",
      "Epoch 584/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7977e-04 - my_r2: 0.9199 - val_loss: 4.5341e-05 - val_my_r2: 0.9837\n",
      "Epoch 585/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9292e-04 - my_r2: 0.9336 - val_loss: 4.3598e-05 - val_my_r2: 0.9847\n",
      "Epoch 586/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4085e-04 - my_r2: 0.9498 - val_loss: 4.2373e-05 - val_my_r2: 0.9848\n",
      "Epoch 587/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0867e-04 - my_r2: 0.9087 - val_loss: 4.5783e-05 - val_my_r2: 0.9837\n",
      "Epoch 588/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3132e-04 - my_r2: 0.9065 - val_loss: 5.4315e-05 - val_my_r2: 0.9817\n",
      "Epoch 589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6732e-04 - my_r2: 0.9073 - val_loss: 5.8546e-05 - val_my_r2: 0.9814\n",
      "Epoch 590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5506e-04 - my_r2: 0.8862 - val_loss: 4.6377e-05 - val_my_r2: 0.9848\n",
      "Epoch 591/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7231e-04 - my_r2: 0.9250 - val_loss: 4.8975e-05 - val_my_r2: 0.9838\n",
      "Epoch 592/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1993e-04 - my_r2: 0.9046 - val_loss: 5.5563e-05 - val_my_r2: 0.9823\n",
      "Epoch 593/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8931e-04 - my_r2: 0.9438 - val_loss: 5.1294e-05 - val_my_r2: 0.9829\n",
      "Epoch 594/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8388e-04 - my_r2: 0.8164 - val_loss: 4.5584e-05 - val_my_r2: 0.9835\n",
      "Epoch 595/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4335e-04 - my_r2: 0.9156 - val_loss: 4.7528e-05 - val_my_r2: 0.9828\n",
      "Epoch 596/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5910e-04 - my_r2: 0.9218 - val_loss: 4.3067e-05 - val_my_r2: 0.9842\n",
      "Epoch 597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7113e-04 - my_r2: 0.8970 - val_loss: 4.2556e-05 - val_my_r2: 0.9839\n",
      "Epoch 598/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9479e-04 - my_r2: 0.8951 - val_loss: 4.3259e-05 - val_my_r2: 0.9831\n",
      "Epoch 599/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9898e-04 - my_r2: 0.9240 - val_loss: 4.6892e-05 - val_my_r2: 0.9811\n",
      "Epoch 600/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4959e-04 - my_r2: 0.9312 - val_loss: 4.3165e-05 - val_my_r2: 0.9842\n",
      "Epoch 601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2847e-04 - my_r2: 0.8945 - val_loss: 5.9961e-05 - val_my_r2: 0.9808\n",
      "Epoch 602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4571e-04 - my_r2: 0.8939 - val_loss: 4.5304e-05 - val_my_r2: 0.9847\n",
      "Epoch 603/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7379e-04 - my_r2: 0.9295 - val_loss: 4.1818e-05 - val_my_r2: 0.9858\n",
      "Epoch 604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8806e-04 - my_r2: 0.9138 - val_loss: 4.0374e-05 - val_my_r2: 0.9861\n",
      "Epoch 605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7513e-04 - my_r2: 0.9193 - val_loss: 4.1029e-05 - val_my_r2: 0.9855\n",
      "Epoch 606/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2820e-04 - my_r2: 0.8805 - val_loss: 4.0020e-05 - val_my_r2: 0.9860\n",
      "Epoch 607/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2222e-04 - my_r2: 0.8733 - val_loss: 4.1584e-05 - val_my_r2: 0.9846\n",
      "Epoch 608/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0394e-04 - my_r2: 0.9108 - val_loss: 3.9052e-05 - val_my_r2: 0.9857\n",
      "Epoch 609/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0196e-04 - my_r2: 0.8588 - val_loss: 3.8824e-05 - val_my_r2: 0.9860\n",
      "Epoch 610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4473e-04 - my_r2: 0.9322 - val_loss: 3.9621e-05 - val_my_r2: 0.9862\n",
      "Epoch 611/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3386e-04 - my_r2: 0.9547 - val_loss: 4.0022e-05 - val_my_r2: 0.9862\n",
      "Epoch 612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4018e-04 - my_r2: 0.9072 - val_loss: 3.7852e-05 - val_my_r2: 0.9866\n",
      "Epoch 613/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0377e-04 - my_r2: 0.8754 - val_loss: 3.9040e-05 - val_my_r2: 0.9851\n",
      "Epoch 614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4576e-04 - my_r2: 0.9324 - val_loss: 4.1758e-05 - val_my_r2: 0.9836\n",
      "Epoch 615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6538e-04 - my_r2: 0.8605 - val_loss: 4.4255e-05 - val_my_r2: 0.9830\n",
      "Epoch 616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2067e-04 - my_r2: 0.9239 - val_loss: 4.3942e-05 - val_my_r2: 0.9832\n",
      "Epoch 617/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9967e-04 - my_r2: 0.9104 - val_loss: 4.6860e-05 - val_my_r2: 0.9825\n",
      "Epoch 618/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8273e-04 - my_r2: 0.9322 - val_loss: 4.3135e-05 - val_my_r2: 0.9847\n",
      "Epoch 619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2511e-04 - my_r2: 0.9035 - val_loss: 3.8662e-05 - val_my_r2: 0.9862\n",
      "Epoch 620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7877e-04 - my_r2: 0.9170 - val_loss: 3.7656e-05 - val_my_r2: 0.9859\n",
      "Epoch 621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7641e-04 - my_r2: 0.9306 - val_loss: 3.7258e-05 - val_my_r2: 0.9862\n",
      "Epoch 622/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5393e-04 - my_r2: 0.8811 - val_loss: 3.7384e-05 - val_my_r2: 0.9860\n",
      "Epoch 623/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2605e-04 - my_r2: 0.9019 - val_loss: 3.7468e-05 - val_my_r2: 0.9857\n",
      "Epoch 624/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2854e-04 - my_r2: 0.8567 - val_loss: 3.9948e-05 - val_my_r2: 0.9852\n",
      "Epoch 625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0765e-04 - my_r2: 0.9501 - val_loss: 4.0797e-05 - val_my_r2: 0.9851\n",
      "Epoch 626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8309e-04 - my_r2: 0.9040 - val_loss: 3.7274e-05 - val_my_r2: 0.9860\n",
      "Epoch 627/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3334e-04 - my_r2: 0.9533 - val_loss: 4.0045e-05 - val_my_r2: 0.9845\n",
      "Epoch 628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7446e-04 - my_r2: 0.6730 - val_loss: 3.5243e-05 - val_my_r2: 0.9865\n",
      "Epoch 629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3186e-04 - my_r2: 0.9163 - val_loss: 3.4713e-05 - val_my_r2: 0.9867\n",
      "Epoch 630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4849e-04 - my_r2: 0.9411 - val_loss: 3.5675e-05 - val_my_r2: 0.9861\n",
      "Epoch 631/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4619e-04 - my_r2: 0.9215 - val_loss: 4.0239e-05 - val_my_r2: 0.9854\n",
      "Epoch 632/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2398e-04 - my_r2: 0.9369 - val_loss: 3.9097e-05 - val_my_r2: 0.9850\n",
      "Epoch 633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9772e-04 - my_r2: 0.9294 - val_loss: 3.7695e-05 - val_my_r2: 0.9852\n",
      "Epoch 634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8449e-04 - my_r2: 0.9067 - val_loss: 3.7110e-05 - val_my_r2: 0.9862\n",
      "Epoch 635/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2439e-04 - my_r2: 0.9152 - val_loss: 4.0769e-05 - val_my_r2: 0.9850\n",
      "Epoch 636/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8140e-04 - my_r2: 0.8892 - val_loss: 5.3065e-05 - val_my_r2: 0.9817\n",
      "Epoch 637/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8443e-04 - my_r2: 0.9328 - val_loss: 4.2553e-05 - val_my_r2: 0.9850\n",
      "Epoch 638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4518e-04 - my_r2: 0.8947 - val_loss: 3.6925e-05 - val_my_r2: 0.9869\n",
      "Epoch 639/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3959e-04 - my_r2: 0.9139 - val_loss: 3.3726e-05 - val_my_r2: 0.9877\n",
      "Epoch 640/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4517e-04 - my_r2: 0.8848 - val_loss: 3.6277e-05 - val_my_r2: 0.9863\n",
      "Epoch 641/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8350e-04 - my_r2: 0.8999 - val_loss: 3.4060e-05 - val_my_r2: 0.9876\n",
      "Epoch 642/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6782e-04 - my_r2: 0.9325 - val_loss: 3.2022e-05 - val_my_r2: 0.9885\n",
      "Epoch 643/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6088e-04 - my_r2: 0.9107 - val_loss: 3.8975e-05 - val_my_r2: 0.9870\n",
      "Epoch 644/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9472e-04 - my_r2: 0.9201 - val_loss: 3.4218e-05 - val_my_r2: 0.9880\n",
      "Epoch 645/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5126e-04 - my_r2: 0.9385 - val_loss: 3.5595e-05 - val_my_r2: 0.9870\n",
      "Epoch 646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4625e-04 - my_r2: 0.9009 - val_loss: 3.7480e-05 - val_my_r2: 0.9867\n",
      "Epoch 647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0003e-04 - my_r2: 0.9242 - val_loss: 4.5948e-05 - val_my_r2: 0.9850\n",
      "Epoch 648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5691e-04 - my_r2: 0.9263 - val_loss: 4.4646e-05 - val_my_r2: 0.9856\n",
      "Epoch 649/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.5656e-04 - my_r2: 0.9114 - val_loss: 4.5331e-05 - val_my_r2: 0.9842\n",
      "Epoch 650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0638e-04 - my_r2: 0.9119 - val_loss: 4.6648e-05 - val_my_r2: 0.9834\n",
      "Epoch 651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8319e-04 - my_r2: 0.9238 - val_loss: 3.8367e-05 - val_my_r2: 0.9864\n",
      "Epoch 652/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6475e-04 - my_r2: 0.9208 - val_loss: 3.8630e-05 - val_my_r2: 0.9866\n",
      "Epoch 653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7670e-04 - my_r2: 0.9335 - val_loss: 4.2166e-05 - val_my_r2: 0.9854\n",
      "Epoch 654/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9476e-04 - my_r2: 0.8999 - val_loss: 3.5690e-05 - val_my_r2: 0.9871\n",
      "Epoch 655/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2271e-04 - my_r2: 0.9356 - val_loss: 3.1650e-05 - val_my_r2: 0.9883\n",
      "Epoch 656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0509e-04 - my_r2: 0.9259 - val_loss: 3.3831e-05 - val_my_r2: 0.9875\n",
      "Epoch 657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3228e-04 - my_r2: 0.8519 - val_loss: 3.1933e-05 - val_my_r2: 0.9879\n",
      "Epoch 658/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2347e-04 - my_r2: 0.9367 - val_loss: 3.6621e-05 - val_my_r2: 0.9865\n",
      "Epoch 659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4223e-04 - my_r2: 0.9487 - val_loss: 3.8630e-05 - val_my_r2: 0.9860\n",
      "Epoch 660/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3110e-04 - my_r2: 0.8966 - val_loss: 3.7026e-05 - val_my_r2: 0.9872\n",
      "Epoch 661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4774e-04 - my_r2: 0.9025 - val_loss: 3.3584e-05 - val_my_r2: 0.9884\n",
      "Epoch 662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5206e-04 - my_r2: 0.9229 - val_loss: 3.8833e-05 - val_my_r2: 0.9869\n",
      "Epoch 663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2765e-04 - my_r2: 0.8945 - val_loss: 4.1854e-05 - val_my_r2: 0.9864\n",
      "Epoch 664/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0381e-04 - my_r2: 0.9270 - val_loss: 3.4009e-05 - val_my_r2: 0.9884\n",
      "Epoch 665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9364e-04 - my_r2: 0.9235 - val_loss: 3.1214e-05 - val_my_r2: 0.9892\n",
      "Epoch 666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7751e-04 - my_r2: 0.8624 - val_loss: 3.2022e-05 - val_my_r2: 0.9889\n",
      "Epoch 667/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5917e-04 - my_r2: 0.9292 - val_loss: 3.1102e-05 - val_my_r2: 0.9886\n",
      "Epoch 668/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3953e-04 - my_r2: 0.9157 - val_loss: 3.0675e-05 - val_my_r2: 0.9883\n",
      "Epoch 669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9101e-04 - my_r2: 0.8932 - val_loss: 3.5665e-05 - val_my_r2: 0.9864\n",
      "Epoch 670/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2902e-04 - my_r2: 0.9214 - val_loss: 3.0655e-05 - val_my_r2: 0.9883\n",
      "Epoch 671/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8611e-04 - my_r2: 0.8151 - val_loss: 2.9684e-05 - val_my_r2: 0.9889\n",
      "Epoch 672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7313e-04 - my_r2: 0.9330 - val_loss: 3.3667e-05 - val_my_r2: 0.9881\n",
      "Epoch 673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0942e-04 - my_r2: 0.9014 - val_loss: 3.3935e-05 - val_my_r2: 0.9878\n",
      "Epoch 674/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2399e-04 - my_r2: 0.9169 - val_loss: 3.6955e-05 - val_my_r2: 0.9866\n",
      "Epoch 675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6720e-04 - my_r2: 0.8938 - val_loss: 3.3608e-05 - val_my_r2: 0.9876\n",
      "Epoch 676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9549e-04 - my_r2: 0.9331 - val_loss: 3.2597e-05 - val_my_r2: 0.9880\n",
      "Epoch 677/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6842e-04 - my_r2: 0.9402 - val_loss: 3.0414e-05 - val_my_r2: 0.9891\n",
      "Epoch 678/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0938e-04 - my_r2: 0.9126 - val_loss: 2.8938e-05 - val_my_r2: 0.9893\n",
      "Epoch 679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5616e-04 - my_r2: 0.8876 - val_loss: 3.1594e-05 - val_my_r2: 0.9882\n",
      "Epoch 680/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8087e-04 - my_r2: 0.9152 - val_loss: 3.3961e-05 - val_my_r2: 0.9870\n",
      "Epoch 681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3099e-04 - my_r2: 0.9444 - val_loss: 3.3586e-05 - val_my_r2: 0.9876\n",
      "Epoch 682/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9888e-04 - my_r2: 0.9368 - val_loss: 3.1866e-05 - val_my_r2: 0.9885\n",
      "Epoch 683/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9125e-04 - my_r2: 0.9339 - val_loss: 3.3118e-05 - val_my_r2: 0.9878\n",
      "Epoch 684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5777e-04 - my_r2: 0.8870 - val_loss: 3.5909e-05 - val_my_r2: 0.9864\n",
      "Epoch 685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7068e-04 - my_r2: 0.9275 - val_loss: 2.9458e-05 - val_my_r2: 0.9892\n",
      "Epoch 686/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9134e-04 - my_r2: 0.8535 - val_loss: 2.8069e-05 - val_my_r2: 0.9898\n",
      "Epoch 687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6934e-04 - my_r2: 0.8708 - val_loss: 2.9511e-05 - val_my_r2: 0.9896\n",
      "Epoch 688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8472e-04 - my_r2: 0.9372 - val_loss: 3.0575e-05 - val_my_r2: 0.9892\n",
      "Epoch 689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4799e-04 - my_r2: 0.9225 - val_loss: 3.0876e-05 - val_my_r2: 0.9889\n",
      "Epoch 690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3521e-04 - my_r2: 0.9178 - val_loss: 3.0107e-05 - val_my_r2: 0.9889\n",
      "Epoch 691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9400e-04 - my_r2: 0.9188 - val_loss: 3.0878e-05 - val_my_r2: 0.9892\n",
      "Epoch 692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3399e-04 - my_r2: 0.9490 - val_loss: 3.4198e-05 - val_my_r2: 0.9880\n",
      "Epoch 693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3186e-04 - my_r2: 0.9017 - val_loss: 3.3155e-05 - val_my_r2: 0.9882\n",
      "Epoch 694/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5711e-04 - my_r2: 0.9138 - val_loss: 2.9570e-05 - val_my_r2: 0.9889\n",
      "Epoch 695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2164e-04 - my_r2: 0.8730 - val_loss: 2.9040e-05 - val_my_r2: 0.9890\n",
      "Epoch 696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7277e-04 - my_r2: 0.8987 - val_loss: 3.3624e-05 - val_my_r2: 0.9877\n",
      "Epoch 697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8390e-04 - my_r2: 0.9077 - val_loss: 3.2659e-05 - val_my_r2: 0.9879\n",
      "Epoch 698/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8002e-04 - my_r2: 0.8964 - val_loss: 3.2556e-05 - val_my_r2: 0.9878\n",
      "Epoch 699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1580e-04 - my_r2: 0.8952 - val_loss: 3.2716e-05 - val_my_r2: 0.9880\n",
      "Epoch 700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5632e-04 - my_r2: 0.9315 - val_loss: 3.2189e-05 - val_my_r2: 0.9881\n",
      "Epoch 701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3131e-04 - my_r2: 0.9079 - val_loss: 2.8018e-05 - val_my_r2: 0.9896\n",
      "Epoch 702/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2601e-04 - my_r2: 0.9304 - val_loss: 2.9943e-05 - val_my_r2: 0.9890\n",
      "Epoch 703/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6965e-04 - my_r2: 0.9487 - val_loss: 2.9557e-05 - val_my_r2: 0.9891\n",
      "Epoch 704/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2605e-04 - my_r2: 0.9480 - val_loss: 3.1160e-05 - val_my_r2: 0.9885\n",
      "Epoch 705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7158e-04 - my_r2: 0.8835 - val_loss: 2.6570e-05 - val_my_r2: 0.9903\n",
      "Epoch 706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0855e-04 - my_r2: 0.9617 - val_loss: 2.6546e-05 - val_my_r2: 0.9903\n",
      "Epoch 707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2576e-04 - my_r2: 0.9307 - val_loss: 2.6592e-05 - val_my_r2: 0.9902\n",
      "Epoch 708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1516e-04 - my_r2: 0.9398 - val_loss: 2.8186e-05 - val_my_r2: 0.9897\n",
      "Epoch 709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7255e-04 - my_r2: 0.9282 - val_loss: 2.8887e-05 - val_my_r2: 0.9897\n",
      "Epoch 710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3850e-04 - my_r2: 0.9061 - val_loss: 2.9812e-05 - val_my_r2: 0.9893\n",
      "Epoch 711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5188e-04 - my_r2: 0.9369 - val_loss: 3.1161e-05 - val_my_r2: 0.9890\n",
      "Epoch 712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6200e-04 - my_r2: 0.9325 - val_loss: 2.7555e-05 - val_my_r2: 0.9903\n",
      "Epoch 713/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4681e-04 - my_r2: 0.9190 - val_loss: 2.9671e-05 - val_my_r2: 0.9895\n",
      "Epoch 714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5798e-04 - my_r2: 0.8440 - val_loss: 2.6568e-05 - val_my_r2: 0.9906\n",
      "Epoch 715/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7671e-04 - my_r2: 0.9229 - val_loss: 2.6244e-05 - val_my_r2: 0.9904\n",
      "Epoch 716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6683e-04 - my_r2: 0.9354 - val_loss: 2.6614e-05 - val_my_r2: 0.9900\n",
      "Epoch 717/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0755e-04 - my_r2: 0.8870 - val_loss: 2.9395e-05 - val_my_r2: 0.9898\n",
      "Epoch 718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1650e-04 - my_r2: 0.9351 - val_loss: 4.0179e-05 - val_my_r2: 0.9866\n",
      "Epoch 719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3151e-04 - my_r2: 0.8939 - val_loss: 2.9014e-05 - val_my_r2: 0.9892\n",
      "Epoch 720/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0390e-04 - my_r2: 0.9306 - val_loss: 3.3957e-05 - val_my_r2: 0.9864\n",
      "Epoch 721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8750e-04 - my_r2: 0.8791 - val_loss: 2.9479e-05 - val_my_r2: 0.9885\n",
      "Epoch 722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9294e-04 - my_r2: 0.9199 - val_loss: 2.6741e-05 - val_my_r2: 0.9902\n",
      "Epoch 723/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3297e-04 - my_r2: 0.9420 - val_loss: 2.7771e-05 - val_my_r2: 0.9895\n",
      "Epoch 724/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6463e-04 - my_r2: 0.9073 - val_loss: 2.7204e-05 - val_my_r2: 0.9902\n",
      "Epoch 725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5830e-04 - my_r2: 0.9179 - val_loss: 2.6812e-05 - val_my_r2: 0.9908\n",
      "Epoch 726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5511e-04 - my_r2: 0.9210 - val_loss: 2.6256e-05 - val_my_r2: 0.9912\n",
      "Epoch 727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2944e-04 - my_r2: 0.8724 - val_loss: 2.6289e-05 - val_my_r2: 0.9900\n",
      "Epoch 728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4539e-04 - my_r2: 0.9400 - val_loss: 2.8614e-05 - val_my_r2: 0.9889\n",
      "Epoch 729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6789e-04 - my_r2: 0.9048 - val_loss: 3.0212e-05 - val_my_r2: 0.9892\n",
      "Epoch 730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3610e-04 - my_r2: 0.9140 - val_loss: 2.7451e-05 - val_my_r2: 0.9897\n",
      "Epoch 731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5172e-04 - my_r2: 0.9073 - val_loss: 2.9428e-05 - val_my_r2: 0.9890\n",
      "Epoch 732/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9595e-04 - my_r2: 0.9438 - val_loss: 3.0035e-05 - val_my_r2: 0.9894\n",
      "Epoch 733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7435e-04 - my_r2: 0.8936 - val_loss: 2.7901e-05 - val_my_r2: 0.9907\n",
      "Epoch 734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6523e-04 - my_r2: 0.9144 - val_loss: 2.7500e-05 - val_my_r2: 0.9901\n",
      "Epoch 735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7802e-04 - my_r2: 0.8847 - val_loss: 2.6515e-05 - val_my_r2: 0.9911\n",
      "Epoch 736/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2619e-04 - my_r2: 0.8903 - val_loss: 3.3931e-05 - val_my_r2: 0.9893\n",
      "Epoch 737/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0287e-04 - my_r2: 0.9335 - val_loss: 2.7591e-05 - val_my_r2: 0.9907\n",
      "Epoch 738/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6616e-04 - my_r2: 0.8674 - val_loss: 2.9963e-05 - val_my_r2: 0.9888\n",
      "Epoch 739/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1025e-04 - my_r2: 0.9242 - val_loss: 2.6029e-05 - val_my_r2: 0.9909\n",
      "Epoch 740/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1671e-04 - my_r2: 0.8990 - val_loss: 2.6586e-05 - val_my_r2: 0.9907\n",
      "Epoch 741/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6469e-04 - my_r2: 0.9221 - val_loss: 2.3102e-05 - val_my_r2: 0.9916\n",
      "Epoch 742/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8830e-04 - my_r2: 0.9261 - val_loss: 2.3640e-05 - val_my_r2: 0.9913\n",
      "Epoch 743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1551e-04 - my_r2: 0.9094 - val_loss: 2.6149e-05 - val_my_r2: 0.9900\n",
      "Epoch 744/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6909e-04 - my_r2: 0.9365 - val_loss: 2.4395e-05 - val_my_r2: 0.9906\n",
      "Epoch 745/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8632e-04 - my_r2: 0.9236 - val_loss: 2.5470e-05 - val_my_r2: 0.9896\n",
      "Epoch 746/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5842e-04 - my_r2: 0.9290 - val_loss: 2.6728e-05 - val_my_r2: 0.9890\n",
      "Epoch 747/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6171e-04 - my_r2: 0.9101 - val_loss: 2.9513e-05 - val_my_r2: 0.9885\n",
      "Epoch 748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8251e-04 - my_r2: 0.9008 - val_loss: 3.0358e-05 - val_my_r2: 0.9887\n",
      "Epoch 749/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2029e-04 - my_r2: 0.9080 - val_loss: 4.5327e-05 - val_my_r2: 0.9838\n",
      "Epoch 750/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4325e-04 - my_r2: 0.8655 - val_loss: 2.6729e-05 - val_my_r2: 0.9905\n",
      "Epoch 751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8259e-04 - my_r2: 0.9316 - val_loss: 2.6078e-05 - val_my_r2: 0.9907\n",
      "Epoch 752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5261e-04 - my_r2: 0.9104 - val_loss: 2.6798e-05 - val_my_r2: 0.9907\n",
      "Epoch 753/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3939e-04 - my_r2: 0.9301 - val_loss: 2.7706e-05 - val_my_r2: 0.9904\n",
      "Epoch 754/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9105e-04 - my_r2: 0.8706 - val_loss: 2.6464e-05 - val_my_r2: 0.9907\n",
      "Epoch 755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8877e-04 - my_r2: 0.9251 - val_loss: 2.6804e-05 - val_my_r2: 0.9906\n",
      "Epoch 756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1681e-04 - my_r2: 0.9422 - val_loss: 2.7522e-05 - val_my_r2: 0.9904\n",
      "Epoch 757/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8450e-04 - my_r2: 0.9107 - val_loss: 2.7450e-05 - val_my_r2: 0.9898\n",
      "Epoch 758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2362e-04 - my_r2: 0.8981 - val_loss: 2.2782e-05 - val_my_r2: 0.9916\n",
      "Epoch 759/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0952e-04 - my_r2: 0.9274 - val_loss: 2.2276e-05 - val_my_r2: 0.9917\n",
      "Epoch 760/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4473e-04 - my_r2: 0.8728 - val_loss: 2.3665e-05 - val_my_r2: 0.9911\n",
      "Epoch 761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1095e-04 - my_r2: 0.9200 - val_loss: 2.6700e-05 - val_my_r2: 0.9898\n",
      "Epoch 762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3808e-04 - my_r2: 0.9223 - val_loss: 2.8626e-05 - val_my_r2: 0.9892\n",
      "Epoch 763/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2701e-04 - my_r2: 0.9016 - val_loss: 3.0159e-05 - val_my_r2: 0.9887\n",
      "Epoch 764/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8942e-04 - my_r2: 0.9262 - val_loss: 2.6119e-05 - val_my_r2: 0.9898\n",
      "Epoch 765/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4214e-04 - my_r2: 0.9449 - val_loss: 2.5981e-05 - val_my_r2: 0.9898\n",
      "Epoch 766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2937e-04 - my_r2: 0.8924 - val_loss: 2.6275e-05 - val_my_r2: 0.9894\n",
      "Epoch 767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6870e-04 - my_r2: 0.9032 - val_loss: 2.3565e-05 - val_my_r2: 0.9905\n",
      "Epoch 768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0652e-04 - my_r2: 0.8848 - val_loss: 2.4010e-05 - val_my_r2: 0.9904\n",
      "Epoch 769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8571e-04 - my_r2: 0.9325 - val_loss: 2.2419e-05 - val_my_r2: 0.9910\n",
      "Epoch 770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5838e-04 - my_r2: 0.8916 - val_loss: 2.4041e-05 - val_my_r2: 0.9908\n",
      "Epoch 771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8731e-04 - my_r2: 0.8960 - val_loss: 2.8025e-05 - val_my_r2: 0.9891\n",
      "Epoch 772/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9124e-04 - my_r2: 0.2641 - val_loss: 2.1780e-05 - val_my_r2: 0.9913\n",
      "Epoch 773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3717e-04 - my_r2: 0.9264 - val_loss: 2.4089e-05 - val_my_r2: 0.9906\n",
      "Epoch 774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4116e-04 - my_r2: 0.9309 - val_loss: 2.6137e-05 - val_my_r2: 0.9905\n",
      "Epoch 775/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2176e-04 - my_r2: 0.9465 - val_loss: 2.5577e-05 - val_my_r2: 0.9905\n",
      "Epoch 776/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1096e-04 - my_r2: 0.9198 - val_loss: 2.4012e-05 - val_my_r2: 0.9909\n",
      "Epoch 777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1948e-04 - my_r2: 0.9277 - val_loss: 2.6070e-05 - val_my_r2: 0.9906\n",
      "Epoch 778/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7010e-04 - my_r2: 0.9485 - val_loss: 2.1945e-05 - val_my_r2: 0.9917\n",
      "Epoch 779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1011e-04 - my_r2: 0.9211 - val_loss: 2.2994e-05 - val_my_r2: 0.9917\n",
      "Epoch 780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6506e-04 - my_r2: 0.9369 - val_loss: 2.3971e-05 - val_my_r2: 0.9908\n",
      "Epoch 781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8744e-04 - my_r2: 0.9282 - val_loss: 2.0569e-05 - val_my_r2: 0.9921\n",
      "Epoch 782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6337e-04 - my_r2: 0.8000 - val_loss: 2.2327e-05 - val_my_r2: 0.9916\n",
      "Epoch 783/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8770e-04 - my_r2: 0.9046 - val_loss: 2.5038e-05 - val_my_r2: 0.9912\n",
      "Epoch 784/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3405e-04 - my_r2: 0.9231 - val_loss: 2.5697e-05 - val_my_r2: 0.9913\n",
      "Epoch 785/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6234e-04 - my_r2: 0.9118 - val_loss: 2.5674e-05 - val_my_r2: 0.9916\n",
      "Epoch 786/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0258e-04 - my_r2: 0.9263 - val_loss: 2.3004e-05 - val_my_r2: 0.9922\n",
      "Epoch 787/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7945e-04 - my_r2: 0.9138 - val_loss: 2.4439e-05 - val_my_r2: 0.9919\n",
      "Epoch 788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8126e-04 - my_r2: 0.9246 - val_loss: 2.1177e-05 - val_my_r2: 0.9921\n",
      "Epoch 789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5115e-04 - my_r2: 0.9303 - val_loss: 2.5641e-05 - val_my_r2: 0.9903\n",
      "Epoch 790/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2900e-04 - my_r2: 0.9294 - val_loss: 2.4325e-05 - val_my_r2: 0.9905\n",
      "Epoch 791/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3440e-04 - my_r2: 0.9420 - val_loss: 2.3288e-05 - val_my_r2: 0.9914\n",
      "Epoch 792/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2244e-04 - my_r2: 0.8331 - val_loss: 2.1715e-05 - val_my_r2: 0.9920\n",
      "Epoch 793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7552e-04 - my_r2: 0.9417 - val_loss: 2.3544e-05 - val_my_r2: 0.9914\n",
      "Epoch 794/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7236e-04 - my_r2: 0.9032 - val_loss: 2.3018e-05 - val_my_r2: 0.9912\n",
      "Epoch 795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4678e-04 - my_r2: 0.9187 - val_loss: 2.6775e-05 - val_my_r2: 0.9903\n",
      "Epoch 796/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1355e-04 - my_r2: 0.9162 - val_loss: 2.2071e-05 - val_my_r2: 0.9922\n",
      "Epoch 797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7747e-04 - my_r2: 0.9315 - val_loss: 2.0664e-05 - val_my_r2: 0.9927\n",
      "Epoch 798/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7547e-04 - my_r2: 0.9015 - val_loss: 2.3911e-05 - val_my_r2: 0.9912\n",
      "Epoch 799/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3918e-04 - my_r2: 0.9062 - val_loss: 2.3373e-05 - val_my_r2: 0.9915\n",
      "Epoch 800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8814e-04 - my_r2: 0.9060 - val_loss: 2.3011e-05 - val_my_r2: 0.9919\n",
      "Epoch 801/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9810e-04 - my_r2: 0.9061 - val_loss: 2.9424e-05 - val_my_r2: 0.9902\n",
      "Epoch 802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2312e-04 - my_r2: 0.9129 - val_loss: 2.7458e-05 - val_my_r2: 0.9905\n",
      "Epoch 803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2733e-04 - my_r2: 0.7769 - val_loss: 2.4248e-05 - val_my_r2: 0.9912\n",
      "Epoch 804/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2008e-04 - my_r2: 0.8919 - val_loss: 2.4538e-05 - val_my_r2: 0.9914\n",
      "Epoch 805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9215e-04 - my_r2: 0.8887 - val_loss: 2.7244e-05 - val_my_r2: 0.9904\n",
      "Epoch 806/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6904e-04 - my_r2: 0.9193 - val_loss: 3.5159e-05 - val_my_r2: 0.9874\n",
      "Epoch 807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8874e-04 - my_r2: 0.9238 - val_loss: 2.5521e-05 - val_my_r2: 0.9912\n",
      "Epoch 808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2251e-04 - my_r2: 0.9485 - val_loss: 2.4776e-05 - val_my_r2: 0.9913\n",
      "Epoch 809/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4697e-04 - my_r2: 0.9366 - val_loss: 2.5303e-05 - val_my_r2: 0.9909\n",
      "Epoch 810/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5326e-04 - my_r2: 0.9124 - val_loss: 2.3365e-05 - val_my_r2: 0.9912\n",
      "Epoch 811/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5059e-04 - my_r2: 0.9209 - val_loss: 2.5097e-05 - val_my_r2: 0.9909\n",
      "Epoch 812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2121e-04 - my_r2: 0.9262 - val_loss: 2.8180e-05 - val_my_r2: 0.9902\n",
      "Epoch 813/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4903e-04 - my_r2: 0.9064 - val_loss: 2.2578e-05 - val_my_r2: 0.9922\n",
      "Epoch 814/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2345e-04 - my_r2: 0.9471 - val_loss: 2.0548e-05 - val_my_r2: 0.9926\n",
      "Epoch 815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6285e-04 - my_r2: 0.9450 - val_loss: 2.1190e-05 - val_my_r2: 0.9922\n",
      "Epoch 816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4040e-04 - my_r2: 0.9486 - val_loss: 2.9123e-05 - val_my_r2: 0.9896\n",
      "Epoch 817/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9226e-04 - my_r2: 0.8656 - val_loss: 2.5392e-05 - val_my_r2: 0.9912\n",
      "Epoch 818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7198e-04 - my_r2: 0.9060 - val_loss: 2.1598e-05 - val_my_r2: 0.9922\n",
      "Epoch 819/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4929e-04 - my_r2: 0.9406 - val_loss: 2.1505e-05 - val_my_r2: 0.9919\n",
      "Epoch 820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1232e-04 - my_r2: 0.9131 - val_loss: 2.0929e-05 - val_my_r2: 0.9925\n",
      "Epoch 821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0738e-04 - my_r2: 0.8989 - val_loss: 2.2961e-05 - val_my_r2: 0.9921\n",
      "Epoch 822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9882e-04 - my_r2: 0.9347 - val_loss: 2.8180e-05 - val_my_r2: 0.9903\n",
      "Epoch 823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9046e-04 - my_r2: 0.9275 - val_loss: 2.8578e-05 - val_my_r2: 0.9907\n",
      "Epoch 824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0330e-04 - my_r2: 0.9128 - val_loss: 2.3547e-05 - val_my_r2: 0.9925\n",
      "Epoch 825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5623e-04 - my_r2: 0.9354 - val_loss: 2.3320e-05 - val_my_r2: 0.9920\n",
      "Epoch 826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3079e-04 - my_r2: 0.9498 - val_loss: 2.2401e-05 - val_my_r2: 0.9922\n",
      "Epoch 827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3880e-04 - my_r2: 0.9440 - val_loss: 2.8562e-05 - val_my_r2: 0.9903\n",
      "Epoch 828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5134e-04 - my_r2: 0.9489 - val_loss: 2.8559e-05 - val_my_r2: 0.9904\n",
      "Epoch 829/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8702e-04 - my_r2: 0.9184 - val_loss: 2.4125e-05 - val_my_r2: 0.9919\n",
      "Epoch 830/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2309e-04 - my_r2: 0.9215 - val_loss: 2.5360e-05 - val_my_r2: 0.9915\n",
      "Epoch 831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2995e-04 - my_r2: 0.8896 - val_loss: 1.7612e-05 - val_my_r2: 0.9937\n",
      "Epoch 832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7209e-04 - my_r2: 0.9271 - val_loss: 2.0791e-05 - val_my_r2: 0.9923\n",
      "Epoch 833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3972e-04 - my_r2: 0.9035 - val_loss: 2.1601e-05 - val_my_r2: 0.9922\n",
      "Epoch 834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5885e-04 - my_r2: 0.8599 - val_loss: 2.3096e-05 - val_my_r2: 0.9923\n",
      "Epoch 835/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1438e-04 - my_r2: 0.9295 - val_loss: 2.4730e-05 - val_my_r2: 0.9915\n",
      "Epoch 836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6378e-04 - my_r2: 0.9256 - val_loss: 1.8726e-05 - val_my_r2: 0.9933\n",
      "Epoch 837/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7248e-04 - my_r2: 0.9315 - val_loss: 2.2090e-05 - val_my_r2: 0.9923\n",
      "Epoch 838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7849e-04 - my_r2: 0.9192 - val_loss: 2.2753e-05 - val_my_r2: 0.9922\n",
      "Epoch 839/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8424e-04 - my_r2: 0.9253 - val_loss: 1.9610e-05 - val_my_r2: 0.9932\n",
      "Epoch 840/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5339e-04 - my_r2: 0.9213 - val_loss: 2.2042e-05 - val_my_r2: 0.9920\n",
      "Epoch 841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5567e-04 - my_r2: 0.9429 - val_loss: 2.2056e-05 - val_my_r2: 0.9919\n",
      "Epoch 842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3704e-04 - my_r2: 0.9329 - val_loss: 2.1966e-05 - val_my_r2: 0.9918\n",
      "Epoch 843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5419e-04 - my_r2: 0.9265 - val_loss: 2.2493e-05 - val_my_r2: 0.9916\n",
      "Epoch 844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9697e-04 - my_r2: 0.9362 - val_loss: 2.4453e-05 - val_my_r2: 0.9910\n",
      "Epoch 845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4809e-04 - my_r2: 0.9491 - val_loss: 2.3595e-05 - val_my_r2: 0.9909\n",
      "Epoch 846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2802e-04 - my_r2: 0.9526 - val_loss: 2.4620e-05 - val_my_r2: 0.9904\n",
      "Epoch 847/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7868e-04 - my_r2: 0.9297 - val_loss: 2.0593e-05 - val_my_r2: 0.9926\n",
      "Epoch 848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5678e-04 - my_r2: 0.9055 - val_loss: 2.0926e-05 - val_my_r2: 0.9928\n",
      "Epoch 849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9664e-04 - my_r2: 0.9294 - val_loss: 2.0539e-05 - val_my_r2: 0.9925\n",
      "Epoch 850/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7917e-04 - my_r2: 0.9208 - val_loss: 2.1919e-05 - val_my_r2: 0.9921\n",
      "Epoch 851/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8065e-04 - my_r2: 0.9250 - val_loss: 2.1855e-05 - val_my_r2: 0.9921\n",
      "Epoch 852/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4603e-04 - my_r2: 0.9058 - val_loss: 2.4505e-05 - val_my_r2: 0.9908\n",
      "Epoch 853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0383e-04 - my_r2: 0.9309 - val_loss: 2.0852e-05 - val_my_r2: 0.9925\n",
      "Epoch 854/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2419e-04 - my_r2: 0.9352 - val_loss: 2.5760e-05 - val_my_r2: 0.9913\n",
      "Epoch 855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4030e-04 - my_r2: 0.9238 - val_loss: 1.9151e-05 - val_my_r2: 0.9931\n",
      "Epoch 856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0311e-04 - my_r2: 0.9118 - val_loss: 2.1780e-05 - val_my_r2: 0.9924\n",
      "Epoch 857/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1654e-04 - my_r2: 0.8914 - val_loss: 1.9948e-05 - val_my_r2: 0.9928\n",
      "Epoch 858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6037e-04 - my_r2: 0.8626 - val_loss: 1.9640e-05 - val_my_r2: 0.9927\n",
      "Epoch 859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9487e-04 - my_r2: 0.9185 - val_loss: 1.8854e-05 - val_my_r2: 0.9932\n",
      "Epoch 860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9109e-04 - my_r2: 0.9315 - val_loss: 2.0758e-05 - val_my_r2: 0.9925\n",
      "Epoch 861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7737e-04 - my_r2: 0.9279 - val_loss: 1.8057e-05 - val_my_r2: 0.9934\n",
      "Epoch 862/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1970e-04 - my_r2: 0.8411 - val_loss: 1.8331e-05 - val_my_r2: 0.9932\n",
      "Epoch 863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3358e-04 - my_r2: 0.9444 - val_loss: 2.0051e-05 - val_my_r2: 0.9923\n",
      "Epoch 864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5033e-04 - my_r2: 0.9494 - val_loss: 2.0092e-05 - val_my_r2: 0.9925\n",
      "Epoch 865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5638e-04 - my_r2: 0.9231 - val_loss: 1.9710e-05 - val_my_r2: 0.9926\n",
      "Epoch 866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9578e-04 - my_r2: 0.9122 - val_loss: 2.1090e-05 - val_my_r2: 0.9917\n",
      "Epoch 867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5619e-04 - my_r2: 0.9062 - val_loss: 1.8475e-05 - val_my_r2: 0.9931\n",
      "Epoch 868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3075e-04 - my_r2: 0.9075 - val_loss: 2.0686e-05 - val_my_r2: 0.9927\n",
      "Epoch 869/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1217e-04 - my_r2: 0.9282 - val_loss: 2.1966e-05 - val_my_r2: 0.9922\n",
      "Epoch 870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6755e-04 - my_r2: 0.9339 - val_loss: 1.9425e-05 - val_my_r2: 0.9928\n",
      "Epoch 871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6477e-04 - my_r2: 0.8837 - val_loss: 1.8763e-05 - val_my_r2: 0.9927\n",
      "Epoch 872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1097e-04 - my_r2: 0.9306 - val_loss: 1.8466e-05 - val_my_r2: 0.9928\n",
      "Epoch 873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2906e-04 - my_r2: 0.9263 - val_loss: 1.8272e-05 - val_my_r2: 0.9932\n",
      "Epoch 874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4948e-04 - my_r2: 0.8995 - val_loss: 2.4009e-05 - val_my_r2: 0.9912\n",
      "Epoch 875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9495e-04 - my_r2: 0.9326 - val_loss: 2.0593e-05 - val_my_r2: 0.9924\n",
      "Epoch 876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9266e-04 - my_r2: 0.9537 - val_loss: 2.3643e-05 - val_my_r2: 0.9918\n",
      "Epoch 877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0960e-04 - my_r2: 0.9094 - val_loss: 2.2522e-05 - val_my_r2: 0.9923\n",
      "Epoch 878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4573e-04 - my_r2: 0.5279 - val_loss: 1.9795e-05 - val_my_r2: 0.9932\n",
      "Epoch 879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1341e-04 - my_r2: 0.9302 - val_loss: 1.9728e-05 - val_my_r2: 0.9929\n",
      "Epoch 880/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0394e-04 - my_r2: 0.9094 - val_loss: 2.7674e-05 - val_my_r2: 0.9894\n",
      "Epoch 881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9893e-04 - my_r2: 0.9410 - val_loss: 1.9659e-05 - val_my_r2: 0.9935\n",
      "Epoch 882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3871e-04 - my_r2: 0.9364 - val_loss: 2.2115e-05 - val_my_r2: 0.9931\n",
      "Epoch 883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4153e-04 - my_r2: 0.9426 - val_loss: 2.1685e-05 - val_my_r2: 0.9924\n",
      "Epoch 884/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9849e-04 - my_r2: 0.9130 - val_loss: 2.3385e-05 - val_my_r2: 0.9926\n",
      "Epoch 885/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9334e-04 - my_r2: 0.9350 - val_loss: 2.5315e-05 - val_my_r2: 0.9923\n",
      "Epoch 886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8290e-04 - my_r2: 0.8655 - val_loss: 1.7997e-05 - val_my_r2: 0.9942\n",
      "Epoch 887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8011e-04 - my_r2: 0.9025 - val_loss: 1.8785e-05 - val_my_r2: 0.9936\n",
      "Epoch 888/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5438e-04 - my_r2: 0.9358 - val_loss: 1.8182e-05 - val_my_r2: 0.9939\n",
      "Epoch 889/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6994e-04 - my_r2: 0.9304 - val_loss: 1.8248e-05 - val_my_r2: 0.9935\n",
      "Epoch 890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5378e-04 - my_r2: 0.9443 - val_loss: 2.2072e-05 - val_my_r2: 0.9918\n",
      "Epoch 891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9870e-04 - my_r2: 0.9241 - val_loss: 1.9505e-05 - val_my_r2: 0.9932\n",
      "Epoch 892/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5544e-04 - my_r2: 0.8432 - val_loss: 1.9284e-05 - val_my_r2: 0.9934\n",
      "Epoch 893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8310e-04 - my_r2: 0.9041 - val_loss: 1.8196e-05 - val_my_r2: 0.9934\n",
      "Epoch 894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2131e-04 - my_r2: 0.9325 - val_loss: 1.6766e-05 - val_my_r2: 0.9941\n",
      "Epoch 895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3362e-04 - my_r2: 0.9254 - val_loss: 1.6906e-05 - val_my_r2: 0.9944\n",
      "Epoch 896/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9050e-04 - my_r2: 0.8987 - val_loss: 1.8591e-05 - val_my_r2: 0.9942\n",
      "Epoch 897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2498e-04 - my_r2: 0.9478 - val_loss: 1.9046e-05 - val_my_r2: 0.9940\n",
      "Epoch 898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6288e-04 - my_r2: 0.9486 - val_loss: 1.9218e-05 - val_my_r2: 0.9938\n",
      "Epoch 899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5573e-04 - my_r2: 0.9409 - val_loss: 1.8027e-05 - val_my_r2: 0.9943\n",
      "Epoch 900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6746e-04 - my_r2: 0.9102 - val_loss: 1.8829e-05 - val_my_r2: 0.9941\n",
      "Epoch 901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7275e-04 - my_r2: 0.9342 - val_loss: 2.4294e-05 - val_my_r2: 0.9925\n",
      "Epoch 902/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0207e-04 - my_r2: 0.9070 - val_loss: 1.9059e-05 - val_my_r2: 0.9939\n",
      "Epoch 903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3781e-04 - my_r2: 0.9501 - val_loss: 1.7974e-05 - val_my_r2: 0.9940\n",
      "Epoch 904/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5565e-04 - my_r2: 0.9524 - val_loss: 1.7646e-05 - val_my_r2: 0.9941\n",
      "Epoch 905/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6120e-04 - my_r2: 0.9302 - val_loss: 1.8838e-05 - val_my_r2: 0.9934\n",
      "Epoch 906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0610e-04 - my_r2: 0.8758 - val_loss: 2.1512e-05 - val_my_r2: 0.9929\n",
      "Epoch 907/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5912e-04 - my_r2: 0.9362 - val_loss: 1.7992e-05 - val_my_r2: 0.9940\n",
      "Epoch 908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7242e-04 - my_r2: 0.9393 - val_loss: 1.8108e-05 - val_my_r2: 0.9938\n",
      "Epoch 909/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7110e-04 - my_r2: 0.9327 - val_loss: 1.7383e-05 - val_my_r2: 0.9939\n",
      "Epoch 910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1656e-04 - my_r2: 0.9437 - val_loss: 1.8972e-05 - val_my_r2: 0.9935\n",
      "Epoch 911/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4290e-04 - my_r2: 0.9328 - val_loss: 2.2534e-05 - val_my_r2: 0.9924\n",
      "Epoch 912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6016e-04 - my_r2: 0.9032 - val_loss: 2.3039e-05 - val_my_r2: 0.9921\n",
      "Epoch 913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0485e-04 - my_r2: 0.9301 - val_loss: 2.2878e-05 - val_my_r2: 0.9919\n",
      "Epoch 914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9406e-04 - my_r2: 0.9106 - val_loss: 1.9637e-05 - val_my_r2: 0.9933\n",
      "Epoch 915/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9969e-04 - my_r2: 0.8435 - val_loss: 1.8117e-05 - val_my_r2: 0.9937\n",
      "Epoch 916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2769e-04 - my_r2: 0.8904 - val_loss: 1.9953e-05 - val_my_r2: 0.9928\n",
      "Epoch 917/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3078e-04 - my_r2: 0.9271 - val_loss: 2.8834e-05 - val_my_r2: 0.9903\n",
      "Epoch 918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9880e-04 - my_r2: 0.9262 - val_loss: 2.4557e-05 - val_my_r2: 0.9918\n",
      "Epoch 919/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1298e-04 - my_r2: 0.9211 - val_loss: 1.8847e-05 - val_my_r2: 0.9933\n",
      "Epoch 920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5164e-04 - my_r2: 0.9468 - val_loss: 1.8374e-05 - val_my_r2: 0.9935\n",
      "Epoch 921/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5361e-04 - my_r2: 0.9381 - val_loss: 2.0932e-05 - val_my_r2: 0.9927\n",
      "Epoch 922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5250e-04 - my_r2: 0.9394 - val_loss: 2.0967e-05 - val_my_r2: 0.9926\n",
      "Epoch 923/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3427e-04 - my_r2: 0.9408 - val_loss: 2.0954e-05 - val_my_r2: 0.9925\n",
      "Epoch 924/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6795e-04 - my_r2: 0.9522 - val_loss: 1.8340e-05 - val_my_r2: 0.9930\n",
      "Epoch 925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5117e-04 - my_r2: 0.9395 - val_loss: 2.0621e-05 - val_my_r2: 0.9920\n",
      "Epoch 926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1889e-04 - my_r2: 0.9444 - val_loss: 1.8797e-05 - val_my_r2: 0.9928\n",
      "Epoch 927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8822e-04 - my_r2: 0.9261 - val_loss: 2.7092e-05 - val_my_r2: 0.9900\n",
      "Epoch 928/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4795e-04 - my_r2: 0.9371 - val_loss: 3.0027e-05 - val_my_r2: 0.9883\n",
      "Epoch 929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4562e-04 - my_r2: 0.8977 - val_loss: 2.1954e-05 - val_my_r2: 0.9915\n",
      "Epoch 930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9803e-04 - my_r2: 0.9318 - val_loss: 2.1539e-05 - val_my_r2: 0.9917\n",
      "Epoch 931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0890e-04 - my_r2: 0.9497 - val_loss: 2.0908e-05 - val_my_r2: 0.9916\n",
      "Epoch 932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7750e-04 - my_r2: 0.9275 - val_loss: 2.3012e-05 - val_my_r2: 0.9906\n",
      "Epoch 933/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5084e-04 - my_r2: 0.9509 - val_loss: 2.1959e-05 - val_my_r2: 0.9911\n",
      "Epoch 934/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8752e-04 - my_r2: 0.9433 - val_loss: 2.1778e-05 - val_my_r2: 0.9909\n",
      "Epoch 935/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9783e-04 - my_r2: 0.9201 - val_loss: 2.6521e-05 - val_my_r2: 0.9891\n",
      "Epoch 936/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0618e-04 - my_r2: 0.8961 - val_loss: 1.7927e-05 - val_my_r2: 0.9929\n",
      "Epoch 937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8453e-04 - my_r2: 0.9298 - val_loss: 2.0308e-05 - val_my_r2: 0.9919\n",
      "Epoch 938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8637e-04 - my_r2: 0.9135 - val_loss: 1.9894e-05 - val_my_r2: 0.9920\n",
      "Epoch 939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2295e-04 - my_r2: 0.9569 - val_loss: 1.8168e-05 - val_my_r2: 0.9933\n",
      "Epoch 940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9944e-04 - my_r2: 0.9203 - val_loss: 1.8862e-05 - val_my_r2: 0.9930\n",
      "Epoch 941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7343e-04 - my_r2: 0.9345 - val_loss: 3.1649e-05 - val_my_r2: 0.9884\n",
      "Epoch 942/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3278e-04 - my_r2: 0.9027 - val_loss: 2.6721e-05 - val_my_r2: 0.9901\n",
      "Epoch 943/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8116e-04 - my_r2: 0.8917 - val_loss: 2.2792e-05 - val_my_r2: 0.9923\n",
      "Epoch 944/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6738e-04 - my_r2: 0.9372 - val_loss: 1.9520e-05 - val_my_r2: 0.9935\n",
      "Epoch 945/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1094e-04 - my_r2: 0.9381 - val_loss: 1.8482e-05 - val_my_r2: 0.9938\n",
      "Epoch 946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2168e-04 - my_r2: 0.9180 - val_loss: 1.7596e-05 - val_my_r2: 0.9942\n",
      "Epoch 947/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0601e-04 - my_r2: 0.9343 - val_loss: 1.9144e-05 - val_my_r2: 0.9932\n",
      "Epoch 948/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8941e-04 - my_r2: 0.9442 - val_loss: 1.8028e-05 - val_my_r2: 0.9933\n",
      "Epoch 949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8160e-04 - my_r2: 0.9084 - val_loss: 1.9163e-05 - val_my_r2: 0.9932\n",
      "Epoch 950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1412e-04 - my_r2: 0.9482 - val_loss: 1.6856e-05 - val_my_r2: 0.9940\n",
      "Epoch 951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0210e-04 - my_r2: 0.9337 - val_loss: 1.6853e-05 - val_my_r2: 0.9943\n",
      "Epoch 952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7643e-04 - my_r2: 0.9568 - val_loss: 1.7443e-05 - val_my_r2: 0.9940\n",
      "Epoch 953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1167e-04 - my_r2: 0.8757 - val_loss: 1.6181e-05 - val_my_r2: 0.9945\n",
      "Epoch 954/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1457e-04 - my_r2: 0.9495 - val_loss: 1.6000e-05 - val_my_r2: 0.9943\n",
      "Epoch 955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6005e-04 - my_r2: 0.9280 - val_loss: 1.6395e-05 - val_my_r2: 0.9938\n",
      "Epoch 956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2899e-04 - my_r2: 0.8857 - val_loss: 1.6521e-05 - val_my_r2: 0.9938\n",
      "Epoch 957/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1504e-04 - my_r2: 0.9159 - val_loss: 1.5279e-05 - val_my_r2: 0.9943\n",
      "Epoch 958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3743e-04 - my_r2: 0.9366 - val_loss: 1.6388e-05 - val_my_r2: 0.9939\n",
      "Epoch 959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0413e-04 - my_r2: 0.9192 - val_loss: 1.7396e-05 - val_my_r2: 0.9932\n",
      "Epoch 960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2901e-04 - my_r2: 0.8847 - val_loss: 1.5940e-05 - val_my_r2: 0.9939\n",
      "Epoch 961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3048e-04 - my_r2: 0.9191 - val_loss: 2.1249e-05 - val_my_r2: 0.9924\n",
      "Epoch 962/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7135e-04 - my_r2: 0.9138 - val_loss: 2.2634e-05 - val_my_r2: 0.9919\n",
      "Epoch 963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8000e-04 - my_r2: 0.9334 - val_loss: 1.8030e-05 - val_my_r2: 0.9928\n",
      "Epoch 964/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4310e-04 - my_r2: 0.9186 - val_loss: 1.6167e-05 - val_my_r2: 0.9937\n",
      "Epoch 965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2376e-04 - my_r2: 0.9077 - val_loss: 1.5185e-05 - val_my_r2: 0.9946\n",
      "Epoch 966/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5799e-04 - my_r2: 0.9329 - val_loss: 1.6939e-05 - val_my_r2: 0.9939\n",
      "Epoch 967/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5246e-04 - my_r2: 0.9137 - val_loss: 1.9713e-05 - val_my_r2: 0.9938\n",
      "Epoch 968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5023e-04 - my_r2: 0.9168 - val_loss: 1.9221e-05 - val_my_r2: 0.9941\n",
      "Epoch 969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5824e-04 - my_r2: 0.9435 - val_loss: 1.7510e-05 - val_my_r2: 0.9941\n",
      "Epoch 970/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5689e-04 - my_r2: 0.9393 - val_loss: 1.8774e-05 - val_my_r2: 0.9931\n",
      "Epoch 971/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0369e-04 - my_r2: 0.9085 - val_loss: 1.8238e-05 - val_my_r2: 0.9933\n",
      "Epoch 972/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1142e-04 - my_r2: 0.8702 - val_loss: 1.6620e-05 - val_my_r2: 0.9939\n",
      "Epoch 973/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9393e-04 - my_r2: 0.9354 - val_loss: 1.9155e-05 - val_my_r2: 0.9933\n",
      "Epoch 974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4430e-04 - my_r2: 0.9261 - val_loss: 2.3126e-05 - val_my_r2: 0.9915\n",
      "Epoch 975/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5079e-04 - my_r2: 0.9099 - val_loss: 1.8491e-05 - val_my_r2: 0.9930\n",
      "Epoch 976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6547e-04 - my_r2: 0.9440 - val_loss: 1.7774e-05 - val_my_r2: 0.9933\n",
      "Epoch 977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0762e-04 - my_r2: 0.8775 - val_loss: 1.7932e-05 - val_my_r2: 0.9935\n",
      "Epoch 978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8763e-04 - my_r2: 0.9322 - val_loss: 1.6034e-05 - val_my_r2: 0.9940\n",
      "Epoch 979/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4812e-04 - my_r2: 0.9331 - val_loss: 1.4561e-05 - val_my_r2: 0.9947\n",
      "Epoch 980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7569e-04 - my_r2: 0.8802 - val_loss: 1.5660e-05 - val_my_r2: 0.9943\n",
      "Epoch 981/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0988e-04 - my_r2: 0.9546 - val_loss: 1.8615e-05 - val_my_r2: 0.9930\n",
      "Epoch 982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3065e-04 - my_r2: 0.9173 - val_loss: 2.0664e-05 - val_my_r2: 0.9928\n",
      "Epoch 983/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.0900e-04 - my_r2: 0.9107 - val_loss: 2.1491e-05 - val_my_r2: 0.9927\n",
      "Epoch 984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1699e-04 - my_r2: 0.8906 - val_loss: 1.8752e-05 - val_my_r2: 0.9935\n",
      "Epoch 985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2108e-04 - my_r2: 0.8942 - val_loss: 1.9914e-05 - val_my_r2: 0.9932\n",
      "Epoch 986/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1848e-04 - my_r2: 0.9260 - val_loss: 2.1413e-05 - val_my_r2: 0.9925\n",
      "Epoch 987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2539e-04 - my_r2: 0.9170 - val_loss: 1.9550e-05 - val_my_r2: 0.9927\n",
      "Epoch 988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1469e-04 - my_r2: 0.9558 - val_loss: 2.1123e-05 - val_my_r2: 0.9918\n",
      "Epoch 989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1882e-04 - my_r2: 0.9558 - val_loss: 2.0749e-05 - val_my_r2: 0.9921\n",
      "Epoch 990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5966e-04 - my_r2: 0.9158 - val_loss: 1.8472e-05 - val_my_r2: 0.9931\n",
      "Epoch 991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3590e-04 - my_r2: 0.9257 - val_loss: 1.9636e-05 - val_my_r2: 0.9928\n",
      "Epoch 992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2638e-04 - my_r2: 0.9243 - val_loss: 2.1232e-05 - val_my_r2: 0.9924\n",
      "Epoch 993/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6671e-04 - my_r2: 0.9424 - val_loss: 2.0539e-05 - val_my_r2: 0.9928\n",
      "Epoch 994/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7348e-04 - my_r2: 0.9274 - val_loss: 1.6261e-05 - val_my_r2: 0.9940\n",
      "Epoch 995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1921e-04 - my_r2: 0.8867 - val_loss: 1.6476e-05 - val_my_r2: 0.9936\n",
      "Epoch 996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8025e-04 - my_r2: 0.9414 - val_loss: 1.8293e-05 - val_my_r2: 0.9930\n",
      "Epoch 997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1429e-04 - my_r2: 0.8645 - val_loss: 1.8204e-05 - val_my_r2: 0.9935\n",
      "Epoch 998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4070e-04 - my_r2: 0.9307 - val_loss: 1.8339e-05 - val_my_r2: 0.9927\n",
      "Epoch 999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1484e-04 - my_r2: 0.9598 - val_loss: 1.7427e-05 - val_my_r2: 0.9926\n",
      "Epoch 1000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0203e-04 - my_r2: 0.9320 - val_loss: 1.9795e-05 - val_my_r2: 0.9916\n",
      "Epoch 1001/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8319e-04 - my_r2: 0.9259 - val_loss: 1.8297e-05 - val_my_r2: 0.9931\n",
      "Epoch 1002/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3339e-04 - my_r2: 0.9494 - val_loss: 1.8594e-05 - val_my_r2: 0.9930\n",
      "Epoch 1003/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4490e-04 - my_r2: 0.9432 - val_loss: 1.7886e-05 - val_my_r2: 0.9934\n",
      "Epoch 1004/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3463e-04 - my_r2: 0.9436 - val_loss: 1.6946e-05 - val_my_r2: 0.9935\n",
      "Epoch 1005/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0769e-04 - my_r2: 0.9441 - val_loss: 1.6265e-05 - val_my_r2: 0.9938\n",
      "Epoch 1006/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8035e-04 - my_r2: 0.8960 - val_loss: 1.7939e-05 - val_my_r2: 0.9924\n",
      "Epoch 1007/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6052e-04 - my_r2: 0.8968 - val_loss: 2.2066e-05 - val_my_r2: 0.9916\n",
      "Epoch 1008/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6800e-04 - my_r2: 0.8817 - val_loss: 1.7608e-05 - val_my_r2: 0.9930\n",
      "Epoch 1009/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3830e-04 - my_r2: 0.9564 - val_loss: 1.7338e-05 - val_my_r2: 0.9937\n",
      "Epoch 1010/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3200e-04 - my_r2: 0.9386 - val_loss: 1.9156e-05 - val_my_r2: 0.9933\n",
      "Epoch 1011/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0287e-04 - my_r2: 0.8956 - val_loss: 1.8149e-05 - val_my_r2: 0.9933\n",
      "Epoch 1012/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2306e-04 - my_r2: 0.9430 - val_loss: 2.3199e-05 - val_my_r2: 0.9912\n",
      "Epoch 1013/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6981e-04 - my_r2: 0.9362 - val_loss: 2.0225e-05 - val_my_r2: 0.9924\n",
      "Epoch 1014/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5122e-04 - my_r2: 0.9241 - val_loss: 2.0446e-05 - val_my_r2: 0.9928\n",
      "Epoch 1015/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1833e-04 - my_r2: 0.9466 - val_loss: 1.9457e-05 - val_my_r2: 0.9932\n",
      "Epoch 1016/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5683e-04 - my_r2: 0.9152 - val_loss: 1.7146e-05 - val_my_r2: 0.9940\n",
      "Epoch 1017/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9543e-04 - my_r2: 0.9119 - val_loss: 1.6938e-05 - val_my_r2: 0.9942\n",
      "Epoch 1018/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0273e-04 - my_r2: 0.9279 - val_loss: 1.5361e-05 - val_my_r2: 0.9948\n",
      "Epoch 1019/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6444e-04 - my_r2: 0.9024 - val_loss: 1.6475e-05 - val_my_r2: 0.9945\n",
      "Epoch 1020/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0495e-04 - my_r2: 0.9265 - val_loss: 1.9540e-05 - val_my_r2: 0.9936\n",
      "Epoch 1021/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4232e-04 - my_r2: 0.9348 - val_loss: 2.5582e-05 - val_my_r2: 0.9918\n",
      "Epoch 1022/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9074e-04 - my_r2: 0.9282 - val_loss: 3.6030e-05 - val_my_r2: 0.9883\n",
      "Epoch 1023/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0093e-04 - my_r2: 0.8789 - val_loss: 1.6900e-05 - val_my_r2: 0.9944\n",
      "Epoch 1024/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9949e-04 - my_r2: 0.9330 - val_loss: 1.7158e-05 - val_my_r2: 0.9940\n",
      "Epoch 1025/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5797e-04 - my_r2: 0.9042 - val_loss: 1.7521e-05 - val_my_r2: 0.9938\n",
      "Epoch 1026/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0889e-04 - my_r2: 0.9094 - val_loss: 1.6292e-05 - val_my_r2: 0.9941\n",
      "Epoch 1027/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3637e-04 - my_r2: 0.9294 - val_loss: 1.8213e-05 - val_my_r2: 0.9939\n",
      "Epoch 1028/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6286e-04 - my_r2: 0.8775 - val_loss: 1.7219e-05 - val_my_r2: 0.9943\n",
      "Epoch 1029/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1621e-04 - my_r2: 0.8914 - val_loss: 1.6959e-05 - val_my_r2: 0.9943\n",
      "Epoch 1030/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5493e-04 - my_r2: 0.9335 - val_loss: 1.6213e-05 - val_my_r2: 0.9940\n",
      "Epoch 1031/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2449e-04 - my_r2: 0.9083 - val_loss: 1.8081e-05 - val_my_r2: 0.9934\n",
      "Epoch 1032/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1704e-04 - my_r2: 0.9289 - val_loss: 2.1780e-05 - val_my_r2: 0.9927\n",
      "Epoch 1033/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1821e-04 - my_r2: 0.9167 - val_loss: 2.1175e-05 - val_my_r2: 0.9930\n",
      "Epoch 1034/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1191e-04 - my_r2: 0.9414 - val_loss: 1.4872e-05 - val_my_r2: 0.9945\n",
      "Epoch 1035/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4912e-04 - my_r2: 0.9360 - val_loss: 1.3188e-05 - val_my_r2: 0.9950\n",
      "Epoch 1036/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6145e-04 - my_r2: 0.9092 - val_loss: 1.2581e-05 - val_my_r2: 0.9954\n",
      "Epoch 1037/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4924e-04 - my_r2: 0.9281 - val_loss: 1.6307e-05 - val_my_r2: 0.9937\n",
      "Epoch 1038/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2392e-04 - my_r2: 0.9166 - val_loss: 1.5329e-05 - val_my_r2: 0.9946\n",
      "Epoch 1039/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0925e-04 - my_r2: 0.9160 - val_loss: 1.3860e-05 - val_my_r2: 0.9946\n",
      "Epoch 1040/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7187e-04 - my_r2: 0.9311 - val_loss: 1.4515e-05 - val_my_r2: 0.9943\n",
      "Epoch 1041/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4056e-04 - my_r2: 0.9176 - val_loss: 1.2908e-05 - val_my_r2: 0.9949\n",
      "Epoch 1042/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9362e-04 - my_r2: 0.9295 - val_loss: 1.1939e-05 - val_my_r2: 0.9953\n",
      "Epoch 1043/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4514e-04 - my_r2: 0.9301 - val_loss: 1.1886e-05 - val_my_r2: 0.9958\n",
      "Epoch 1044/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0595e-04 - my_r2: 0.9047 - val_loss: 1.2576e-05 - val_my_r2: 0.9955\n",
      "Epoch 1045/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3038e-04 - my_r2: 0.9509 - val_loss: 1.5989e-05 - val_my_r2: 0.9949\n",
      "Epoch 1046/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8968e-04 - my_r2: 0.9156 - val_loss: 1.5966e-05 - val_my_r2: 0.9948\n",
      "Epoch 1047/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2989e-04 - my_r2: 0.8875 - val_loss: 2.1041e-05 - val_my_r2: 0.9932\n",
      "Epoch 1048/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0410e-04 - my_r2: 0.9366 - val_loss: 1.8906e-05 - val_my_r2: 0.9938\n",
      "Epoch 1049/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7662e-04 - my_r2: 0.9422 - val_loss: 1.3794e-05 - val_my_r2: 0.9950\n",
      "Epoch 1050/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9078e-04 - my_r2: 0.8885 - val_loss: 1.5787e-05 - val_my_r2: 0.9942\n",
      "Epoch 1051/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3012e-04 - my_r2: 0.9371 - val_loss: 2.1159e-05 - val_my_r2: 0.9929\n",
      "Epoch 1052/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3974e-04 - my_r2: 0.9258 - val_loss: 1.7992e-05 - val_my_r2: 0.9939\n",
      "Epoch 1053/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6599e-04 - my_r2: 0.9377 - val_loss: 1.4357e-05 - val_my_r2: 0.9953\n",
      "Epoch 1054/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6580e-04 - my_r2: 0.9431 - val_loss: 1.6308e-05 - val_my_r2: 0.9946\n",
      "Epoch 1055/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5657e-04 - my_r2: 0.9514 - val_loss: 1.4761e-05 - val_my_r2: 0.9949\n",
      "Epoch 1056/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6774e-04 - my_r2: 0.9079 - val_loss: 1.3164e-05 - val_my_r2: 0.9954\n",
      "Epoch 1057/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5385e-04 - my_r2: 0.9083 - val_loss: 1.3176e-05 - val_my_r2: 0.9956\n",
      "Epoch 1058/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4032e-04 - my_r2: 0.9125 - val_loss: 1.2962e-05 - val_my_r2: 0.9958\n",
      "Epoch 1059/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5199e-04 - my_r2: 0.9188 - val_loss: 1.4560e-05 - val_my_r2: 0.9952\n",
      "Epoch 1060/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3692e-04 - my_r2: 0.9223 - val_loss: 1.4702e-05 - val_my_r2: 0.9952\n",
      "Epoch 1061/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3947e-04 - my_r2: 0.9486 - val_loss: 1.3935e-05 - val_my_r2: 0.9954\n",
      "Epoch 1062/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6468e-04 - my_r2: 0.9229 - val_loss: 1.4271e-05 - val_my_r2: 0.9953\n",
      "Epoch 1063/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6636e-04 - my_r2: 0.7492 - val_loss: 1.1568e-05 - val_my_r2: 0.9961\n",
      "Epoch 1064/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2671e-04 - my_r2: 0.9459 - val_loss: 1.2531e-05 - val_my_r2: 0.9958\n",
      "Epoch 1065/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3817e-04 - my_r2: 0.9303 - val_loss: 1.1329e-05 - val_my_r2: 0.9962\n",
      "Epoch 1066/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6541e-04 - my_r2: 0.9075 - val_loss: 1.4584e-05 - val_my_r2: 0.9953\n",
      "Epoch 1067/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2427e-04 - my_r2: 0.8753 - val_loss: 1.4057e-05 - val_my_r2: 0.9953\n",
      "Epoch 1068/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7911e-04 - my_r2: 0.8947 - val_loss: 1.6903e-05 - val_my_r2: 0.9945\n",
      "Epoch 1069/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1582e-04 - my_r2: 0.9303 - val_loss: 1.7237e-05 - val_my_r2: 0.9943\n",
      "Epoch 1070/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3658e-04 - my_r2: 0.9441 - val_loss: 1.6051e-05 - val_my_r2: 0.9949\n",
      "Epoch 1071/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7035e-04 - my_r2: 0.9480 - val_loss: 1.8904e-05 - val_my_r2: 0.9937\n",
      "Epoch 1072/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8499e-04 - my_r2: 0.9348 - val_loss: 1.7592e-05 - val_my_r2: 0.9933\n",
      "Epoch 1073/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3686e-04 - my_r2: 0.9506 - val_loss: 1.9552e-05 - val_my_r2: 0.9924\n",
      "Epoch 1074/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2804e-04 - my_r2: 0.9450 - val_loss: 1.2955e-05 - val_my_r2: 0.9952\n",
      "Epoch 1075/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3859e-04 - my_r2: 0.9370 - val_loss: 1.4090e-05 - val_my_r2: 0.9953\n",
      "Epoch 1076/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5448e-04 - my_r2: 0.9258 - val_loss: 1.3738e-05 - val_my_r2: 0.9953\n",
      "Epoch 1077/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1401e-04 - my_r2: 0.9541 - val_loss: 1.3509e-05 - val_my_r2: 0.9950\n",
      "Epoch 1078/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8281e-04 - my_r2: 0.9007 - val_loss: 1.4593e-05 - val_my_r2: 0.9950\n",
      "Epoch 1079/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8563e-04 - my_r2: 0.9078 - val_loss: 1.3718e-05 - val_my_r2: 0.9951\n",
      "Epoch 1080/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1561e-04 - my_r2: 0.9447 - val_loss: 1.1031e-05 - val_my_r2: 0.9962\n",
      "Epoch 1081/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0262e-04 - my_r2: 0.9397 - val_loss: 1.2828e-05 - val_my_r2: 0.9960\n",
      "Epoch 1082/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7344e-04 - my_r2: 0.9051 - val_loss: 1.3189e-05 - val_my_r2: 0.9959\n",
      "Epoch 1083/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0169e-04 - my_r2: 0.9387 - val_loss: 1.4534e-05 - val_my_r2: 0.9949\n",
      "Epoch 1084/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1416e-04 - my_r2: 0.9231 - val_loss: 1.5242e-05 - val_my_r2: 0.9947\n",
      "Epoch 1085/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5928e-04 - my_r2: 0.9357 - val_loss: 1.4488e-05 - val_my_r2: 0.9948\n",
      "Epoch 1086/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0056e-04 - my_r2: 0.9527 - val_loss: 1.3015e-05 - val_my_r2: 0.9952\n",
      "Epoch 1087/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9024e-04 - my_r2: 0.9307 - val_loss: 1.2158e-05 - val_my_r2: 0.9956\n",
      "Epoch 1088/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4167e-04 - my_r2: 0.9457 - val_loss: 1.1638e-05 - val_my_r2: 0.9959\n",
      "Epoch 1089/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3993e-04 - my_r2: 0.9315 - val_loss: 1.2555e-05 - val_my_r2: 0.9952\n",
      "Epoch 1090/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7335e-04 - my_r2: 0.9338 - val_loss: 1.4083e-05 - val_my_r2: 0.9951\n",
      "Epoch 1091/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3215e-04 - my_r2: 0.9088 - val_loss: 1.5204e-05 - val_my_r2: 0.9946\n",
      "Epoch 1092/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8760e-04 - my_r2: 0.9071 - val_loss: 1.6804e-05 - val_my_r2: 0.9940\n",
      "Epoch 1093/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6875e-04 - my_r2: 0.9312 - val_loss: 1.8263e-05 - val_my_r2: 0.9939\n",
      "Epoch 1094/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3642e-04 - my_r2: 0.9104 - val_loss: 1.4816e-05 - val_my_r2: 0.9947\n",
      "Epoch 1095/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6175e-04 - my_r2: 0.9162 - val_loss: 1.3906e-05 - val_my_r2: 0.9952\n",
      "Epoch 1096/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0856e-04 - my_r2: 0.9495 - val_loss: 1.2985e-05 - val_my_r2: 0.9954\n",
      "Epoch 1097/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7112e-04 - my_r2: 0.9283 - val_loss: 2.3951e-05 - val_my_r2: 0.9914\n",
      "Epoch 1098/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5782e-04 - my_r2: 0.9183 - val_loss: 2.0220e-05 - val_my_r2: 0.9930\n",
      "Epoch 1099/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4436e-04 - my_r2: 0.9485 - val_loss: 1.8696e-05 - val_my_r2: 0.9931\n",
      "Epoch 1100/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1299e-04 - my_r2: 0.8976 - val_loss: 1.4953e-05 - val_my_r2: 0.9946\n",
      "Epoch 1101/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7017e-04 - my_r2: 0.9285 - val_loss: 1.3952e-05 - val_my_r2: 0.9949\n",
      "Epoch 1102/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4394e-04 - my_r2: 0.9309 - val_loss: 1.4840e-05 - val_my_r2: 0.9943\n",
      "Epoch 1103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0829e-04 - my_r2: 0.9468 - val_loss: 1.3183e-05 - val_my_r2: 0.9950\n",
      "Epoch 1104/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1043e-04 - my_r2: 0.9314 - val_loss: 1.2633e-05 - val_my_r2: 0.9954\n",
      "Epoch 1105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7516e-04 - my_r2: 0.9338 - val_loss: 1.2186e-05 - val_my_r2: 0.9957\n",
      "Epoch 1106/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8807e-04 - my_r2: 0.8989 - val_loss: 1.7340e-05 - val_my_r2: 0.9943\n",
      "Epoch 1107/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1677e-04 - my_r2: 0.9560 - val_loss: 1.4181e-05 - val_my_r2: 0.9955\n",
      "Epoch 1108/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4567e-04 - my_r2: 0.9316 - val_loss: 1.2245e-05 - val_my_r2: 0.9961\n",
      "Epoch 1109/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2210e-04 - my_r2: 0.9215 - val_loss: 1.4152e-05 - val_my_r2: 0.9952\n",
      "Epoch 1110/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2649e-04 - my_r2: 0.8711 - val_loss: 1.4695e-05 - val_my_r2: 0.9951\n",
      "Epoch 1111/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1448e-04 - my_r2: 0.9465 - val_loss: 1.6404e-05 - val_my_r2: 0.9946\n",
      "Epoch 1112/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6192e-04 - my_r2: 0.9452 - val_loss: 1.3078e-05 - val_my_r2: 0.9955\n",
      "Epoch 1113/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0999e-04 - my_r2: 0.8814 - val_loss: 1.5344e-05 - val_my_r2: 0.9948\n",
      "Epoch 1114/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3640e-04 - my_r2: 0.9503 - val_loss: 1.4333e-05 - val_my_r2: 0.9951\n",
      "Epoch 1115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2902e-04 - my_r2: 0.9526 - val_loss: 1.2879e-05 - val_my_r2: 0.9955\n",
      "Epoch 1116/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1852e-04 - my_r2: 0.9505 - val_loss: 1.1308e-05 - val_my_r2: 0.9961\n",
      "Epoch 1117/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4930e-04 - my_r2: 0.9413 - val_loss: 1.3244e-05 - val_my_r2: 0.9953\n",
      "Epoch 1118/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3659e-04 - my_r2: 0.9396 - val_loss: 1.4135e-05 - val_my_r2: 0.9950\n",
      "Epoch 1119/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0613e-04 - my_r2: 0.9146 - val_loss: 1.1903e-05 - val_my_r2: 0.9957\n",
      "Epoch 1120/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6742e-04 - my_r2: 0.9030 - val_loss: 1.3294e-05 - val_my_r2: 0.9954\n",
      "Epoch 1121/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4905e-04 - my_r2: 0.9321 - val_loss: 1.4695e-05 - val_my_r2: 0.9949\n",
      "Epoch 1122/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0223e-04 - my_r2: 0.9077 - val_loss: 1.4244e-05 - val_my_r2: 0.9954\n",
      "Epoch 1123/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9583e-04 - my_r2: 0.8978 - val_loss: 1.1801e-05 - val_my_r2: 0.9960\n",
      "Epoch 1124/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8386e-04 - my_r2: 0.9306 - val_loss: 1.6242e-05 - val_my_r2: 0.9945\n",
      "Epoch 1125/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7463e-04 - my_r2: 0.9498 - val_loss: 2.1506e-05 - val_my_r2: 0.9932\n",
      "Epoch 1126/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6375e-04 - my_r2: 0.9311 - val_loss: 2.0626e-05 - val_my_r2: 0.9937\n",
      "Epoch 1127/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4130e-04 - my_r2: 0.9451 - val_loss: 1.9776e-05 - val_my_r2: 0.9938\n",
      "Epoch 1128/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4472e-04 - my_r2: 0.9432 - val_loss: 1.6883e-05 - val_my_r2: 0.9946\n",
      "Epoch 1129/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3711e-04 - my_r2: 0.9363 - val_loss: 1.4295e-05 - val_my_r2: 0.9952\n",
      "Epoch 1130/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3278e-04 - my_r2: 0.9388 - val_loss: 1.7007e-05 - val_my_r2: 0.9945\n",
      "Epoch 1131/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8307e-04 - my_r2: 0.8667 - val_loss: 1.5185e-05 - val_my_r2: 0.9947\n",
      "Epoch 1132/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8610e-04 - my_r2: 0.9367 - val_loss: 1.7128e-05 - val_my_r2: 0.9937\n",
      "Epoch 1133/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6507e-04 - my_r2: 0.8897 - val_loss: 1.4238e-05 - val_my_r2: 0.9946\n",
      "Epoch 1134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3510e-04 - my_r2: 0.8926 - val_loss: 1.5752e-05 - val_my_r2: 0.9938\n",
      "Epoch 1135/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4159e-04 - my_r2: 0.9307 - val_loss: 1.3967e-05 - val_my_r2: 0.9949\n",
      "Epoch 1136/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3692e-04 - my_r2: 0.9201 - val_loss: 1.4522e-05 - val_my_r2: 0.9949\n",
      "Epoch 1137/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3100e-04 - my_r2: 0.9378 - val_loss: 1.6727e-05 - val_my_r2: 0.9944\n",
      "Epoch 1138/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9030e-04 - my_r2: 0.9348 - val_loss: 1.6302e-05 - val_my_r2: 0.9940\n",
      "Epoch 1139/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9985e-04 - my_r2: 0.9538 - val_loss: 1.5505e-05 - val_my_r2: 0.9945\n",
      "Epoch 1140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7677e-04 - my_r2: 0.8904 - val_loss: 1.7034e-05 - val_my_r2: 0.9942\n",
      "Epoch 1141/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5219e-04 - my_r2: 0.9356 - val_loss: 1.6385e-05 - val_my_r2: 0.9947\n",
      "Epoch 1142/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6784e-04 - my_r2: 0.9339 - val_loss: 1.4714e-05 - val_my_r2: 0.9948\n",
      "Epoch 1143/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3374e-04 - my_r2: 0.9260 - val_loss: 1.5217e-05 - val_my_r2: 0.9945\n",
      "Epoch 1144/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8884e-04 - my_r2: 0.8755 - val_loss: 1.5419e-05 - val_my_r2: 0.9946\n",
      "Epoch 1145/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7335e-04 - my_r2: 0.9168 - val_loss: 2.2560e-05 - val_my_r2: 0.9918\n",
      "Epoch 1146/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6010e-04 - my_r2: 0.8843 - val_loss: 1.5263e-05 - val_my_r2: 0.9940\n",
      "Epoch 1147/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5189e-04 - my_r2: 0.9402 - val_loss: 1.5532e-05 - val_my_r2: 0.9937\n",
      "Epoch 1148/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2112e-04 - my_r2: 0.9192 - val_loss: 1.6518e-05 - val_my_r2: 0.9938\n",
      "Epoch 1149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7077e-04 - my_r2: 0.9351 - val_loss: 1.3782e-05 - val_my_r2: 0.9946\n",
      "Epoch 1150/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5694e-04 - my_r2: 0.9341 - val_loss: 1.1443e-05 - val_my_r2: 0.9957\n",
      "Epoch 1151/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0417e-04 - my_r2: 0.9373 - val_loss: 1.4048e-05 - val_my_r2: 0.9950\n",
      "Epoch 1152/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3055e-04 - my_r2: 0.9246 - val_loss: 1.4465e-05 - val_my_r2: 0.9944\n",
      "Epoch 1153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6635e-04 - my_r2: 0.9249 - val_loss: 1.2929e-05 - val_my_r2: 0.9949\n",
      "Epoch 1154/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7993e-04 - my_r2: 0.7943 - val_loss: 1.3062e-05 - val_my_r2: 0.9954\n",
      "Epoch 1155/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1817e-04 - my_r2: 0.9144 - val_loss: 1.4549e-05 - val_my_r2: 0.9949\n",
      "Epoch 1156/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9646e-04 - my_r2: 0.9193 - val_loss: 1.6654e-05 - val_my_r2: 0.9945\n",
      "Epoch 1157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6592e-04 - my_r2: 0.9291 - val_loss: 1.5316e-05 - val_my_r2: 0.9949\n",
      "Epoch 1158/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9207e-04 - my_r2: 0.9153 - val_loss: 1.6967e-05 - val_my_r2: 0.9947\n",
      "Epoch 1159/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3216e-04 - my_r2: 0.9129 - val_loss: 1.4328e-05 - val_my_r2: 0.9949\n",
      "Epoch 1160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3696e-04 - my_r2: 0.9502 - val_loss: 1.4530e-05 - val_my_r2: 0.9947\n",
      "Epoch 1161/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3865e-04 - my_r2: 0.9461 - val_loss: 1.7519e-05 - val_my_r2: 0.9935\n",
      "Epoch 1162/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5804e-04 - my_r2: 0.9425 - val_loss: 2.0256e-05 - val_my_r2: 0.9930\n",
      "Epoch 1163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3526e-04 - my_r2: 0.9547 - val_loss: 2.0472e-05 - val_my_r2: 0.9930\n",
      "Epoch 1164/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9720e-04 - my_r2: 0.9307 - val_loss: 1.4584e-05 - val_my_r2: 0.9951\n",
      "Epoch 1165/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4738e-04 - my_r2: 0.8914 - val_loss: 1.3044e-05 - val_my_r2: 0.9955\n",
      "Epoch 1166/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1903e-04 - my_r2: 0.9029 - val_loss: 1.2568e-05 - val_my_r2: 0.9957\n",
      "Epoch 1167/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5049e-04 - my_r2: 0.9372 - val_loss: 1.1712e-05 - val_my_r2: 0.9960\n",
      "Epoch 1168/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7517e-04 - my_r2: 0.8973 - val_loss: 1.2434e-05 - val_my_r2: 0.9958\n",
      "Epoch 1169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2551e-04 - my_r2: 0.9345 - val_loss: 1.5104e-05 - val_my_r2: 0.9950\n",
      "Epoch 1170/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6518e-04 - my_r2: 0.9326 - val_loss: 1.3643e-05 - val_my_r2: 0.9954\n",
      "Epoch 1171/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2905e-04 - my_r2: 0.9268 - val_loss: 1.5138e-05 - val_my_r2: 0.9948\n",
      "Epoch 1172/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0050e-04 - my_r2: 0.9422 - val_loss: 1.5659e-05 - val_my_r2: 0.9938\n",
      "Epoch 1173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2133e-04 - my_r2: 0.9212 - val_loss: 1.4928e-05 - val_my_r2: 0.9940\n",
      "Epoch 1174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5651e-04 - my_r2: 0.9164 - val_loss: 1.3186e-05 - val_my_r2: 0.9948\n",
      "Epoch 1175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3835e-04 - my_r2: 0.9342 - val_loss: 1.4862e-05 - val_my_r2: 0.9942\n",
      "Epoch 1176/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3962e-04 - my_r2: 0.9426 - val_loss: 1.2395e-05 - val_my_r2: 0.9955\n",
      "Epoch 1177/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0637e-04 - my_r2: 0.9256 - val_loss: 1.2706e-05 - val_my_r2: 0.9958\n",
      "Epoch 1178/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8625e-04 - my_r2: 0.9296 - val_loss: 1.1927e-05 - val_my_r2: 0.9955\n",
      "Epoch 1179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2159e-04 - my_r2: 0.9522 - val_loss: 1.4814e-05 - val_my_r2: 0.9941\n",
      "Epoch 1180/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1948e-04 - my_r2: 0.9507 - val_loss: 1.3119e-05 - val_my_r2: 0.9947\n",
      "Epoch 1181/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5264e-04 - my_r2: 0.9308 - val_loss: 1.2532e-05 - val_my_r2: 0.9949\n",
      "Epoch 1182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1030e-04 - my_r2: 0.8430 - val_loss: 1.3319e-05 - val_my_r2: 0.9953\n",
      "Epoch 1183/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2571e-04 - my_r2: 0.9543 - val_loss: 1.4380e-05 - val_my_r2: 0.9951\n",
      "Epoch 1184/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0832e-04 - my_r2: 0.9405 - val_loss: 1.8902e-05 - val_my_r2: 0.9932\n",
      "Epoch 1185/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3093e-04 - my_r2: 0.9385 - val_loss: 2.0838e-05 - val_my_r2: 0.9925\n",
      "Epoch 1186/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6450e-04 - my_r2: 0.9216 - val_loss: 1.3849e-05 - val_my_r2: 0.9950\n",
      "Epoch 1187/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0876e-04 - my_r2: 0.8955 - val_loss: 1.4499e-05 - val_my_r2: 0.9950\n",
      "Epoch 1188/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9447e-04 - my_r2: 0.9408 - val_loss: 1.2761e-05 - val_my_r2: 0.9954\n",
      "Epoch 1189/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8308e-04 - my_r2: 0.9243 - val_loss: 1.4249e-05 - val_my_r2: 0.9947\n",
      "Epoch 1190/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3572e-04 - my_r2: 0.9276 - val_loss: 1.2388e-05 - val_my_r2: 0.9956\n",
      "Epoch 1191/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1445e-04 - my_r2: 0.9419 - val_loss: 1.7911e-05 - val_my_r2: 0.9940\n",
      "Epoch 1192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7342e-04 - my_r2: 0.8798 - val_loss: 1.7091e-05 - val_my_r2: 0.9937\n",
      "Epoch 1193/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6494e-04 - my_r2: 0.8973 - val_loss: 1.7700e-05 - val_my_r2: 0.9936\n",
      "Epoch 1194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8916e-04 - my_r2: 0.9235 - val_loss: 1.5533e-05 - val_my_r2: 0.9941\n",
      "Epoch 1195/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7426e-04 - my_r2: 0.9262 - val_loss: 1.3758e-05 - val_my_r2: 0.9948\n",
      "Epoch 1196/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4748e-04 - my_r2: 0.8312 - val_loss: 1.4816e-05 - val_my_r2: 0.9944\n",
      "Epoch 1197/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2655e-04 - my_r2: 0.9315 - val_loss: 1.4055e-05 - val_my_r2: 0.9951\n",
      "Epoch 1198/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4910e-04 - my_r2: 0.9355 - val_loss: 1.4868e-05 - val_my_r2: 0.9950\n",
      "Epoch 1199/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3728e-04 - my_r2: 0.9204 - val_loss: 1.6519e-05 - val_my_r2: 0.9947\n",
      "Epoch 1200/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8508e-04 - my_r2: 0.9227 - val_loss: 1.6188e-05 - val_my_r2: 0.9948\n",
      "Epoch 1201/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2162e-04 - my_r2: 0.9477 - val_loss: 1.3665e-05 - val_my_r2: 0.9950\n",
      "Epoch 1202/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9214e-04 - my_r2: 0.9588 - val_loss: 1.3831e-05 - val_my_r2: 0.9949\n",
      "Epoch 1203/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2218e-04 - my_r2: 0.9421 - val_loss: 1.3131e-05 - val_my_r2: 0.9952\n",
      "Epoch 1204/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8412e-04 - my_r2: 0.9186 - val_loss: 1.2108e-05 - val_my_r2: 0.9956\n",
      "Epoch 1205/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8024e-04 - my_r2: 0.9081 - val_loss: 9.6552e-06 - val_my_r2: 0.9964\n",
      "Epoch 1206/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7151e-04 - my_r2: 0.8883 - val_loss: 1.2172e-05 - val_my_r2: 0.9951\n",
      "Epoch 1207/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8185e-04 - my_r2: 0.8950 - val_loss: 1.3092e-05 - val_my_r2: 0.9945\n",
      "Epoch 1208/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9533e-04 - my_r2: 0.9198 - val_loss: 1.1069e-05 - val_my_r2: 0.9955\n",
      "Epoch 1209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1168e-04 - my_r2: 0.9315 - val_loss: 1.1761e-05 - val_my_r2: 0.9954\n",
      "Epoch 1210/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4158e-04 - my_r2: 0.9544 - val_loss: 9.5125e-06 - val_my_r2: 0.9963\n",
      "Epoch 1211/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7903e-04 - my_r2: 0.9383 - val_loss: 1.0623e-05 - val_my_r2: 0.9956\n",
      "Epoch 1212/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3891e-04 - my_r2: 0.9459 - val_loss: 1.2015e-05 - val_my_r2: 0.9957\n",
      "Epoch 1213/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5661e-04 - my_r2: 0.9275 - val_loss: 1.1745e-05 - val_my_r2: 0.9961\n",
      "Epoch 1214/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7377e-04 - my_r2: 0.9265 - val_loss: 1.7228e-05 - val_my_r2: 0.9946\n",
      "Epoch 1215/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7920e-04 - my_r2: 0.9394 - val_loss: 1.2224e-05 - val_my_r2: 0.9952\n",
      "Epoch 1216/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8870e-04 - my_r2: 0.9121 - val_loss: 1.4683e-05 - val_my_r2: 0.9950\n",
      "Epoch 1217/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0397e-04 - my_r2: 0.9614 - val_loss: 1.1445e-05 - val_my_r2: 0.9958\n",
      "Epoch 1218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7259e-04 - my_r2: 0.9493 - val_loss: 1.2839e-05 - val_my_r2: 0.9956\n",
      "Epoch 1219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8691e-04 - my_r2: 0.9314 - val_loss: 1.4531e-05 - val_my_r2: 0.9946\n",
      "Epoch 1220/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8883e-04 - my_r2: 0.9125 - val_loss: 1.3980e-05 - val_my_r2: 0.9946\n",
      "Epoch 1221/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7573e-04 - my_r2: 0.9119 - val_loss: 1.0492e-05 - val_my_r2: 0.9963\n",
      "Epoch 1222/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4485e-04 - my_r2: 0.9451 - val_loss: 1.1316e-05 - val_my_r2: 0.9958\n",
      "Epoch 1223/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8108e-04 - my_r2: 0.9373 - val_loss: 1.1061e-05 - val_my_r2: 0.9955\n",
      "Epoch 1224/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4711e-04 - my_r2: 0.8987 - val_loss: 1.1213e-05 - val_my_r2: 0.9954\n",
      "Epoch 1225/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4036e-04 - my_r2: 0.9284 - val_loss: 1.2671e-05 - val_my_r2: 0.9944\n",
      "Epoch 1226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5611e-04 - my_r2: 0.8746 - val_loss: 1.1024e-05 - val_my_r2: 0.9956\n",
      "Epoch 1227/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1455e-04 - my_r2: 0.9396 - val_loss: 1.1270e-05 - val_my_r2: 0.9961\n",
      "Epoch 1228/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5919e-04 - my_r2: 0.9203 - val_loss: 1.3322e-05 - val_my_r2: 0.9951\n",
      "Epoch 1229/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9598e-04 - my_r2: 0.9175 - val_loss: 1.0802e-05 - val_my_r2: 0.9963\n",
      "Epoch 1230/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5802e-04 - my_r2: 0.9369 - val_loss: 1.1213e-05 - val_my_r2: 0.9958\n",
      "Epoch 1231/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6494e-04 - my_r2: 0.9415 - val_loss: 1.3051e-05 - val_my_r2: 0.9952\n",
      "Epoch 1232/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8258e-04 - my_r2: 0.9361 - val_loss: 1.4335e-05 - val_my_r2: 0.9943\n",
      "Epoch 1233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2196e-04 - my_r2: 0.9213 - val_loss: 1.3594e-05 - val_my_r2: 0.9947\n",
      "Epoch 1234/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5521e-04 - my_r2: 0.9326 - val_loss: 1.4826e-05 - val_my_r2: 0.9948\n",
      "Epoch 1235/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1055e-04 - my_r2: 0.9235 - val_loss: 1.1412e-05 - val_my_r2: 0.9959\n",
      "Epoch 1236/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0827e-04 - my_r2: 0.9219 - val_loss: 2.0644e-05 - val_my_r2: 0.9934\n",
      "Epoch 1237/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4974e-04 - my_r2: 0.9143 - val_loss: 2.3525e-05 - val_my_r2: 0.9929\n",
      "Epoch 1238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3371e-04 - my_r2: 0.9332 - val_loss: 1.1687e-05 - val_my_r2: 0.9962\n",
      "Epoch 1239/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6701e-04 - my_r2: 0.9196 - val_loss: 1.2327e-05 - val_my_r2: 0.9959\n",
      "Epoch 1240/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8734e-04 - my_r2: 0.8986 - val_loss: 1.1216e-05 - val_my_r2: 0.9959\n",
      "Epoch 1241/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5771e-04 - my_r2: 0.9278 - val_loss: 1.1962e-05 - val_my_r2: 0.9955\n",
      "Epoch 1242/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7921e-04 - my_r2: 0.8847 - val_loss: 1.3705e-05 - val_my_r2: 0.9949\n",
      "Epoch 1243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7815e-04 - my_r2: 0.9332 - val_loss: 1.8190e-05 - val_my_r2: 0.9932\n",
      "Epoch 1244/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9904e-04 - my_r2: 0.9043 - val_loss: 1.9910e-05 - val_my_r2: 0.9925\n",
      "Epoch 1245/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8949e-04 - my_r2: 0.8874 - val_loss: 1.7500e-05 - val_my_r2: 0.9934\n",
      "Epoch 1246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0538e-04 - my_r2: 0.9021 - val_loss: 1.8377e-05 - val_my_r2: 0.9927\n",
      "Epoch 1247/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2957e-04 - my_r2: 0.9124 - val_loss: 1.5839e-05 - val_my_r2: 0.9935\n",
      "Epoch 1248/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2710e-04 - my_r2: 0.9511 - val_loss: 1.9540e-05 - val_my_r2: 0.9917\n",
      "Epoch 1249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7498e-04 - my_r2: 0.9228 - val_loss: 2.0231e-05 - val_my_r2: 0.9919\n",
      "Epoch 1250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9443e-04 - my_r2: 0.9308 - val_loss: 1.8228e-05 - val_my_r2: 0.9930\n",
      "Epoch 1251/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3211e-04 - my_r2: 0.9115 - val_loss: 2.1890e-05 - val_my_r2: 0.9913\n",
      "Epoch 1252/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2723e-04 - my_r2: 0.9383 - val_loss: 1.6799e-05 - val_my_r2: 0.9936\n",
      "Epoch 1253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8760e-04 - my_r2: 0.9130 - val_loss: 1.3146e-05 - val_my_r2: 0.9948\n",
      "Epoch 1254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4625e-04 - my_r2: 0.9154 - val_loss: 1.7494e-05 - val_my_r2: 0.9933\n",
      "Epoch 1255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4953e-04 - my_r2: 0.9321 - val_loss: 2.2435e-05 - val_my_r2: 0.9910\n",
      "Epoch 1256/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2713e-04 - my_r2: 0.9488 - val_loss: 1.8951e-05 - val_my_r2: 0.9924\n",
      "Epoch 1257/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9407e-04 - my_r2: 0.9430 - val_loss: 1.4990e-05 - val_my_r2: 0.9943\n",
      "Epoch 1258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4183e-04 - my_r2: 0.9258 - val_loss: 1.5368e-05 - val_my_r2: 0.9936\n",
      "Epoch 1259/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7977e-04 - my_r2: 0.9269 - val_loss: 1.6115e-05 - val_my_r2: 0.9930\n",
      "Epoch 1260/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7259e-04 - my_r2: 0.9362 - val_loss: 1.4485e-05 - val_my_r2: 0.9938\n",
      "Epoch 1261/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9408e-04 - my_r2: 0.8795 - val_loss: 1.5654e-05 - val_my_r2: 0.9932\n",
      "Epoch 1262/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4710e-04 - my_r2: 0.9190 - val_loss: 1.9497e-05 - val_my_r2: 0.9926\n",
      "Epoch 1263/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8727e-04 - my_r2: 0.9143 - val_loss: 2.1831e-05 - val_my_r2: 0.9916\n",
      "Epoch 1264/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4769e-04 - my_r2: 0.9289 - val_loss: 1.7628e-05 - val_my_r2: 0.9931\n",
      "Epoch 1265/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0969e-04 - my_r2: 0.9296 - val_loss: 1.5979e-05 - val_my_r2: 0.9937\n",
      "Epoch 1266/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5564e-04 - my_r2: 0.9498 - val_loss: 1.2669e-05 - val_my_r2: 0.9951\n",
      "Epoch 1267/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9175e-04 - my_r2: 0.9315 - val_loss: 1.7849e-05 - val_my_r2: 0.9931\n",
      "Epoch 1268/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5720e-04 - my_r2: 0.9192 - val_loss: 1.8634e-05 - val_my_r2: 0.9929\n",
      "Epoch 1269/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9838e-04 - my_r2: 0.9456 - val_loss: 1.5636e-05 - val_my_r2: 0.9943\n",
      "Epoch 1270/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3932e-04 - my_r2: 0.9161 - val_loss: 1.5250e-05 - val_my_r2: 0.9945\n",
      "Epoch 1271/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3294e-04 - my_r2: 0.9217 - val_loss: 1.5809e-05 - val_my_r2: 0.9941\n",
      "Epoch 1272/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8741e-04 - my_r2: 0.9174 - val_loss: 1.5229e-05 - val_my_r2: 0.9944\n",
      "Epoch 1273/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4270e-04 - my_r2: 0.9351 - val_loss: 1.3811e-05 - val_my_r2: 0.9952\n",
      "Epoch 1274/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6150e-04 - my_r2: 0.9370 - val_loss: 1.3256e-05 - val_my_r2: 0.9953\n",
      "Epoch 1275/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8347e-04 - my_r2: 0.9364 - val_loss: 1.0960e-05 - val_my_r2: 0.9961\n",
      "Epoch 1276/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5423e-04 - my_r2: 0.9172 - val_loss: 1.2118e-05 - val_my_r2: 0.9956\n",
      "Epoch 1277/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1188e-04 - my_r2: 0.9421 - val_loss: 1.5421e-05 - val_my_r2: 0.9946\n",
      "Epoch 1278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5018e-04 - my_r2: 0.9102 - val_loss: 1.3326e-05 - val_my_r2: 0.9954\n",
      "Epoch 1279/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0967e-04 - my_r2: 0.9221 - val_loss: 1.1533e-05 - val_my_r2: 0.9957\n",
      "Epoch 1280/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8724e-04 - my_r2: 0.9469 - val_loss: 1.2618e-05 - val_my_r2: 0.9953\n",
      "Epoch 1281/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3532e-04 - my_r2: 0.9266 - val_loss: 1.5829e-05 - val_my_r2: 0.9945\n",
      "Epoch 1282/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8504e-04 - my_r2: 0.9256 - val_loss: 1.3714e-05 - val_my_r2: 0.9948\n",
      "Epoch 1283/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1461e-04 - my_r2: 0.8961 - val_loss: 1.9613e-05 - val_my_r2: 0.9924\n",
      "Epoch 1284/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9029e-04 - my_r2: 0.9186 - val_loss: 1.2327e-05 - val_my_r2: 0.9958\n",
      "Epoch 1285/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4966e-04 - my_r2: 0.9397 - val_loss: 1.6130e-05 - val_my_r2: 0.9949\n",
      "Epoch 1286/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2441e-04 - my_r2: 0.9481 - val_loss: 1.3800e-05 - val_my_r2: 0.9955\n",
      "Epoch 1287/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7665e-04 - my_r2: 0.9237 - val_loss: 1.0963e-05 - val_my_r2: 0.9961\n",
      "Epoch 1288/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8093e-04 - my_r2: 0.9132 - val_loss: 9.6467e-06 - val_my_r2: 0.9967\n",
      "Epoch 1289/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2028e-04 - my_r2: 0.9282 - val_loss: 1.2580e-05 - val_my_r2: 0.9952\n",
      "Epoch 1290/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4798e-04 - my_r2: 0.9040 - val_loss: 1.1340e-05 - val_my_r2: 0.9961\n",
      "Epoch 1291/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4221e-04 - my_r2: 0.9165 - val_loss: 1.1371e-05 - val_my_r2: 0.9960\n",
      "Epoch 1292/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9514e-04 - my_r2: 0.9210 - val_loss: 1.1270e-05 - val_my_r2: 0.9963\n",
      "Epoch 1293/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3627e-04 - my_r2: 0.9193 - val_loss: 1.2857e-05 - val_my_r2: 0.9957\n",
      "Epoch 1294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3951e-04 - my_r2: 0.9404 - val_loss: 1.3112e-05 - val_my_r2: 0.9954\n",
      "Epoch 1295/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7843e-04 - my_r2: 0.9160 - val_loss: 1.2176e-05 - val_my_r2: 0.9957\n",
      "Epoch 1296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5029e-04 - my_r2: 0.9254 - val_loss: 1.4423e-05 - val_my_r2: 0.9950\n",
      "Epoch 1297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5740e-04 - my_r2: 0.9308 - val_loss: 1.3824e-05 - val_my_r2: 0.9951\n",
      "Epoch 1298/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8602e-04 - my_r2: 0.9279 - val_loss: 1.2035e-05 - val_my_r2: 0.9957\n",
      "Epoch 1299/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6891e-04 - my_r2: 0.7908 - val_loss: 1.2305e-05 - val_my_r2: 0.9957\n",
      "Epoch 1300/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3008e-04 - my_r2: 0.9328 - val_loss: 1.3258e-05 - val_my_r2: 0.9953\n",
      "Epoch 1301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1025e-04 - my_r2: 0.9406 - val_loss: 1.5682e-05 - val_my_r2: 0.9944\n",
      "Epoch 1302/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3246e-04 - my_r2: 0.9380 - val_loss: 1.3368e-05 - val_my_r2: 0.9953\n",
      "Epoch 1303/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8641e-04 - my_r2: 0.9180 - val_loss: 1.1157e-05 - val_my_r2: 0.9964\n",
      "Epoch 1304/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8420e-04 - my_r2: 0.9335 - val_loss: 1.2053e-05 - val_my_r2: 0.9961\n",
      "Epoch 1305/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3715e-04 - my_r2: 0.9340 - val_loss: 1.5064e-05 - val_my_r2: 0.9954\n",
      "Epoch 1306/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6074e-04 - my_r2: 0.9287 - val_loss: 1.9399e-05 - val_my_r2: 0.9941\n",
      "Epoch 1307/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2631e-04 - my_r2: 0.9513 - val_loss: 1.5462e-05 - val_my_r2: 0.9954\n",
      "Epoch 1308/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7033e-04 - my_r2: 0.8985 - val_loss: 1.5309e-05 - val_my_r2: 0.9947\n",
      "Epoch 1309/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3686e-04 - my_r2: 0.9323 - val_loss: 1.2880e-05 - val_my_r2: 0.9956\n",
      "Epoch 1310/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8898e-04 - my_r2: 0.9262 - val_loss: 1.3624e-05 - val_my_r2: 0.9954\n",
      "Epoch 1311/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2482e-04 - my_r2: 0.9363 - val_loss: 1.7296e-05 - val_my_r2: 0.9945\n",
      "Epoch 1312/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9503e-04 - my_r2: 0.9568 - val_loss: 1.3223e-05 - val_my_r2: 0.9958\n",
      "Epoch 1313/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5198e-04 - my_r2: 0.9086 - val_loss: 1.4764e-05 - val_my_r2: 0.9955\n",
      "Epoch 1314/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5413e-04 - my_r2: 0.9341 - val_loss: 1.6491e-05 - val_my_r2: 0.9947\n",
      "Epoch 1315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5718e-04 - my_r2: 0.9154 - val_loss: 1.1542e-05 - val_my_r2: 0.9963\n",
      "Epoch 1316/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1769e-04 - my_r2: 0.9473 - val_loss: 9.9323e-06 - val_my_r2: 0.9969\n",
      "Epoch 1317/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4782e-04 - my_r2: 0.9444 - val_loss: 1.0605e-05 - val_my_r2: 0.9967\n",
      "Epoch 1318/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2276e-04 - my_r2: 0.9539 - val_loss: 1.4100e-05 - val_my_r2: 0.9955\n",
      "Epoch 1319/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6167e-04 - my_r2: 0.9438 - val_loss: 1.3164e-05 - val_my_r2: 0.9957\n",
      "Epoch 1320/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3831e-04 - my_r2: 0.9489 - val_loss: 1.4719e-05 - val_my_r2: 0.9954\n",
      "Epoch 1321/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7965e-04 - my_r2: 0.9345 - val_loss: 2.0500e-05 - val_my_r2: 0.9929\n",
      "Epoch 1322/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3436e-04 - my_r2: 0.9314 - val_loss: 3.2762e-05 - val_my_r2: 0.9886\n",
      "Epoch 1323/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8305e-04 - my_r2: 0.9342 - val_loss: 1.4569e-05 - val_my_r2: 0.9950\n",
      "Epoch 1324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9221e-04 - my_r2: 0.9365 - val_loss: 1.4467e-05 - val_my_r2: 0.9953\n",
      "Epoch 1325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5640e-04 - my_r2: 0.9087 - val_loss: 1.5719e-05 - val_my_r2: 0.9947\n",
      "Epoch 1326/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8283e-04 - my_r2: 0.8943 - val_loss: 1.6639e-05 - val_my_r2: 0.9944\n",
      "Epoch 1327/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6309e-04 - my_r2: 0.9154 - val_loss: 1.5525e-05 - val_my_r2: 0.9949\n",
      "Epoch 1328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3118e-04 - my_r2: 0.9386 - val_loss: 1.4777e-05 - val_my_r2: 0.9949\n",
      "Epoch 1329/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6718e-04 - my_r2: 0.8756 - val_loss: 1.5343e-05 - val_my_r2: 0.9947\n",
      "Epoch 1330/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2331e-04 - my_r2: 0.9226 - val_loss: 1.6517e-05 - val_my_r2: 0.9942\n",
      "Epoch 1331/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4898e-04 - my_r2: 0.9058 - val_loss: 1.6467e-05 - val_my_r2: 0.9940\n",
      "Epoch 1332/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9488e-04 - my_r2: 0.9308 - val_loss: 1.3256e-05 - val_my_r2: 0.9951\n",
      "Epoch 1333/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5192e-04 - my_r2: 0.9451 - val_loss: 1.4801e-05 - val_my_r2: 0.9948\n",
      "Epoch 1334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3308e-04 - my_r2: 0.9255 - val_loss: 1.5415e-05 - val_my_r2: 0.9941\n",
      "Epoch 1335/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6321e-04 - my_r2: 0.8928 - val_loss: 1.4575e-05 - val_my_r2: 0.9948\n",
      "Epoch 1336/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9876e-04 - my_r2: 0.9425 - val_loss: 1.3642e-05 - val_my_r2: 0.9952\n",
      "Epoch 1337/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1974e-04 - my_r2: 0.9500 - val_loss: 1.3662e-05 - val_my_r2: 0.9953\n",
      "Epoch 1338/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0539e-04 - my_r2: 0.9594 - val_loss: 1.1090e-05 - val_my_r2: 0.9963\n",
      "Epoch 1339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7518e-04 - my_r2: 0.9372 - val_loss: 1.2425e-05 - val_my_r2: 0.9958\n",
      "Epoch 1340/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3092e-04 - my_r2: 0.9381 - val_loss: 1.3709e-05 - val_my_r2: 0.9948\n",
      "Epoch 1341/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4089e-04 - my_r2: 0.9410 - val_loss: 1.3874e-05 - val_my_r2: 0.9946\n",
      "Epoch 1342/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2117e-04 - my_r2: 0.9335 - val_loss: 1.7093e-05 - val_my_r2: 0.9931\n",
      "Epoch 1343/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8740e-04 - my_r2: 0.8862 - val_loss: 1.0682e-05 - val_my_r2: 0.9960\n",
      "Epoch 1344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5583e-04 - my_r2: 0.9501 - val_loss: 1.1632e-05 - val_my_r2: 0.9959\n",
      "Epoch 1345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9116e-04 - my_r2: 0.9483 - val_loss: 1.4499e-05 - val_my_r2: 0.9950\n",
      "Epoch 1346/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8043e-04 - my_r2: 0.8860 - val_loss: 1.6782e-05 - val_my_r2: 0.9944\n",
      "Epoch 1347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4418e-04 - my_r2: 0.9120 - val_loss: 1.0996e-05 - val_my_r2: 0.9962\n",
      "Epoch 1348/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8238e-04 - my_r2: 0.9334 - val_loss: 1.2781e-05 - val_my_r2: 0.9956\n",
      "Epoch 1349/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2289e-04 - my_r2: 0.9350 - val_loss: 1.1322e-05 - val_my_r2: 0.9962\n",
      "Epoch 1350/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3601e-04 - my_r2: 0.9500 - val_loss: 1.5654e-05 - val_my_r2: 0.9950\n",
      "Epoch 1351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6845e-04 - my_r2: 0.9288 - val_loss: 1.5700e-05 - val_my_r2: 0.9950\n",
      "Epoch 1352/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8044e-04 - my_r2: 0.8953 - val_loss: 1.7682e-05 - val_my_r2: 0.9945\n",
      "Epoch 1353/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7786e-04 - my_r2: 0.9140 - val_loss: 1.5929e-05 - val_my_r2: 0.9945\n",
      "Epoch 1354/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4536e-04 - my_r2: 0.8843 - val_loss: 1.4988e-05 - val_my_r2: 0.9945\n",
      "Epoch 1355/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4681e-04 - my_r2: 0.9347 - val_loss: 1.1307e-05 - val_my_r2: 0.9953\n",
      "Epoch 1356/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6249e-04 - my_r2: 0.9133 - val_loss: 1.0505e-05 - val_my_r2: 0.9957\n",
      "Epoch 1357/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1145e-04 - my_r2: 0.9444 - val_loss: 1.4273e-05 - val_my_r2: 0.9946\n",
      "Epoch 1358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0840e-04 - my_r2: 0.9323 - val_loss: 1.1647e-05 - val_my_r2: 0.9957\n",
      "Epoch 1359/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1157e-04 - my_r2: 0.9047 - val_loss: 1.0621e-05 - val_my_r2: 0.9963\n",
      "Epoch 1360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2630e-04 - my_r2: 0.9121 - val_loss: 1.0335e-05 - val_my_r2: 0.9966\n",
      "Epoch 1361/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8427e-04 - my_r2: 0.9385 - val_loss: 1.5193e-05 - val_my_r2: 0.9944\n",
      "Epoch 1362/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0052e-04 - my_r2: 0.7710 - val_loss: 1.0155e-05 - val_my_r2: 0.9963\n",
      "Epoch 1363/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5908e-04 - my_r2: 0.8952 - val_loss: 1.2414e-05 - val_my_r2: 0.9953\n",
      "Epoch 1364/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7512e-04 - my_r2: 0.9173 - val_loss: 1.8280e-05 - val_my_r2: 0.9933\n",
      "Epoch 1365/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0845e-04 - my_r2: 0.9558 - val_loss: 1.4839e-05 - val_my_r2: 0.9948\n",
      "Epoch 1366/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7959e-04 - my_r2: 0.9352 - val_loss: 1.0334e-05 - val_my_r2: 0.9965\n",
      "Epoch 1367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6952e-04 - my_r2: 0.8988 - val_loss: 8.7806e-06 - val_my_r2: 0.9969\n",
      "Epoch 1368/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1959e-04 - my_r2: 0.9447 - val_loss: 8.8866e-06 - val_my_r2: 0.9969\n",
      "Epoch 1369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9177e-04 - my_r2: 0.8618 - val_loss: 1.1736e-05 - val_my_r2: 0.9961\n",
      "Epoch 1370/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5351e-04 - my_r2: 0.9509 - val_loss: 1.1132e-05 - val_my_r2: 0.9964\n",
      "Epoch 1371/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4491e-04 - my_r2: 0.9365 - val_loss: 8.8350e-06 - val_my_r2: 0.9970\n",
      "Epoch 1372/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6099e-04 - my_r2: 0.9440 - val_loss: 1.0995e-05 - val_my_r2: 0.9961\n",
      "Epoch 1373/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3657e-04 - my_r2: 0.9296 - val_loss: 1.0079e-05 - val_my_r2: 0.9967\n",
      "Epoch 1374/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6201e-04 - my_r2: 0.9176 - val_loss: 9.8786e-06 - val_my_r2: 0.9967\n",
      "Epoch 1375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1456e-04 - my_r2: 0.9415 - val_loss: 8.9027e-06 - val_my_r2: 0.9969\n",
      "Epoch 1376/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4176e-04 - my_r2: 0.9229 - val_loss: 7.9490e-06 - val_my_r2: 0.9971\n",
      "Epoch 1377/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7257e-04 - my_r2: 0.9320 - val_loss: 8.5576e-06 - val_my_r2: 0.9967\n",
      "Epoch 1378/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5523e-04 - my_r2: 0.8947 - val_loss: 9.6106e-06 - val_my_r2: 0.9966\n",
      "Epoch 1379/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4137e-04 - my_r2: 0.9396 - val_loss: 1.1679e-05 - val_my_r2: 0.9956\n",
      "Epoch 1380/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4855e-04 - my_r2: 0.9400 - val_loss: 1.0519e-05 - val_my_r2: 0.9959\n",
      "Epoch 1381/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9258e-04 - my_r2: 0.9205 - val_loss: 1.4008e-05 - val_my_r2: 0.9946\n",
      "Epoch 1382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9956e-04 - my_r2: 0.8453 - val_loss: 1.6625e-05 - val_my_r2: 0.9934\n",
      "Epoch 1383/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4925e-04 - my_r2: 0.8986 - val_loss: 1.5802e-05 - val_my_r2: 0.9941\n",
      "Epoch 1384/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5562e-04 - my_r2: 0.9473 - val_loss: 1.1644e-05 - val_my_r2: 0.9956\n",
      "Epoch 1385/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5821e-04 - my_r2: 0.8783 - val_loss: 1.4040e-05 - val_my_r2: 0.9949\n",
      "Epoch 1386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4242e-04 - my_r2: 0.9446 - val_loss: 9.9964e-06 - val_my_r2: 0.9963\n",
      "Epoch 1387/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4171e-04 - my_r2: 0.8702 - val_loss: 1.3164e-05 - val_my_r2: 0.9952\n",
      "Epoch 1388/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8001e-04 - my_r2: 0.9571 - val_loss: 1.6542e-05 - val_my_r2: 0.9940\n",
      "Epoch 1389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3738e-04 - my_r2: 0.9178 - val_loss: 1.4341e-05 - val_my_r2: 0.9949\n",
      "Epoch 1390/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1205e-04 - my_r2: 0.9219 - val_loss: 1.4461e-05 - val_my_r2: 0.9950\n",
      "Epoch 1391/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3963e-04 - my_r2: 0.9522 - val_loss: 1.5405e-05 - val_my_r2: 0.9950\n",
      "Epoch 1392/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8360e-04 - my_r2: 0.8973 - val_loss: 1.6209e-05 - val_my_r2: 0.9946\n",
      "Epoch 1393/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6135e-04 - my_r2: 0.9355 - val_loss: 1.4219e-05 - val_my_r2: 0.9949\n",
      "Epoch 1394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8553e-04 - my_r2: 0.9191 - val_loss: 1.3071e-05 - val_my_r2: 0.9949\n",
      "Epoch 1395/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8476e-04 - my_r2: 0.9159 - val_loss: 1.2299e-05 - val_my_r2: 0.9960\n",
      "Epoch 1396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7664e-04 - my_r2: 0.9439 - val_loss: 1.2582e-05 - val_my_r2: 0.9957\n",
      "Epoch 1397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5811e-04 - my_r2: 0.9336 - val_loss: 1.7732e-05 - val_my_r2: 0.9939\n",
      "Epoch 1398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1327e-04 - my_r2: 0.9357 - val_loss: 1.6490e-05 - val_my_r2: 0.9940\n",
      "Epoch 1399/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4001e-04 - my_r2: 0.9400 - val_loss: 1.1079e-05 - val_my_r2: 0.9957\n",
      "Epoch 1400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1050e-04 - my_r2: 0.9475 - val_loss: 1.0869e-05 - val_my_r2: 0.9960\n",
      "Epoch 1401/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7121e-04 - my_r2: 0.9235 - val_loss: 9.5772e-06 - val_my_r2: 0.9968\n",
      "Epoch 1402/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2842e-04 - my_r2: 0.9120 - val_loss: 1.1243e-05 - val_my_r2: 0.9963\n",
      "Epoch 1403/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1767e-04 - my_r2: 0.9315 - val_loss: 1.1479e-05 - val_my_r2: 0.9963\n",
      "Epoch 1404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8883e-04 - my_r2: 0.9289 - val_loss: 1.0287e-05 - val_my_r2: 0.9966\n",
      "Epoch 1405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1173e-04 - my_r2: 0.9572 - val_loss: 1.1055e-05 - val_my_r2: 0.9964\n",
      "Epoch 1406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0155e-04 - my_r2: 0.9289 - val_loss: 1.1011e-05 - val_my_r2: 0.9964\n",
      "Epoch 1407/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5013e-04 - my_r2: 0.8650 - val_loss: 1.7070e-05 - val_my_r2: 0.9945\n",
      "Epoch 1408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4451e-04 - my_r2: 0.8925 - val_loss: 1.0850e-05 - val_my_r2: 0.9961\n",
      "Epoch 1409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2150e-04 - my_r2: 0.9543 - val_loss: 1.1863e-05 - val_my_r2: 0.9952\n",
      "Epoch 1410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8343e-04 - my_r2: 0.9355 - val_loss: 1.6622e-05 - val_my_r2: 0.9936\n",
      "Epoch 1411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6519e-04 - my_r2: 0.8976 - val_loss: 1.9027e-05 - val_my_r2: 0.9934\n",
      "Epoch 1412/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3685e-04 - my_r2: 0.9473 - val_loss: 9.6622e-06 - val_my_r2: 0.9965\n",
      "Epoch 1413/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8879e-04 - my_r2: 0.8965 - val_loss: 1.0443e-05 - val_my_r2: 0.9964\n",
      "Epoch 1414/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1858e-04 - my_r2: 0.9414 - val_loss: 1.4635e-05 - val_my_r2: 0.9946\n",
      "Epoch 1415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9365e-04 - my_r2: 0.9012 - val_loss: 1.6987e-05 - val_my_r2: 0.9939\n",
      "Epoch 1416/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6456e-04 - my_r2: 0.8987 - val_loss: 1.4646e-05 - val_my_r2: 0.9948\n",
      "Epoch 1417/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6765e-04 - my_r2: 0.9240 - val_loss: 1.3550e-05 - val_my_r2: 0.9951\n",
      "Epoch 1418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8612e-04 - my_r2: 0.8912 - val_loss: 1.3168e-05 - val_my_r2: 0.9952\n",
      "Epoch 1419/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7896e-04 - my_r2: 0.8581 - val_loss: 1.1910e-05 - val_my_r2: 0.9956\n",
      "Epoch 1420/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1379e-04 - my_r2: 0.8775 - val_loss: 1.1269e-05 - val_my_r2: 0.9961\n",
      "Epoch 1421/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2719e-04 - my_r2: 0.9448 - val_loss: 9.2399e-06 - val_my_r2: 0.9968\n",
      "Epoch 1422/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1199e-04 - my_r2: 0.9300 - val_loss: 1.0688e-05 - val_my_r2: 0.9963\n",
      "Epoch 1423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0950e-04 - my_r2: 0.9113 - val_loss: 9.5170e-06 - val_my_r2: 0.9965\n",
      "Epoch 1424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4122e-04 - my_r2: 0.9309 - val_loss: 8.7496e-06 - val_my_r2: 0.9969\n",
      "Epoch 1425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1668e-04 - my_r2: 0.9135 - val_loss: 1.1317e-05 - val_my_r2: 0.9958\n",
      "Epoch 1426/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2136e-04 - my_r2: 0.9224 - val_loss: 1.1169e-05 - val_my_r2: 0.9958\n",
      "Epoch 1427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0289e-04 - my_r2: 0.9445 - val_loss: 9.0883e-06 - val_my_r2: 0.9969\n",
      "Epoch 1428/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2355e-04 - my_r2: 0.9394 - val_loss: 1.1058e-05 - val_my_r2: 0.9962\n",
      "Epoch 1429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6688e-04 - my_r2: 0.8911 - val_loss: 1.2784e-05 - val_my_r2: 0.9953\n",
      "Epoch 1430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6882e-04 - my_r2: 0.9195 - val_loss: 1.4125e-05 - val_my_r2: 0.9948\n",
      "Epoch 1431/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9343e-04 - my_r2: 0.9034 - val_loss: 1.0649e-05 - val_my_r2: 0.9963\n",
      "Epoch 1432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3161e-04 - my_r2: 0.9322 - val_loss: 1.3809e-05 - val_my_r2: 0.9954\n",
      "Epoch 1433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2197e-04 - my_r2: 0.9327 - val_loss: 1.2255e-05 - val_my_r2: 0.9958\n",
      "Epoch 1434/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2025e-04 - my_r2: 0.9254 - val_loss: 1.2499e-05 - val_my_r2: 0.9952\n",
      "Epoch 1435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6812e-04 - my_r2: 0.9180 - val_loss: 1.2775e-05 - val_my_r2: 0.9957\n",
      "Epoch 1436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3649e-04 - my_r2: 0.9251 - val_loss: 1.0957e-05 - val_my_r2: 0.9956\n",
      "Epoch 1437/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3563e-04 - my_r2: 0.8767 - val_loss: 1.1719e-05 - val_my_r2: 0.9954\n",
      "Epoch 1438/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4624e-04 - my_r2: 0.9388 - val_loss: 1.4713e-05 - val_my_r2: 0.9944\n",
      "Epoch 1439/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5747e-04 - my_r2: 0.9076 - val_loss: 1.4219e-05 - val_my_r2: 0.9945\n",
      "Epoch 1440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0186e-04 - my_r2: 0.9119 - val_loss: 1.2184e-05 - val_my_r2: 0.9955\n",
      "Epoch 1441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4206e-04 - my_r2: 0.9312 - val_loss: 1.3718e-05 - val_my_r2: 0.9951\n",
      "Epoch 1442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9073e-04 - my_r2: 0.9319 - val_loss: 1.2928e-05 - val_my_r2: 0.9956\n",
      "Epoch 1443/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4078e-04 - my_r2: 0.9229 - val_loss: 1.3285e-05 - val_my_r2: 0.9954\n",
      "Epoch 1444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1850e-04 - my_r2: 0.8599 - val_loss: 1.7390e-05 - val_my_r2: 0.9942\n",
      "Epoch 1445/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8706e-04 - my_r2: 0.9058 - val_loss: 1.6821e-05 - val_my_r2: 0.9948\n",
      "Epoch 1446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8687e-04 - my_r2: 0.9637 - val_loss: 1.1960e-05 - val_my_r2: 0.9967\n",
      "Epoch 1447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6579e-04 - my_r2: 0.9308 - val_loss: 1.3817e-05 - val_my_r2: 0.9958\n",
      "Epoch 1448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7183e-04 - my_r2: 0.9342 - val_loss: 1.0287e-05 - val_my_r2: 0.9967\n",
      "Epoch 1449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0138e-04 - my_r2: 0.9507 - val_loss: 9.0237e-06 - val_my_r2: 0.9972\n",
      "Epoch 1450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5810e-04 - my_r2: 0.9210 - val_loss: 1.0125e-05 - val_my_r2: 0.9969\n",
      "Epoch 1451/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3986e-04 - my_r2: 0.9102 - val_loss: 9.9857e-06 - val_my_r2: 0.9966\n",
      "Epoch 1452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7342e-04 - my_r2: 0.9196 - val_loss: 1.0942e-05 - val_my_r2: 0.9959\n",
      "Epoch 1453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9415e-04 - my_r2: 0.9541 - val_loss: 1.2625e-05 - val_my_r2: 0.9949\n",
      "Epoch 1454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5931e-04 - my_r2: 0.9416 - val_loss: 1.2023e-05 - val_my_r2: 0.9955\n",
      "Epoch 1455/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6783e-04 - my_r2: 0.9327 - val_loss: 1.3036e-05 - val_my_r2: 0.9958\n",
      "Epoch 1456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5039e-04 - my_r2: 0.9498 - val_loss: 1.0412e-05 - val_my_r2: 0.9962\n",
      "Epoch 1457/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0924e-04 - my_r2: 0.9124 - val_loss: 1.1633e-05 - val_my_r2: 0.9954\n",
      "Epoch 1458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9957e-04 - my_r2: 0.8841 - val_loss: 1.6471e-05 - val_my_r2: 0.9935\n",
      "Epoch 1459/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6174e-04 - my_r2: 0.9303 - val_loss: 1.5878e-05 - val_my_r2: 0.9934\n",
      "Epoch 1460/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7956e-04 - my_r2: 0.8987 - val_loss: 1.2965e-05 - val_my_r2: 0.9951\n",
      "Epoch 1461/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4572e-04 - my_r2: 0.9342 - val_loss: 1.1408e-05 - val_my_r2: 0.9961\n",
      "Epoch 1462/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2652e-04 - my_r2: 0.8914 - val_loss: 9.0873e-06 - val_my_r2: 0.9971\n",
      "Epoch 1463/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9863e-04 - my_r2: 0.9639 - val_loss: 1.2520e-05 - val_my_r2: 0.9962\n",
      "Epoch 1464/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4778e-04 - my_r2: 0.9220 - val_loss: 1.2352e-05 - val_my_r2: 0.9960\n",
      "Epoch 1465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5537e-04 - my_r2: 0.9085 - val_loss: 1.1492e-05 - val_my_r2: 0.9962\n",
      "Epoch 1466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5141e-04 - my_r2: 0.9478 - val_loss: 1.1973e-05 - val_my_r2: 0.9962\n",
      "Epoch 1467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9569e-04 - my_r2: 0.9253 - val_loss: 1.6378e-05 - val_my_r2: 0.9948\n",
      "Epoch 1468/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7463e-04 - my_r2: 0.9348 - val_loss: 2.3430e-05 - val_my_r2: 0.9926\n",
      "Epoch 1469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5288e-04 - my_r2: 0.9308 - val_loss: 1.3954e-05 - val_my_r2: 0.9957\n",
      "Epoch 1470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0928e-04 - my_r2: 0.9570 - val_loss: 1.4431e-05 - val_my_r2: 0.9958\n",
      "Epoch 1471/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5109e-04 - my_r2: 0.9224 - val_loss: 1.3401e-05 - val_my_r2: 0.9961\n",
      "Epoch 1472/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3243e-04 - my_r2: 0.8944 - val_loss: 8.7661e-06 - val_my_r2: 0.9973\n",
      "Epoch 1473/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6544e-04 - my_r2: 0.9312 - val_loss: 9.1122e-06 - val_my_r2: 0.9969\n",
      "Epoch 1474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6197e-04 - my_r2: 0.9107 - val_loss: 1.0129e-05 - val_my_r2: 0.9971\n",
      "Epoch 1475/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0890e-04 - my_r2: 0.9392 - val_loss: 8.0698e-06 - val_my_r2: 0.9976\n",
      "Epoch 1476/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1446e-04 - my_r2: 0.9310 - val_loss: 9.4431e-06 - val_my_r2: 0.9970\n",
      "Epoch 1477/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9943e-04 - my_r2: 0.9384 - val_loss: 1.8668e-05 - val_my_r2: 0.9944\n",
      "Epoch 1478/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9190e-04 - my_r2: 0.9427 - val_loss: 1.0446e-05 - val_my_r2: 0.9968\n",
      "Epoch 1479/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9920e-04 - my_r2: 0.9430 - val_loss: 8.2518e-06 - val_my_r2: 0.9971\n",
      "Epoch 1480/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6243e-04 - my_r2: 0.9426 - val_loss: 9.7714e-06 - val_my_r2: 0.9966\n",
      "Epoch 1481/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2561e-04 - my_r2: 0.9271 - val_loss: 1.1198e-05 - val_my_r2: 0.9966\n",
      "Epoch 1482/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9501e-04 - my_r2: 0.9227 - val_loss: 1.0271e-05 - val_my_r2: 0.9967\n",
      "Epoch 1483/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1121e-04 - my_r2: 0.9574 - val_loss: 1.2183e-05 - val_my_r2: 0.9962\n",
      "Epoch 1484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6552e-04 - my_r2: 0.9353 - val_loss: 8.3379e-06 - val_my_r2: 0.9973\n",
      "Epoch 1485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2151e-04 - my_r2: 0.9387 - val_loss: 8.5246e-06 - val_my_r2: 0.9971\n",
      "Epoch 1486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2180e-04 - my_r2: 0.9413 - val_loss: 8.1423e-06 - val_my_r2: 0.9972\n",
      "Epoch 1487/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4283e-04 - my_r2: 0.9467 - val_loss: 8.4800e-06 - val_my_r2: 0.9971\n",
      "Epoch 1488/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3506e-04 - my_r2: 0.9455 - val_loss: 9.6991e-06 - val_my_r2: 0.9967\n",
      "Epoch 1489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7076e-04 - my_r2: 0.9104 - val_loss: 9.2873e-06 - val_my_r2: 0.9970\n",
      "Epoch 1490/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4562e-04 - my_r2: 0.9448 - val_loss: 1.0015e-05 - val_my_r2: 0.9968\n",
      "Epoch 1491/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6754e-04 - my_r2: 0.9434 - val_loss: 1.0033e-05 - val_my_r2: 0.9969\n",
      "Epoch 1492/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3397e-04 - my_r2: 0.9262 - val_loss: 1.1200e-05 - val_my_r2: 0.9966\n",
      "Epoch 1493/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4062e-04 - my_r2: 0.9512 - val_loss: 1.1162e-05 - val_my_r2: 0.9965\n",
      "Epoch 1494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7772e-04 - my_r2: 0.9318 - val_loss: 8.7714e-06 - val_my_r2: 0.9971\n",
      "Epoch 1495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4375e-04 - my_r2: 0.9202 - val_loss: 7.4694e-06 - val_my_r2: 0.9976\n",
      "Epoch 1496/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7689e-04 - my_r2: 0.8765 - val_loss: 1.0921e-05 - val_my_r2: 0.9966\n",
      "Epoch 1497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3140e-04 - my_r2: 0.9532 - val_loss: 1.2066e-05 - val_my_r2: 0.9960\n",
      "Epoch 1498/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4086e-04 - my_r2: 0.8817 - val_loss: 1.0538e-05 - val_my_r2: 0.9964\n",
      "Epoch 1499/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9999e-04 - my_r2: 0.9368 - val_loss: 1.4307e-05 - val_my_r2: 0.9948\n",
      "Epoch 1500/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0963e-04 - my_r2: 0.8903 - val_loss: 1.2862e-05 - val_my_r2: 0.9952\n",
      "Epoch 1501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6387e-04 - my_r2: 0.9470 - val_loss: 1.4752e-05 - val_my_r2: 0.9947\n",
      "Epoch 1502/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3251e-04 - my_r2: 0.9545 - val_loss: 1.1614e-05 - val_my_r2: 0.9958\n",
      "Epoch 1503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4975e-04 - my_r2: 0.9285 - val_loss: 9.6073e-06 - val_my_r2: 0.9967\n",
      "Epoch 1504/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3353e-04 - my_r2: 0.9565 - val_loss: 8.9194e-06 - val_my_r2: 0.9970\n",
      "Epoch 1505/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5373e-04 - my_r2: 0.9101 - val_loss: 1.0419e-05 - val_my_r2: 0.9963\n",
      "Epoch 1506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5489e-04 - my_r2: 0.9372 - val_loss: 1.1855e-05 - val_my_r2: 0.9962\n",
      "Epoch 1507/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1261e-04 - my_r2: 0.8806 - val_loss: 1.8738e-05 - val_my_r2: 0.9941\n",
      "Epoch 1508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5636e-04 - my_r2: 0.9231 - val_loss: 1.3942e-05 - val_my_r2: 0.9954\n",
      "Epoch 1509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0450e-04 - my_r2: 0.9448 - val_loss: 8.6276e-06 - val_my_r2: 0.9970\n",
      "Epoch 1510/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4229e-04 - my_r2: 0.9151 - val_loss: 7.8663e-06 - val_my_r2: 0.9973\n",
      "Epoch 1511/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6127e-04 - my_r2: 0.9342 - val_loss: 9.3808e-06 - val_my_r2: 0.9969\n",
      "Epoch 1512/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0424e-04 - my_r2: 0.8959 - val_loss: 9.7553e-06 - val_my_r2: 0.9968\n",
      "Epoch 1513/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4154e-04 - my_r2: 0.9379 - val_loss: 1.1621e-05 - val_my_r2: 0.9964\n",
      "Epoch 1514/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6695e-04 - my_r2: 0.9187 - val_loss: 1.2915e-05 - val_my_r2: 0.9960\n",
      "Epoch 1515/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9527e-04 - my_r2: 0.9528 - val_loss: 9.5700e-06 - val_my_r2: 0.9967\n",
      "Epoch 1516/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5449e-04 - my_r2: 0.9340 - val_loss: 1.3423e-05 - val_my_r2: 0.9947\n",
      "Epoch 1517/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9826e-04 - my_r2: 0.8978 - val_loss: 1.1890e-05 - val_my_r2: 0.9957\n",
      "Epoch 1518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4587e-04 - my_r2: 0.9394 - val_loss: 1.6235e-05 - val_my_r2: 0.9944\n",
      "Epoch 1519/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7870e-04 - my_r2: 0.9137 - val_loss: 1.5377e-05 - val_my_r2: 0.9949\n",
      "Epoch 1520/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8457e-04 - my_r2: 0.9147 - val_loss: 9.5489e-06 - val_my_r2: 0.9968\n",
      "Epoch 1521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0382e-04 - my_r2: 0.8915 - val_loss: 9.1637e-06 - val_my_r2: 0.9970\n",
      "Epoch 1522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3770e-04 - my_r2: 0.9227 - val_loss: 9.2231e-06 - val_my_r2: 0.9971\n",
      "Epoch 1523/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3314e-04 - my_r2: 0.9338 - val_loss: 1.3205e-05 - val_my_r2: 0.9956\n",
      "Epoch 1524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7600e-04 - my_r2: 0.9132 - val_loss: 9.6674e-06 - val_my_r2: 0.9961\n",
      "Epoch 1525/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5102e-04 - my_r2: 0.9132 - val_loss: 1.2293e-05 - val_my_r2: 0.9953\n",
      "Epoch 1526/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3364e-04 - my_r2: 0.9350 - val_loss: 9.9187e-06 - val_my_r2: 0.9959\n",
      "Epoch 1527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5566e-04 - my_r2: 0.9031 - val_loss: 1.1589e-05 - val_my_r2: 0.9948\n",
      "Epoch 1528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1796e-04 - my_r2: 0.9312 - val_loss: 1.0933e-05 - val_my_r2: 0.9950\n",
      "Epoch 1529/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4343e-04 - my_r2: 0.9429 - val_loss: 1.3842e-05 - val_my_r2: 0.9950\n",
      "Epoch 1530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5008e-04 - my_r2: 0.9456 - val_loss: 1.2614e-05 - val_my_r2: 0.9958\n",
      "Epoch 1531/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2498e-04 - my_r2: 0.9342 - val_loss: 9.9512e-06 - val_my_r2: 0.9963\n",
      "Epoch 1532/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1759e-04 - my_r2: 0.9058 - val_loss: 8.2470e-06 - val_my_r2: 0.9970\n",
      "Epoch 1533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2890e-04 - my_r2: 0.9209 - val_loss: 1.0745e-05 - val_my_r2: 0.9967\n",
      "Epoch 1534/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5013e-04 - my_r2: 0.9089 - val_loss: 9.7346e-06 - val_my_r2: 0.9968\n",
      "Epoch 1535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0070e-04 - my_r2: 0.8849 - val_loss: 8.2906e-06 - val_my_r2: 0.9970\n",
      "Epoch 1536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4140e-04 - my_r2: 0.9137 - val_loss: 1.0838e-05 - val_my_r2: 0.9961\n",
      "Epoch 1537/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7744e-04 - my_r2: 0.9094 - val_loss: 1.2309e-05 - val_my_r2: 0.9961\n",
      "Epoch 1538/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4071e-04 - my_r2: 0.9431 - val_loss: 1.7378e-05 - val_my_r2: 0.9946\n",
      "Epoch 1539/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7056e-04 - my_r2: 0.9467 - val_loss: 1.1288e-05 - val_my_r2: 0.9965\n",
      "Epoch 1540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5511e-04 - my_r2: 0.9393 - val_loss: 1.4121e-05 - val_my_r2: 0.9954\n",
      "Epoch 1541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3909e-04 - my_r2: 0.9487 - val_loss: 1.4935e-05 - val_my_r2: 0.9949\n",
      "Epoch 1542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2161e-04 - my_r2: 0.9474 - val_loss: 1.4657e-05 - val_my_r2: 0.9951\n",
      "Epoch 1543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5352e-04 - my_r2: 0.9405 - val_loss: 1.1722e-05 - val_my_r2: 0.9958\n",
      "Epoch 1544/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6813e-04 - my_r2: 0.8903 - val_loss: 1.4836e-05 - val_my_r2: 0.9949\n",
      "Epoch 1545/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2871e-04 - my_r2: 0.9246 - val_loss: 1.1327e-05 - val_my_r2: 0.9960\n",
      "Epoch 1546/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3651e-04 - my_r2: 0.9177 - val_loss: 1.0250e-05 - val_my_r2: 0.9967\n",
      "Epoch 1547/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4650e-04 - my_r2: 0.9454 - val_loss: 7.1941e-06 - val_my_r2: 0.9979\n",
      "Epoch 1548/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3453e-04 - my_r2: 0.9242 - val_loss: 8.8297e-06 - val_my_r2: 0.9972\n",
      "Epoch 1549/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5556e-04 - my_r2: 0.9124 - val_loss: 1.2198e-05 - val_my_r2: 0.9954\n",
      "Epoch 1550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0978e-04 - my_r2: 0.9440 - val_loss: 1.6115e-05 - val_my_r2: 0.9940\n",
      "Epoch 1551/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8316e-04 - my_r2: 0.9610 - val_loss: 1.2970e-05 - val_my_r2: 0.9953\n",
      "Epoch 1552/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4907e-04 - my_r2: 0.9134 - val_loss: 1.0438e-05 - val_my_r2: 0.9966\n",
      "Epoch 1553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2331e-04 - my_r2: 0.9429 - val_loss: 9.8499e-06 - val_my_r2: 0.9966\n",
      "Epoch 1554/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9357e-04 - my_r2: 0.9082 - val_loss: 1.1647e-05 - val_my_r2: 0.9963\n",
      "Epoch 1555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5877e-04 - my_r2: 0.9336 - val_loss: 9.4308e-06 - val_my_r2: 0.9968\n",
      "Epoch 1556/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8393e-04 - my_r2: 0.9410 - val_loss: 8.8298e-06 - val_my_r2: 0.9972\n",
      "Epoch 1557/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4790e-04 - my_r2: 0.9423 - val_loss: 9.3849e-06 - val_my_r2: 0.9970\n",
      "Epoch 1558/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7526e-04 - my_r2: 0.9497 - val_loss: 8.7779e-06 - val_my_r2: 0.9972\n",
      "Epoch 1559/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9083e-04 - my_r2: 0.9175 - val_loss: 7.3827e-06 - val_my_r2: 0.9974\n",
      "Epoch 1560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5121e-04 - my_r2: 0.9449 - val_loss: 9.0188e-06 - val_my_r2: 0.9968\n",
      "Epoch 1561/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2837e-04 - my_r2: 0.9212 - val_loss: 8.0200e-06 - val_my_r2: 0.9970\n",
      "Epoch 1562/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1463e-04 - my_r2: 0.9517 - val_loss: 7.9742e-06 - val_my_r2: 0.9968\n",
      "Epoch 1563/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6223e-04 - my_r2: 0.9395 - val_loss: 1.1325e-05 - val_my_r2: 0.9957\n",
      "Epoch 1564/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6649e-04 - my_r2: 0.9239 - val_loss: 1.3576e-05 - val_my_r2: 0.9952\n",
      "Epoch 1565/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2653e-04 - my_r2: 0.9557 - val_loss: 9.9806e-06 - val_my_r2: 0.9963\n",
      "Epoch 1566/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6775e-04 - my_r2: 0.9250 - val_loss: 7.8636e-06 - val_my_r2: 0.9970\n",
      "Epoch 1567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8649e-04 - my_r2: 0.9163 - val_loss: 6.1074e-06 - val_my_r2: 0.9980\n",
      "Epoch 1568/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3525e-04 - my_r2: 0.9203 - val_loss: 7.3009e-06 - val_my_r2: 0.9977\n",
      "Epoch 1569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4608e-04 - my_r2: 0.8746 - val_loss: 1.1334e-05 - val_my_r2: 0.9964\n",
      "Epoch 1570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3182e-04 - my_r2: 0.9472 - val_loss: 7.8216e-06 - val_my_r2: 0.9975\n",
      "Epoch 1571/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0059e-04 - my_r2: 0.9444 - val_loss: 7.6109e-06 - val_my_r2: 0.9973\n",
      "Epoch 1572/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2194e-04 - my_r2: 0.9336 - val_loss: 8.1687e-06 - val_my_r2: 0.9973\n",
      "Epoch 1573/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5360e-04 - my_r2: 0.9458 - val_loss: 1.2220e-05 - val_my_r2: 0.9956\n",
      "Epoch 1574/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4237e-04 - my_r2: 0.9458 - val_loss: 1.5316e-05 - val_my_r2: 0.9943\n",
      "Epoch 1575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6328e-04 - my_r2: 0.8676 - val_loss: 1.0514e-05 - val_my_r2: 0.9964\n",
      "Epoch 1576/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0471e-04 - my_r2: 0.9550 - val_loss: 1.0378e-05 - val_my_r2: 0.9966\n",
      "Epoch 1577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8588e-04 - my_r2: 0.9473 - val_loss: 9.9436e-06 - val_my_r2: 0.9967\n",
      "Epoch 1578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6193e-04 - my_r2: 0.9501 - val_loss: 9.2148e-06 - val_my_r2: 0.9968\n",
      "Epoch 1579/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8644e-04 - my_r2: 0.9529 - val_loss: 1.2691e-05 - val_my_r2: 0.9954\n",
      "Epoch 1580/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9392e-04 - my_r2: 0.8874 - val_loss: 9.1931e-06 - val_my_r2: 0.9967\n",
      "Epoch 1581/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2595e-04 - my_r2: 0.8875 - val_loss: 8.6290e-06 - val_my_r2: 0.9971\n",
      "Epoch 1582/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5106e-04 - my_r2: 0.9485 - val_loss: 1.0459e-05 - val_my_r2: 0.9966\n",
      "Epoch 1583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5002e-04 - my_r2: 0.9224 - val_loss: 7.9370e-06 - val_my_r2: 0.9974\n",
      "Epoch 1584/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0519e-04 - my_r2: 0.9075 - val_loss: 7.2523e-06 - val_my_r2: 0.9976\n",
      "Epoch 1585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7753e-04 - my_r2: 0.8935 - val_loss: 6.9356e-06 - val_my_r2: 0.9976\n",
      "Epoch 1586/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4030e-04 - my_r2: 0.9342 - val_loss: 9.0384e-06 - val_my_r2: 0.9971\n",
      "Epoch 1587/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0332e-04 - my_r2: 0.9588 - val_loss: 1.0312e-05 - val_my_r2: 0.9969\n",
      "Epoch 1588/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7790e-04 - my_r2: 0.9233 - val_loss: 1.3865e-05 - val_my_r2: 0.9959\n",
      "Epoch 1589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3657e-04 - my_r2: 0.9457 - val_loss: 1.0782e-05 - val_my_r2: 0.9968\n",
      "Epoch 1590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6621e-04 - my_r2: 0.9257 - val_loss: 9.1081e-06 - val_my_r2: 0.9972\n",
      "Epoch 1591/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0253e-04 - my_r2: 0.9408 - val_loss: 1.2316e-05 - val_my_r2: 0.9954\n",
      "Epoch 1592/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8713e-04 - my_r2: 0.9591 - val_loss: 1.0324e-05 - val_my_r2: 0.9963\n",
      "Epoch 1593/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5393e-04 - my_r2: 0.9243 - val_loss: 9.7083e-06 - val_my_r2: 0.9967\n",
      "Epoch 1594/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5275e-04 - my_r2: 0.9151 - val_loss: 1.0283e-05 - val_my_r2: 0.9967\n",
      "Epoch 1595/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9924e-04 - my_r2: 0.9505 - val_loss: 1.2056e-05 - val_my_r2: 0.9960\n",
      "Epoch 1596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9586e-04 - my_r2: 0.9348 - val_loss: 1.1059e-05 - val_my_r2: 0.9962\n",
      "Epoch 1597/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4316e-04 - my_r2: 0.9328 - val_loss: 9.9791e-06 - val_my_r2: 0.9968\n",
      "Epoch 1598/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3835e-04 - my_r2: 0.9236 - val_loss: 1.3418e-05 - val_my_r2: 0.9957\n",
      "Epoch 1599/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8217e-04 - my_r2: 0.8984 - val_loss: 1.0637e-05 - val_my_r2: 0.9964\n",
      "Epoch 1600/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4394e-04 - my_r2: 0.9445 - val_loss: 7.3904e-06 - val_my_r2: 0.9975\n",
      "Epoch 1601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7069e-04 - my_r2: 0.9343 - val_loss: 5.2909e-06 - val_my_r2: 0.9982\n",
      "Epoch 1602/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4179e-04 - my_r2: 0.9280 - val_loss: 6.2914e-06 - val_my_r2: 0.9978\n",
      "Epoch 1603/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8288e-04 - my_r2: 0.9427 - val_loss: 8.7912e-06 - val_my_r2: 0.9969\n",
      "Epoch 1604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6730e-04 - my_r2: 0.9262 - val_loss: 1.0526e-05 - val_my_r2: 0.9959\n",
      "Epoch 1605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5981e-04 - my_r2: 0.9475 - val_loss: 9.4808e-06 - val_my_r2: 0.9967\n",
      "Epoch 1606/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5174e-04 - my_r2: 0.9457 - val_loss: 1.1946e-05 - val_my_r2: 0.9959\n",
      "Epoch 1607/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9766e-04 - my_r2: 0.9361 - val_loss: 1.4697e-05 - val_my_r2: 0.9952\n",
      "Epoch 1608/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0101e-04 - my_r2: 0.9163 - val_loss: 1.0125e-05 - val_my_r2: 0.9966\n",
      "Epoch 1609/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1257e-04 - my_r2: 0.9385 - val_loss: 8.4356e-06 - val_my_r2: 0.9972\n",
      "Epoch 1610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7541e-04 - my_r2: 0.9208 - val_loss: 9.2807e-06 - val_my_r2: 0.9970\n",
      "Epoch 1611/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0815e-04 - my_r2: 0.9563 - val_loss: 1.0039e-05 - val_my_r2: 0.9965\n",
      "Epoch 1612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4013e-04 - my_r2: 0.6689 - val_loss: 1.0794e-05 - val_my_r2: 0.9962\n",
      "Epoch 1613/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5928e-04 - my_r2: 0.9442 - val_loss: 1.8373e-05 - val_my_r2: 0.9947\n",
      "Epoch 1614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0870e-04 - my_r2: 0.9529 - val_loss: 1.3661e-05 - val_my_r2: 0.9957\n",
      "Epoch 1615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0796e-04 - my_r2: 0.9489 - val_loss: 1.5489e-05 - val_my_r2: 0.9953\n",
      "Epoch 1616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8158e-04 - my_r2: 0.9594 - val_loss: 1.4405e-05 - val_my_r2: 0.9956\n",
      "Epoch 1617/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5522e-04 - my_r2: 0.9465 - val_loss: 1.5879e-05 - val_my_r2: 0.9955\n",
      "Epoch 1618/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3974e-04 - my_r2: 0.9022 - val_loss: 1.2429e-05 - val_my_r2: 0.9964\n",
      "Epoch 1619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3862e-04 - my_r2: 0.9374 - val_loss: 7.6887e-06 - val_my_r2: 0.9975\n",
      "Epoch 1620/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1426e-04 - my_r2: 0.9429 - val_loss: 1.0714e-05 - val_my_r2: 0.9959\n",
      "Epoch 1621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2026e-04 - my_r2: 0.9439 - val_loss: 8.1192e-06 - val_my_r2: 0.9971\n",
      "Epoch 1622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6357e-04 - my_r2: 0.9407 - val_loss: 8.9164e-06 - val_my_r2: 0.9971\n",
      "Epoch 1623/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7195e-04 - my_r2: 0.8560 - val_loss: 7.4775e-06 - val_my_r2: 0.9975\n",
      "Epoch 1624/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1114e-04 - my_r2: 0.9452 - val_loss: 6.3912e-06 - val_my_r2: 0.9979\n",
      "Epoch 1625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6928e-04 - my_r2: 0.8970 - val_loss: 8.2734e-06 - val_my_r2: 0.9972\n",
      "Epoch 1626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5249e-04 - my_r2: 0.9390 - val_loss: 9.9908e-06 - val_my_r2: 0.9963\n",
      "Epoch 1627/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6746e-04 - my_r2: 0.8125 - val_loss: 1.1188e-05 - val_my_r2: 0.9961\n",
      "Epoch 1628/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1866e-04 - my_r2: 0.9149 - val_loss: 9.9596e-06 - val_my_r2: 0.9967\n",
      "Epoch 1629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5318e-04 - my_r2: 0.9276 - val_loss: 8.7174e-06 - val_my_r2: 0.9973\n",
      "Epoch 1630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7158e-04 - my_r2: 0.8452 - val_loss: 8.0607e-06 - val_my_r2: 0.9978\n",
      "Epoch 1631/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4108e-04 - my_r2: 0.8870 - val_loss: 1.0756e-05 - val_my_r2: 0.9968\n",
      "Epoch 1632/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4023e-04 - my_r2: 0.8935 - val_loss: 9.2230e-06 - val_my_r2: 0.9972\n",
      "Epoch 1633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2261e-04 - my_r2: 0.8738 - val_loss: 1.4302e-05 - val_my_r2: 0.9956\n",
      "Epoch 1634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1623e-04 - my_r2: 0.9055 - val_loss: 9.7559e-06 - val_my_r2: 0.9970\n",
      "Epoch 1635/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1253e-04 - my_r2: 0.9447 - val_loss: 8.7534e-06 - val_my_r2: 0.9973\n",
      "Epoch 1636/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7134e-04 - my_r2: 0.9405 - val_loss: 7.3578e-06 - val_my_r2: 0.9978\n",
      "Epoch 1637/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9741e-04 - my_r2: 0.9538 - val_loss: 6.5526e-06 - val_my_r2: 0.9979\n",
      "Epoch 1638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7380e-04 - my_r2: 0.9286 - val_loss: 6.6966e-06 - val_my_r2: 0.9977\n",
      "Epoch 1639/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7211e-04 - my_r2: 0.9224 - val_loss: 9.4117e-06 - val_my_r2: 0.9964\n",
      "Epoch 1640/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7602e-04 - my_r2: 0.9332 - val_loss: 1.0525e-05 - val_my_r2: 0.9958\n",
      "Epoch 1641/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6424e-04 - my_r2: 0.9159 - val_loss: 8.7757e-06 - val_my_r2: 0.9969\n",
      "Epoch 1642/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7520e-04 - my_r2: 0.9039 - val_loss: 7.0851e-06 - val_my_r2: 0.9974\n",
      "Epoch 1643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8201e-04 - my_r2: 0.9584 - val_loss: 7.0395e-06 - val_my_r2: 0.9977\n",
      "Epoch 1644/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4132e-04 - my_r2: 0.9261 - val_loss: 8.0501e-06 - val_my_r2: 0.9976\n",
      "Epoch 1645/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8296e-04 - my_r2: 0.9515 - val_loss: 1.0230e-05 - val_my_r2: 0.9970\n",
      "Epoch 1646/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6009e-04 - my_r2: 0.9338 - val_loss: 9.1238e-06 - val_my_r2: 0.9972\n",
      "Epoch 1647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5348e-04 - my_r2: 0.9213 - val_loss: 7.9603e-06 - val_my_r2: 0.9976\n",
      "Epoch 1648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5144e-04 - my_r2: 0.9108 - val_loss: 8.5491e-06 - val_my_r2: 0.9972\n",
      "Epoch 1649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5976e-04 - my_r2: 0.9242 - val_loss: 9.4363e-06 - val_my_r2: 0.9966\n",
      "Epoch 1650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3103e-04 - my_r2: 0.9440 - val_loss: 1.1399e-05 - val_my_r2: 0.9957\n",
      "Epoch 1651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6497e-04 - my_r2: 0.9183 - val_loss: 1.8485e-05 - val_my_r2: 0.9933\n",
      "Epoch 1652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7209e-04 - my_r2: 0.9244 - val_loss: 1.5115e-05 - val_my_r2: 0.9950\n",
      "Epoch 1653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9811e-04 - my_r2: 0.9169 - val_loss: 9.8185e-06 - val_my_r2: 0.9968\n",
      "Epoch 1654/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6543e-04 - my_r2: 0.9072 - val_loss: 9.3492e-06 - val_my_r2: 0.9970\n",
      "Epoch 1655/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0965e-04 - my_r2: 0.9411 - val_loss: 7.9036e-06 - val_my_r2: 0.9973\n",
      "Epoch 1656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3132e-04 - my_r2: 0.9348 - val_loss: 8.4368e-06 - val_my_r2: 0.9970\n",
      "Epoch 1657/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3474e-04 - my_r2: 0.9276 - val_loss: 1.0068e-05 - val_my_r2: 0.9963\n",
      "Epoch 1658/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9646e-04 - my_r2: 0.9494 - val_loss: 1.0913e-05 - val_my_r2: 0.9961\n",
      "Epoch 1659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1369e-04 - my_r2: 0.9451 - val_loss: 8.9157e-06 - val_my_r2: 0.9969\n",
      "Epoch 1660/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7207e-04 - my_r2: 0.9383 - val_loss: 8.5007e-06 - val_my_r2: 0.9972\n",
      "Epoch 1661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7175e-04 - my_r2: 0.9342 - val_loss: 1.0187e-05 - val_my_r2: 0.9966\n",
      "Epoch 1662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1437e-04 - my_r2: 0.9131 - val_loss: 1.1276e-05 - val_my_r2: 0.9963\n",
      "Epoch 1663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0126e-04 - my_r2: 0.9197 - val_loss: 9.1324e-06 - val_my_r2: 0.9970\n",
      "Epoch 1664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1749e-04 - my_r2: 0.9209 - val_loss: 1.0306e-05 - val_my_r2: 0.9962\n",
      "Epoch 1665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2240e-04 - my_r2: 0.9317 - val_loss: 1.3231e-05 - val_my_r2: 0.9951\n",
      "Epoch 1666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6433e-04 - my_r2: 0.9453 - val_loss: 1.3523e-05 - val_my_r2: 0.9954\n",
      "Epoch 1667/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7710e-04 - my_r2: 0.9372 - val_loss: 1.1820e-05 - val_my_r2: 0.9960\n",
      "Epoch 1668/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4605e-04 - my_r2: 0.9471 - val_loss: 8.9922e-06 - val_my_r2: 0.9971\n",
      "Epoch 1669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0901e-04 - my_r2: 0.9105 - val_loss: 9.1581e-06 - val_my_r2: 0.9971\n",
      "Epoch 1670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2193e-04 - my_r2: 0.9116 - val_loss: 1.0592e-05 - val_my_r2: 0.9966\n",
      "Epoch 1671/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8178e-04 - my_r2: 0.9237 - val_loss: 1.3723e-05 - val_my_r2: 0.9957\n",
      "Epoch 1672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8893e-04 - my_r2: 0.8857 - val_loss: 2.3070e-05 - val_my_r2: 0.9931\n",
      "Epoch 1673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6597e-04 - my_r2: 0.9279 - val_loss: 2.4236e-05 - val_my_r2: 0.9927\n",
      "Epoch 1674/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0687e-04 - my_r2: 0.9407 - val_loss: 1.6748e-05 - val_my_r2: 0.9950\n",
      "Epoch 1675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5906e-04 - my_r2: 0.9154 - val_loss: 1.0874e-05 - val_my_r2: 0.9967\n",
      "Epoch 1676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1108e-04 - my_r2: 0.9376 - val_loss: 8.6464e-06 - val_my_r2: 0.9974\n",
      "Epoch 1677/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8525e-04 - my_r2: 0.9213 - val_loss: 7.9360e-06 - val_my_r2: 0.9975\n",
      "Epoch 1678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8636e-04 - my_r2: 0.9356 - val_loss: 1.0739e-05 - val_my_r2: 0.9964\n",
      "Epoch 1679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1963e-04 - my_r2: 0.9285 - val_loss: 9.7778e-06 - val_my_r2: 0.9965\n",
      "Epoch 1680/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5020e-04 - my_r2: 0.8811 - val_loss: 8.3350e-06 - val_my_r2: 0.9971\n",
      "Epoch 1681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6650e-04 - my_r2: 0.9155 - val_loss: 7.6690e-06 - val_my_r2: 0.9974\n",
      "Epoch 1682/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8698e-04 - my_r2: 0.9264 - val_loss: 7.1132e-06 - val_my_r2: 0.9977\n",
      "Epoch 1683/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3665e-04 - my_r2: 0.8969 - val_loss: 1.0330e-05 - val_my_r2: 0.9962\n",
      "Epoch 1684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5820e-04 - my_r2: 0.9217 - val_loss: 1.6809e-05 - val_my_r2: 0.9939\n",
      "Epoch 1685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2058e-04 - my_r2: 0.8806 - val_loss: 1.0150e-05 - val_my_r2: 0.9962\n",
      "Epoch 1686/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1988e-04 - my_r2: 0.9167 - val_loss: 9.6641e-06 - val_my_r2: 0.9966\n",
      "Epoch 1687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1091e-04 - my_r2: 0.9582 - val_loss: 6.5581e-06 - val_my_r2: 0.9976\n",
      "Epoch 1688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0679e-04 - my_r2: 0.9447 - val_loss: 1.1587e-05 - val_my_r2: 0.9961\n",
      "Epoch 1689/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0172e-04 - my_r2: 0.9605 - val_loss: 7.6220e-06 - val_my_r2: 0.9975\n",
      "Epoch 1690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1896e-04 - my_r2: 0.9395 - val_loss: 7.1127e-06 - val_my_r2: 0.9974\n",
      "Epoch 1691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3848e-04 - my_r2: 0.9239 - val_loss: 7.9253e-06 - val_my_r2: 0.9971\n",
      "Epoch 1692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5450e-04 - my_r2: 0.9455 - val_loss: 9.9639e-06 - val_my_r2: 0.9965\n",
      "Epoch 1693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2442e-04 - my_r2: 0.9334 - val_loss: 9.0763e-06 - val_my_r2: 0.9972\n",
      "Epoch 1694/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3532e-04 - my_r2: 0.9449 - val_loss: 6.9829e-06 - val_my_r2: 0.9979\n",
      "Epoch 1695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8967e-04 - my_r2: 0.9030 - val_loss: 7.5594e-06 - val_my_r2: 0.9972\n",
      "Epoch 1696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1056e-04 - my_r2: 0.9537 - val_loss: 7.8973e-06 - val_my_r2: 0.9968\n",
      "Epoch 1697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7237e-04 - my_r2: 0.9103 - val_loss: 9.5746e-06 - val_my_r2: 0.9968\n",
      "Epoch 1698/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3013e-04 - my_r2: 0.9467 - val_loss: 8.1438e-06 - val_my_r2: 0.9970\n",
      "Epoch 1699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3956e-04 - my_r2: 0.9451 - val_loss: 1.2401e-05 - val_my_r2: 0.9951\n",
      "Epoch 1700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9989e-04 - my_r2: 0.9374 - val_loss: 1.2195e-05 - val_my_r2: 0.9950\n",
      "Epoch 1701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6287e-04 - my_r2: 0.9349 - val_loss: 1.4958e-05 - val_my_r2: 0.9955\n",
      "Epoch 1702/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0726e-04 - my_r2: 0.9331 - val_loss: 1.3719e-05 - val_my_r2: 0.9958\n",
      "Epoch 1703/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2333e-04 - my_r2: 0.9495 - val_loss: 1.1268e-05 - val_my_r2: 0.9966\n",
      "Epoch 1704/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4322e-04 - my_r2: 0.9342 - val_loss: 9.5978e-06 - val_my_r2: 0.9970\n",
      "Epoch 1705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5756e-04 - my_r2: 0.9458 - val_loss: 8.6701e-06 - val_my_r2: 0.9972\n",
      "Epoch 1706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8830e-04 - my_r2: 0.8908 - val_loss: 1.3634e-05 - val_my_r2: 0.9958\n",
      "Epoch 1707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2808e-04 - my_r2: 0.9448 - val_loss: 8.6952e-06 - val_my_r2: 0.9971\n",
      "Epoch 1708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8340e-04 - my_r2: 0.9461 - val_loss: 9.6712e-06 - val_my_r2: 0.9966\n",
      "Epoch 1709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4726e-04 - my_r2: 0.9535 - val_loss: 1.4361e-05 - val_my_r2: 0.9943\n",
      "Epoch 1710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7401e-04 - my_r2: 0.8952 - val_loss: 1.2519e-05 - val_my_r2: 0.9953\n",
      "Epoch 1711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1725e-04 - my_r2: 0.9460 - val_loss: 1.2549e-05 - val_my_r2: 0.9952\n",
      "Epoch 1712/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0706e-04 - my_r2: 0.9511 - val_loss: 1.0075e-05 - val_my_r2: 0.9963\n",
      "Epoch 1713/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1796e-04 - my_r2: 0.9265 - val_loss: 1.2161e-05 - val_my_r2: 0.9956\n",
      "Epoch 1714/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7119e-04 - my_r2: 0.8979 - val_loss: 1.2840e-05 - val_my_r2: 0.9952\n",
      "Epoch 1715/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4871e-04 - my_r2: 0.7488 - val_loss: 1.4111e-05 - val_my_r2: 0.9948\n",
      "Epoch 1716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6366e-04 - my_r2: 0.7075 - val_loss: 7.7826e-06 - val_my_r2: 0.9972\n",
      "Epoch 1717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5472e-04 - my_r2: 0.9086 - val_loss: 8.7214e-06 - val_my_r2: 0.9971\n",
      "Epoch 1718/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6778e-04 - my_r2: 0.9262 - val_loss: 8.8756e-06 - val_my_r2: 0.9970\n",
      "Epoch 1719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2689e-04 - my_r2: 0.9459 - val_loss: 6.8611e-06 - val_my_r2: 0.9976\n",
      "Epoch 1720/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2777e-04 - my_r2: 0.9303 - val_loss: 8.0324e-06 - val_my_r2: 0.9974\n",
      "Epoch 1721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0472e-04 - my_r2: 0.9513 - val_loss: 9.2029e-06 - val_my_r2: 0.9973\n",
      "Epoch 1722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0096e-04 - my_r2: 0.9208 - val_loss: 9.1673e-06 - val_my_r2: 0.9973\n",
      "Epoch 1723/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3099e-04 - my_r2: 0.9449 - val_loss: 1.0083e-05 - val_my_r2: 0.9971\n",
      "Epoch 1724/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0411e-04 - my_r2: 0.9282 - val_loss: 1.1241e-05 - val_my_r2: 0.9969\n",
      "Epoch 1725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8487e-04 - my_r2: 0.9431 - val_loss: 9.8562e-06 - val_my_r2: 0.9971\n",
      "Epoch 1726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8066e-04 - my_r2: 0.9005 - val_loss: 1.0159e-05 - val_my_r2: 0.9966\n",
      "Epoch 1727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7626e-04 - my_r2: 0.9247 - val_loss: 1.3937e-05 - val_my_r2: 0.9955\n",
      "Epoch 1728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1357e-04 - my_r2: 0.9476 - val_loss: 1.0343e-05 - val_my_r2: 0.9969\n",
      "Epoch 1729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6233e-04 - my_r2: 0.9436 - val_loss: 9.4825e-06 - val_my_r2: 0.9972\n",
      "Epoch 1730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6100e-04 - my_r2: 0.9253 - val_loss: 1.1453e-05 - val_my_r2: 0.9967\n",
      "Epoch 1731/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4375e-04 - my_r2: 0.9498 - val_loss: 1.1577e-05 - val_my_r2: 0.9963\n",
      "Epoch 1732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8135e-04 - my_r2: 0.9275 - val_loss: 8.9150e-06 - val_my_r2: 0.9974\n",
      "Epoch 1733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9868e-04 - my_r2: 0.9518 - val_loss: 1.0955e-05 - val_my_r2: 0.9967\n",
      "Epoch 1734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5503e-04 - my_r2: 0.9312 - val_loss: 9.3705e-06 - val_my_r2: 0.9971\n",
      "Epoch 1735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4996e-04 - my_r2: 0.9342 - val_loss: 1.6427e-05 - val_my_r2: 0.9947\n",
      "Epoch 1736/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4518e-04 - my_r2: 0.9335 - val_loss: 1.6137e-05 - val_my_r2: 0.9945\n",
      "Epoch 1737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1254e-04 - my_r2: 0.8789 - val_loss: 1.2666e-05 - val_my_r2: 0.9954\n",
      "Epoch 1738/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9201e-04 - my_r2: 0.9047 - val_loss: 1.0467e-05 - val_my_r2: 0.9962\n",
      "Epoch 1739/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8006e-04 - my_r2: 0.8634 - val_loss: 8.7028e-06 - val_my_r2: 0.9973\n",
      "Epoch 1740/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3852e-04 - my_r2: 0.9312 - val_loss: 8.6336e-06 - val_my_r2: 0.9973\n",
      "Epoch 1741/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8907e-04 - my_r2: 0.9541 - val_loss: 7.5728e-06 - val_my_r2: 0.9975\n",
      "Epoch 1742/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9447e-04 - my_r2: 0.9458 - val_loss: 9.4134e-06 - val_my_r2: 0.9966\n",
      "Epoch 1743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1112e-04 - my_r2: 0.9411 - val_loss: 1.0552e-05 - val_my_r2: 0.9960\n",
      "Epoch 1744/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0468e-04 - my_r2: 0.9540 - val_loss: 8.6197e-06 - val_my_r2: 0.9969\n",
      "Epoch 1745/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0839e-04 - my_r2: 0.9551 - val_loss: 1.2258e-05 - val_my_r2: 0.9956\n",
      "Epoch 1746/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9182e-04 - my_r2: 0.9318 - val_loss: 9.4536e-06 - val_my_r2: 0.9968\n",
      "Epoch 1747/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3952e-04 - my_r2: 0.8983 - val_loss: 8.5997e-06 - val_my_r2: 0.9973\n",
      "Epoch 1748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8207e-04 - my_r2: 0.9304 - val_loss: 8.8823e-06 - val_my_r2: 0.9973\n",
      "Epoch 1749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9953e-04 - my_r2: 0.8890 - val_loss: 7.1534e-06 - val_my_r2: 0.9977\n",
      "Epoch 1750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4698e-04 - my_r2: 0.8930 - val_loss: 1.5690e-05 - val_my_r2: 0.9947\n",
      "Epoch 1751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4542e-04 - my_r2: 0.9454 - val_loss: 1.3582e-05 - val_my_r2: 0.9951\n",
      "Epoch 1752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2018e-04 - my_r2: 0.9559 - val_loss: 1.5725e-05 - val_my_r2: 0.9941\n",
      "Epoch 1753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8727e-04 - my_r2: 0.9619 - val_loss: 1.0903e-05 - val_my_r2: 0.9958\n",
      "Epoch 1754/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8618e-04 - my_r2: 0.8837 - val_loss: 1.1634e-05 - val_my_r2: 0.9959\n",
      "Epoch 1755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7725e-04 - my_r2: 0.9166 - val_loss: 8.8198e-06 - val_my_r2: 0.9971\n",
      "Epoch 1756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1594e-04 - my_r2: 0.9495 - val_loss: 1.1600e-05 - val_my_r2: 0.9964\n",
      "Epoch 1757/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3141e-04 - my_r2: 0.9016 - val_loss: 9.0698e-06 - val_my_r2: 0.9971\n",
      "Epoch 1758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3073e-04 - my_r2: 0.9516 - val_loss: 9.7814e-06 - val_my_r2: 0.9967\n",
      "Epoch 1759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6884e-04 - my_r2: 0.9154 - val_loss: 9.4205e-06 - val_my_r2: 0.9965\n",
      "Epoch 1760/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2267e-04 - my_r2: 0.9124 - val_loss: 9.5333e-06 - val_my_r2: 0.9963\n",
      "Epoch 1761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2799e-04 - my_r2: 0.9430 - val_loss: 9.3052e-06 - val_my_r2: 0.9964\n",
      "Epoch 1762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1923e-04 - my_r2: 0.9431 - val_loss: 1.0403e-05 - val_my_r2: 0.9961\n",
      "Epoch 1763/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5677e-04 - my_r2: 0.9468 - val_loss: 1.0132e-05 - val_my_r2: 0.9965\n",
      "Epoch 1764/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8768e-04 - my_r2: 0.9163 - val_loss: 1.0362e-05 - val_my_r2: 0.9965\n",
      "Epoch 1765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4765e-04 - my_r2: 0.9272 - val_loss: 1.3149e-05 - val_my_r2: 0.9954\n",
      "Epoch 1766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7227e-04 - my_r2: 0.9229 - val_loss: 1.1602e-05 - val_my_r2: 0.9963\n",
      "Epoch 1767/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5986e-04 - my_r2: 0.9326 - val_loss: 1.0267e-05 - val_my_r2: 0.9968\n",
      "Epoch 1768/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2330e-04 - my_r2: 0.9294 - val_loss: 8.8329e-06 - val_my_r2: 0.9972\n",
      "Epoch 1769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5704e-04 - my_r2: 0.9365 - val_loss: 1.0278e-05 - val_my_r2: 0.9969\n",
      "Epoch 1770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8933e-04 - my_r2: 0.9328 - val_loss: 1.2568e-05 - val_my_r2: 0.9962\n",
      "Epoch 1771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7386e-04 - my_r2: 0.9365 - val_loss: 1.2640e-05 - val_my_r2: 0.9961\n",
      "Epoch 1772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7325e-04 - my_r2: 0.9193 - val_loss: 1.1202e-05 - val_my_r2: 0.9964\n",
      "Epoch 1773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9665e-04 - my_r2: 0.9307 - val_loss: 8.1026e-06 - val_my_r2: 0.9972\n",
      "Epoch 1774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2352e-04 - my_r2: 0.8976 - val_loss: 1.6424e-05 - val_my_r2: 0.9944\n",
      "Epoch 1775/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0976e-04 - my_r2: 0.9460 - val_loss: 1.0687e-05 - val_my_r2: 0.9960\n",
      "Epoch 1776/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2608e-04 - my_r2: 0.9409 - val_loss: 1.2540e-05 - val_my_r2: 0.9953\n",
      "Epoch 1777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1928e-04 - my_r2: 0.9040 - val_loss: 1.4033e-05 - val_my_r2: 0.9945\n",
      "Epoch 1778/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5197e-04 - my_r2: 0.9106 - val_loss: 9.8370e-06 - val_my_r2: 0.9961\n",
      "Epoch 1779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9475e-04 - my_r2: 0.8763 - val_loss: 1.0670e-05 - val_my_r2: 0.9959\n",
      "Epoch 1780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6101e-04 - my_r2: 0.9256 - val_loss: 9.3450e-06 - val_my_r2: 0.9968\n",
      "Epoch 1781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5433e-04 - my_r2: 0.9415 - val_loss: 1.0278e-05 - val_my_r2: 0.9965\n",
      "Epoch 1782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5034e-04 - my_r2: 0.9477 - val_loss: 7.6274e-06 - val_my_r2: 0.9975\n",
      "Epoch 1783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8292e-04 - my_r2: 0.9089 - val_loss: 7.6998e-06 - val_my_r2: 0.9975\n",
      "Epoch 1784/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1851e-04 - my_r2: 0.9531 - val_loss: 9.7855e-06 - val_my_r2: 0.9966\n",
      "Epoch 1785/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7316e-04 - my_r2: 0.9300 - val_loss: 1.0427e-05 - val_my_r2: 0.9963\n",
      "Epoch 1786/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2920e-04 - my_r2: 0.9396 - val_loss: 7.3812e-06 - val_my_r2: 0.9976\n",
      "Epoch 1787/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9305e-04 - my_r2: 0.8823 - val_loss: 8.6297e-06 - val_my_r2: 0.9974\n",
      "Epoch 1788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6302e-04 - my_r2: 0.8828 - val_loss: 1.1405e-05 - val_my_r2: 0.9964\n",
      "Epoch 1789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2648e-04 - my_r2: 0.8779 - val_loss: 9.8341e-06 - val_my_r2: 0.9970\n",
      "Epoch 1790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3916e-04 - my_r2: 0.9447 - val_loss: 8.8650e-06 - val_my_r2: 0.9971\n",
      "Epoch 1791/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8661e-04 - my_r2: 0.9342 - val_loss: 9.8335e-06 - val_my_r2: 0.9964\n",
      "Epoch 1792/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0163e-04 - my_r2: 0.8999 - val_loss: 9.2255e-06 - val_my_r2: 0.9971\n",
      "Epoch 1793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2732e-04 - my_r2: 0.8847 - val_loss: 9.4223e-06 - val_my_r2: 0.9973\n",
      "Epoch 1794/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2807e-04 - my_r2: 0.9410 - val_loss: 1.1588e-05 - val_my_r2: 0.9961\n",
      "Epoch 1795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6163e-04 - my_r2: 0.9383 - val_loss: 1.5175e-05 - val_my_r2: 0.9948\n",
      "Epoch 1796/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8177e-04 - my_r2: 0.9374 - val_loss: 1.5796e-05 - val_my_r2: 0.9948\n",
      "Epoch 1797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5691e-04 - my_r2: 0.9363 - val_loss: 1.2797e-05 - val_my_r2: 0.9958\n",
      "Epoch 1798/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9090e-04 - my_r2: 0.9239 - val_loss: 1.3366e-05 - val_my_r2: 0.9956\n",
      "Epoch 1799/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7125e-04 - my_r2: 0.9335 - val_loss: 1.3618e-05 - val_my_r2: 0.9953\n",
      "Epoch 1800/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7428e-04 - my_r2: 0.9103 - val_loss: 7.7230e-06 - val_my_r2: 0.9972\n",
      "Epoch 1801/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2737e-04 - my_r2: 0.9415 - val_loss: 1.0673e-05 - val_my_r2: 0.9963\n",
      "Epoch 1802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0291e-04 - my_r2: 0.9203 - val_loss: 6.8672e-06 - val_my_r2: 0.9976\n",
      "Epoch 1803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4947e-04 - my_r2: 0.9400 - val_loss: 5.6626e-06 - val_my_r2: 0.9981\n",
      "Epoch 1804/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1680e-04 - my_r2: 0.9109 - val_loss: 6.5353e-06 - val_my_r2: 0.9979\n",
      "Epoch 1805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7977e-04 - my_r2: 0.9269 - val_loss: 5.9543e-06 - val_my_r2: 0.9981\n",
      "Epoch 1806/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2521e-04 - my_r2: 0.9388 - val_loss: 6.3660e-06 - val_my_r2: 0.9981\n",
      "Epoch 1807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5687e-04 - my_r2: 0.9369 - val_loss: 7.0617e-06 - val_my_r2: 0.9979\n",
      "Epoch 1808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8625e-04 - my_r2: 0.9336 - val_loss: 1.6396e-05 - val_my_r2: 0.9948\n",
      "Epoch 1809/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3656e-04 - my_r2: 0.9414 - val_loss: 1.4638e-05 - val_my_r2: 0.9956\n",
      "Epoch 1810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0197e-04 - my_r2: 0.9300 - val_loss: 1.2264e-05 - val_my_r2: 0.9964\n",
      "Epoch 1811/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6827e-04 - my_r2: 0.9323 - val_loss: 1.0628e-05 - val_my_r2: 0.9971\n",
      "Epoch 1812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4995e-04 - my_r2: 0.9405 - val_loss: 8.3431e-06 - val_my_r2: 0.9979\n",
      "Epoch 1813/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3391e-04 - my_r2: 0.9361 - val_loss: 9.6738e-06 - val_my_r2: 0.9972\n",
      "Epoch 1814/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6316e-04 - my_r2: 0.9312 - val_loss: 1.0446e-05 - val_my_r2: 0.9969\n",
      "Epoch 1815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5376e-04 - my_r2: 0.9270 - val_loss: 1.2756e-05 - val_my_r2: 0.9959\n",
      "Epoch 1816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9116e-04 - my_r2: 0.9528 - val_loss: 1.2813e-05 - val_my_r2: 0.9958\n",
      "Epoch 1817/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7905e-04 - my_r2: 0.9130 - val_loss: 1.0940e-05 - val_my_r2: 0.9968\n",
      "Epoch 1818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2537e-04 - my_r2: 0.9351 - val_loss: 1.0044e-05 - val_my_r2: 0.9972\n",
      "Epoch 1819/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1593e-04 - my_r2: 0.9454 - val_loss: 1.0640e-05 - val_my_r2: 0.9969\n",
      "Epoch 1820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5125e-04 - my_r2: 0.9350 - val_loss: 1.0602e-05 - val_my_r2: 0.9969\n",
      "Epoch 1821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2493e-04 - my_r2: 0.9357 - val_loss: 1.5114e-05 - val_my_r2: 0.9954\n",
      "Epoch 1822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5043e-04 - my_r2: 0.9439 - val_loss: 1.1261e-05 - val_my_r2: 0.9965\n",
      "Epoch 1823/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7966e-04 - my_r2: 0.9154 - val_loss: 1.0059e-05 - val_my_r2: 0.9969\n",
      "Epoch 1824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8539e-04 - my_r2: 0.9566 - val_loss: 9.8839e-06 - val_my_r2: 0.9967\n",
      "Epoch 1825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9019e-04 - my_r2: 0.9430 - val_loss: 1.2747e-05 - val_my_r2: 0.9958\n",
      "Epoch 1826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8317e-04 - my_r2: 0.9537 - val_loss: 1.1935e-05 - val_my_r2: 0.9960\n",
      "Epoch 1827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8874e-04 - my_r2: 0.9272 - val_loss: 1.2916e-05 - val_my_r2: 0.9959\n",
      "Epoch 1828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9412e-04 - my_r2: 0.9193 - val_loss: 1.1931e-05 - val_my_r2: 0.9964\n",
      "Epoch 1829/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5442e-04 - my_r2: 0.9419 - val_loss: 1.0259e-05 - val_my_r2: 0.9972\n",
      "Epoch 1830/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7760e-04 - my_r2: 0.9272 - val_loss: 1.2268e-05 - val_my_r2: 0.9965\n",
      "Epoch 1831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0175e-04 - my_r2: 0.9302 - val_loss: 1.1860e-05 - val_my_r2: 0.9964\n",
      "Epoch 1832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4672e-04 - my_r2: 0.9231 - val_loss: 1.0740e-05 - val_my_r2: 0.9966\n",
      "Epoch 1833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9479e-04 - my_r2: 0.9469 - val_loss: 8.8593e-06 - val_my_r2: 0.9974\n",
      "Epoch 1834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9915e-04 - my_r2: 0.8894 - val_loss: 9.8898e-06 - val_my_r2: 0.9970\n",
      "Epoch 1835/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9374e-04 - my_r2: 0.8409 - val_loss: 8.8076e-06 - val_my_r2: 0.9974\n",
      "Epoch 1836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5745e-04 - my_r2: 0.9263 - val_loss: 1.0614e-05 - val_my_r2: 0.9969\n",
      "Epoch 1837/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6754e-04 - my_r2: 0.9162 - val_loss: 9.4546e-06 - val_my_r2: 0.9971\n",
      "Epoch 1838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4293e-04 - my_r2: 0.9342 - val_loss: 9.4588e-06 - val_my_r2: 0.9970\n",
      "Epoch 1839/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8372e-04 - my_r2: 0.9319 - val_loss: 1.3241e-05 - val_my_r2: 0.9957\n",
      "Epoch 1840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9806e-04 - my_r2: 0.9056 - val_loss: 1.1024e-05 - val_my_r2: 0.9966\n",
      "Epoch 1841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2087e-04 - my_r2: 0.9162 - val_loss: 1.3627e-05 - val_my_r2: 0.9959\n",
      "Epoch 1842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8234e-04 - my_r2: 0.9608 - val_loss: 1.2241e-05 - val_my_r2: 0.9964\n",
      "Epoch 1843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3792e-04 - my_r2: 0.9275 - val_loss: 1.0176e-05 - val_my_r2: 0.9969\n",
      "Epoch 1844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5760e-04 - my_r2: 0.9381 - val_loss: 1.0596e-05 - val_my_r2: 0.9968\n",
      "Epoch 1845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3521e-04 - my_r2: 0.9497 - val_loss: 1.6403e-05 - val_my_r2: 0.9946\n",
      "Epoch 1846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6209e-04 - my_r2: 0.9234 - val_loss: 1.0373e-05 - val_my_r2: 0.9968\n",
      "Epoch 1847/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8311e-04 - my_r2: 0.9580 - val_loss: 9.7889e-06 - val_my_r2: 0.9971\n",
      "Epoch 1848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1348e-04 - my_r2: 0.9531 - val_loss: 1.4177e-05 - val_my_r2: 0.9955\n",
      "Epoch 1849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2328e-04 - my_r2: 0.9362 - val_loss: 1.7957e-05 - val_my_r2: 0.9939\n",
      "Epoch 1850/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4252e-04 - my_r2: 0.9441 - val_loss: 1.5209e-05 - val_my_r2: 0.9951\n",
      "Epoch 1851/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6637e-04 - my_r2: 0.8929 - val_loss: 1.4688e-05 - val_my_r2: 0.9953\n",
      "Epoch 1852/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4991e-04 - my_r2: 0.8928 - val_loss: 1.1515e-05 - val_my_r2: 0.9965\n",
      "Epoch 1853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2075e-04 - my_r2: 0.9370 - val_loss: 1.3988e-05 - val_my_r2: 0.9957\n",
      "Epoch 1854/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3042e-04 - my_r2: 0.8746 - val_loss: 1.2216e-05 - val_my_r2: 0.9963\n",
      "Epoch 1855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7784e-04 - my_r2: 0.9370 - val_loss: 9.4100e-06 - val_my_r2: 0.9971\n",
      "Epoch 1856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4067e-04 - my_r2: 0.9217 - val_loss: 1.0076e-05 - val_my_r2: 0.9970\n",
      "Epoch 1857/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4234e-04 - my_r2: 0.9279 - val_loss: 1.0338e-05 - val_my_r2: 0.9968\n",
      "Epoch 1858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2508e-04 - my_r2: 0.9374 - val_loss: 1.3001e-05 - val_my_r2: 0.9960\n",
      "Epoch 1859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4804e-04 - my_r2: 0.9080 - val_loss: 1.0929e-05 - val_my_r2: 0.9967\n",
      "Epoch 1860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5311e-04 - my_r2: 0.9383 - val_loss: 1.0470e-05 - val_my_r2: 0.9966\n",
      "Epoch 1861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3920e-04 - my_r2: 0.9471 - val_loss: 1.1426e-05 - val_my_r2: 0.9962\n",
      "Epoch 1862/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9796e-04 - my_r2: 0.9647 - val_loss: 1.0368e-05 - val_my_r2: 0.9967\n",
      "Epoch 1863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3531e-04 - my_r2: 0.9407 - val_loss: 9.7096e-06 - val_my_r2: 0.9971\n",
      "Epoch 1864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5580e-04 - my_r2: 0.9479 - val_loss: 1.0143e-05 - val_my_r2: 0.9969\n",
      "Epoch 1865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2203e-04 - my_r2: 0.9180 - val_loss: 9.4506e-06 - val_my_r2: 0.9973\n",
      "Epoch 1866/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5051e-04 - my_r2: 0.9338 - val_loss: 1.1950e-05 - val_my_r2: 0.9965\n",
      "Epoch 1867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7347e-04 - my_r2: 0.9366 - val_loss: 1.1678e-05 - val_my_r2: 0.9966\n",
      "Epoch 1868/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4167e-04 - my_r2: 0.9273 - val_loss: 8.6955e-06 - val_my_r2: 0.9972\n",
      "Epoch 1869/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0738e-04 - my_r2: 0.9299 - val_loss: 8.0576e-06 - val_my_r2: 0.9976\n",
      "Epoch 1870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3199e-04 - my_r2: 0.9154 - val_loss: 8.8147e-06 - val_my_r2: 0.9973\n",
      "Epoch 1871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3843e-04 - my_r2: 0.9468 - val_loss: 8.9427e-06 - val_my_r2: 0.9973\n",
      "Epoch 1872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2587e-04 - my_r2: 0.9318 - val_loss: 9.8040e-06 - val_my_r2: 0.9970\n",
      "Epoch 1873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8617e-04 - my_r2: 0.9218 - val_loss: 1.1070e-05 - val_my_r2: 0.9965\n",
      "Epoch 1874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8749e-04 - my_r2: 0.9447 - val_loss: 1.0919e-05 - val_my_r2: 0.9964\n",
      "Epoch 1875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7335e-04 - my_r2: 0.9056 - val_loss: 8.8579e-06 - val_my_r2: 0.9972\n",
      "Epoch 1876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3339e-04 - my_r2: 0.9374 - val_loss: 9.4483e-06 - val_my_r2: 0.9972\n",
      "Epoch 1877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1914e-04 - my_r2: 0.9546 - val_loss: 1.0550e-05 - val_my_r2: 0.9967\n",
      "Epoch 1878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7710e-04 - my_r2: 0.9001 - val_loss: 7.5452e-06 - val_my_r2: 0.9976\n",
      "Epoch 1879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7168e-04 - my_r2: 0.8763 - val_loss: 6.4860e-06 - val_my_r2: 0.9981\n",
      "Epoch 1880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6841e-04 - my_r2: 0.9119 - val_loss: 7.2595e-06 - val_my_r2: 0.9977\n",
      "Epoch 1881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9154e-04 - my_r2: 0.9605 - val_loss: 8.4200e-06 - val_my_r2: 0.9969\n",
      "Epoch 1882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9292e-04 - my_r2: 0.9521 - val_loss: 8.6075e-06 - val_my_r2: 0.9968\n",
      "Epoch 1883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4114e-04 - my_r2: 0.9427 - val_loss: 6.1131e-06 - val_my_r2: 0.9978\n",
      "Epoch 1884/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5991e-04 - my_r2: 0.9194 - val_loss: 6.0165e-06 - val_my_r2: 0.9979\n",
      "Epoch 1885/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1699e-04 - my_r2: 0.9350 - val_loss: 7.7636e-06 - val_my_r2: 0.9972\n",
      "Epoch 1886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6356e-04 - my_r2: 0.8816 - val_loss: 9.5594e-06 - val_my_r2: 0.9962\n",
      "Epoch 1887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0013e-04 - my_r2: 0.9625 - val_loss: 9.2199e-06 - val_my_r2: 0.9963\n",
      "Epoch 1888/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9712e-04 - my_r2: 0.9363 - val_loss: 6.9092e-06 - val_my_r2: 0.9973\n",
      "Epoch 1889/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8245e-04 - my_r2: 0.9209 - val_loss: 5.5634e-06 - val_my_r2: 0.9977\n",
      "Epoch 1890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8721e-04 - my_r2: 0.8646 - val_loss: 1.2065e-05 - val_my_r2: 0.9957\n",
      "Epoch 1891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9991e-04 - my_r2: 0.9591 - val_loss: 9.9425e-06 - val_my_r2: 0.9965\n",
      "Epoch 1892/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8691e-04 - my_r2: 0.9245 - val_loss: 1.5885e-05 - val_my_r2: 0.9945\n",
      "Epoch 1893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1900e-04 - my_r2: 0.9247 - val_loss: 1.0564e-05 - val_my_r2: 0.9964\n",
      "Epoch 1894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0936e-04 - my_r2: 0.9327 - val_loss: 9.3643e-06 - val_my_r2: 0.9967\n",
      "Epoch 1895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3130e-04 - my_r2: 0.8435 - val_loss: 7.5080e-06 - val_my_r2: 0.9974\n",
      "Epoch 1896/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2464e-04 - my_r2: 0.9148 - val_loss: 8.0276e-06 - val_my_r2: 0.9974\n",
      "Epoch 1897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6148e-04 - my_r2: 0.9236 - val_loss: 1.1423e-05 - val_my_r2: 0.9963\n",
      "Epoch 1898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4445e-04 - my_r2: 0.9433 - val_loss: 8.7548e-06 - val_my_r2: 0.9974\n",
      "Epoch 1899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7212e-04 - my_r2: 0.9415 - val_loss: 1.0678e-05 - val_my_r2: 0.9967\n",
      "Epoch 1900/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7218e-04 - my_r2: 0.9456 - val_loss: 7.9468e-06 - val_my_r2: 0.9974\n",
      "Epoch 1901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3989e-04 - my_r2: 0.9463 - val_loss: 1.1922e-05 - val_my_r2: 0.9960\n",
      "Epoch 1902/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1076e-04 - my_r2: 0.9087 - val_loss: 1.2380e-05 - val_my_r2: 0.9960\n",
      "Epoch 1903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3996e-04 - my_r2: 0.9521 - val_loss: 1.0709e-05 - val_my_r2: 0.9962\n",
      "Epoch 1904/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6188e-04 - my_r2: 0.9542 - val_loss: 1.0402e-05 - val_my_r2: 0.9962\n",
      "Epoch 1905/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0712e-04 - my_r2: 0.9303 - val_loss: 9.6394e-06 - val_my_r2: 0.9967\n",
      "Epoch 1906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2711e-04 - my_r2: 0.9473 - val_loss: 1.2571e-05 - val_my_r2: 0.9955\n",
      "Epoch 1907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3651e-04 - my_r2: 0.9460 - val_loss: 1.0756e-05 - val_my_r2: 0.9965\n",
      "Epoch 1908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3052e-04 - my_r2: 0.9322 - val_loss: 1.0534e-05 - val_my_r2: 0.9966\n",
      "Epoch 1909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3854e-04 - my_r2: 0.9432 - val_loss: 8.1323e-06 - val_my_r2: 0.9973\n",
      "Epoch 1910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1273e-04 - my_r2: 0.8939 - val_loss: 1.3134e-05 - val_my_r2: 0.9958\n",
      "Epoch 1911/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6138e-04 - my_r2: 0.8745 - val_loss: 1.3047e-05 - val_my_r2: 0.9958\n",
      "Epoch 1912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8355e-04 - my_r2: 0.9430 - val_loss: 1.2606e-05 - val_my_r2: 0.9961\n",
      "Epoch 1913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5119e-04 - my_r2: 0.9197 - val_loss: 1.0063e-05 - val_my_r2: 0.9970\n",
      "Epoch 1914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4648e-04 - my_r2: 0.9409 - val_loss: 9.1871e-06 - val_my_r2: 0.9968\n",
      "Epoch 1915/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8358e-04 - my_r2: 0.9193 - val_loss: 1.2639e-05 - val_my_r2: 0.9955\n",
      "Epoch 1916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0166e-04 - my_r2: 0.9544 - val_loss: 1.5314e-05 - val_my_r2: 0.9951\n",
      "Epoch 1917/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2034e-04 - my_r2: 0.9519 - val_loss: 1.1214e-05 - val_my_r2: 0.9965\n",
      "Epoch 1918/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3562e-04 - my_r2: 0.9317 - val_loss: 1.0781e-05 - val_my_r2: 0.9968\n",
      "Epoch 1919/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4144e-04 - my_r2: 0.9409 - val_loss: 1.0165e-05 - val_my_r2: 0.9968\n",
      "Epoch 1920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8501e-04 - my_r2: 0.9168 - val_loss: 9.5787e-06 - val_my_r2: 0.9968\n",
      "Epoch 1921/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2847e-04 - my_r2: 0.9074 - val_loss: 8.5330e-06 - val_my_r2: 0.9971\n",
      "Epoch 1922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0778e-04 - my_r2: 0.9273 - val_loss: 8.6918e-06 - val_my_r2: 0.9967\n",
      "Epoch 1923/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4547e-04 - my_r2: 0.9436 - val_loss: 1.2152e-05 - val_my_r2: 0.9957\n",
      "Epoch 1924/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6399e-04 - my_r2: 0.9118 - val_loss: 9.7532e-06 - val_my_r2: 0.9970\n",
      "Epoch 1925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8393e-04 - my_r2: 0.9150 - val_loss: 9.3582e-06 - val_my_r2: 0.9970\n",
      "Epoch 1926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4129e-04 - my_r2: 0.8614 - val_loss: 8.6137e-06 - val_my_r2: 0.9972\n",
      "Epoch 1927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4456e-04 - my_r2: 0.9347 - val_loss: 1.0892e-05 - val_my_r2: 0.9965\n",
      "Epoch 1928/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3376e-04 - my_r2: 0.9538 - val_loss: 1.0268e-05 - val_my_r2: 0.9962\n",
      "Epoch 1929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6802e-04 - my_r2: 0.9007 - val_loss: 7.9139e-06 - val_my_r2: 0.9971\n",
      "Epoch 1930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5077e-04 - my_r2: 0.9447 - val_loss: 7.4269e-06 - val_my_r2: 0.9972\n",
      "Epoch 1931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5986e-04 - my_r2: 0.9287 - val_loss: 8.6711e-06 - val_my_r2: 0.9964\n",
      "Epoch 1932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5239e-04 - my_r2: 0.9348 - val_loss: 9.0542e-06 - val_my_r2: 0.9965\n",
      "Epoch 1933/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9582e-04 - my_r2: 0.9491 - val_loss: 1.0714e-05 - val_my_r2: 0.9960\n",
      "Epoch 1934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0310e-04 - my_r2: 0.9355 - val_loss: 1.5033e-05 - val_my_r2: 0.9951\n",
      "Epoch 1935/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5902e-04 - my_r2: 0.9319 - val_loss: 1.0699e-05 - val_my_r2: 0.9968\n",
      "Epoch 1936/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0147e-04 - my_r2: 0.9376 - val_loss: 1.0294e-05 - val_my_r2: 0.9966\n",
      "Epoch 1937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6217e-04 - my_r2: 0.8499 - val_loss: 9.3752e-06 - val_my_r2: 0.9967\n",
      "Epoch 1938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2127e-04 - my_r2: 0.9400 - val_loss: 1.0661e-05 - val_my_r2: 0.9960\n",
      "Epoch 1939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8001e-04 - my_r2: 0.9385 - val_loss: 1.1920e-05 - val_my_r2: 0.9956\n",
      "Epoch 1940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5742e-04 - my_r2: 0.9435 - val_loss: 9.8507e-06 - val_my_r2: 0.9964\n",
      "Epoch 1941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3055e-04 - my_r2: 0.9238 - val_loss: 6.1348e-06 - val_my_r2: 0.9975\n",
      "Epoch 1942/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7782e-04 - my_r2: 0.9299 - val_loss: 9.2650e-06 - val_my_r2: 0.9968\n",
      "Epoch 1943/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0147e-04 - my_r2: 0.9262 - val_loss: 9.3138e-06 - val_my_r2: 0.9968\n",
      "Epoch 1944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4697e-04 - my_r2: 0.9448 - val_loss: 1.0267e-05 - val_my_r2: 0.9965\n",
      "Epoch 1945/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1529e-04 - my_r2: 0.9128 - val_loss: 1.0075e-05 - val_my_r2: 0.9968\n",
      "Epoch 1946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5348e-04 - my_r2: 0.9181 - val_loss: 8.9211e-06 - val_my_r2: 0.9966\n",
      "Epoch 1947/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0713e-04 - my_r2: 0.9246 - val_loss: 8.8560e-06 - val_my_r2: 0.9966\n",
      "Epoch 1948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8372e-04 - my_r2: 0.9455 - val_loss: 8.4352e-06 - val_my_r2: 0.9972\n",
      "Epoch 1949/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1735e-04 - my_r2: 0.9281 - val_loss: 7.0352e-06 - val_my_r2: 0.9976\n",
      "Epoch 1950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4722e-04 - my_r2: 0.9095 - val_loss: 1.0056e-05 - val_my_r2: 0.9961\n",
      "Epoch 1951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6295e-04 - my_r2: 0.9047 - val_loss: 8.0621e-06 - val_my_r2: 0.9968\n",
      "Epoch 1952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4744e-04 - my_r2: 0.9297 - val_loss: 9.5635e-06 - val_my_r2: 0.9967\n",
      "Epoch 1953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6770e-04 - my_r2: 0.9148 - val_loss: 8.3331e-06 - val_my_r2: 0.9970\n",
      "Epoch 1954/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 2.9366e-04 - my_r2: 0.9435 - val_loss: 5.9130e-06 - val_my_r2: 0.9979\n",
      "Epoch 1955/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.5549e-04 - my_r2: 0.9515 - val_loss: 1.0105e-05 - val_my_r2: 0.9966\n",
      "Epoch 1956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4761e-04 - my_r2: 0.9366 - val_loss: 8.5903e-06 - val_my_r2: 0.9969\n",
      "Epoch 1957/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2635e-04 - my_r2: 0.9524 - val_loss: 9.1370e-06 - val_my_r2: 0.9966\n",
      "Epoch 1958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9105e-04 - my_r2: 0.9127 - val_loss: 1.0435e-05 - val_my_r2: 0.9961\n",
      "Epoch 1959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4039e-04 - my_r2: 0.9157 - val_loss: 1.0385e-05 - val_my_r2: 0.9962\n",
      "Epoch 1960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3916e-04 - my_r2: 0.9331 - val_loss: 9.4200e-06 - val_my_r2: 0.9965\n",
      "Epoch 1961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6072e-04 - my_r2: 0.9372 - val_loss: 8.9491e-06 - val_my_r2: 0.9965\n",
      "Epoch 1962/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3976e-04 - my_r2: 0.8767 - val_loss: 9.5061e-06 - val_my_r2: 0.9961\n",
      "Epoch 1963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6876e-04 - my_r2: 0.9219 - val_loss: 1.1079e-05 - val_my_r2: 0.9956\n",
      "Epoch 1964/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4907e-04 - my_r2: 0.9336 - val_loss: 1.0090e-05 - val_my_r2: 0.9962\n",
      "Epoch 1965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7186e-04 - my_r2: 0.9343 - val_loss: 1.0347e-05 - val_my_r2: 0.9957\n",
      "Epoch 1966/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7351e-04 - my_r2: 0.9561 - val_loss: 1.2136e-05 - val_my_r2: 0.9949\n",
      "Epoch 1967/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2876e-04 - my_r2: 0.9168 - val_loss: 1.4152e-05 - val_my_r2: 0.9940\n",
      "Epoch 1968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8520e-04 - my_r2: 0.9400 - val_loss: 1.5656e-05 - val_my_r2: 0.9931\n",
      "Epoch 1969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8217e-04 - my_r2: 0.9356 - val_loss: 1.1607e-05 - val_my_r2: 0.9951\n",
      "Epoch 1970/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8414e-04 - my_r2: 0.9392 - val_loss: 1.1009e-05 - val_my_r2: 0.9960\n",
      "Epoch 1971/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5790e-04 - my_r2: 0.9111 - val_loss: 9.3542e-06 - val_my_r2: 0.9967\n",
      "Epoch 1972/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8169e-04 - my_r2: 0.9124 - val_loss: 1.0000e-05 - val_my_r2: 0.9960\n",
      "Epoch 1973/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8033e-04 - my_r2: 0.9631 - val_loss: 8.3129e-06 - val_my_r2: 0.9966\n",
      "Epoch 1974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9674e-04 - my_r2: 0.9562 - val_loss: 6.9018e-06 - val_my_r2: 0.9974\n",
      "Epoch 1975/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5390e-04 - my_r2: 0.8939 - val_loss: 9.7677e-06 - val_my_r2: 0.9966\n",
      "Epoch 1976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9134e-04 - my_r2: 0.9215 - val_loss: 1.4015e-05 - val_my_r2: 0.9952\n",
      "Epoch 1977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6704e-04 - my_r2: 0.8985 - val_loss: 1.1592e-05 - val_my_r2: 0.9959\n",
      "Epoch 1978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7660e-04 - my_r2: 0.8778 - val_loss: 7.6572e-06 - val_my_r2: 0.9972\n",
      "Epoch 1979/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3866e-04 - my_r2: 0.8818 - val_loss: 9.1469e-06 - val_my_r2: 0.9968\n",
      "Epoch 1980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4043e-04 - my_r2: 0.9454 - val_loss: 7.4161e-06 - val_my_r2: 0.9973\n",
      "Epoch 1981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2212e-04 - my_r2: 0.9424 - val_loss: 8.1376e-06 - val_my_r2: 0.9971\n",
      "Epoch 1982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4215e-04 - my_r2: 0.8275 - val_loss: 7.7291e-06 - val_my_r2: 0.9974\n",
      "Epoch 1983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7798e-04 - my_r2: 0.9280 - val_loss: 9.5808e-06 - val_my_r2: 0.9971\n",
      "Epoch 1984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6384e-04 - my_r2: 0.9411 - val_loss: 7.3181e-06 - val_my_r2: 0.9978\n",
      "Epoch 1985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3293e-04 - my_r2: 0.9294 - val_loss: 9.1501e-06 - val_my_r2: 0.9974\n",
      "Epoch 1986/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3715e-04 - my_r2: 0.9344 - val_loss: 1.0698e-05 - val_my_r2: 0.9968\n",
      "Epoch 1987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4108e-04 - my_r2: 0.9416 - val_loss: 1.0819e-05 - val_my_r2: 0.9969\n",
      "Epoch 1988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2683e-04 - my_r2: 0.9400 - val_loss: 1.3320e-05 - val_my_r2: 0.9958\n",
      "Epoch 1989/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8988e-04 - my_r2: 0.8887 - val_loss: 1.5510e-05 - val_my_r2: 0.9948\n",
      "Epoch 1990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8636e-04 - my_r2: 0.9259 - val_loss: 8.6546e-06 - val_my_r2: 0.9973\n",
      "Epoch 1991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5635e-04 - my_r2: 0.9274 - val_loss: 1.0229e-05 - val_my_r2: 0.9967\n",
      "Epoch 1992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6547e-04 - my_r2: 0.8298 - val_loss: 1.2679e-05 - val_my_r2: 0.9963\n",
      "Epoch 1993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7067e-04 - my_r2: 0.9387 - val_loss: 1.0668e-05 - val_my_r2: 0.9971\n",
      "Epoch 1994/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5094e-04 - my_r2: 0.9443 - val_loss: 7.6840e-06 - val_my_r2: 0.9976\n",
      "Epoch 1995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0798e-04 - my_r2: 0.9454 - val_loss: 6.9495e-06 - val_my_r2: 0.9978\n",
      "Epoch 1996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6366e-04 - my_r2: 0.9074 - val_loss: 9.1937e-06 - val_my_r2: 0.9969\n",
      "Epoch 1997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6464e-04 - my_r2: 0.8894 - val_loss: 9.2463e-06 - val_my_r2: 0.9974\n",
      "Epoch 1998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5346e-04 - my_r2: 0.8463 - val_loss: 7.2156e-06 - val_my_r2: 0.9978\n",
      "Epoch 1999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2746e-04 - my_r2: 0.9518 - val_loss: 1.0175e-05 - val_my_r2: 0.9968\n",
      "Epoch 2000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0158e-04 - my_r2: 0.9578 - val_loss: 1.0274e-05 - val_my_r2: 0.9966\n",
      "---------- 1\n",
      "---------- 1\n",
      "train = 1.00 test = 1.00 loss-train = -1.000000 loss-test = -1.000000 iter=0\n",
      "Stats for iML1515_ec6_UB_AMN_QP CPU-time 1499.8865\n",
      "R2 = 0.9986 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Q2 = 0.9986 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Iter 0 Collated Q2 0.9985552063293269\n",
      "number of reactions:  1186 1186\n",
      "number of metabolites:  2084\n",
      "filtered measurements size:  1\n",
      "RC reservoir file: ./Reservoir/iML1515_ec6_UB_AMN_QP\n",
      "RC model type: RC\n",
      "RC scaler: 0.0\n",
      "RC model input dim: 38\n",
      "RC model output dim: 1\n",
      "RC model medium bound: UB\n",
      "training set size (110, 38) (110, 1)\n",
      "reservoir S, Pin, Pout matrices (2084, 1186) (38, 1186) (1, 1186)\n",
      "RC training epochs: 2000\n",
      "RC training regression: True\n",
      "RC training learn rate: 0.0001\n",
      "RC training dropout: 0.25\n",
      "RC training batch size: 5\n",
      "RC training validation iter: 0\n",
      "RC training xfold: 0\n",
      "RC training early stopping: False\n",
      "--------prior network --------\n",
      "training file: None\n",
      "model type: ANN_Dense\n",
      "model scaler: 0.0\n",
      "model input dim: 10\n",
      "model output dim: 10\n",
      "model medium bound: \n",
      "timestep: 0\n",
      "no training set provided\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "--------reservoir network-----\n",
      "training file: ./Dataset_model/iML1515_ec6_UB\n",
      "model type: AMN_QP\n",
      "model scaler: 7.95\n",
      "model input dim: 38\n",
      "model output dim: 2376\n",
      "model medium bound: UB\n",
      "timestep: 4\n",
      "training set size (11000, 38) (11000, 1)\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "gradient learn rate: 0.001\n",
      "gradient decay rate: 0.9\n",
      "training epochs: 20\n",
      "training regression: True\n",
      "training learn rate: 0.001\n",
      "training dropout: 0.25\n",
      "training batch size: 100\n",
      "training validation iter: 0\n",
      "training xfold: 5\n",
      "training early stopping: False\n",
      "AMN scaler 0.0\n",
      "RC input shape (110, 38) (110, 1)\n",
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "Physical devices cannot be modified after being initialized\n",
      "----------------------------------- RC\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 10 relu True\n",
      "Prior inputs and outputs (None, 10) (None, 10)\n",
      "Res inputs added to Prior_outputs 28\n",
      "Res inputs (final) (None, 38)\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 1186 relu False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 13:19:01.418659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMN output shapes for PoutV, SV, PinV, Vpos, V, outputs (None, 1) (None, 1) (None, 1) (None, 1) (None, 1186) (None, 2376)\n",
      "=======================\n",
      "PoutV: (None, 1)\n",
      "SV: (None, 1)\n",
      "PinV: (None, 1)\n",
      "Vpos: (None, 1)\n",
      "V: (None, 1186)\n",
      "V0: KerasTensor(type_spec=TensorSpec(shape=(None, 1186), dtype=tf.float32, name=None), name='tf.__operators__.add_15/AddV2:0', description=\"created by layer 'tf.__operators__.add_15'\")\n",
      "Vin: KerasTensor(type_spec=TensorSpec(shape=(None, 38), dtype=tf.float32, name=None), name='tf.math.truediv_32/truediv:0', description=\"created by layer 'tf.math.truediv_32'\")\n",
      "Vout: tf.Tensor([], shape=(0, 0), dtype=float32)\n",
      "Res_outputs-------------------- (None, 2376)\n",
      "SV, PinV, Vpos, V-------------- (None, 1) (None, 1) (None, 1) (None, 1186)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_8 (Lambda)              (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 500)          5500        ['lambda_8[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 500)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 28)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 10)           5010        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 38)           0           ['lambda_7[0][0]',               \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_14 (TFOp  (None, 38)          0           ['input_2[0][0]',                \n",
      " Lambda)                                                          'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_28 (TFOpLambd  (None, 38)          0           ['concatenate_3[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_14[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_32 (TFOpLambda  (None, 38)          0           ['tf.math.multiply_28[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 500)          19500       ['tf.math.truediv_32[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 500)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1186)         594186      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_20 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_32[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_11 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_20[0][0]',    \n",
      " a)                                                               'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " tf.nn.relu_11 (TFOpLambda)     (None, 1186)         0           ['tf.math.subtract_11[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_15 (TFOp  (None, 1186)        0           ['tf.nn.relu_11[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_11[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract_12 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_15[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_29 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_15[0][0]'\n",
      " a)                                                              , 'dense_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply_30 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_12[0][0]',    \n",
      " a)                                                               'tf.linalg.matmul_20[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_31 (TFOpLambd  (None, 1186)        0           ['dense_7[0][0]']                \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 1186)        0           ['tf.math.multiply_29[0][0]',    \n",
      " ambda)                                                           'tf.math.multiply_30[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 1186)        0           ['tf.math.multiply_31[0][0]',    \n",
      " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.linalg.matmul_23 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_15[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_13 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_23[0][0]',    \n",
      " a)                                                               'tf.math.truediv_32[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_12 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_13[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_9 (TFOpLambda  (None, 1186)        0           ['tf.__operators__.add_15[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_21 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_15[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_16 (TFOp  (None, 38)          0           ['tf.nn.relu_12[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_12[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_13 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_9[0][0]']     \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_22 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_21[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_33 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_12[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_16[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_17 (TFOp  (None, 1186)        0           ['tf.nn.relu_13[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_13[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_34 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_22[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_24 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_33[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_10 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_17[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_35 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_34[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_37 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_24[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_34 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_13[0][0]',          \n",
      " a)                                                               'tf.math.negative_10[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 1186)        0           ['tf.math.truediv_35[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_37[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_39 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_34[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_16[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_39[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_32 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_15[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_35 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_17[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_36 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_32[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_37 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_35[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_14 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_36[0][0]',    \n",
      " a)                                                               'tf.math.multiply_37[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_15[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_14[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_27 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_18[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_15 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_27[0][0]',    \n",
      " a)                                                               'tf.math.truediv_32[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_14 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_15[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_11 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_18[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_25 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_18[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_19 (TFOp  (None, 38)          0           ['tf.nn.relu_14[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_14[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_15 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_11[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_26 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_25[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_38 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_14[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_19[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_20 (TFOp  (None, 1186)        0           ['tf.nn.relu_15[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_15[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_41 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_26[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_28 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_38[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_12 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_20[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_42 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_41[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_44 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_28[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_39 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_15[0][0]',          \n",
      " a)                                                               'tf.math.negative_12[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 1186)        0           ['tf.math.truediv_42[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_44[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_46 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_39[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_19[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_46[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_40 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_20[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_41 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_14[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_42 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_40[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_16 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_41[0][0]',    \n",
      " a)                                                               'tf.math.multiply_42[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_18[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_16[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_31 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_21[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_17 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_31[0][0]',    \n",
      " a)                                                               'tf.math.truediv_32[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_16 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_17[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_13 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_21[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_29 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_21[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_22 (TFOp  (None, 38)          0           ['tf.nn.relu_16[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_16[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_17 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_13[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_30 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_29[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_43 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_16[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_22[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_23 (TFOp  (None, 1186)        0           ['tf.nn.relu_17[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_17[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_48 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_30[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_32 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_43[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_14 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_23[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_49 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_48[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_51 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_32[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_44 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_17[0][0]',          \n",
      " a)                                                               'tf.math.negative_14[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 1186)        0           ['tf.math.truediv_49[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_51[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_53 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_44[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_22[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_53[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_45 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_23[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_46 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_16[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_47 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_45[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_18 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_46[0][0]',    \n",
      " a)                                                               'tf.math.multiply_47[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_21[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_18[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_35 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_24[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_19 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_35[0][0]',    \n",
      " a)                                                               'tf.math.truediv_32[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_18 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_19[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_15 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_24[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_33 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_24[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_25 (TFOp  (None, 38)          0           ['tf.nn.relu_18[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_18[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_19 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_15[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_34 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_33[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_48 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_18[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_25[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_26 (TFOp  (None, 1186)        0           ['tf.nn.relu_19[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_19[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_55 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_34[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_36 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_48[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_16 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_26[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_56 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_55[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_58 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_36[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_49 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_19[0][0]',          \n",
      " a)                                                               'tf.math.negative_16[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 1186)        0           ['tf.math.truediv_56[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_58[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_60 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_49[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_25[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_60[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_50 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_26[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_51 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_18[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_52 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_50[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_20 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_51[0][0]',    \n",
      " a)                                                               'tf.math.multiply_52[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_24[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_20[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_39 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_27[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_21 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_39[0][0]',    \n",
      " a)                                                               'tf.math.truediv_32[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.negative_17 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_27[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_38 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_27[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.nn.relu_20 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_21[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_21 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_17[0][0]']    \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_27 (TFOpLamb  (None, 1)           0           ['tf.linalg.matmul_38[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_28 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_20[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_29 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_21[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_37 (TFOpLambd  (None, 1)           0           ['tf.__operators__.add_27[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_61 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_27[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_62 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_28[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_63 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_29[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 2376)         0           ['tf.linalg.matmul_37[0][0]',    \n",
      "                                                                  'tf.math.truediv_61[0][0]',     \n",
      "                                                                  'tf.math.truediv_62[0][0]',     \n",
      "                                                                  'tf.math.truediv_63[0][0]',     \n",
      "                                                                  'tf.__operators__.add_27[0][0]',\n",
      "                                                                  'tf.__operators__.add_15[0][0]']\n",
      "                                                                                                  \n",
      " lambda_9 (Lambda)              (None, 1)            0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_10 (Lambda)             (None, 1)            0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_11 (Lambda)             (None, 1)            0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_12 (Lambda)             (None, 1)            0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_13 (Lambda)             (None, 1186)         0           ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 1228)         0           ['lambda_9[0][0]',               \n",
      "                                                                  'lambda_10[0][0]',              \n",
      "                                                                  'lambda_11[0][0]',              \n",
      "                                                                  'lambda_12[0][0]',              \n",
      "                                                                  'lambda_13[0][0]',              \n",
      "                                                                  'tf.math.truediv_32[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 624,196\n",
      "Trainable params: 10,510\n",
      "Non-trainable params: 613,686\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "nbr parameters: 624196\n",
      "Epoch 1/2000\n",
      "22/22 [==============================] - 2s 46ms/step - loss: 0.0372 - my_r2: -9.7330 - val_loss: 0.0349 - val_my_r2: -9.8637\n",
      "Epoch 2/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0332 - my_r2: -6.5252 - val_loss: 0.0306 - val_my_r2: -8.5182\n",
      "Epoch 3/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0276 - my_r2: -4.9042 - val_loss: 0.0262 - val_my_r2: -7.1741\n",
      "Epoch 4/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0245 - my_r2: -8.5769 - val_loss: 0.0224 - val_my_r2: -6.0131\n",
      "Epoch 5/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0206 - my_r2: -5.0841 - val_loss: 0.0186 - val_my_r2: -4.8525\n",
      "Epoch 6/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0181 - my_r2: -3.1778 - val_loss: 0.0156 - val_my_r2: -3.9002\n",
      "Epoch 7/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0144 - my_r2: -1.8589 - val_loss: 0.0130 - val_my_r2: -3.0792\n",
      "Epoch 8/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0127 - my_r2: -4.4471 - val_loss: 0.0109 - val_my_r2: -2.4314\n",
      "Epoch 9/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0108 - my_r2: -5.0129 - val_loss: 0.0093 - val_my_r2: -1.9260\n",
      "Epoch 10/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0089 - my_r2: -0.8383 - val_loss: 0.0080 - val_my_r2: -1.5217\n",
      "Epoch 11/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0077 - my_r2: -1.2851 - val_loss: 0.0070 - val_my_r2: -1.1996\n",
      "Epoch 12/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0068 - my_r2: -1.4275 - val_loss: 0.0062 - val_my_r2: -0.9644\n",
      "Epoch 13/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0060 - my_r2: -0.1877 - val_loss: 0.0055 - val_my_r2: -0.7482\n",
      "Epoch 14/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0059 - my_r2: -0.5858 - val_loss: 0.0049 - val_my_r2: -0.5567\n",
      "Epoch 15/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0048 - my_r2: -0.2635 - val_loss: 0.0045 - val_my_r2: -0.4226\n",
      "Epoch 16/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0046 - my_r2: 0.0605 - val_loss: 0.0041 - val_my_r2: -0.3027\n",
      "Epoch 17/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0044 - my_r2: -0.5175 - val_loss: 0.0038 - val_my_r2: -0.1995\n",
      "Epoch 18/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0035 - my_r2: 0.2709 - val_loss: 0.0035 - val_my_r2: -0.1222\n",
      "Epoch 19/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0037 - my_r2: -0.1144 - val_loss: 0.0033 - val_my_r2: -0.0543\n",
      "Epoch 20/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0032 - my_r2: -0.0226 - val_loss: 0.0031 - val_my_r2: 0.0050\n",
      "Epoch 21/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0034 - my_r2: -0.2624 - val_loss: 0.0030 - val_my_r2: 0.0531\n",
      "Epoch 22/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0030 - my_r2: 0.2267 - val_loss: 0.0028 - val_my_r2: 0.0997\n",
      "Epoch 23/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0031 - my_r2: -0.1679 - val_loss: 0.0027 - val_my_r2: 0.1413\n",
      "Epoch 24/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0031 - my_r2: 0.1536 - val_loss: 0.0026 - val_my_r2: 0.1754\n",
      "Epoch 25/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0031 - my_r2: 0.3467 - val_loss: 0.0025 - val_my_r2: 0.2092\n",
      "Epoch 26/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0026 - my_r2: 0.1161 - val_loss: 0.0024 - val_my_r2: 0.2385\n",
      "Epoch 27/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0026 - my_r2: 0.4710 - val_loss: 0.0023 - val_my_r2: 0.2680\n",
      "Epoch 28/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0025 - my_r2: 0.2562 - val_loss: 0.0022 - val_my_r2: 0.2934\n",
      "Epoch 29/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0027 - my_r2: 0.1596 - val_loss: 0.0021 - val_my_r2: 0.3177\n",
      "Epoch 30/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0022 - my_r2: 0.5359 - val_loss: 0.0021 - val_my_r2: 0.3387\n",
      "Epoch 31/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0022 - my_r2: 0.2491 - val_loss: 0.0020 - val_my_r2: 0.3580\n",
      "Epoch 32/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0024 - my_r2: 0.2328 - val_loss: 0.0019 - val_my_r2: 0.3777\n",
      "Epoch 33/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0023 - my_r2: 0.2334 - val_loss: 0.0019 - val_my_r2: 0.3961\n",
      "Epoch 34/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0020 - my_r2: 0.0704 - val_loss: 0.0018 - val_my_r2: 0.4123\n",
      "Epoch 35/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: -0.1671 - val_loss: 0.0018 - val_my_r2: 0.4270\n",
      "Epoch 36/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0018 - my_r2: 0.2828 - val_loss: 0.0017 - val_my_r2: 0.4398\n",
      "Epoch 37/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0019 - my_r2: 0.4904 - val_loss: 0.0017 - val_my_r2: 0.4555\n",
      "Epoch 38/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0020 - my_r2: 0.1088 - val_loss: 0.0016 - val_my_r2: 0.4693\n",
      "Epoch 39/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0018 - my_r2: 0.4915 - val_loss: 0.0016 - val_my_r2: 0.4818\n",
      "Epoch 40/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0017 - my_r2: 0.4138 - val_loss: 0.0015 - val_my_r2: 0.4941\n",
      "Epoch 41/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.3666 - val_loss: 0.0015 - val_my_r2: 0.5064\n",
      "Epoch 42/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.5225 - val_loss: 0.0015 - val_my_r2: 0.5178\n",
      "Epoch 43/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.4929 - val_loss: 0.0014 - val_my_r2: 0.5293\n",
      "Epoch 44/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0018 - my_r2: 0.4088 - val_loss: 0.0014 - val_my_r2: 0.5397\n",
      "Epoch 45/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0017 - my_r2: 0.5721 - val_loss: 0.0014 - val_my_r2: 0.5483\n",
      "Epoch 46/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.4154 - val_loss: 0.0013 - val_my_r2: 0.5590\n",
      "Epoch 47/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0015 - my_r2: 0.6108 - val_loss: 0.0013 - val_my_r2: 0.5676\n",
      "Epoch 48/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.6782 - val_loss: 0.0013 - val_my_r2: 0.5752\n",
      "Epoch 49/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0015 - my_r2: 0.5862 - val_loss: 0.0013 - val_my_r2: 0.5843\n",
      "Epoch 50/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0016 - my_r2: 0.4868 - val_loss: 0.0012 - val_my_r2: 0.5901\n",
      "Epoch 51/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.6716 - val_loss: 0.0012 - val_my_r2: 0.5985\n",
      "Epoch 52/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.6424 - val_loss: 0.0012 - val_my_r2: 0.6074\n",
      "Epoch 53/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.3529 - val_loss: 0.0012 - val_my_r2: 0.6125\n",
      "Epoch 54/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.4217 - val_loss: 0.0011 - val_my_r2: 0.6196\n",
      "Epoch 55/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6103 - val_loss: 0.0011 - val_my_r2: 0.6256\n",
      "Epoch 56/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.5676 - val_loss: 0.0011 - val_my_r2: 0.6325\n",
      "Epoch 57/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7634 - val_loss: 0.0011 - val_my_r2: 0.6378\n",
      "Epoch 58/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.5325 - val_loss: 0.0011 - val_my_r2: 0.6417\n",
      "Epoch 59/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.5144 - val_loss: 0.0010 - val_my_r2: 0.6477\n",
      "Epoch 60/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.6831 - val_loss: 0.0010 - val_my_r2: 0.6526\n",
      "Epoch 61/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.7021 - val_loss: 0.0010 - val_my_r2: 0.6586\n",
      "Epoch 62/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.7089 - val_loss: 9.9788e-04 - val_my_r2: 0.6629\n",
      "Epoch 63/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.6065 - val_loss: 9.8256e-04 - val_my_r2: 0.6683\n",
      "Epoch 64/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.6449 - val_loss: 9.6982e-04 - val_my_r2: 0.6744\n",
      "Epoch 65/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.6842 - val_loss: 9.5342e-04 - val_my_r2: 0.6783\n",
      "Epoch 66/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.7562 - val_loss: 9.3976e-04 - val_my_r2: 0.6816\n",
      "Epoch 67/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7601 - val_loss: 9.2757e-04 - val_my_r2: 0.6868\n",
      "Epoch 68/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.5059 - val_loss: 9.1277e-04 - val_my_r2: 0.6912\n",
      "Epoch 69/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6780 - val_loss: 9.0179e-04 - val_my_r2: 0.6953\n",
      "Epoch 70/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7631 - val_loss: 8.8873e-04 - val_my_r2: 0.6998\n",
      "Epoch 71/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.6268 - val_loss: 8.7931e-04 - val_my_r2: 0.7042\n",
      "Epoch 72/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.6436 - val_loss: 8.6745e-04 - val_my_r2: 0.7079\n",
      "Epoch 73/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.5967 - val_loss: 8.5136e-04 - val_my_r2: 0.7113\n",
      "Epoch 74/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.7910e-04 - my_r2: 0.7611 - val_loss: 8.4115e-04 - val_my_r2: 0.7147\n",
      "Epoch 75/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7200 - val_loss: 8.2889e-04 - val_my_r2: 0.7171\n",
      "Epoch 76/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7063 - val_loss: 8.1715e-04 - val_my_r2: 0.7209\n",
      "Epoch 77/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7243 - val_loss: 8.0308e-04 - val_my_r2: 0.7229\n",
      "Epoch 78/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7142 - val_loss: 7.9333e-04 - val_my_r2: 0.7265\n",
      "Epoch 79/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.9526e-04 - my_r2: 0.7862 - val_loss: 7.8385e-04 - val_my_r2: 0.7303\n",
      "Epoch 80/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.1199e-04 - my_r2: 0.6374 - val_loss: 7.7441e-04 - val_my_r2: 0.7316\n",
      "Epoch 81/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6523 - val_loss: 7.6419e-04 - val_my_r2: 0.7348\n",
      "Epoch 82/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.7520e-04 - my_r2: 0.5214 - val_loss: 7.5316e-04 - val_my_r2: 0.7379\n",
      "Epoch 83/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7384 - val_loss: 7.4236e-04 - val_my_r2: 0.7436\n",
      "Epoch 84/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8346e-04 - my_r2: 0.8181 - val_loss: 7.3303e-04 - val_my_r2: 0.7450\n",
      "Epoch 85/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.1455e-04 - my_r2: 0.6377 - val_loss: 7.2457e-04 - val_my_r2: 0.7494\n",
      "Epoch 86/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7036 - val_loss: 7.1661e-04 - val_my_r2: 0.7515\n",
      "Epoch 87/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.5888e-04 - my_r2: 0.6493 - val_loss: 7.0800e-04 - val_my_r2: 0.7542\n",
      "Epoch 88/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.6289 - val_loss: 6.9934e-04 - val_my_r2: 0.7560\n",
      "Epoch 89/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.9563e-04 - my_r2: 0.5355 - val_loss: 6.9131e-04 - val_my_r2: 0.7610\n",
      "Epoch 90/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.1735e-04 - my_r2: 0.7141 - val_loss: 6.8398e-04 - val_my_r2: 0.7638\n",
      "Epoch 91/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.9458e-04 - my_r2: 0.7685 - val_loss: 6.7642e-04 - val_my_r2: 0.7664\n",
      "Epoch 92/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.7919e-04 - my_r2: 0.7760 - val_loss: 6.6891e-04 - val_my_r2: 0.7681\n",
      "Epoch 93/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.9380e-04 - my_r2: 0.7146 - val_loss: 6.6173e-04 - val_my_r2: 0.7699\n",
      "Epoch 94/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 9.5606e-04 - my_r2: 0.7741 - val_loss: 6.5319e-04 - val_my_r2: 0.7726\n",
      "Epoch 95/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.8768e-04 - my_r2: 0.6119 - val_loss: 6.4486e-04 - val_my_r2: 0.7753\n",
      "Epoch 96/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7488 - val_loss: 6.3735e-04 - val_my_r2: 0.7782\n",
      "Epoch 97/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.2943e-04 - my_r2: 0.7969 - val_loss: 6.3076e-04 - val_my_r2: 0.7801\n",
      "Epoch 98/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.6818e-04 - my_r2: 0.7324 - val_loss: 6.2509e-04 - val_my_r2: 0.7818\n",
      "Epoch 99/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8439e-04 - my_r2: 0.6866 - val_loss: 6.1761e-04 - val_my_r2: 0.7860\n",
      "Epoch 100/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8267e-04 - my_r2: 0.7981 - val_loss: 6.0973e-04 - val_my_r2: 0.7879\n",
      "Epoch 101/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.5139e-04 - my_r2: 0.7869 - val_loss: 6.0265e-04 - val_my_r2: 0.7911\n",
      "Epoch 102/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.0968e-04 - my_r2: 0.7569 - val_loss: 5.9620e-04 - val_my_r2: 0.7927\n",
      "Epoch 103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7051 - val_loss: 5.9153e-04 - val_my_r2: 0.7944\n",
      "Epoch 104/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 7.9520e-04 - my_r2: 0.7412 - val_loss: 5.8385e-04 - val_my_r2: 0.7969\n",
      "Epoch 105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.1213e-04 - my_r2: 0.7732 - val_loss: 5.7609e-04 - val_my_r2: 0.7984\n",
      "Epoch 106/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.4945e-04 - my_r2: 0.7637 - val_loss: 5.7062e-04 - val_my_r2: 0.7997\n",
      "Epoch 107/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7677e-04 - my_r2: 0.8186 - val_loss: 5.6652e-04 - val_my_r2: 0.8023\n",
      "Epoch 108/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.4102e-04 - my_r2: 0.4911 - val_loss: 5.6189e-04 - val_my_r2: 0.8040\n",
      "Epoch 109/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.2694e-04 - my_r2: 0.8288 - val_loss: 5.5839e-04 - val_my_r2: 0.8079\n",
      "Epoch 110/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.5078e-04 - my_r2: 0.7199 - val_loss: 5.5249e-04 - val_my_r2: 0.8083\n",
      "Epoch 111/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.6692e-04 - my_r2: 0.6050 - val_loss: 5.4915e-04 - val_my_r2: 0.8107\n",
      "Epoch 112/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 8.6135e-04 - my_r2: 0.7642 - val_loss: 5.4187e-04 - val_my_r2: 0.8124\n",
      "Epoch 113/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.0084e-04 - my_r2: 0.8527 - val_loss: 5.3636e-04 - val_my_r2: 0.8131\n",
      "Epoch 114/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8422e-04 - my_r2: 0.7177 - val_loss: 5.2908e-04 - val_my_r2: 0.8137\n",
      "Epoch 115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.6708e-04 - my_r2: 0.7279 - val_loss: 5.2285e-04 - val_my_r2: 0.8149\n",
      "Epoch 116/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0644e-04 - my_r2: 0.8433 - val_loss: 5.1723e-04 - val_my_r2: 0.8162\n",
      "Epoch 117/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.8926e-04 - my_r2: 0.7834 - val_loss: 5.1268e-04 - val_my_r2: 0.8161\n",
      "Epoch 118/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.0662e-04 - my_r2: 0.7584 - val_loss: 5.0886e-04 - val_my_r2: 0.8183\n",
      "Epoch 119/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.3850e-04 - my_r2: 0.8336 - val_loss: 5.0506e-04 - val_my_r2: 0.8205\n",
      "Epoch 120/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.3525e-04 - my_r2: 0.7513 - val_loss: 5.0071e-04 - val_my_r2: 0.8234\n",
      "Epoch 121/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.1290e-04 - my_r2: 0.8189 - val_loss: 4.9476e-04 - val_my_r2: 0.8267\n",
      "Epoch 122/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7171e-04 - my_r2: 0.8214 - val_loss: 4.9014e-04 - val_my_r2: 0.8280\n",
      "Epoch 123/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.3054e-04 - my_r2: 0.7249 - val_loss: 4.8625e-04 - val_my_r2: 0.8282\n",
      "Epoch 124/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3247e-04 - my_r2: 0.8316 - val_loss: 4.8184e-04 - val_my_r2: 0.8292\n",
      "Epoch 125/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.5704e-04 - my_r2: 0.8502 - val_loss: 4.8006e-04 - val_my_r2: 0.8311\n",
      "Epoch 126/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.1170e-04 - my_r2: 0.8439 - val_loss: 4.8130e-04 - val_my_r2: 0.8326\n",
      "Epoch 127/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.1697e-04 - my_r2: 0.7886 - val_loss: 4.7469e-04 - val_my_r2: 0.8334\n",
      "Epoch 128/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.2709e-04 - my_r2: 0.8413 - val_loss: 4.7024e-04 - val_my_r2: 0.8340\n",
      "Epoch 129/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.5781e-04 - my_r2: 0.7133 - val_loss: 4.6246e-04 - val_my_r2: 0.8366\n",
      "Epoch 130/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8077e-04 - my_r2: 0.7795 - val_loss: 4.5743e-04 - val_my_r2: 0.8367\n",
      "Epoch 131/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.9706e-04 - my_r2: 0.7990 - val_loss: 4.5426e-04 - val_my_r2: 0.8375\n",
      "Epoch 132/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0119e-04 - my_r2: 0.8392 - val_loss: 4.5204e-04 - val_my_r2: 0.8389\n",
      "Epoch 133/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.7465e-04 - my_r2: 0.8145 - val_loss: 4.5161e-04 - val_my_r2: 0.8397\n",
      "Epoch 134/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.0264e-04 - my_r2: 0.6139 - val_loss: 4.4809e-04 - val_my_r2: 0.8412\n",
      "Epoch 135/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.6310e-04 - my_r2: 0.7945 - val_loss: 4.4436e-04 - val_my_r2: 0.8428\n",
      "Epoch 136/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.9515e-04 - my_r2: 0.8424 - val_loss: 4.4077e-04 - val_my_r2: 0.8457\n",
      "Epoch 137/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.4759e-04 - my_r2: 0.7017 - val_loss: 4.3443e-04 - val_my_r2: 0.8478\n",
      "Epoch 138/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0923e-04 - my_r2: 0.8442 - val_loss: 4.3061e-04 - val_my_r2: 0.8473\n",
      "Epoch 139/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.3873e-04 - my_r2: 0.7912 - val_loss: 4.2748e-04 - val_my_r2: 0.8499\n",
      "Epoch 140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7469e-04 - my_r2: 0.8724 - val_loss: 4.2635e-04 - val_my_r2: 0.8472\n",
      "Epoch 141/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1764e-04 - my_r2: 0.8627 - val_loss: 4.1989e-04 - val_my_r2: 0.8502\n",
      "Epoch 142/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.7746e-04 - my_r2: 0.8728 - val_loss: 4.1603e-04 - val_my_r2: 0.8522\n",
      "Epoch 143/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.9174e-04 - my_r2: 0.8291 - val_loss: 4.1347e-04 - val_my_r2: 0.8542\n",
      "Epoch 144/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.5437e-04 - my_r2: 0.8804 - val_loss: 4.1133e-04 - val_my_r2: 0.8541\n",
      "Epoch 145/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2211e-04 - my_r2: 0.7699 - val_loss: 4.0878e-04 - val_my_r2: 0.8558\n",
      "Epoch 146/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.3409e-04 - my_r2: 0.8589 - val_loss: 4.0523e-04 - val_my_r2: 0.8580\n",
      "Epoch 147/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1505e-04 - my_r2: 0.8420 - val_loss: 4.0348e-04 - val_my_r2: 0.8582\n",
      "Epoch 148/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8613e-04 - my_r2: 0.8156 - val_loss: 4.0064e-04 - val_my_r2: 0.8606\n",
      "Epoch 149/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.5156e-04 - my_r2: 0.8033 - val_loss: 3.9893e-04 - val_my_r2: 0.8613\n",
      "Epoch 150/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.2664e-04 - my_r2: 0.7717 - val_loss: 3.9496e-04 - val_my_r2: 0.8633\n",
      "Epoch 151/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.9321e-04 - my_r2: 0.8072 - val_loss: 3.9315e-04 - val_my_r2: 0.8659\n",
      "Epoch 152/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2837e-04 - my_r2: 0.8703 - val_loss: 3.9251e-04 - val_my_r2: 0.8661\n",
      "Epoch 153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3014e-04 - my_r2: 0.7515 - val_loss: 3.8884e-04 - val_my_r2: 0.8662\n",
      "Epoch 154/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2296e-04 - my_r2: 0.7168 - val_loss: 3.8611e-04 - val_my_r2: 0.8668\n",
      "Epoch 155/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5181e-04 - my_r2: 0.8812 - val_loss: 3.8253e-04 - val_my_r2: 0.8679\n",
      "Epoch 156/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5759e-04 - my_r2: 0.8424 - val_loss: 3.7840e-04 - val_my_r2: 0.8690\n",
      "Epoch 157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.9191e-04 - my_r2: 0.6634 - val_loss: 3.7541e-04 - val_my_r2: 0.8692\n",
      "Epoch 158/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6186e-04 - my_r2: 0.8511 - val_loss: 3.7611e-04 - val_my_r2: 0.8692\n",
      "Epoch 159/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.0074e-04 - my_r2: 0.8420 - val_loss: 3.7022e-04 - val_my_r2: 0.8686\n",
      "Epoch 160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.6431e-04 - my_r2: 0.8297 - val_loss: 3.6734e-04 - val_my_r2: 0.8697\n",
      "Epoch 161/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0192e-04 - my_r2: 0.8775 - val_loss: 3.6390e-04 - val_my_r2: 0.8715\n",
      "Epoch 162/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.2451e-04 - my_r2: 0.8509 - val_loss: 3.6196e-04 - val_my_r2: 0.8729\n",
      "Epoch 163/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0602e-04 - my_r2: 0.8333 - val_loss: 3.5859e-04 - val_my_r2: 0.8741\n",
      "Epoch 164/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6102e-04 - my_r2: 0.8585 - val_loss: 3.5775e-04 - val_my_r2: 0.8754\n",
      "Epoch 165/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6201e-04 - my_r2: 0.8146 - val_loss: 3.5709e-04 - val_my_r2: 0.8765\n",
      "Epoch 166/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.6265e-04 - my_r2: 0.8519 - val_loss: 3.5182e-04 - val_my_r2: 0.8774\n",
      "Epoch 167/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.9249e-04 - my_r2: 0.7756 - val_loss: 3.5167e-04 - val_my_r2: 0.8770\n",
      "Epoch 168/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9944e-04 - my_r2: 0.7981 - val_loss: 3.4704e-04 - val_my_r2: 0.8788\n",
      "Epoch 169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2792e-04 - my_r2: 0.7856 - val_loss: 3.4351e-04 - val_my_r2: 0.8789\n",
      "Epoch 170/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4832e-04 - my_r2: 0.8130 - val_loss: 3.4217e-04 - val_my_r2: 0.8784\n",
      "Epoch 171/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.6113e-04 - my_r2: 0.8039 - val_loss: 3.4106e-04 - val_my_r2: 0.8784\n",
      "Epoch 172/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3982e-04 - my_r2: 0.8859 - val_loss: 3.3799e-04 - val_my_r2: 0.8799\n",
      "Epoch 173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9842e-04 - my_r2: 0.8591 - val_loss: 3.3645e-04 - val_my_r2: 0.8820\n",
      "Epoch 174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2636e-04 - my_r2: 0.7067 - val_loss: 3.3435e-04 - val_my_r2: 0.8839\n",
      "Epoch 175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3323e-04 - my_r2: 0.8195 - val_loss: 3.3437e-04 - val_my_r2: 0.8837\n",
      "Epoch 176/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2585e-04 - my_r2: 0.8585 - val_loss: 3.3091e-04 - val_my_r2: 0.8829\n",
      "Epoch 177/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.4972e-04 - my_r2: 0.8597 - val_loss: 3.2972e-04 - val_my_r2: 0.8821\n",
      "Epoch 178/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.8932e-04 - my_r2: 0.8096 - val_loss: 3.2603e-04 - val_my_r2: 0.8845\n",
      "Epoch 179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2952e-04 - my_r2: 0.8639 - val_loss: 3.2447e-04 - val_my_r2: 0.8850\n",
      "Epoch 180/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5723e-04 - my_r2: 0.7154 - val_loss: 3.2158e-04 - val_my_r2: 0.8861\n",
      "Epoch 181/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.2886e-04 - my_r2: 0.8510 - val_loss: 3.1872e-04 - val_my_r2: 0.8868\n",
      "Epoch 182/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1049e-04 - my_r2: 0.8064 - val_loss: 3.1627e-04 - val_my_r2: 0.8870\n",
      "Epoch 183/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.5112e-04 - my_r2: 0.8384 - val_loss: 3.1479e-04 - val_my_r2: 0.8870\n",
      "Epoch 184/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2464e-04 - my_r2: 0.8769 - val_loss: 3.1056e-04 - val_my_r2: 0.8904\n",
      "Epoch 185/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6038e-04 - my_r2: 0.9083 - val_loss: 3.0970e-04 - val_my_r2: 0.8903\n",
      "Epoch 186/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5790e-04 - my_r2: 0.8855 - val_loss: 3.0787e-04 - val_my_r2: 0.8905\n",
      "Epoch 187/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6079e-04 - my_r2: 0.8762 - val_loss: 3.0704e-04 - val_my_r2: 0.8904\n",
      "Epoch 188/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6788e-04 - my_r2: 0.7992 - val_loss: 3.0507e-04 - val_my_r2: 0.8915\n",
      "Epoch 189/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.8753e-04 - my_r2: 0.8343 - val_loss: 3.0197e-04 - val_my_r2: 0.8909\n",
      "Epoch 190/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.7320e-04 - my_r2: 0.8220 - val_loss: 2.9895e-04 - val_my_r2: 0.8934\n",
      "Epoch 191/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2096e-04 - my_r2: 0.8505 - val_loss: 2.9673e-04 - val_my_r2: 0.8943\n",
      "Epoch 192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5043e-04 - my_r2: 0.8824 - val_loss: 2.9538e-04 - val_my_r2: 0.8943\n",
      "Epoch 193/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.2398e-04 - my_r2: 0.8813 - val_loss: 2.9378e-04 - val_my_r2: 0.8964\n",
      "Epoch 194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0445e-04 - my_r2: 0.8775 - val_loss: 2.9245e-04 - val_my_r2: 0.8968\n",
      "Epoch 195/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3799e-04 - my_r2: 0.8335 - val_loss: 2.9272e-04 - val_my_r2: 0.8968\n",
      "Epoch 196/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1888e-04 - my_r2: 0.6301 - val_loss: 2.9176e-04 - val_my_r2: 0.8973\n",
      "Epoch 197/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3207e-04 - my_r2: 0.8698 - val_loss: 2.8819e-04 - val_my_r2: 0.8988\n",
      "Epoch 198/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1713e-04 - my_r2: 0.8773 - val_loss: 2.8842e-04 - val_my_r2: 0.8993\n",
      "Epoch 199/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.4259e-04 - my_r2: 0.8680 - val_loss: 2.8628e-04 - val_my_r2: 0.8999\n",
      "Epoch 200/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.1167e-04 - my_r2: 0.8612 - val_loss: 2.8669e-04 - val_my_r2: 0.8992\n",
      "Epoch 201/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2761e-04 - my_r2: 0.8884 - val_loss: 2.8463e-04 - val_my_r2: 0.9001\n",
      "Epoch 202/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2046e-04 - my_r2: 0.8744 - val_loss: 2.8228e-04 - val_my_r2: 0.9005\n",
      "Epoch 203/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.5303e-04 - my_r2: 0.8674 - val_loss: 2.7638e-04 - val_my_r2: 0.9033\n",
      "Epoch 204/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0181e-04 - my_r2: 0.8980 - val_loss: 2.7482e-04 - val_my_r2: 0.9049\n",
      "Epoch 205/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.0351e-04 - my_r2: 0.8473 - val_loss: 2.7193e-04 - val_my_r2: 0.9048\n",
      "Epoch 206/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2097e-04 - my_r2: 0.8685 - val_loss: 2.6890e-04 - val_my_r2: 0.9061\n",
      "Epoch 207/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8964e-04 - my_r2: 0.8498 - val_loss: 2.7001e-04 - val_my_r2: 0.9060\n",
      "Epoch 208/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3384e-04 - my_r2: 0.8464 - val_loss: 2.7984e-04 - val_my_r2: 0.9039\n",
      "Epoch 209/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6222e-04 - my_r2: 0.7535 - val_loss: 2.6482e-04 - val_my_r2: 0.9076\n",
      "Epoch 210/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3007e-04 - my_r2: 0.8969 - val_loss: 2.6683e-04 - val_my_r2: 0.9059\n",
      "Epoch 211/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.5113e-04 - my_r2: 0.7955 - val_loss: 2.6088e-04 - val_my_r2: 0.9079\n",
      "Epoch 212/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3937e-04 - my_r2: 0.8856 - val_loss: 2.5871e-04 - val_my_r2: 0.9079\n",
      "Epoch 213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1480e-04 - my_r2: 0.8647 - val_loss: 2.5880e-04 - val_my_r2: 0.9078\n",
      "Epoch 214/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2937e-04 - my_r2: 0.8607 - val_loss: 2.6123e-04 - val_my_r2: 0.9063\n",
      "Epoch 215/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7125e-04 - my_r2: 0.8205 - val_loss: 2.5969e-04 - val_my_r2: 0.9070\n",
      "Epoch 216/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5154e-04 - my_r2: 0.6630 - val_loss: 2.5357e-04 - val_my_r2: 0.9101\n",
      "Epoch 217/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8638e-04 - my_r2: 0.4791 - val_loss: 2.5488e-04 - val_my_r2: 0.9102\n",
      "Epoch 218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7959e-04 - my_r2: 0.8262 - val_loss: 2.5636e-04 - val_my_r2: 0.9102\n",
      "Epoch 219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4330e-04 - my_r2: 0.9089 - val_loss: 2.5191e-04 - val_my_r2: 0.9118\n",
      "Epoch 220/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5246e-04 - my_r2: 0.8488 - val_loss: 2.5211e-04 - val_my_r2: 0.9128\n",
      "Epoch 221/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9723e-04 - my_r2: 0.8968 - val_loss: 2.5019e-04 - val_my_r2: 0.9139\n",
      "Epoch 222/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4892e-04 - my_r2: 0.8693 - val_loss: 2.4639e-04 - val_my_r2: 0.9159\n",
      "Epoch 223/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2081e-04 - my_r2: 0.9060 - val_loss: 2.4487e-04 - val_my_r2: 0.9165\n",
      "Epoch 224/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5919e-04 - my_r2: 0.7905 - val_loss: 2.4432e-04 - val_my_r2: 0.9160\n",
      "Epoch 225/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7810e-04 - my_r2: 0.8450 - val_loss: 2.4182e-04 - val_my_r2: 0.9168\n",
      "Epoch 226/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3507e-04 - my_r2: 0.8769 - val_loss: 2.4077e-04 - val_my_r2: 0.9165\n",
      "Epoch 227/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1384e-04 - my_r2: 0.8536 - val_loss: 2.3845e-04 - val_my_r2: 0.9175\n",
      "Epoch 228/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3862e-04 - my_r2: 0.8903 - val_loss: 2.3647e-04 - val_my_r2: 0.9195\n",
      "Epoch 229/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5634e-04 - my_r2: 0.8326 - val_loss: 2.3561e-04 - val_my_r2: 0.9200\n",
      "Epoch 230/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0924e-04 - my_r2: 0.8327 - val_loss: 2.3392e-04 - val_my_r2: 0.9192\n",
      "Epoch 231/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0834e-04 - my_r2: 0.8854 - val_loss: 2.3350e-04 - val_my_r2: 0.9194\n",
      "Epoch 232/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7476e-04 - my_r2: 0.8574 - val_loss: 2.3043e-04 - val_my_r2: 0.9211\n",
      "Epoch 233/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3520e-04 - my_r2: 0.8885 - val_loss: 2.2900e-04 - val_my_r2: 0.9219\n",
      "Epoch 234/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 4.4782e-04 - my_r2: 0.9106 - val_loss: 2.3133e-04 - val_my_r2: 0.9201\n",
      "Epoch 235/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6861e-04 - my_r2: 0.8371 - val_loss: 2.2743e-04 - val_my_r2: 0.9223\n",
      "Epoch 236/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7680e-04 - my_r2: 0.8676 - val_loss: 2.2709e-04 - val_my_r2: 0.9219\n",
      "Epoch 237/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9898e-04 - my_r2: 0.8681 - val_loss: 2.2798e-04 - val_my_r2: 0.9209\n",
      "Epoch 238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6597e-04 - my_r2: 0.8831 - val_loss: 2.2562e-04 - val_my_r2: 0.9213\n",
      "Epoch 239/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0348e-04 - my_r2: 0.8922 - val_loss: 2.2403e-04 - val_my_r2: 0.9221\n",
      "Epoch 240/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2014e-04 - my_r2: 0.7442 - val_loss: 2.2302e-04 - val_my_r2: 0.9224\n",
      "Epoch 241/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1286e-04 - my_r2: 0.8649 - val_loss: 2.1939e-04 - val_my_r2: 0.9232\n",
      "Epoch 242/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8147e-04 - my_r2: 0.9024 - val_loss: 2.2005e-04 - val_my_r2: 0.9213\n",
      "Epoch 243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4950e-04 - my_r2: 0.8798 - val_loss: 2.2014e-04 - val_my_r2: 0.9221\n",
      "Epoch 244/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6558e-04 - my_r2: 0.8913 - val_loss: 2.1411e-04 - val_my_r2: 0.9258\n",
      "Epoch 245/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.5417e-04 - my_r2: 0.8643 - val_loss: 2.1768e-04 - val_my_r2: 0.9250\n",
      "Epoch 246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6763e-04 - my_r2: 0.8918 - val_loss: 2.1823e-04 - val_my_r2: 0.9250\n",
      "Epoch 247/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0855e-04 - my_r2: 0.8060 - val_loss: 2.0883e-04 - val_my_r2: 0.9280\n",
      "Epoch 248/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9171e-04 - my_r2: 0.8242 - val_loss: 2.0905e-04 - val_my_r2: 0.9288\n",
      "Epoch 249/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7247e-04 - my_r2: 0.8912 - val_loss: 2.0697e-04 - val_my_r2: 0.9285\n",
      "Epoch 250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2160e-04 - my_r2: 0.8808 - val_loss: 2.0648e-04 - val_my_r2: 0.9296\n",
      "Epoch 251/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2307e-04 - my_r2: 0.8754 - val_loss: 2.1171e-04 - val_my_r2: 0.9287\n",
      "Epoch 252/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5485e-04 - my_r2: 0.8714 - val_loss: 2.0572e-04 - val_my_r2: 0.9293\n",
      "Epoch 253/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9179e-04 - my_r2: 0.9001 - val_loss: 2.0555e-04 - val_my_r2: 0.9298\n",
      "Epoch 254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8756e-04 - my_r2: 0.8940 - val_loss: 2.0136e-04 - val_my_r2: 0.9318\n",
      "Epoch 255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3445e-04 - my_r2: 0.7854 - val_loss: 2.0244e-04 - val_my_r2: 0.9327\n",
      "Epoch 256/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2197e-04 - my_r2: 0.8864 - val_loss: 2.0063e-04 - val_my_r2: 0.9325\n",
      "Epoch 257/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4896e-04 - my_r2: 0.8840 - val_loss: 2.0094e-04 - val_my_r2: 0.9315\n",
      "Epoch 258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5232e-04 - my_r2: 0.8442 - val_loss: 1.9722e-04 - val_my_r2: 0.9332\n",
      "Epoch 259/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8592e-04 - my_r2: 0.7726 - val_loss: 1.9626e-04 - val_my_r2: 0.9333\n",
      "Epoch 260/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6665e-04 - my_r2: 0.9042 - val_loss: 1.9436e-04 - val_my_r2: 0.9325\n",
      "Epoch 261/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1317e-04 - my_r2: 0.8852 - val_loss: 1.9647e-04 - val_my_r2: 0.9331\n",
      "Epoch 262/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8902e-04 - my_r2: 0.8920 - val_loss: 1.9228e-04 - val_my_r2: 0.9338\n",
      "Epoch 263/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7978e-04 - my_r2: 0.8907 - val_loss: 1.9028e-04 - val_my_r2: 0.9342\n",
      "Epoch 264/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4568e-04 - my_r2: 0.6909 - val_loss: 1.9052e-04 - val_my_r2: 0.9342\n",
      "Epoch 265/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5576e-04 - my_r2: 0.8765 - val_loss: 1.9229e-04 - val_my_r2: 0.9336\n",
      "Epoch 266/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8340e-04 - my_r2: 0.8633 - val_loss: 1.8834e-04 - val_my_r2: 0.9349\n",
      "Epoch 267/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3822e-04 - my_r2: 0.9188 - val_loss: 1.8517e-04 - val_my_r2: 0.9368\n",
      "Epoch 268/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.4913e-04 - my_r2: 0.8720 - val_loss: 1.8442e-04 - val_my_r2: 0.9362\n",
      "Epoch 269/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5179e-04 - my_r2: 0.8924 - val_loss: 1.8353e-04 - val_my_r2: 0.9377\n",
      "Epoch 270/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5110e-04 - my_r2: 0.7186 - val_loss: 1.8499e-04 - val_my_r2: 0.9376\n",
      "Epoch 271/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8410e-04 - my_r2: 0.8260 - val_loss: 1.8222e-04 - val_my_r2: 0.9376\n",
      "Epoch 272/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5442e-04 - my_r2: 0.8937 - val_loss: 1.8495e-04 - val_my_r2: 0.9363\n",
      "Epoch 273/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7033e-04 - my_r2: 0.8859 - val_loss: 1.8207e-04 - val_my_r2: 0.9371\n",
      "Epoch 274/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2819e-04 - my_r2: 0.8868 - val_loss: 1.7945e-04 - val_my_r2: 0.9394\n",
      "Epoch 275/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6766e-04 - my_r2: 0.8465 - val_loss: 1.7634e-04 - val_my_r2: 0.9402\n",
      "Epoch 276/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5491e-04 - my_r2: 0.9194 - val_loss: 1.7674e-04 - val_my_r2: 0.9393\n",
      "Epoch 277/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1171e-04 - my_r2: 0.8695 - val_loss: 1.8135e-04 - val_my_r2: 0.9390\n",
      "Epoch 278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4385e-04 - my_r2: 0.8628 - val_loss: 1.8645e-04 - val_my_r2: 0.9385\n",
      "Epoch 279/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2074e-04 - my_r2: 0.8751 - val_loss: 1.7803e-04 - val_my_r2: 0.9403\n",
      "Epoch 280/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3006e-04 - my_r2: 0.8848 - val_loss: 1.7320e-04 - val_my_r2: 0.9411\n",
      "Epoch 281/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7798e-04 - my_r2: 0.8848 - val_loss: 1.7343e-04 - val_my_r2: 0.9410\n",
      "Epoch 282/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1628e-04 - my_r2: 0.8909 - val_loss: 1.6948e-04 - val_my_r2: 0.9410\n",
      "Epoch 283/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8444e-04 - my_r2: 0.8964 - val_loss: 1.7271e-04 - val_my_r2: 0.9377\n",
      "Epoch 284/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9570e-04 - my_r2: 0.8338 - val_loss: 1.6625e-04 - val_my_r2: 0.9414\n",
      "Epoch 285/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8012e-04 - my_r2: 0.8670 - val_loss: 1.7573e-04 - val_my_r2: 0.9402\n",
      "Epoch 286/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9168e-04 - my_r2: 0.8984 - val_loss: 1.6595e-04 - val_my_r2: 0.9421\n",
      "Epoch 287/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4953e-04 - my_r2: 0.8785 - val_loss: 1.6499e-04 - val_my_r2: 0.9421\n",
      "Epoch 288/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0167e-04 - my_r2: 0.8794 - val_loss: 1.6520e-04 - val_my_r2: 0.9427\n",
      "Epoch 289/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3441e-04 - my_r2: 0.8862 - val_loss: 1.6358e-04 - val_my_r2: 0.9430\n",
      "Epoch 290/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9802e-04 - my_r2: 0.8728 - val_loss: 1.6155e-04 - val_my_r2: 0.9439\n",
      "Epoch 291/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1553e-04 - my_r2: 0.8741 - val_loss: 1.5955e-04 - val_my_r2: 0.9441\n",
      "Epoch 292/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3509e-04 - my_r2: 0.9012 - val_loss: 1.5838e-04 - val_my_r2: 0.9447\n",
      "Epoch 293/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1658e-04 - my_r2: 0.8947 - val_loss: 1.6356e-04 - val_my_r2: 0.9449\n",
      "Epoch 294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5105e-04 - my_r2: 0.8576 - val_loss: 1.5997e-04 - val_my_r2: 0.9464\n",
      "Epoch 295/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7969e-04 - my_r2: 0.8987 - val_loss: 1.6414e-04 - val_my_r2: 0.9441\n",
      "Epoch 296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5561e-04 - my_r2: 0.8301 - val_loss: 1.5877e-04 - val_my_r2: 0.9467\n",
      "Epoch 297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8663e-04 - my_r2: 0.7352 - val_loss: 1.6365e-04 - val_my_r2: 0.9451\n",
      "Epoch 298/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9050e-04 - my_r2: 0.8895 - val_loss: 1.5966e-04 - val_my_r2: 0.9452\n",
      "Epoch 299/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8440e-04 - my_r2: 0.9041 - val_loss: 1.6007e-04 - val_my_r2: 0.9440\n",
      "Epoch 300/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9567e-04 - my_r2: 0.9145 - val_loss: 1.5978e-04 - val_my_r2: 0.9463\n",
      "Epoch 301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3216e-04 - my_r2: 0.9092 - val_loss: 1.5721e-04 - val_my_r2: 0.9477\n",
      "Epoch 302/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6462e-04 - my_r2: 0.8511 - val_loss: 1.5727e-04 - val_my_r2: 0.9470\n",
      "Epoch 303/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7804e-04 - my_r2: 0.9021 - val_loss: 1.6094e-04 - val_my_r2: 0.9457\n",
      "Epoch 304/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1736e-04 - my_r2: 0.8557 - val_loss: 1.5313e-04 - val_my_r2: 0.9482\n",
      "Epoch 305/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3352e-04 - my_r2: 0.9009 - val_loss: 1.5627e-04 - val_my_r2: 0.9460\n",
      "Epoch 306/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8830e-04 - my_r2: 0.9143 - val_loss: 1.5075e-04 - val_my_r2: 0.9479\n",
      "Epoch 307/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0931e-04 - my_r2: 0.8972 - val_loss: 1.4681e-04 - val_my_r2: 0.9486\n",
      "Epoch 308/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2348e-04 - my_r2: 0.8747 - val_loss: 1.4831e-04 - val_my_r2: 0.9483\n",
      "Epoch 309/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1609e-04 - my_r2: 0.9360 - val_loss: 1.4626e-04 - val_my_r2: 0.9486\n",
      "Epoch 310/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7801e-04 - my_r2: 0.8463 - val_loss: 1.4604e-04 - val_my_r2: 0.9496\n",
      "Epoch 311/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3893e-04 - my_r2: 0.8643 - val_loss: 1.4570e-04 - val_my_r2: 0.9497\n",
      "Epoch 312/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6082e-04 - my_r2: 0.8799 - val_loss: 1.5123e-04 - val_my_r2: 0.9476\n",
      "Epoch 313/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8576e-04 - my_r2: 0.8842 - val_loss: 1.5303e-04 - val_my_r2: 0.9475\n",
      "Epoch 314/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7745e-04 - my_r2: 0.8868 - val_loss: 1.4363e-04 - val_my_r2: 0.9508\n",
      "Epoch 315/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4587e-04 - my_r2: 0.8631 - val_loss: 1.4055e-04 - val_my_r2: 0.9514\n",
      "Epoch 316/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9236e-04 - my_r2: 0.8926 - val_loss: 1.3961e-04 - val_my_r2: 0.9514\n",
      "Epoch 317/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7711e-04 - my_r2: 0.8885 - val_loss: 1.4005e-04 - val_my_r2: 0.9512\n",
      "Epoch 318/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6602e-04 - my_r2: 0.9001 - val_loss: 1.4160e-04 - val_my_r2: 0.9507\n",
      "Epoch 319/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5280e-04 - my_r2: 0.8853 - val_loss: 1.4338e-04 - val_my_r2: 0.9511\n",
      "Epoch 320/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3194e-04 - my_r2: 0.9065 - val_loss: 1.5132e-04 - val_my_r2: 0.9496\n",
      "Epoch 321/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1820e-04 - my_r2: 0.8935 - val_loss: 1.4325e-04 - val_my_r2: 0.9520\n",
      "Epoch 322/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2748e-04 - my_r2: 0.8678 - val_loss: 1.3636e-04 - val_my_r2: 0.9535\n",
      "Epoch 323/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4621e-04 - my_r2: 0.8800 - val_loss: 1.3842e-04 - val_my_r2: 0.9521\n",
      "Epoch 324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6840e-04 - my_r2: 0.8735 - val_loss: 1.4090e-04 - val_my_r2: 0.9508\n",
      "Epoch 325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9886e-04 - my_r2: 0.8763 - val_loss: 1.3843e-04 - val_my_r2: 0.9505\n",
      "Epoch 326/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6184e-04 - my_r2: 0.9098 - val_loss: 1.4329e-04 - val_my_r2: 0.9491\n",
      "Epoch 327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7984e-04 - my_r2: 0.9081 - val_loss: 1.3508e-04 - val_my_r2: 0.9517\n",
      "Epoch 328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7831e-04 - my_r2: 0.8723 - val_loss: 1.3292e-04 - val_my_r2: 0.9524\n",
      "Epoch 329/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6984e-04 - my_r2: 0.9140 - val_loss: 1.3366e-04 - val_my_r2: 0.9518\n",
      "Epoch 330/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0761e-04 - my_r2: 0.9399 - val_loss: 1.2999e-04 - val_my_r2: 0.9533\n",
      "Epoch 331/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1104e-04 - my_r2: 0.9069 - val_loss: 1.3256e-04 - val_my_r2: 0.9515\n",
      "Epoch 332/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1826e-04 - my_r2: 0.8712 - val_loss: 1.3280e-04 - val_my_r2: 0.9522\n",
      "Epoch 333/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6721e-04 - my_r2: 0.8428 - val_loss: 1.2768e-04 - val_my_r2: 0.9549\n",
      "Epoch 334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7066e-04 - my_r2: 0.9023 - val_loss: 1.2635e-04 - val_my_r2: 0.9558\n",
      "Epoch 335/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0143e-04 - my_r2: 0.8694 - val_loss: 1.2497e-04 - val_my_r2: 0.9559\n",
      "Epoch 336/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8351e-04 - my_r2: 0.9180 - val_loss: 1.2721e-04 - val_my_r2: 0.9559\n",
      "Epoch 337/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3872e-04 - my_r2: 0.8485 - val_loss: 1.2487e-04 - val_my_r2: 0.9562\n",
      "Epoch 338/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7394e-04 - my_r2: 0.8551 - val_loss: 1.2626e-04 - val_my_r2: 0.9556\n",
      "Epoch 339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4874e-04 - my_r2: 0.9197 - val_loss: 1.2867e-04 - val_my_r2: 0.9541\n",
      "Epoch 340/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2918e-04 - my_r2: 0.8669 - val_loss: 1.2539e-04 - val_my_r2: 0.9548\n",
      "Epoch 341/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9853e-04 - my_r2: 0.9156 - val_loss: 1.2896e-04 - val_my_r2: 0.9528\n",
      "Epoch 342/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4503e-04 - my_r2: 0.8687 - val_loss: 1.2516e-04 - val_my_r2: 0.9544\n",
      "Epoch 343/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6452e-04 - my_r2: 0.9215 - val_loss: 1.2424e-04 - val_my_r2: 0.9547\n",
      "Epoch 344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5621e-04 - my_r2: 0.8885 - val_loss: 1.2411e-04 - val_my_r2: 0.9552\n",
      "Epoch 345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3960e-04 - my_r2: 0.9243 - val_loss: 1.2635e-04 - val_my_r2: 0.9544\n",
      "Epoch 346/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3219e-04 - my_r2: 0.8780 - val_loss: 1.2404e-04 - val_my_r2: 0.9558\n",
      "Epoch 347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5211e-04 - my_r2: 0.9222 - val_loss: 1.2061e-04 - val_my_r2: 0.9572\n",
      "Epoch 348/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3573e-04 - my_r2: 0.9255 - val_loss: 1.2270e-04 - val_my_r2: 0.9566\n",
      "Epoch 349/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.7811e-04 - my_r2: 0.8427 - val_loss: 1.2142e-04 - val_my_r2: 0.9564\n",
      "Epoch 350/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4344e-04 - my_r2: 0.8767 - val_loss: 1.1797e-04 - val_my_r2: 0.9570\n",
      "Epoch 351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5379e-04 - my_r2: 0.7871 - val_loss: 1.2121e-04 - val_my_r2: 0.9567\n",
      "Epoch 352/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9721e-04 - my_r2: 0.9052 - val_loss: 1.1923e-04 - val_my_r2: 0.9575\n",
      "Epoch 353/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6026e-04 - my_r2: 0.8598 - val_loss: 1.1903e-04 - val_my_r2: 0.9578\n",
      "Epoch 354/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3682e-04 - my_r2: 0.9234 - val_loss: 1.1997e-04 - val_my_r2: 0.9576\n",
      "Epoch 355/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5543e-04 - my_r2: 0.9307 - val_loss: 1.1878e-04 - val_my_r2: 0.9584\n",
      "Epoch 356/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1788e-04 - my_r2: 0.8958 - val_loss: 1.1551e-04 - val_my_r2: 0.9597\n",
      "Epoch 357/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4091e-04 - my_r2: 0.7862 - val_loss: 1.1262e-04 - val_my_r2: 0.9607\n",
      "Epoch 358/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4068e-04 - my_r2: 0.9152 - val_loss: 1.1295e-04 - val_my_r2: 0.9603\n",
      "Epoch 359/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0163e-04 - my_r2: 0.8933 - val_loss: 1.1423e-04 - val_my_r2: 0.9593\n",
      "Epoch 360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1436e-04 - my_r2: 0.8774 - val_loss: 1.1535e-04 - val_my_r2: 0.9593\n",
      "Epoch 361/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7945e-04 - my_r2: 0.9235 - val_loss: 1.1443e-04 - val_my_r2: 0.9595\n",
      "Epoch 362/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4798e-04 - my_r2: 0.9134 - val_loss: 1.1195e-04 - val_my_r2: 0.9603\n",
      "Epoch 363/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5301e-04 - my_r2: 0.9201 - val_loss: 1.0940e-04 - val_my_r2: 0.9608\n",
      "Epoch 364/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5160e-04 - my_r2: 0.9091 - val_loss: 1.0870e-04 - val_my_r2: 0.9610\n",
      "Epoch 365/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6765e-04 - my_r2: 0.9201 - val_loss: 1.0898e-04 - val_my_r2: 0.9611\n",
      "Epoch 366/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0218e-04 - my_r2: 0.9301 - val_loss: 1.0754e-04 - val_my_r2: 0.9615\n",
      "Epoch 367/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4403e-04 - my_r2: 0.9321 - val_loss: 1.1278e-04 - val_my_r2: 0.9603\n",
      "Epoch 368/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5605e-04 - my_r2: 0.9018 - val_loss: 1.0803e-04 - val_my_r2: 0.9628\n",
      "Epoch 369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4222e-04 - my_r2: 0.9143 - val_loss: 1.0750e-04 - val_my_r2: 0.9630\n",
      "Epoch 370/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7090e-04 - my_r2: 0.8881 - val_loss: 1.0531e-04 - val_my_r2: 0.9637\n",
      "Epoch 371/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6591e-04 - my_r2: 0.9080 - val_loss: 1.0423e-04 - val_my_r2: 0.9638\n",
      "Epoch 372/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4923e-04 - my_r2: 0.8933 - val_loss: 1.0528e-04 - val_my_r2: 0.9628\n",
      "Epoch 373/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3586e-04 - my_r2: 0.9010 - val_loss: 1.0746e-04 - val_my_r2: 0.9626\n",
      "Epoch 374/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6713e-04 - my_r2: 0.9236 - val_loss: 1.0561e-04 - val_my_r2: 0.9631\n",
      "Epoch 375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4679e-04 - my_r2: 0.8851 - val_loss: 1.0265e-04 - val_my_r2: 0.9638\n",
      "Epoch 376/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3352e-04 - my_r2: 0.8628 - val_loss: 1.0361e-04 - val_my_r2: 0.9637\n",
      "Epoch 377/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9384e-04 - my_r2: 0.8774 - val_loss: 1.0649e-04 - val_my_r2: 0.9633\n",
      "Epoch 378/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1773e-04 - my_r2: 0.9146 - val_loss: 1.0640e-04 - val_my_r2: 0.9633\n",
      "Epoch 379/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0594e-04 - my_r2: 0.9273 - val_loss: 1.0346e-04 - val_my_r2: 0.9634\n",
      "Epoch 380/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0344e-04 - my_r2: 0.9233 - val_loss: 1.0013e-04 - val_my_r2: 0.9650\n",
      "Epoch 381/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0710e-04 - my_r2: 0.7827 - val_loss: 9.9749e-05 - val_my_r2: 0.9647\n",
      "Epoch 382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6979e-04 - my_r2: 0.9285 - val_loss: 1.0249e-04 - val_my_r2: 0.9643\n",
      "Epoch 383/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1747e-04 - my_r2: 0.8952 - val_loss: 1.0788e-04 - val_my_r2: 0.9610\n",
      "Epoch 384/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6334e-04 - my_r2: 0.8186 - val_loss: 1.0248e-04 - val_my_r2: 0.9638\n",
      "Epoch 385/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0351e-04 - my_r2: 0.8905 - val_loss: 9.8778e-05 - val_my_r2: 0.9650\n",
      "Epoch 386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3108e-04 - my_r2: 0.8899 - val_loss: 1.0176e-04 - val_my_r2: 0.9639\n",
      "Epoch 387/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3309e-04 - my_r2: 0.9112 - val_loss: 1.0637e-04 - val_my_r2: 0.9623\n",
      "Epoch 388/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5245e-04 - my_r2: 0.8623 - val_loss: 1.0053e-04 - val_my_r2: 0.9641\n",
      "Epoch 389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2814e-04 - my_r2: 0.9216 - val_loss: 9.8492e-05 - val_my_r2: 0.9650\n",
      "Epoch 390/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7814e-04 - my_r2: 0.8939 - val_loss: 9.7972e-05 - val_my_r2: 0.9654\n",
      "Epoch 391/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0262e-04 - my_r2: 0.9032 - val_loss: 9.4868e-05 - val_my_r2: 0.9661\n",
      "Epoch 392/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2812e-04 - my_r2: 0.9246 - val_loss: 9.4978e-05 - val_my_r2: 0.9663\n",
      "Epoch 393/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9339e-04 - my_r2: 0.8324 - val_loss: 9.3724e-05 - val_my_r2: 0.9665\n",
      "Epoch 394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4877e-04 - my_r2: 0.8829 - val_loss: 9.5565e-05 - val_my_r2: 0.9661\n",
      "Epoch 395/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3470e-04 - my_r2: 0.9046 - val_loss: 9.8966e-05 - val_my_r2: 0.9646\n",
      "Epoch 396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5950e-04 - my_r2: 0.9032 - val_loss: 9.8302e-05 - val_my_r2: 0.9652\n",
      "Epoch 397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3522e-04 - my_r2: 0.9154 - val_loss: 1.0169e-04 - val_my_r2: 0.9643\n",
      "Epoch 398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6689e-04 - my_r2: 0.9145 - val_loss: 9.4594e-05 - val_my_r2: 0.9664\n",
      "Epoch 399/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4547e-04 - my_r2: 0.9126 - val_loss: 9.3813e-05 - val_my_r2: 0.9667\n",
      "Epoch 400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2926e-04 - my_r2: 0.9216 - val_loss: 9.0986e-05 - val_my_r2: 0.9675\n",
      "Epoch 401/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7279e-04 - my_r2: 0.9146 - val_loss: 9.0929e-05 - val_my_r2: 0.9676\n",
      "Epoch 402/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5834e-04 - my_r2: 0.9151 - val_loss: 9.3066e-05 - val_my_r2: 0.9671\n",
      "Epoch 403/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6479e-04 - my_r2: 0.9129 - val_loss: 9.4953e-05 - val_my_r2: 0.9666\n",
      "Epoch 404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8508e-04 - my_r2: 0.8677 - val_loss: 9.0359e-05 - val_my_r2: 0.9680\n",
      "Epoch 405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9563e-04 - my_r2: 0.8888 - val_loss: 8.9710e-05 - val_my_r2: 0.9674\n",
      "Epoch 406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3002e-04 - my_r2: 0.9025 - val_loss: 8.9101e-05 - val_my_r2: 0.9681\n",
      "Epoch 407/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1915e-04 - my_r2: 0.8977 - val_loss: 8.9342e-05 - val_my_r2: 0.9673\n",
      "Epoch 408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6656e-04 - my_r2: 0.9220 - val_loss: 8.9310e-05 - val_my_r2: 0.9678\n",
      "Epoch 409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9106e-04 - my_r2: 0.9053 - val_loss: 9.1149e-05 - val_my_r2: 0.9683\n",
      "Epoch 410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9258e-04 - my_r2: 0.9198 - val_loss: 8.7665e-05 - val_my_r2: 0.9687\n",
      "Epoch 411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5985e-04 - my_r2: 0.8953 - val_loss: 8.5861e-05 - val_my_r2: 0.9691\n",
      "Epoch 412/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1108e-04 - my_r2: 0.9168 - val_loss: 8.5757e-05 - val_my_r2: 0.9690\n",
      "Epoch 413/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8952e-04 - my_r2: 0.9207 - val_loss: 8.5996e-05 - val_my_r2: 0.9686\n",
      "Epoch 414/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1903e-04 - my_r2: 0.9188 - val_loss: 8.9006e-05 - val_my_r2: 0.9670\n",
      "Epoch 415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1896e-04 - my_r2: 0.9086 - val_loss: 8.4872e-05 - val_my_r2: 0.9682\n",
      "Epoch 416/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3703e-04 - my_r2: 0.9139 - val_loss: 8.6134e-05 - val_my_r2: 0.9682\n",
      "Epoch 417/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6376e-04 - my_r2: 0.8941 - val_loss: 8.3715e-05 - val_my_r2: 0.9694\n",
      "Epoch 418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2492e-04 - my_r2: 0.8960 - val_loss: 8.5724e-05 - val_my_r2: 0.9699\n",
      "Epoch 419/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8914e-04 - my_r2: 0.8711 - val_loss: 8.5096e-05 - val_my_r2: 0.9702\n",
      "Epoch 420/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3867e-04 - my_r2: 0.8476 - val_loss: 8.4234e-05 - val_my_r2: 0.9705\n",
      "Epoch 421/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6838e-04 - my_r2: 0.9259 - val_loss: 8.4512e-05 - val_my_r2: 0.9694\n",
      "Epoch 422/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7506e-04 - my_r2: 0.9370 - val_loss: 8.6983e-05 - val_my_r2: 0.9687\n",
      "Epoch 423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9204e-04 - my_r2: 0.8133 - val_loss: 1.0168e-04 - val_my_r2: 0.9621\n",
      "Epoch 424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5115e-04 - my_r2: 0.8117 - val_loss: 8.4588e-05 - val_my_r2: 0.9695\n",
      "Epoch 425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8334e-04 - my_r2: 0.9052 - val_loss: 8.2377e-05 - val_my_r2: 0.9708\n",
      "Epoch 426/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4413e-04 - my_r2: 0.9026 - val_loss: 8.3863e-05 - val_my_r2: 0.9703\n",
      "Epoch 427/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7657e-04 - my_r2: 0.8932 - val_loss: 8.1206e-05 - val_my_r2: 0.9709\n",
      "Epoch 428/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5868e-04 - my_r2: 0.9260 - val_loss: 8.1532e-05 - val_my_r2: 0.9710\n",
      "Epoch 429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3032e-04 - my_r2: 0.9197 - val_loss: 8.2640e-05 - val_my_r2: 0.9706\n",
      "Epoch 430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0065e-04 - my_r2: 0.9313 - val_loss: 8.3521e-05 - val_my_r2: 0.9697\n",
      "Epoch 431/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6077e-04 - my_r2: 0.9328 - val_loss: 8.2594e-05 - val_my_r2: 0.9704\n",
      "Epoch 432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8583e-04 - my_r2: 0.8716 - val_loss: 7.9635e-05 - val_my_r2: 0.9714\n",
      "Epoch 433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1448e-04 - my_r2: 0.9080 - val_loss: 8.4486e-05 - val_my_r2: 0.9702\n",
      "Epoch 434/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2043e-04 - my_r2: 0.9252 - val_loss: 8.1681e-05 - val_my_r2: 0.9710\n",
      "Epoch 435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1733e-04 - my_r2: 0.7344 - val_loss: 7.8396e-05 - val_my_r2: 0.9719\n",
      "Epoch 436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8401e-04 - my_r2: 0.9069 - val_loss: 7.7855e-05 - val_my_r2: 0.9718\n",
      "Epoch 437/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4947e-04 - my_r2: 0.9213 - val_loss: 7.9009e-05 - val_my_r2: 0.9717\n",
      "Epoch 438/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6954e-04 - my_r2: 0.9054 - val_loss: 8.0301e-05 - val_my_r2: 0.9716\n",
      "Epoch 439/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6505e-04 - my_r2: 0.9177 - val_loss: 7.8677e-05 - val_my_r2: 0.9721\n",
      "Epoch 440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0141e-04 - my_r2: 0.9317 - val_loss: 7.4059e-05 - val_my_r2: 0.9732\n",
      "Epoch 441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3263e-04 - my_r2: 0.9001 - val_loss: 7.2989e-05 - val_my_r2: 0.9736\n",
      "Epoch 442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4559e-04 - my_r2: 0.9042 - val_loss: 7.2423e-05 - val_my_r2: 0.9741\n",
      "Epoch 443/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1785e-04 - my_r2: 0.9044 - val_loss: 7.4501e-05 - val_my_r2: 0.9732\n",
      "Epoch 444/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9198e-04 - my_r2: 0.9388 - val_loss: 7.6324e-05 - val_my_r2: 0.9725\n",
      "Epoch 445/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4456e-04 - my_r2: 0.8803 - val_loss: 7.8235e-05 - val_my_r2: 0.9724\n",
      "Epoch 446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4991e-04 - my_r2: 0.9070 - val_loss: 8.0112e-05 - val_my_r2: 0.9717\n",
      "Epoch 447/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1231e-04 - my_r2: 0.9089 - val_loss: 7.4176e-05 - val_my_r2: 0.9739\n",
      "Epoch 448/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0270e-04 - my_r2: 0.8726 - val_loss: 7.3538e-05 - val_my_r2: 0.9739\n",
      "Epoch 449/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6086e-04 - my_r2: 0.8924 - val_loss: 7.0936e-05 - val_my_r2: 0.9745\n",
      "Epoch 450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4547e-04 - my_r2: 0.9456 - val_loss: 7.1976e-05 - val_my_r2: 0.9741\n",
      "Epoch 451/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1505e-04 - my_r2: 0.9310 - val_loss: 7.2866e-05 - val_my_r2: 0.9735\n",
      "Epoch 452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2878e-04 - my_r2: 0.8976 - val_loss: 7.2621e-05 - val_my_r2: 0.9734\n",
      "Epoch 453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6079e-04 - my_r2: 0.9218 - val_loss: 7.4276e-05 - val_my_r2: 0.9731\n",
      "Epoch 454/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5118e-04 - my_r2: 0.9150 - val_loss: 7.6827e-05 - val_my_r2: 0.9721\n",
      "Epoch 455/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8476e-04 - my_r2: 0.9349 - val_loss: 7.4181e-05 - val_my_r2: 0.9731\n",
      "Epoch 456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1215e-04 - my_r2: 0.9000 - val_loss: 7.4334e-05 - val_my_r2: 0.9736\n",
      "Epoch 457/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1727e-04 - my_r2: 0.9242 - val_loss: 7.2585e-05 - val_my_r2: 0.9744\n",
      "Epoch 458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2063e-04 - my_r2: 0.9383 - val_loss: 7.1388e-05 - val_my_r2: 0.9747\n",
      "Epoch 459/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1095e-04 - my_r2: 0.9098 - val_loss: 7.1598e-05 - val_my_r2: 0.9742\n",
      "Epoch 460/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4048e-04 - my_r2: 0.9142 - val_loss: 7.3717e-05 - val_my_r2: 0.9726\n",
      "Epoch 461/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2015e-04 - my_r2: 0.8714 - val_loss: 8.0352e-05 - val_my_r2: 0.9719\n",
      "Epoch 462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1415e-04 - my_r2: 0.9198 - val_loss: 7.0527e-05 - val_my_r2: 0.9732\n",
      "Epoch 463/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1605e-04 - my_r2: 0.9154 - val_loss: 7.1024e-05 - val_my_r2: 0.9733\n",
      "Epoch 464/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3040e-04 - my_r2: 0.8707 - val_loss: 6.7042e-05 - val_my_r2: 0.9757\n",
      "Epoch 465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3477e-04 - my_r2: 0.8753 - val_loss: 6.5358e-05 - val_my_r2: 0.9763\n",
      "Epoch 466/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7970e-04 - my_r2: 0.9255 - val_loss: 6.4502e-05 - val_my_r2: 0.9769\n",
      "Epoch 467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0406e-04 - my_r2: 0.9139 - val_loss: 6.7496e-05 - val_my_r2: 0.9760\n",
      "Epoch 468/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7056e-04 - my_r2: 0.8771 - val_loss: 6.5934e-05 - val_my_r2: 0.9761\n",
      "Epoch 469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6520e-04 - my_r2: 0.8968 - val_loss: 6.8253e-05 - val_my_r2: 0.9752\n",
      "Epoch 470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4487e-04 - my_r2: 0.9238 - val_loss: 6.7121e-05 - val_my_r2: 0.9758\n",
      "Epoch 471/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3183e-04 - my_r2: 0.8945 - val_loss: 6.6954e-05 - val_my_r2: 0.9760\n",
      "Epoch 472/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0803e-04 - my_r2: 0.8925 - val_loss: 6.4503e-05 - val_my_r2: 0.9763\n",
      "Epoch 473/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7379e-04 - my_r2: 0.7978 - val_loss: 6.4652e-05 - val_my_r2: 0.9757\n",
      "Epoch 474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0377e-04 - my_r2: 0.8809 - val_loss: 6.8767e-05 - val_my_r2: 0.9744\n",
      "Epoch 475/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4353e-04 - my_r2: 0.9089 - val_loss: 7.0670e-05 - val_my_r2: 0.9740\n",
      "Epoch 476/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7024e-04 - my_r2: 0.9080 - val_loss: 6.7685e-05 - val_my_r2: 0.9749\n",
      "Epoch 477/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1691e-04 - my_r2: 0.8867 - val_loss: 6.8759e-05 - val_my_r2: 0.9745\n",
      "Epoch 478/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4049e-04 - my_r2: 0.9095 - val_loss: 6.8318e-05 - val_my_r2: 0.9748\n",
      "Epoch 479/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5423e-04 - my_r2: 0.9254 - val_loss: 6.6223e-05 - val_my_r2: 0.9754\n",
      "Epoch 480/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7541e-04 - my_r2: 0.9030 - val_loss: 6.6242e-05 - val_my_r2: 0.9753\n",
      "Epoch 481/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6816e-04 - my_r2: 0.9377 - val_loss: 6.4614e-05 - val_my_r2: 0.9768\n",
      "Epoch 482/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1156e-04 - my_r2: 0.8955 - val_loss: 6.9511e-05 - val_my_r2: 0.9750\n",
      "Epoch 483/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1835e-04 - my_r2: 0.8798 - val_loss: 6.6584e-05 - val_my_r2: 0.9761\n",
      "Epoch 484/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2660e-04 - my_r2: 0.8797 - val_loss: 6.4000e-05 - val_my_r2: 0.9769\n",
      "Epoch 485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0214e-04 - my_r2: 0.9269 - val_loss: 6.4405e-05 - val_my_r2: 0.9764\n",
      "Epoch 486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8588e-04 - my_r2: 0.9437 - val_loss: 6.1471e-05 - val_my_r2: 0.9777\n",
      "Epoch 487/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2072e-04 - my_r2: 0.8939 - val_loss: 6.2260e-05 - val_my_r2: 0.9776\n",
      "Epoch 488/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5600e-04 - my_r2: 0.9312 - val_loss: 6.3640e-05 - val_my_r2: 0.9770\n",
      "Epoch 489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9222e-04 - my_r2: 0.8976 - val_loss: 6.2647e-05 - val_my_r2: 0.9771\n",
      "Epoch 490/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2932e-04 - my_r2: 0.9156 - val_loss: 6.2050e-05 - val_my_r2: 0.9770\n",
      "Epoch 491/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1338e-04 - my_r2: 0.9201 - val_loss: 6.2019e-05 - val_my_r2: 0.9768\n",
      "Epoch 492/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0593e-04 - my_r2: 0.9264 - val_loss: 6.0217e-05 - val_my_r2: 0.9772\n",
      "Epoch 493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1979e-04 - my_r2: 0.8839 - val_loss: 6.2983e-05 - val_my_r2: 0.9763\n",
      "Epoch 494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1316e-04 - my_r2: 0.8914 - val_loss: 5.9823e-05 - val_my_r2: 0.9775\n",
      "Epoch 495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2706e-04 - my_r2: 0.9187 - val_loss: 6.4544e-05 - val_my_r2: 0.9773\n",
      "Epoch 496/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2091e-04 - my_r2: 0.9000 - val_loss: 6.0723e-05 - val_my_r2: 0.9789\n",
      "Epoch 497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8553e-04 - my_r2: 0.9106 - val_loss: 5.8769e-05 - val_my_r2: 0.9788\n",
      "Epoch 498/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9003e-04 - my_r2: 0.8934 - val_loss: 6.0549e-05 - val_my_r2: 0.9782\n",
      "Epoch 499/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9198e-04 - my_r2: 0.9317 - val_loss: 5.8720e-05 - val_my_r2: 0.9787\n",
      "Epoch 500/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.9074e-04 - my_r2: 0.8555 - val_loss: 5.6181e-05 - val_my_r2: 0.9797\n",
      "Epoch 501/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2250e-04 - my_r2: 0.8997 - val_loss: 6.0969e-05 - val_my_r2: 0.9776\n",
      "Epoch 502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2884e-04 - my_r2: 0.9066 - val_loss: 5.9999e-05 - val_my_r2: 0.9780\n",
      "Epoch 503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9839e-04 - my_r2: 0.8770 - val_loss: 5.9204e-05 - val_my_r2: 0.9783\n",
      "Epoch 504/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4916e-04 - my_r2: 0.9030 - val_loss: 5.9151e-05 - val_my_r2: 0.9783\n",
      "Epoch 505/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4930e-04 - my_r2: 0.8869 - val_loss: 6.6193e-05 - val_my_r2: 0.9761\n",
      "Epoch 506/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6172e-04 - my_r2: 0.8732 - val_loss: 6.0593e-05 - val_my_r2: 0.9780\n",
      "Epoch 507/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2415e-04 - my_r2: 0.9056 - val_loss: 5.6514e-05 - val_my_r2: 0.9793\n",
      "Epoch 508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7523e-04 - my_r2: 0.9042 - val_loss: 5.8497e-05 - val_my_r2: 0.9780\n",
      "Epoch 509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9343e-04 - my_r2: 0.8740 - val_loss: 5.5201e-05 - val_my_r2: 0.9794\n",
      "Epoch 510/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6248e-04 - my_r2: 0.8819 - val_loss: 6.1908e-05 - val_my_r2: 0.9781\n",
      "Epoch 511/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.7738e-04 - my_r2: 0.9417 - val_loss: 5.6762e-05 - val_my_r2: 0.9792\n",
      "Epoch 512/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7640e-04 - my_r2: 0.9285 - val_loss: 6.2937e-05 - val_my_r2: 0.9777\n",
      "Epoch 513/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5370e-04 - my_r2: 0.8389 - val_loss: 6.7289e-05 - val_my_r2: 0.9769\n",
      "Epoch 514/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9623e-04 - my_r2: 0.9085 - val_loss: 6.8111e-05 - val_my_r2: 0.9764\n",
      "Epoch 515/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2680e-04 - my_r2: 0.9204 - val_loss: 6.4770e-05 - val_my_r2: 0.9770\n",
      "Epoch 516/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2561e-04 - my_r2: 0.8955 - val_loss: 5.8775e-05 - val_my_r2: 0.9788\n",
      "Epoch 517/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5866e-04 - my_r2: 0.9261 - val_loss: 5.7534e-05 - val_my_r2: 0.9793\n",
      "Epoch 518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6224e-04 - my_r2: 0.7712 - val_loss: 5.7312e-05 - val_my_r2: 0.9794\n",
      "Epoch 519/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5238e-04 - my_r2: 0.9208 - val_loss: 6.4202e-05 - val_my_r2: 0.9778\n",
      "Epoch 520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9629e-04 - my_r2: 0.9228 - val_loss: 6.2645e-05 - val_my_r2: 0.9779\n",
      "Epoch 521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1755e-04 - my_r2: 0.9045 - val_loss: 5.2085e-05 - val_my_r2: 0.9812\n",
      "Epoch 522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1255e-04 - my_r2: 0.9287 - val_loss: 5.2975e-05 - val_my_r2: 0.9810\n",
      "Epoch 523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2790e-04 - my_r2: 0.9089 - val_loss: 5.2962e-05 - val_my_r2: 0.9811\n",
      "Epoch 524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8886e-04 - my_r2: 0.9028 - val_loss: 5.2685e-05 - val_my_r2: 0.9816\n",
      "Epoch 525/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3931e-04 - my_r2: 0.9245 - val_loss: 5.7437e-05 - val_my_r2: 0.9805\n",
      "Epoch 526/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3697e-04 - my_r2: 0.9410 - val_loss: 5.5380e-05 - val_my_r2: 0.9801\n",
      "Epoch 527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2775e-04 - my_r2: 0.8861 - val_loss: 6.1186e-05 - val_my_r2: 0.9777\n",
      "Epoch 528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8894e-04 - my_r2: 0.8729 - val_loss: 5.6802e-05 - val_my_r2: 0.9784\n",
      "Epoch 529/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.4565e-04 - my_r2: 0.8902 - val_loss: 5.5265e-05 - val_my_r2: 0.9790\n",
      "Epoch 530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3102e-04 - my_r2: 0.8778 - val_loss: 5.4464e-05 - val_my_r2: 0.9801\n",
      "Epoch 531/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3656e-04 - my_r2: 0.7989 - val_loss: 5.5101e-05 - val_my_r2: 0.9800\n",
      "Epoch 532/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2715e-04 - my_r2: 0.8867 - val_loss: 5.1046e-05 - val_my_r2: 0.9805\n",
      "Epoch 533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2543e-04 - my_r2: 0.9178 - val_loss: 5.1252e-05 - val_my_r2: 0.9803\n",
      "Epoch 534/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9031e-04 - my_r2: 0.9064 - val_loss: 5.7943e-05 - val_my_r2: 0.9791\n",
      "Epoch 535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4032e-04 - my_r2: 0.9292 - val_loss: 5.4612e-05 - val_my_r2: 0.9795\n",
      "Epoch 536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6759e-04 - my_r2: 0.9072 - val_loss: 5.2292e-05 - val_my_r2: 0.9810\n",
      "Epoch 537/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6713e-04 - my_r2: 0.9138 - val_loss: 5.4334e-05 - val_my_r2: 0.9808\n",
      "Epoch 538/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7256e-04 - my_r2: 0.9451 - val_loss: 4.8473e-05 - val_my_r2: 0.9821\n",
      "Epoch 539/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0480e-04 - my_r2: 0.9011 - val_loss: 5.0100e-05 - val_my_r2: 0.9818\n",
      "Epoch 540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8391e-04 - my_r2: 0.9112 - val_loss: 5.0639e-05 - val_my_r2: 0.9816\n",
      "Epoch 541/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3624e-04 - my_r2: 0.9413 - val_loss: 5.1118e-05 - val_my_r2: 0.9815\n",
      "Epoch 542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5556e-04 - my_r2: 0.8738 - val_loss: 5.2667e-05 - val_my_r2: 0.9806\n",
      "Epoch 543/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1367e-04 - my_r2: 0.8956 - val_loss: 4.9173e-05 - val_my_r2: 0.9818\n",
      "Epoch 544/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9465e-04 - my_r2: 0.9107 - val_loss: 4.8731e-05 - val_my_r2: 0.9817\n",
      "Epoch 545/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9703e-04 - my_r2: 0.9078 - val_loss: 5.3492e-05 - val_my_r2: 0.9809\n",
      "Epoch 546/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1887e-04 - my_r2: 0.9311 - val_loss: 5.9078e-05 - val_my_r2: 0.9797\n",
      "Epoch 547/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0329e-04 - my_r2: 0.9373 - val_loss: 5.5848e-05 - val_my_r2: 0.9805\n",
      "Epoch 548/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0637e-04 - my_r2: 0.9042 - val_loss: 5.0002e-05 - val_my_r2: 0.9819\n",
      "Epoch 549/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2869e-04 - my_r2: 0.6726 - val_loss: 4.9124e-05 - val_my_r2: 0.9825\n",
      "Epoch 550/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.1536e-04 - my_r2: 0.8731 - val_loss: 4.8155e-05 - val_my_r2: 0.9828\n",
      "Epoch 551/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2177e-04 - my_r2: 0.9265 - val_loss: 5.8283e-05 - val_my_r2: 0.9793\n",
      "Epoch 552/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2911e-04 - my_r2: 0.8930 - val_loss: 5.1751e-05 - val_my_r2: 0.9817\n",
      "Epoch 553/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5520e-04 - my_r2: 0.7391 - val_loss: 5.1415e-05 - val_my_r2: 0.9818\n",
      "Epoch 554/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2147e-04 - my_r2: 0.9201 - val_loss: 4.8470e-05 - val_my_r2: 0.9825\n",
      "Epoch 555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9768e-04 - my_r2: 0.9351 - val_loss: 4.8596e-05 - val_my_r2: 0.9821\n",
      "Epoch 556/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2701e-04 - my_r2: 0.9228 - val_loss: 5.3756e-05 - val_my_r2: 0.9789\n",
      "Epoch 557/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7245e-04 - my_r2: 0.8913 - val_loss: 4.7535e-05 - val_my_r2: 0.9814\n",
      "Epoch 558/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2547e-04 - my_r2: 0.8541 - val_loss: 4.8396e-05 - val_my_r2: 0.9810\n",
      "Epoch 559/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1220e-04 - my_r2: 0.9067 - val_loss: 5.2413e-05 - val_my_r2: 0.9790\n",
      "Epoch 560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6903e-04 - my_r2: 0.9223 - val_loss: 4.8095e-05 - val_my_r2: 0.9816\n",
      "Epoch 561/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7592e-04 - my_r2: 0.9325 - val_loss: 4.6823e-05 - val_my_r2: 0.9829\n",
      "Epoch 562/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1273e-04 - my_r2: 0.9058 - val_loss: 4.9173e-05 - val_my_r2: 0.9821\n",
      "Epoch 563/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2225e-04 - my_r2: 0.8910 - val_loss: 4.6364e-05 - val_my_r2: 0.9827\n",
      "Epoch 564/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3717e-04 - my_r2: 0.9362 - val_loss: 4.6360e-05 - val_my_r2: 0.9823\n",
      "Epoch 565/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8517e-04 - my_r2: 0.9425 - val_loss: 4.8131e-05 - val_my_r2: 0.9819\n",
      "Epoch 566/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8946e-04 - my_r2: 0.9213 - val_loss: 4.8045e-05 - val_my_r2: 0.9823\n",
      "Epoch 567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5956e-04 - my_r2: 0.7983 - val_loss: 5.0700e-05 - val_my_r2: 0.9817\n",
      "Epoch 568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4397e-04 - my_r2: 0.9385 - val_loss: 5.0093e-05 - val_my_r2: 0.9819\n",
      "Epoch 569/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8817e-04 - my_r2: 0.9284 - val_loss: 4.4387e-05 - val_my_r2: 0.9830\n",
      "Epoch 570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9603e-04 - my_r2: 0.9248 - val_loss: 4.5242e-05 - val_my_r2: 0.9822\n",
      "Epoch 571/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5970e-04 - my_r2: 0.9306 - val_loss: 4.5426e-05 - val_my_r2: 0.9824\n",
      "Epoch 572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1103e-04 - my_r2: 0.9371 - val_loss: 4.2966e-05 - val_my_r2: 0.9837\n",
      "Epoch 573/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0533e-04 - my_r2: 0.9067 - val_loss: 4.1677e-05 - val_my_r2: 0.9843\n",
      "Epoch 574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3188e-04 - my_r2: 0.8833 - val_loss: 4.3578e-05 - val_my_r2: 0.9836\n",
      "Epoch 575/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3909e-04 - my_r2: 0.7999 - val_loss: 4.6859e-05 - val_my_r2: 0.9820\n",
      "Epoch 576/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1995e-04 - my_r2: 0.9288 - val_loss: 4.3890e-05 - val_my_r2: 0.9830\n",
      "Epoch 577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9405e-04 - my_r2: 0.9150 - val_loss: 4.1362e-05 - val_my_r2: 0.9838\n",
      "Epoch 578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0620e-04 - my_r2: 0.9194 - val_loss: 4.1360e-05 - val_my_r2: 0.9839\n",
      "Epoch 579/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0039e-04 - my_r2: 0.9317 - val_loss: 4.4650e-05 - val_my_r2: 0.9828\n",
      "Epoch 580/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9128e-04 - my_r2: 0.9241 - val_loss: 4.5903e-05 - val_my_r2: 0.9825\n",
      "Epoch 581/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7765e-04 - my_r2: 0.8984 - val_loss: 4.3104e-05 - val_my_r2: 0.9839\n",
      "Epoch 582/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9751e-04 - my_r2: 0.9083 - val_loss: 4.4476e-05 - val_my_r2: 0.9838\n",
      "Epoch 583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0660e-04 - my_r2: 0.9319 - val_loss: 4.2551e-05 - val_my_r2: 0.9841\n",
      "Epoch 584/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5061e-04 - my_r2: 0.9123 - val_loss: 4.3451e-05 - val_my_r2: 0.9842\n",
      "Epoch 585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7698e-04 - my_r2: 0.9247 - val_loss: 4.6037e-05 - val_my_r2: 0.9832\n",
      "Epoch 586/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0042e-04 - my_r2: 0.9076 - val_loss: 4.8893e-05 - val_my_r2: 0.9817\n",
      "Epoch 587/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2861e-04 - my_r2: 0.9335 - val_loss: 5.0376e-05 - val_my_r2: 0.9811\n",
      "Epoch 588/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4161e-04 - my_r2: 0.9075 - val_loss: 4.9923e-05 - val_my_r2: 0.9814\n",
      "Epoch 589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0514e-04 - my_r2: 0.9229 - val_loss: 4.6602e-05 - val_my_r2: 0.9829\n",
      "Epoch 590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6900e-04 - my_r2: 0.9153 - val_loss: 4.5074e-05 - val_my_r2: 0.9830\n",
      "Epoch 591/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3326e-04 - my_r2: 0.8927 - val_loss: 4.4064e-05 - val_my_r2: 0.9835\n",
      "Epoch 592/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9904e-04 - my_r2: 0.9112 - val_loss: 5.1015e-05 - val_my_r2: 0.9815\n",
      "Epoch 593/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2356e-04 - my_r2: 0.9242 - val_loss: 4.7591e-05 - val_my_r2: 0.9829\n",
      "Epoch 594/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3695e-04 - my_r2: 0.9106 - val_loss: 4.9749e-05 - val_my_r2: 0.9821\n",
      "Epoch 595/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0721e-04 - my_r2: 0.8976 - val_loss: 4.4997e-05 - val_my_r2: 0.9844\n",
      "Epoch 596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1866e-04 - my_r2: 0.9130 - val_loss: 4.4717e-05 - val_my_r2: 0.9847\n",
      "Epoch 597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0188e-04 - my_r2: 0.9074 - val_loss: 4.0420e-05 - val_my_r2: 0.9858\n",
      "Epoch 598/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5902e-04 - my_r2: 0.9350 - val_loss: 4.4129e-05 - val_my_r2: 0.9847\n",
      "Epoch 599/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8282e-04 - my_r2: 0.9126 - val_loss: 4.2998e-05 - val_my_r2: 0.9845\n",
      "Epoch 600/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2315e-04 - my_r2: 0.9575 - val_loss: 4.2546e-05 - val_my_r2: 0.9846\n",
      "Epoch 601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3883e-04 - my_r2: 0.8667 - val_loss: 4.1446e-05 - val_my_r2: 0.9852\n",
      "Epoch 602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2916e-04 - my_r2: 0.9336 - val_loss: 4.0707e-05 - val_my_r2: 0.9859\n",
      "Epoch 603/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3548e-04 - my_r2: 0.9158 - val_loss: 4.6783e-05 - val_my_r2: 0.9830\n",
      "Epoch 604/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7121e-04 - my_r2: 0.9304 - val_loss: 4.4122e-05 - val_my_r2: 0.9848\n",
      "Epoch 605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3789e-04 - my_r2: 0.9132 - val_loss: 4.4129e-05 - val_my_r2: 0.9847\n",
      "Epoch 606/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2152e-04 - my_r2: 0.9236 - val_loss: 4.6498e-05 - val_my_r2: 0.9846\n",
      "Epoch 607/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4967e-04 - my_r2: 0.9290 - val_loss: 4.1833e-05 - val_my_r2: 0.9852\n",
      "Epoch 608/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0729e-04 - my_r2: 0.9283 - val_loss: 4.7934e-05 - val_my_r2: 0.9819\n",
      "Epoch 609/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9564e-04 - my_r2: 0.9067 - val_loss: 4.0441e-05 - val_my_r2: 0.9855\n",
      "Epoch 610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1681e-04 - my_r2: 0.9025 - val_loss: 3.9385e-05 - val_my_r2: 0.9861\n",
      "Epoch 611/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4570e-04 - my_r2: 0.9009 - val_loss: 4.2387e-05 - val_my_r2: 0.9856\n",
      "Epoch 612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4589e-04 - my_r2: 0.9494 - val_loss: 4.0490e-05 - val_my_r2: 0.9860\n",
      "Epoch 613/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0266e-04 - my_r2: 0.8895 - val_loss: 3.9544e-05 - val_my_r2: 0.9860\n",
      "Epoch 614/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5654e-04 - my_r2: 0.9235 - val_loss: 4.4807e-05 - val_my_r2: 0.9843\n",
      "Epoch 615/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1786e-04 - my_r2: 0.9155 - val_loss: 4.1608e-05 - val_my_r2: 0.9848\n",
      "Epoch 616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1241e-04 - my_r2: 0.9487 - val_loss: 4.2491e-05 - val_my_r2: 0.9845\n",
      "Epoch 617/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1090e-04 - my_r2: 0.9240 - val_loss: 3.7990e-05 - val_my_r2: 0.9861\n",
      "Epoch 618/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0433e-04 - my_r2: 0.9239 - val_loss: 4.0184e-05 - val_my_r2: 0.9847\n",
      "Epoch 619/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0042e-04 - my_r2: 0.9250 - val_loss: 3.9774e-05 - val_my_r2: 0.9860\n",
      "Epoch 620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1258e-04 - my_r2: 0.8937 - val_loss: 3.8195e-05 - val_my_r2: 0.9864\n",
      "Epoch 621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5303e-04 - my_r2: 0.9228 - val_loss: 4.0295e-05 - val_my_r2: 0.9857\n",
      "Epoch 622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3052e-04 - my_r2: 0.8186 - val_loss: 3.9468e-05 - val_my_r2: 0.9853\n",
      "Epoch 623/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5893e-04 - my_r2: 0.9324 - val_loss: 4.0333e-05 - val_my_r2: 0.9849\n",
      "Epoch 624/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0109e-04 - my_r2: 0.9266 - val_loss: 4.3996e-05 - val_my_r2: 0.9836\n",
      "Epoch 625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6148e-04 - my_r2: 0.9294 - val_loss: 4.4410e-05 - val_my_r2: 0.9842\n",
      "Epoch 626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0784e-04 - my_r2: 0.9041 - val_loss: 4.1277e-05 - val_my_r2: 0.9849\n",
      "Epoch 627/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1978e-04 - my_r2: 0.9354 - val_loss: 3.7754e-05 - val_my_r2: 0.9857\n",
      "Epoch 628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9143e-04 - my_r2: 0.9341 - val_loss: 3.8705e-05 - val_my_r2: 0.9852\n",
      "Epoch 629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4850e-04 - my_r2: 0.9401 - val_loss: 3.6911e-05 - val_my_r2: 0.9861\n",
      "Epoch 630/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4336e-04 - my_r2: 0.9223 - val_loss: 3.6983e-05 - val_my_r2: 0.9860\n",
      "Epoch 631/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1217e-04 - my_r2: 0.9362 - val_loss: 3.9063e-05 - val_my_r2: 0.9853\n",
      "Epoch 632/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3358e-04 - my_r2: 0.9272 - val_loss: 3.6153e-05 - val_my_r2: 0.9867\n",
      "Epoch 633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7861e-04 - my_r2: 0.9136 - val_loss: 3.6862e-05 - val_my_r2: 0.9861\n",
      "Epoch 634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0548e-04 - my_r2: 0.9323 - val_loss: 3.9053e-05 - val_my_r2: 0.9853\n",
      "Epoch 635/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8030e-04 - my_r2: 0.9154 - val_loss: 3.9968e-05 - val_my_r2: 0.9851\n",
      "Epoch 636/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4658e-04 - my_r2: 0.9292 - val_loss: 3.6940e-05 - val_my_r2: 0.9864\n",
      "Epoch 637/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2190e-04 - my_r2: 0.9236 - val_loss: 3.6602e-05 - val_my_r2: 0.9870\n",
      "Epoch 638/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2376e-04 - my_r2: 0.9090 - val_loss: 3.9420e-05 - val_my_r2: 0.9864\n",
      "Epoch 639/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9882e-04 - my_r2: 0.8525 - val_loss: 3.7935e-05 - val_my_r2: 0.9861\n",
      "Epoch 640/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7143e-04 - my_r2: 0.9429 - val_loss: 3.6262e-05 - val_my_r2: 0.9868\n",
      "Epoch 641/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3742e-04 - my_r2: 0.9389 - val_loss: 3.3784e-05 - val_my_r2: 0.9881\n",
      "Epoch 642/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8664e-04 - my_r2: 0.9142 - val_loss: 3.3510e-05 - val_my_r2: 0.9878\n",
      "Epoch 643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6165e-04 - my_r2: 0.9292 - val_loss: 3.6097e-05 - val_my_r2: 0.9869\n",
      "Epoch 644/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7626e-04 - my_r2: 0.8386 - val_loss: 3.7263e-05 - val_my_r2: 0.9868\n",
      "Epoch 645/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2907e-04 - my_r2: 0.9206 - val_loss: 3.6552e-05 - val_my_r2: 0.9872\n",
      "Epoch 646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6292e-04 - my_r2: 0.8847 - val_loss: 3.7299e-05 - val_my_r2: 0.9864\n",
      "Epoch 647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1069e-04 - my_r2: 0.9191 - val_loss: 3.2812e-05 - val_my_r2: 0.9879\n",
      "Epoch 648/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1762e-04 - my_r2: 0.9085 - val_loss: 3.6489e-05 - val_my_r2: 0.9868\n",
      "Epoch 649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0483e-04 - my_r2: 0.9546 - val_loss: 3.7346e-05 - val_my_r2: 0.9866\n",
      "Epoch 650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8690e-04 - my_r2: 0.7756 - val_loss: 3.4699e-05 - val_my_r2: 0.9870\n",
      "Epoch 651/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2271e-04 - my_r2: 0.9378 - val_loss: 3.2135e-05 - val_my_r2: 0.9878\n",
      "Epoch 652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5093e-04 - my_r2: 0.9053 - val_loss: 3.3285e-05 - val_my_r2: 0.9874\n",
      "Epoch 653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3292e-04 - my_r2: 0.9104 - val_loss: 3.5581e-05 - val_my_r2: 0.9864\n",
      "Epoch 654/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1235e-04 - my_r2: 0.9116 - val_loss: 3.2930e-05 - val_my_r2: 0.9875\n",
      "Epoch 655/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5135e-04 - my_r2: 0.9223 - val_loss: 3.3697e-05 - val_my_r2: 0.9874\n",
      "Epoch 656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0708e-04 - my_r2: 0.9407 - val_loss: 3.8819e-05 - val_my_r2: 0.9863\n",
      "Epoch 657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1719e-04 - my_r2: 0.9248 - val_loss: 3.5922e-05 - val_my_r2: 0.9873\n",
      "Epoch 658/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9310e-04 - my_r2: 0.9085 - val_loss: 3.9635e-05 - val_my_r2: 0.9860\n",
      "Epoch 659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8931e-04 - my_r2: 0.9404 - val_loss: 3.7049e-05 - val_my_r2: 0.9868\n",
      "Epoch 660/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2660e-04 - my_r2: 0.9130 - val_loss: 3.8258e-05 - val_my_r2: 0.9864\n",
      "Epoch 661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3376e-04 - my_r2: 0.9113 - val_loss: 3.6617e-05 - val_my_r2: 0.9869\n",
      "Epoch 662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5185e-04 - my_r2: 0.9359 - val_loss: 3.4996e-05 - val_my_r2: 0.9872\n",
      "Epoch 663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9644e-04 - my_r2: 0.9157 - val_loss: 3.4031e-05 - val_my_r2: 0.9877\n",
      "Epoch 664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0919e-04 - my_r2: 0.9187 - val_loss: 3.5500e-05 - val_my_r2: 0.9872\n",
      "Epoch 665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9720e-04 - my_r2: 0.9036 - val_loss: 3.2568e-05 - val_my_r2: 0.9884\n",
      "Epoch 666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3440e-04 - my_r2: 0.9349 - val_loss: 3.6862e-05 - val_my_r2: 0.9868\n",
      "Epoch 667/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0367e-04 - my_r2: 0.9098 - val_loss: 3.3679e-05 - val_my_r2: 0.9879\n",
      "Epoch 668/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8607e-04 - my_r2: 0.9253 - val_loss: 3.3349e-05 - val_my_r2: 0.9882\n",
      "Epoch 669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9154e-04 - my_r2: 0.9126 - val_loss: 3.3545e-05 - val_my_r2: 0.9886\n",
      "Epoch 670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4770e-04 - my_r2: 0.8541 - val_loss: 3.1700e-05 - val_my_r2: 0.9888\n",
      "Epoch 671/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6185e-04 - my_r2: 0.9216 - val_loss: 3.4111e-05 - val_my_r2: 0.9880\n",
      "Epoch 672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2918e-04 - my_r2: 0.9281 - val_loss: 3.3016e-05 - val_my_r2: 0.9888\n",
      "Epoch 673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6276e-04 - my_r2: 0.9364 - val_loss: 4.8557e-05 - val_my_r2: 0.9828\n",
      "Epoch 674/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1151e-04 - my_r2: 0.9345 - val_loss: 4.2236e-05 - val_my_r2: 0.9842\n",
      "Epoch 675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0097e-04 - my_r2: 0.9280 - val_loss: 3.1759e-05 - val_my_r2: 0.9886\n",
      "Epoch 676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6084e-04 - my_r2: 0.8720 - val_loss: 3.0282e-05 - val_my_r2: 0.9893\n",
      "Epoch 677/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3182e-04 - my_r2: 0.9228 - val_loss: 3.2873e-05 - val_my_r2: 0.9886\n",
      "Epoch 678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0311e-04 - my_r2: 0.9437 - val_loss: 3.2805e-05 - val_my_r2: 0.9886\n",
      "Epoch 679/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4908e-04 - my_r2: 0.9512 - val_loss: 3.2679e-05 - val_my_r2: 0.9884\n",
      "Epoch 680/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7144e-04 - my_r2: 0.8932 - val_loss: 3.4398e-05 - val_my_r2: 0.9880\n",
      "Epoch 681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9014e-04 - my_r2: 0.9199 - val_loss: 3.3132e-05 - val_my_r2: 0.9886\n",
      "Epoch 682/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7928e-04 - my_r2: 0.9054 - val_loss: 3.2266e-05 - val_my_r2: 0.9884\n",
      "Epoch 683/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2119e-04 - my_r2: 0.9040 - val_loss: 3.6116e-05 - val_my_r2: 0.9866\n",
      "Epoch 684/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8314e-04 - my_r2: 0.9420 - val_loss: 3.3312e-05 - val_my_r2: 0.9883\n",
      "Epoch 685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0597e-04 - my_r2: 0.9597 - val_loss: 3.1645e-05 - val_my_r2: 0.9883\n",
      "Epoch 686/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2900e-04 - my_r2: 0.9172 - val_loss: 3.1544e-05 - val_my_r2: 0.9889\n",
      "Epoch 687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6023e-04 - my_r2: 0.9424 - val_loss: 3.0635e-05 - val_my_r2: 0.9895\n",
      "Epoch 688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2491e-04 - my_r2: 0.9471 - val_loss: 3.3067e-05 - val_my_r2: 0.9883\n",
      "Epoch 689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1757e-04 - my_r2: 0.8887 - val_loss: 3.3214e-05 - val_my_r2: 0.9883\n",
      "Epoch 690/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0836e-04 - my_r2: 0.9219 - val_loss: 3.1207e-05 - val_my_r2: 0.9894\n",
      "Epoch 691/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2360e-04 - my_r2: 0.8996 - val_loss: 3.3437e-05 - val_my_r2: 0.9886\n",
      "Epoch 692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3426e-04 - my_r2: 0.9187 - val_loss: 2.9646e-05 - val_my_r2: 0.9897\n",
      "Epoch 693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2633e-04 - my_r2: 0.9210 - val_loss: 3.1689e-05 - val_my_r2: 0.9884\n",
      "Epoch 694/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2015e-04 - my_r2: 0.9086 - val_loss: 3.0884e-05 - val_my_r2: 0.9885\n",
      "Epoch 695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1793e-04 - my_r2: 0.9107 - val_loss: 2.8559e-05 - val_my_r2: 0.9896\n",
      "Epoch 696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8984e-04 - my_r2: 0.9067 - val_loss: 2.7541e-05 - val_my_r2: 0.9900\n",
      "Epoch 697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2902e-04 - my_r2: 0.9050 - val_loss: 3.1598e-05 - val_my_r2: 0.9885\n",
      "Epoch 698/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9679e-04 - my_r2: 0.9216 - val_loss: 3.0542e-05 - val_my_r2: 0.9887\n",
      "Epoch 699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3498e-04 - my_r2: 0.9196 - val_loss: 3.0370e-05 - val_my_r2: 0.9890\n",
      "Epoch 700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6512e-04 - my_r2: 0.9250 - val_loss: 3.0694e-05 - val_my_r2: 0.9891\n",
      "Epoch 701/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2976e-04 - my_r2: 0.9114 - val_loss: 2.9191e-05 - val_my_r2: 0.9896\n",
      "Epoch 702/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4761e-04 - my_r2: 0.9411 - val_loss: 2.9380e-05 - val_my_r2: 0.9898\n",
      "Epoch 703/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6502e-04 - my_r2: 0.9312 - val_loss: 2.7529e-05 - val_my_r2: 0.9904\n",
      "Epoch 704/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5436e-04 - my_r2: 0.9454 - val_loss: 2.8741e-05 - val_my_r2: 0.9898\n",
      "Epoch 705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6318e-04 - my_r2: 0.9388 - val_loss: 3.0700e-05 - val_my_r2: 0.9893\n",
      "Epoch 706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2246e-04 - my_r2: 0.9204 - val_loss: 3.0761e-05 - val_my_r2: 0.9892\n",
      "Epoch 707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9105e-04 - my_r2: 0.8932 - val_loss: 3.0595e-05 - val_my_r2: 0.9889\n",
      "Epoch 708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9015e-04 - my_r2: 0.9212 - val_loss: 3.1079e-05 - val_my_r2: 0.9892\n",
      "Epoch 709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2520e-04 - my_r2: 0.9068 - val_loss: 3.1017e-05 - val_my_r2: 0.9893\n",
      "Epoch 710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2633e-04 - my_r2: 0.8983 - val_loss: 3.2475e-05 - val_my_r2: 0.9885\n",
      "Epoch 711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0104e-04 - my_r2: 0.9051 - val_loss: 3.4100e-05 - val_my_r2: 0.9872\n",
      "Epoch 712/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8450e-04 - my_r2: 0.9228 - val_loss: 3.7413e-05 - val_my_r2: 0.9856\n",
      "Epoch 713/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7367e-04 - my_r2: 0.9366 - val_loss: 3.5670e-05 - val_my_r2: 0.9865\n",
      "Epoch 714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6007e-04 - my_r2: 0.8795 - val_loss: 3.3211e-05 - val_my_r2: 0.9876\n",
      "Epoch 715/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2758e-04 - my_r2: 0.8527 - val_loss: 3.0541e-05 - val_my_r2: 0.9895\n",
      "Epoch 716/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8152e-04 - my_r2: 0.9351 - val_loss: 2.9827e-05 - val_my_r2: 0.9894\n",
      "Epoch 717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6250e-04 - my_r2: 0.9231 - val_loss: 2.9471e-05 - val_my_r2: 0.9898\n",
      "Epoch 718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0178e-04 - my_r2: 0.9337 - val_loss: 2.9981e-05 - val_my_r2: 0.9891\n",
      "Epoch 719/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7010e-04 - my_r2: 0.9033 - val_loss: 3.1335e-05 - val_my_r2: 0.9882\n",
      "Epoch 720/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0592e-04 - my_r2: 0.9157 - val_loss: 2.7570e-05 - val_my_r2: 0.9899\n",
      "Epoch 721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5642e-04 - my_r2: 0.9343 - val_loss: 2.6572e-05 - val_my_r2: 0.9905\n",
      "Epoch 722/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2877e-04 - my_r2: 0.9493 - val_loss: 3.1029e-05 - val_my_r2: 0.9891\n",
      "Epoch 723/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3196e-04 - my_r2: 0.9401 - val_loss: 3.1392e-05 - val_my_r2: 0.9890\n",
      "Epoch 724/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5406e-04 - my_r2: 0.8978 - val_loss: 2.9165e-05 - val_my_r2: 0.9900\n",
      "Epoch 725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2681e-04 - my_r2: 0.9208 - val_loss: 2.7306e-05 - val_my_r2: 0.9906\n",
      "Epoch 726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0386e-04 - my_r2: 0.8671 - val_loss: 2.5995e-05 - val_my_r2: 0.9911\n",
      "Epoch 727/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1371e-04 - my_r2: 0.9430 - val_loss: 2.8804e-05 - val_my_r2: 0.9901\n",
      "Epoch 728/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6241e-04 - my_r2: 0.8991 - val_loss: 2.8832e-05 - val_my_r2: 0.9903\n",
      "Epoch 729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0236e-04 - my_r2: 0.8617 - val_loss: 3.0775e-05 - val_my_r2: 0.9890\n",
      "Epoch 730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3812e-04 - my_r2: 0.9346 - val_loss: 3.1019e-05 - val_my_r2: 0.9886\n",
      "Epoch 731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5853e-04 - my_r2: 0.9268 - val_loss: 2.6476e-05 - val_my_r2: 0.9906\n",
      "Epoch 732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5361e-04 - my_r2: 0.9439 - val_loss: 2.5881e-05 - val_my_r2: 0.9910\n",
      "Epoch 733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2207e-04 - my_r2: 0.9332 - val_loss: 3.4793e-05 - val_my_r2: 0.9880\n",
      "Epoch 734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9292e-04 - my_r2: 0.8863 - val_loss: 3.1201e-05 - val_my_r2: 0.9891\n",
      "Epoch 735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7689e-04 - my_r2: 0.8886 - val_loss: 3.2429e-05 - val_my_r2: 0.9879\n",
      "Epoch 736/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9107e-04 - my_r2: 0.9350 - val_loss: 3.3544e-05 - val_my_r2: 0.9878\n",
      "Epoch 737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0559e-04 - my_r2: 0.9207 - val_loss: 3.3482e-05 - val_my_r2: 0.9883\n",
      "Epoch 738/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8385e-04 - my_r2: 0.9275 - val_loss: 3.2567e-05 - val_my_r2: 0.9895\n",
      "Epoch 739/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2676e-04 - my_r2: 0.9024 - val_loss: 3.1777e-05 - val_my_r2: 0.9895\n",
      "Epoch 740/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5216e-04 - my_r2: 0.9150 - val_loss: 3.1890e-05 - val_my_r2: 0.9893\n",
      "Epoch 741/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4039e-04 - my_r2: 0.9550 - val_loss: 2.9509e-05 - val_my_r2: 0.9892\n",
      "Epoch 742/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1560e-04 - my_r2: 0.8325 - val_loss: 2.9236e-05 - val_my_r2: 0.9905\n",
      "Epoch 743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8284e-04 - my_r2: 0.9120 - val_loss: 2.9688e-05 - val_my_r2: 0.9903\n",
      "Epoch 744/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9527e-04 - my_r2: 0.9438 - val_loss: 3.1453e-05 - val_my_r2: 0.9896\n",
      "Epoch 745/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3179e-04 - my_r2: 0.8994 - val_loss: 2.8582e-05 - val_my_r2: 0.9905\n",
      "Epoch 746/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2668e-04 - my_r2: 0.9197 - val_loss: 2.9837e-05 - val_my_r2: 0.9895\n",
      "Epoch 747/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3506e-04 - my_r2: 0.9077 - val_loss: 3.3136e-05 - val_my_r2: 0.9883\n",
      "Epoch 748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7709e-04 - my_r2: 0.9382 - val_loss: 2.9321e-05 - val_my_r2: 0.9889\n",
      "Epoch 749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9482e-04 - my_r2: 0.9256 - val_loss: 2.7569e-05 - val_my_r2: 0.9899\n",
      "Epoch 750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4720e-04 - my_r2: 0.9256 - val_loss: 3.4677e-05 - val_my_r2: 0.9884\n",
      "Epoch 751/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3779e-04 - my_r2: 0.9128 - val_loss: 3.0169e-05 - val_my_r2: 0.9896\n",
      "Epoch 752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6048e-04 - my_r2: 0.9197 - val_loss: 3.1140e-05 - val_my_r2: 0.9893\n",
      "Epoch 753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7741e-04 - my_r2: 0.9299 - val_loss: 2.7618e-05 - val_my_r2: 0.9905\n",
      "Epoch 754/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3971e-04 - my_r2: 0.9294 - val_loss: 2.7756e-05 - val_my_r2: 0.9900\n",
      "Epoch 755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8191e-04 - my_r2: 0.9108 - val_loss: 2.6035e-05 - val_my_r2: 0.9908\n",
      "Epoch 756/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0069e-04 - my_r2: 0.9153 - val_loss: 2.6910e-05 - val_my_r2: 0.9904\n",
      "Epoch 757/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9256e-04 - my_r2: 0.9159 - val_loss: 3.2154e-05 - val_my_r2: 0.9888\n",
      "Epoch 758/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1197e-04 - my_r2: 0.8628 - val_loss: 2.5767e-05 - val_my_r2: 0.9910\n",
      "Epoch 759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7293e-04 - my_r2: 0.9143 - val_loss: 3.0360e-05 - val_my_r2: 0.9900\n",
      "Epoch 760/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8416e-04 - my_r2: 0.9192 - val_loss: 2.8608e-05 - val_my_r2: 0.9906\n",
      "Epoch 761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0774e-04 - my_r2: 0.9344 - val_loss: 2.4456e-05 - val_my_r2: 0.9917\n",
      "Epoch 762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5204e-04 - my_r2: 0.9314 - val_loss: 2.5374e-05 - val_my_r2: 0.9910\n",
      "Epoch 763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3031e-04 - my_r2: 0.9457 - val_loss: 2.5705e-05 - val_my_r2: 0.9907\n",
      "Epoch 764/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0438e-04 - my_r2: 0.9210 - val_loss: 2.6108e-05 - val_my_r2: 0.9914\n",
      "Epoch 765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7976e-04 - my_r2: 0.9246 - val_loss: 2.7303e-05 - val_my_r2: 0.9909\n",
      "Epoch 766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0894e-04 - my_r2: 0.9354 - val_loss: 2.5506e-05 - val_my_r2: 0.9916\n",
      "Epoch 767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8892e-04 - my_r2: 0.8474 - val_loss: 2.5049e-05 - val_my_r2: 0.9915\n",
      "Epoch 768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3539e-04 - my_r2: 0.8869 - val_loss: 2.6123e-05 - val_my_r2: 0.9914\n",
      "Epoch 769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9401e-04 - my_r2: 0.9093 - val_loss: 2.6358e-05 - val_my_r2: 0.9915\n",
      "Epoch 770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4267e-04 - my_r2: 0.7493 - val_loss: 2.6480e-05 - val_my_r2: 0.9909\n",
      "Epoch 771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7961e-04 - my_r2: 0.8695 - val_loss: 2.4353e-05 - val_my_r2: 0.9918\n",
      "Epoch 772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2491e-04 - my_r2: 0.9187 - val_loss: 2.6719e-05 - val_my_r2: 0.9904\n",
      "Epoch 773/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9727e-04 - my_r2: 0.9491 - val_loss: 2.4510e-05 - val_my_r2: 0.9915\n",
      "Epoch 774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6538e-04 - my_r2: 0.9279 - val_loss: 2.5027e-05 - val_my_r2: 0.9914\n",
      "Epoch 775/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2392e-04 - my_r2: 0.9237 - val_loss: 2.5162e-05 - val_my_r2: 0.9914\n",
      "Epoch 776/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2717e-04 - my_r2: 0.9181 - val_loss: 3.0781e-05 - val_my_r2: 0.9886\n",
      "Epoch 777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9641e-04 - my_r2: 0.9468 - val_loss: 2.8052e-05 - val_my_r2: 0.9898\n",
      "Epoch 778/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2583e-04 - my_r2: 0.9195 - val_loss: 2.7044e-05 - val_my_r2: 0.9908\n",
      "Epoch 779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4446e-04 - my_r2: 0.9461 - val_loss: 2.6576e-05 - val_my_r2: 0.9912\n",
      "Epoch 780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9841e-04 - my_r2: 0.9235 - val_loss: 2.9819e-05 - val_my_r2: 0.9897\n",
      "Epoch 781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9784e-04 - my_r2: 0.9273 - val_loss: 2.8463e-05 - val_my_r2: 0.9905\n",
      "Epoch 782/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5547e-04 - my_r2: 0.9225 - val_loss: 2.8093e-05 - val_my_r2: 0.9907\n",
      "Epoch 783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4881e-04 - my_r2: 0.9149 - val_loss: 2.3937e-05 - val_my_r2: 0.9916\n",
      "Epoch 784/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2241e-04 - my_r2: 0.9508 - val_loss: 2.5045e-05 - val_my_r2: 0.9910\n",
      "Epoch 785/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4183e-04 - my_r2: 0.9061 - val_loss: 2.6675e-05 - val_my_r2: 0.9903\n",
      "Epoch 786/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9530e-04 - my_r2: 0.9230 - val_loss: 2.7750e-05 - val_my_r2: 0.9900\n",
      "Epoch 787/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7080e-04 - my_r2: 0.9267 - val_loss: 2.7367e-05 - val_my_r2: 0.9902\n",
      "Epoch 788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1042e-04 - my_r2: 0.9285 - val_loss: 2.4300e-05 - val_my_r2: 0.9913\n",
      "Epoch 789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9038e-04 - my_r2: 0.9053 - val_loss: 2.4010e-05 - val_my_r2: 0.9914\n",
      "Epoch 790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4250e-04 - my_r2: 0.8843 - val_loss: 2.5515e-05 - val_my_r2: 0.9913\n",
      "Epoch 791/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7993e-04 - my_r2: 0.8728 - val_loss: 3.1826e-05 - val_my_r2: 0.9893\n",
      "Epoch 792/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0447e-04 - my_r2: 0.9358 - val_loss: 2.5147e-05 - val_my_r2: 0.9909\n",
      "Epoch 793/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2152e-04 - my_r2: 0.9425 - val_loss: 2.2395e-05 - val_my_r2: 0.9921\n",
      "Epoch 794/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7137e-04 - my_r2: 0.9298 - val_loss: 2.1647e-05 - val_my_r2: 0.9923\n",
      "Epoch 795/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8479e-04 - my_r2: 0.9348 - val_loss: 2.4425e-05 - val_my_r2: 0.9913\n",
      "Epoch 796/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8577e-04 - my_r2: 0.9300 - val_loss: 2.4889e-05 - val_my_r2: 0.9916\n",
      "Epoch 797/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5734e-04 - my_r2: 0.9438 - val_loss: 2.3793e-05 - val_my_r2: 0.9915\n",
      "Epoch 798/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3953e-04 - my_r2: 0.8896 - val_loss: 2.6810e-05 - val_my_r2: 0.9904\n",
      "Epoch 799/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0892e-04 - my_r2: 0.9353 - val_loss: 2.2501e-05 - val_my_r2: 0.9920\n",
      "Epoch 800/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4459e-04 - my_r2: 0.9023 - val_loss: 2.3510e-05 - val_my_r2: 0.9916\n",
      "Epoch 801/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3275e-04 - my_r2: 0.3072 - val_loss: 2.4144e-05 - val_my_r2: 0.9915\n",
      "Epoch 802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1936e-04 - my_r2: 0.9464 - val_loss: 2.6995e-05 - val_my_r2: 0.9907\n",
      "Epoch 803/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4217e-04 - my_r2: 0.9466 - val_loss: 2.6803e-05 - val_my_r2: 0.9909\n",
      "Epoch 804/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0784e-04 - my_r2: 0.9357 - val_loss: 2.4981e-05 - val_my_r2: 0.9915\n",
      "Epoch 805/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4753e-04 - my_r2: 0.9439 - val_loss: 2.3084e-05 - val_my_r2: 0.9920\n",
      "Epoch 806/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5384e-04 - my_r2: 0.9423 - val_loss: 2.3843e-05 - val_my_r2: 0.9920\n",
      "Epoch 807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8863e-04 - my_r2: 0.9466 - val_loss: 2.4145e-05 - val_my_r2: 0.9916\n",
      "Epoch 808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3085e-04 - my_r2: 0.9359 - val_loss: 2.2114e-05 - val_my_r2: 0.9925\n",
      "Epoch 809/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9966e-04 - my_r2: 0.9258 - val_loss: 2.0704e-05 - val_my_r2: 0.9924\n",
      "Epoch 810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0214e-04 - my_r2: 0.9333 - val_loss: 2.2674e-05 - val_my_r2: 0.9916\n",
      "Epoch 811/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6483e-04 - my_r2: 0.9392 - val_loss: 2.2935e-05 - val_my_r2: 0.9916\n",
      "Epoch 812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6833e-04 - my_r2: 0.9206 - val_loss: 2.6005e-05 - val_my_r2: 0.9909\n",
      "Epoch 813/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8948e-04 - my_r2: 0.8985 - val_loss: 2.2756e-05 - val_my_r2: 0.9917\n",
      "Epoch 814/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8033e-04 - my_r2: 0.9453 - val_loss: 2.1444e-05 - val_my_r2: 0.9925\n",
      "Epoch 815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3613e-04 - my_r2: 0.9065 - val_loss: 2.2758e-05 - val_my_r2: 0.9917\n",
      "Epoch 816/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1158e-04 - my_r2: 0.9299 - val_loss: 2.1800e-05 - val_my_r2: 0.9922\n",
      "Epoch 817/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5685e-04 - my_r2: 0.9041 - val_loss: 2.6178e-05 - val_my_r2: 0.9907\n",
      "Epoch 818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5699e-04 - my_r2: 0.9317 - val_loss: 2.7741e-05 - val_my_r2: 0.9903\n",
      "Epoch 819/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7604e-04 - my_r2: 0.8858 - val_loss: 2.3597e-05 - val_my_r2: 0.9917\n",
      "Epoch 820/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4303e-04 - my_r2: 0.9310 - val_loss: 2.1219e-05 - val_my_r2: 0.9923\n",
      "Epoch 821/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8203e-04 - my_r2: 0.9186 - val_loss: 2.5404e-05 - val_my_r2: 0.9905\n",
      "Epoch 822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9307e-04 - my_r2: 0.9072 - val_loss: 2.2541e-05 - val_my_r2: 0.9919\n",
      "Epoch 823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4449e-04 - my_r2: 0.9161 - val_loss: 2.2537e-05 - val_my_r2: 0.9918\n",
      "Epoch 824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4102e-04 - my_r2: 0.9355 - val_loss: 2.1863e-05 - val_my_r2: 0.9922\n",
      "Epoch 825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4429e-04 - my_r2: 0.9231 - val_loss: 2.1226e-05 - val_my_r2: 0.9923\n",
      "Epoch 826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4810e-04 - my_r2: 0.9291 - val_loss: 2.0819e-05 - val_my_r2: 0.9925\n",
      "Epoch 827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8294e-04 - my_r2: 0.9300 - val_loss: 2.3129e-05 - val_my_r2: 0.9922\n",
      "Epoch 828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5023e-04 - my_r2: 0.9416 - val_loss: 2.1648e-05 - val_my_r2: 0.9925\n",
      "Epoch 829/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5094e-04 - my_r2: 0.9034 - val_loss: 2.0761e-05 - val_my_r2: 0.9924\n",
      "Epoch 830/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8349e-04 - my_r2: 0.9254 - val_loss: 2.3031e-05 - val_my_r2: 0.9914\n",
      "Epoch 831/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3203e-04 - my_r2: 0.9296 - val_loss: 2.2338e-05 - val_my_r2: 0.9917\n",
      "Epoch 832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6332e-04 - my_r2: 0.9407 - val_loss: 1.8712e-05 - val_my_r2: 0.9931\n",
      "Epoch 833/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6084e-04 - my_r2: 0.9081 - val_loss: 2.1397e-05 - val_my_r2: 0.9927\n",
      "Epoch 834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2178e-04 - my_r2: 0.9046 - val_loss: 2.1167e-05 - val_my_r2: 0.9931\n",
      "Epoch 835/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1183e-04 - my_r2: 0.9409 - val_loss: 2.7398e-05 - val_my_r2: 0.9910\n",
      "Epoch 836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1865e-04 - my_r2: 0.8986 - val_loss: 1.8908e-05 - val_my_r2: 0.9935\n",
      "Epoch 837/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1740e-04 - my_r2: 0.9247 - val_loss: 1.8971e-05 - val_my_r2: 0.9933\n",
      "Epoch 838/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1253e-04 - my_r2: 0.6663 - val_loss: 2.2439e-05 - val_my_r2: 0.9920\n",
      "Epoch 839/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2165e-04 - my_r2: 0.9121 - val_loss: 1.8385e-05 - val_my_r2: 0.9937\n",
      "Epoch 840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1740e-04 - my_r2: 0.8466 - val_loss: 2.2139e-05 - val_my_r2: 0.9926\n",
      "Epoch 841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5224e-04 - my_r2: 0.9481 - val_loss: 2.0744e-05 - val_my_r2: 0.9931\n",
      "Epoch 842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1113e-04 - my_r2: 0.8502 - val_loss: 2.1946e-05 - val_my_r2: 0.9926\n",
      "Epoch 843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3093e-04 - my_r2: 0.9395 - val_loss: 2.3368e-05 - val_my_r2: 0.9919\n",
      "Epoch 844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4009e-04 - my_r2: 0.9442 - val_loss: 2.0462e-05 - val_my_r2: 0.9929\n",
      "Epoch 845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4056e-04 - my_r2: 0.9229 - val_loss: 1.9631e-05 - val_my_r2: 0.9933\n",
      "Epoch 846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4034e-04 - my_r2: 0.9330 - val_loss: 2.4099e-05 - val_my_r2: 0.9916\n",
      "Epoch 847/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3098e-04 - my_r2: 0.9315 - val_loss: 2.0376e-05 - val_my_r2: 0.9930\n",
      "Epoch 848/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5611e-04 - my_r2: 0.9017 - val_loss: 2.1837e-05 - val_my_r2: 0.9923\n",
      "Epoch 849/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3466e-04 - my_r2: 0.9390 - val_loss: 2.3820e-05 - val_my_r2: 0.9908\n",
      "Epoch 850/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0396e-04 - my_r2: 0.9464 - val_loss: 3.1588e-05 - val_my_r2: 0.9881\n",
      "Epoch 851/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2217e-04 - my_r2: 0.9347 - val_loss: 3.4435e-05 - val_my_r2: 0.9871\n",
      "Epoch 852/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4850e-04 - my_r2: 0.9224 - val_loss: 2.5335e-05 - val_my_r2: 0.9903\n",
      "Epoch 853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3174e-04 - my_r2: 0.9483 - val_loss: 2.1933e-05 - val_my_r2: 0.9917\n",
      "Epoch 854/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1855e-04 - my_r2: 0.9093 - val_loss: 2.3476e-05 - val_my_r2: 0.9914\n",
      "Epoch 855/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9985e-04 - my_r2: 0.9138 - val_loss: 2.6089e-05 - val_my_r2: 0.9898\n",
      "Epoch 856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6167e-04 - my_r2: 0.8962 - val_loss: 2.6533e-05 - val_my_r2: 0.9901\n",
      "Epoch 857/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7280e-04 - my_r2: 0.9110 - val_loss: 2.3135e-05 - val_my_r2: 0.9916\n",
      "Epoch 858/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5947e-04 - my_r2: 0.9296 - val_loss: 2.0833e-05 - val_my_r2: 0.9928\n",
      "Epoch 859/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.2893e-04 - my_r2: 0.9429 - val_loss: 2.6823e-05 - val_my_r2: 0.9910\n",
      "Epoch 860/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.8713e-04 - my_r2: 0.9237 - val_loss: 2.0027e-05 - val_my_r2: 0.9933\n",
      "Epoch 861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6101e-04 - my_r2: 0.9162 - val_loss: 2.3824e-05 - val_my_r2: 0.9920\n",
      "Epoch 862/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9001e-04 - my_r2: 0.9226 - val_loss: 2.2119e-05 - val_my_r2: 0.9925\n",
      "Epoch 863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5237e-04 - my_r2: 0.9400 - val_loss: 2.7637e-05 - val_my_r2: 0.9892\n",
      "Epoch 864/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1706e-04 - my_r2: 0.9107 - val_loss: 2.3518e-05 - val_my_r2: 0.9910\n",
      "Epoch 865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5096e-04 - my_r2: 0.8412 - val_loss: 2.5438e-05 - val_my_r2: 0.9912\n",
      "Epoch 866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4597e-04 - my_r2: 0.8994 - val_loss: 2.8746e-05 - val_my_r2: 0.9900\n",
      "Epoch 867/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8835e-04 - my_r2: 0.9054 - val_loss: 1.9615e-05 - val_my_r2: 0.9935\n",
      "Epoch 868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0719e-04 - my_r2: 0.9102 - val_loss: 1.8193e-05 - val_my_r2: 0.9939\n",
      "Epoch 869/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0203e-04 - my_r2: 0.8945 - val_loss: 2.0816e-05 - val_my_r2: 0.9929\n",
      "Epoch 870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3115e-04 - my_r2: 0.9006 - val_loss: 2.6166e-05 - val_my_r2: 0.9910\n",
      "Epoch 871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5922e-04 - my_r2: 0.9432 - val_loss: 2.0016e-05 - val_my_r2: 0.9933\n",
      "Epoch 872/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0778e-04 - my_r2: 0.9082 - val_loss: 2.3003e-05 - val_my_r2: 0.9920\n",
      "Epoch 873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5385e-04 - my_r2: 0.9219 - val_loss: 1.8146e-05 - val_my_r2: 0.9934\n",
      "Epoch 874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5216e-04 - my_r2: 0.8947 - val_loss: 1.8020e-05 - val_my_r2: 0.9933\n",
      "Epoch 875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1130e-04 - my_r2: 0.9047 - val_loss: 1.8195e-05 - val_my_r2: 0.9935\n",
      "Epoch 876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2552e-04 - my_r2: 0.9107 - val_loss: 2.2144e-05 - val_my_r2: 0.9923\n",
      "Epoch 877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3920e-04 - my_r2: 0.9551 - val_loss: 1.8437e-05 - val_my_r2: 0.9932\n",
      "Epoch 878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0980e-04 - my_r2: 0.8778 - val_loss: 1.8592e-05 - val_my_r2: 0.9932\n",
      "Epoch 879/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4139e-04 - my_r2: 0.9127 - val_loss: 1.8517e-05 - val_my_r2: 0.9928\n",
      "Epoch 880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6780e-04 - my_r2: 0.9198 - val_loss: 1.9006e-05 - val_my_r2: 0.9929\n",
      "Epoch 881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5049e-04 - my_r2: 0.9353 - val_loss: 2.0242e-05 - val_my_r2: 0.9921\n",
      "Epoch 882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4891e-04 - my_r2: 0.9219 - val_loss: 2.0929e-05 - val_my_r2: 0.9918\n",
      "Epoch 883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5906e-04 - my_r2: 0.9278 - val_loss: 2.3740e-05 - val_my_r2: 0.9906\n",
      "Epoch 884/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1045e-04 - my_r2: 0.9085 - val_loss: 2.2531e-05 - val_my_r2: 0.9913\n",
      "Epoch 885/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6615e-04 - my_r2: 0.8869 - val_loss: 1.9294e-05 - val_my_r2: 0.9929\n",
      "Epoch 886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3021e-04 - my_r2: 0.9270 - val_loss: 2.2077e-05 - val_my_r2: 0.9923\n",
      "Epoch 887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0858e-04 - my_r2: 0.9543 - val_loss: 2.0273e-05 - val_my_r2: 0.9930\n",
      "Epoch 888/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3099e-04 - my_r2: 0.8693 - val_loss: 2.1264e-05 - val_my_r2: 0.9926\n",
      "Epoch 889/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8716e-04 - my_r2: 0.9392 - val_loss: 2.4465e-05 - val_my_r2: 0.9917\n",
      "Epoch 890/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2831e-04 - my_r2: 0.9093 - val_loss: 1.9272e-05 - val_my_r2: 0.9934\n",
      "Epoch 891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8032e-04 - my_r2: 0.9370 - val_loss: 1.9051e-05 - val_my_r2: 0.9932\n",
      "Epoch 892/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1873e-04 - my_r2: 0.9329 - val_loss: 1.7623e-05 - val_my_r2: 0.9936\n",
      "Epoch 893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3281e-04 - my_r2: 0.9247 - val_loss: 2.0022e-05 - val_my_r2: 0.9925\n",
      "Epoch 894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3821e-04 - my_r2: 0.9366 - val_loss: 2.1369e-05 - val_my_r2: 0.9920\n",
      "Epoch 895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4375e-04 - my_r2: 0.9333 - val_loss: 1.9494e-05 - val_my_r2: 0.9928\n",
      "Epoch 896/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0520e-04 - my_r2: 0.9390 - val_loss: 2.0905e-05 - val_my_r2: 0.9922\n",
      "Epoch 897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9332e-04 - my_r2: 0.9322 - val_loss: 2.6721e-05 - val_my_r2: 0.9905\n",
      "Epoch 898/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2446e-04 - my_r2: 0.9326 - val_loss: 2.5704e-05 - val_my_r2: 0.9904\n",
      "Epoch 899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7730e-04 - my_r2: 0.9259 - val_loss: 1.8664e-05 - val_my_r2: 0.9931\n",
      "Epoch 900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3951e-04 - my_r2: 0.9198 - val_loss: 1.6636e-05 - val_my_r2: 0.9940\n",
      "Epoch 901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8869e-04 - my_r2: 0.9513 - val_loss: 1.7331e-05 - val_my_r2: 0.9940\n",
      "Epoch 902/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4725e-04 - my_r2: 0.9165 - val_loss: 1.7867e-05 - val_my_r2: 0.9935\n",
      "Epoch 903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4108e-04 - my_r2: 0.9432 - val_loss: 1.9815e-05 - val_my_r2: 0.9930\n",
      "Epoch 904/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1406e-04 - my_r2: 0.9366 - val_loss: 1.9852e-05 - val_my_r2: 0.9933\n",
      "Epoch 905/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0421e-04 - my_r2: 0.9346 - val_loss: 1.6563e-05 - val_my_r2: 0.9945\n",
      "Epoch 906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6170e-04 - my_r2: 0.9488 - val_loss: 1.9046e-05 - val_my_r2: 0.9936\n",
      "Epoch 907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9890e-04 - my_r2: 0.9289 - val_loss: 1.8324e-05 - val_my_r2: 0.9939\n",
      "Epoch 908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5588e-04 - my_r2: 0.9477 - val_loss: 1.8687e-05 - val_my_r2: 0.9937\n",
      "Epoch 909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7653e-04 - my_r2: 0.9197 - val_loss: 1.7600e-05 - val_my_r2: 0.9940\n",
      "Epoch 910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7822e-04 - my_r2: 0.9083 - val_loss: 2.0659e-05 - val_my_r2: 0.9931\n",
      "Epoch 911/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8192e-04 - my_r2: 0.9107 - val_loss: 1.8166e-05 - val_my_r2: 0.9938\n",
      "Epoch 912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5230e-04 - my_r2: 0.9479 - val_loss: 1.6396e-05 - val_my_r2: 0.9946\n",
      "Epoch 913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2129e-04 - my_r2: 0.9450 - val_loss: 1.7426e-05 - val_my_r2: 0.9941\n",
      "Epoch 914/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6977e-04 - my_r2: 0.9460 - val_loss: 1.7528e-05 - val_my_r2: 0.9941\n",
      "Epoch 915/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7554e-04 - my_r2: 0.9275 - val_loss: 1.8632e-05 - val_my_r2: 0.9940\n",
      "Epoch 916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6565e-04 - my_r2: 0.8969 - val_loss: 1.8714e-05 - val_my_r2: 0.9940\n",
      "Epoch 917/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6254e-04 - my_r2: 0.9311 - val_loss: 1.8757e-05 - val_my_r2: 0.9937\n",
      "Epoch 918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0646e-04 - my_r2: 0.9326 - val_loss: 1.6729e-05 - val_my_r2: 0.9942\n",
      "Epoch 919/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5446e-04 - my_r2: 0.9440 - val_loss: 1.6563e-05 - val_my_r2: 0.9945\n",
      "Epoch 920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4766e-04 - my_r2: 0.9261 - val_loss: 1.7221e-05 - val_my_r2: 0.9942\n",
      "Epoch 921/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9950e-04 - my_r2: 0.8616 - val_loss: 1.7836e-05 - val_my_r2: 0.9944\n",
      "Epoch 922/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1390e-04 - my_r2: 0.8996 - val_loss: 2.1171e-05 - val_my_r2: 0.9936\n",
      "Epoch 923/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9619e-04 - my_r2: 0.9357 - val_loss: 1.7777e-05 - val_my_r2: 0.9942\n",
      "Epoch 924/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9125e-04 - my_r2: 0.8144 - val_loss: 2.3216e-05 - val_my_r2: 0.9914\n",
      "Epoch 925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5025e-04 - my_r2: 0.9478 - val_loss: 2.4073e-05 - val_my_r2: 0.9906\n",
      "Epoch 926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7043e-04 - my_r2: 0.9085 - val_loss: 2.1535e-05 - val_my_r2: 0.9919\n",
      "Epoch 927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5798e-04 - my_r2: 0.9203 - val_loss: 1.9987e-05 - val_my_r2: 0.9927\n",
      "Epoch 928/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6855e-04 - my_r2: 0.8686 - val_loss: 1.8904e-05 - val_my_r2: 0.9928\n",
      "Epoch 929/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4161e-04 - my_r2: 0.9491 - val_loss: 1.8625e-05 - val_my_r2: 0.9930\n",
      "Epoch 930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6483e-04 - my_r2: 0.9008 - val_loss: 2.2985e-05 - val_my_r2: 0.9914\n",
      "Epoch 931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9764e-04 - my_r2: 0.8863 - val_loss: 2.3378e-05 - val_my_r2: 0.9916\n",
      "Epoch 932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0729e-04 - my_r2: 0.9593 - val_loss: 1.8642e-05 - val_my_r2: 0.9932\n",
      "Epoch 933/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3240e-04 - my_r2: 0.9164 - val_loss: 1.8524e-05 - val_my_r2: 0.9934\n",
      "Epoch 934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5914e-04 - my_r2: 0.8923 - val_loss: 1.8773e-05 - val_my_r2: 0.9932\n",
      "Epoch 935/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6950e-04 - my_r2: 0.9294 - val_loss: 1.8768e-05 - val_my_r2: 0.9932\n",
      "Epoch 936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9565e-04 - my_r2: 0.9193 - val_loss: 1.8360e-05 - val_my_r2: 0.9938\n",
      "Epoch 937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3994e-04 - my_r2: 0.9327 - val_loss: 1.9748e-05 - val_my_r2: 0.9932\n",
      "Epoch 938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5137e-04 - my_r2: 0.8579 - val_loss: 1.6709e-05 - val_my_r2: 0.9943\n",
      "Epoch 939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7873e-04 - my_r2: 0.9285 - val_loss: 1.6803e-05 - val_my_r2: 0.9944\n",
      "Epoch 940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6237e-04 - my_r2: 0.9033 - val_loss: 2.1210e-05 - val_my_r2: 0.9928\n",
      "Epoch 941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0175e-04 - my_r2: 0.9267 - val_loss: 2.3009e-05 - val_my_r2: 0.9928\n",
      "Epoch 942/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4752e-04 - my_r2: 0.9343 - val_loss: 2.3437e-05 - val_my_r2: 0.9927\n",
      "Epoch 943/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5348e-04 - my_r2: 0.9382 - val_loss: 2.5153e-05 - val_my_r2: 0.9920\n",
      "Epoch 944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6518e-04 - my_r2: 0.6034 - val_loss: 1.7906e-05 - val_my_r2: 0.9943\n",
      "Epoch 945/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4970e-04 - my_r2: 0.9347 - val_loss: 1.7197e-05 - val_my_r2: 0.9942\n",
      "Epoch 946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9448e-04 - my_r2: 0.9208 - val_loss: 1.7921e-05 - val_my_r2: 0.9935\n",
      "Epoch 947/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6708e-04 - my_r2: 0.8691 - val_loss: 1.8751e-05 - val_my_r2: 0.9931\n",
      "Epoch 948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1949e-04 - my_r2: 0.9398 - val_loss: 1.8282e-05 - val_my_r2: 0.9935\n",
      "Epoch 949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4598e-04 - my_r2: 0.9316 - val_loss: 1.9888e-05 - val_my_r2: 0.9930\n",
      "Epoch 950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0271e-04 - my_r2: 0.9039 - val_loss: 1.9415e-05 - val_my_r2: 0.9930\n",
      "Epoch 951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6311e-04 - my_r2: 0.9483 - val_loss: 1.6847e-05 - val_my_r2: 0.9937\n",
      "Epoch 952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6907e-04 - my_r2: 0.8771 - val_loss: 1.6544e-05 - val_my_r2: 0.9938\n",
      "Epoch 953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2778e-04 - my_r2: 0.9147 - val_loss: 1.6998e-05 - val_my_r2: 0.9939\n",
      "Epoch 954/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8590e-04 - my_r2: 0.9090 - val_loss: 1.7817e-05 - val_my_r2: 0.9930\n",
      "Epoch 955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1563e-04 - my_r2: 0.9276 - val_loss: 1.9571e-05 - val_my_r2: 0.9926\n",
      "Epoch 956/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0858e-04 - my_r2: 0.9231 - val_loss: 2.6088e-05 - val_my_r2: 0.9900\n",
      "Epoch 957/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7431e-04 - my_r2: 0.9189 - val_loss: 1.9239e-05 - val_my_r2: 0.9932\n",
      "Epoch 958/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7924e-04 - my_r2: 0.9258 - val_loss: 2.1391e-05 - val_my_r2: 0.9925\n",
      "Epoch 959/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4644e-04 - my_r2: 0.8785 - val_loss: 1.8369e-05 - val_my_r2: 0.9939\n",
      "Epoch 960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3508e-04 - my_r2: 0.9416 - val_loss: 1.6961e-05 - val_my_r2: 0.9942\n",
      "Epoch 961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1328e-04 - my_r2: 0.8970 - val_loss: 1.8049e-05 - val_my_r2: 0.9934\n",
      "Epoch 962/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0728e-04 - my_r2: 0.9170 - val_loss: 1.9350e-05 - val_my_r2: 0.9928\n",
      "Epoch 963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9721e-04 - my_r2: 0.9379 - val_loss: 2.4498e-05 - val_my_r2: 0.9909\n",
      "Epoch 964/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6861e-04 - my_r2: 0.9350 - val_loss: 2.4305e-05 - val_my_r2: 0.9908\n",
      "Epoch 965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5032e-04 - my_r2: 0.8909 - val_loss: 2.0474e-05 - val_my_r2: 0.9916\n",
      "Epoch 966/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2387e-04 - my_r2: 0.9275 - val_loss: 1.8223e-05 - val_my_r2: 0.9921\n",
      "Epoch 967/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7381e-04 - my_r2: 0.9294 - val_loss: 1.9323e-05 - val_my_r2: 0.9924\n",
      "Epoch 968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3392e-04 - my_r2: 0.9351 - val_loss: 2.0716e-05 - val_my_r2: 0.9924\n",
      "Epoch 969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2431e-04 - my_r2: 0.9042 - val_loss: 1.8803e-05 - val_my_r2: 0.9931\n",
      "Epoch 970/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5708e-04 - my_r2: 0.9270 - val_loss: 1.8215e-05 - val_my_r2: 0.9931\n",
      "Epoch 971/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3231e-04 - my_r2: 0.9189 - val_loss: 1.8997e-05 - val_my_r2: 0.9928\n",
      "Epoch 972/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2608e-04 - my_r2: 0.9092 - val_loss: 1.8409e-05 - val_my_r2: 0.9929\n",
      "Epoch 973/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1122e-04 - my_r2: 0.8875 - val_loss: 1.7299e-05 - val_my_r2: 0.9933\n",
      "Epoch 974/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6836e-04 - my_r2: 0.9284 - val_loss: 1.7082e-05 - val_my_r2: 0.9932\n",
      "Epoch 975/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8215e-04 - my_r2: 0.9520 - val_loss: 1.6631e-05 - val_my_r2: 0.9933\n",
      "Epoch 976/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3062e-04 - my_r2: 0.9520 - val_loss: 1.9311e-05 - val_my_r2: 0.9930\n",
      "Epoch 977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9912e-04 - my_r2: 0.9415 - val_loss: 1.7551e-05 - val_my_r2: 0.9934\n",
      "Epoch 978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1530e-04 - my_r2: 0.9419 - val_loss: 1.5660e-05 - val_my_r2: 0.9940\n",
      "Epoch 979/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2123e-04 - my_r2: 0.9447 - val_loss: 2.0795e-05 - val_my_r2: 0.9918\n",
      "Epoch 980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4147e-04 - my_r2: 0.9321 - val_loss: 2.2824e-05 - val_my_r2: 0.9909\n",
      "Epoch 981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3979e-04 - my_r2: 0.9336 - val_loss: 2.3833e-05 - val_my_r2: 0.9907\n",
      "Epoch 982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7579e-04 - my_r2: 0.8670 - val_loss: 2.0282e-05 - val_my_r2: 0.9923\n",
      "Epoch 983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1868e-04 - my_r2: 0.9329 - val_loss: 1.5646e-05 - val_my_r2: 0.9942\n",
      "Epoch 984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3807e-04 - my_r2: 0.9446 - val_loss: 1.7055e-05 - val_my_r2: 0.9938\n",
      "Epoch 985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3758e-04 - my_r2: 0.8884 - val_loss: 1.5179e-05 - val_my_r2: 0.9947\n",
      "Epoch 986/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6533e-04 - my_r2: 0.8614 - val_loss: 1.7587e-05 - val_my_r2: 0.9937\n",
      "Epoch 987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4100e-04 - my_r2: 0.9156 - val_loss: 1.5236e-05 - val_my_r2: 0.9942\n",
      "Epoch 988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0021e-04 - my_r2: 0.9457 - val_loss: 2.1144e-05 - val_my_r2: 0.9923\n",
      "Epoch 989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6086e-04 - my_r2: 0.9262 - val_loss: 1.8980e-05 - val_my_r2: 0.9931\n",
      "Epoch 990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9623e-04 - my_r2: 0.8589 - val_loss: 1.8441e-05 - val_my_r2: 0.9931\n",
      "Epoch 991/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5280e-04 - my_r2: 0.9243 - val_loss: 2.3518e-05 - val_my_r2: 0.9915\n",
      "Epoch 992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6468e-04 - my_r2: 0.9288 - val_loss: 2.0599e-05 - val_my_r2: 0.9922\n",
      "Epoch 993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8415e-04 - my_r2: 0.9293 - val_loss: 2.0007e-05 - val_my_r2: 0.9925\n",
      "Epoch 994/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7116e-04 - my_r2: 0.9075 - val_loss: 1.8312e-05 - val_my_r2: 0.9933\n",
      "Epoch 995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5579e-04 - my_r2: 0.8617 - val_loss: 1.8792e-05 - val_my_r2: 0.9931\n",
      "Epoch 996/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5073e-04 - my_r2: 0.9456 - val_loss: 1.6476e-05 - val_my_r2: 0.9940\n",
      "Epoch 997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8415e-04 - my_r2: 0.8938 - val_loss: 1.7028e-05 - val_my_r2: 0.9938\n",
      "Epoch 998/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6556e-04 - my_r2: 0.9318 - val_loss: 1.4916e-05 - val_my_r2: 0.9948\n",
      "Epoch 999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8281e-04 - my_r2: 0.9088 - val_loss: 1.5241e-05 - val_my_r2: 0.9949\n",
      "Epoch 1000/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5706e-04 - my_r2: 0.9239 - val_loss: 1.7341e-05 - val_my_r2: 0.9941\n",
      "Epoch 1001/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8712e-04 - my_r2: 0.8918 - val_loss: 1.8685e-05 - val_my_r2: 0.9933\n",
      "Epoch 1002/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8319e-04 - my_r2: 0.9021 - val_loss: 1.9439e-05 - val_my_r2: 0.9930\n",
      "Epoch 1003/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4659e-04 - my_r2: 0.9432 - val_loss: 1.9277e-05 - val_my_r2: 0.9931\n",
      "Epoch 1004/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5596e-04 - my_r2: 0.9167 - val_loss: 2.5865e-05 - val_my_r2: 0.9911\n",
      "Epoch 1005/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3795e-04 - my_r2: 0.9295 - val_loss: 2.0081e-05 - val_my_r2: 0.9926\n",
      "Epoch 1006/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6310e-04 - my_r2: 0.9253 - val_loss: 1.7083e-05 - val_my_r2: 0.9938\n",
      "Epoch 1007/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5801e-04 - my_r2: 0.9237 - val_loss: 2.3356e-05 - val_my_r2: 0.9922\n",
      "Epoch 1008/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7977e-04 - my_r2: 0.9308 - val_loss: 1.9590e-05 - val_my_r2: 0.9938\n",
      "Epoch 1009/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1395e-04 - my_r2: 0.9393 - val_loss: 2.3076e-05 - val_my_r2: 0.9926\n",
      "Epoch 1010/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0701e-04 - my_r2: 0.8950 - val_loss: 1.8117e-05 - val_my_r2: 0.9943\n",
      "Epoch 1011/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8320e-04 - my_r2: 0.9294 - val_loss: 1.6705e-05 - val_my_r2: 0.9945\n",
      "Epoch 1012/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8050e-04 - my_r2: 0.9252 - val_loss: 1.6003e-05 - val_my_r2: 0.9947\n",
      "Epoch 1013/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1094e-04 - my_r2: 0.9452 - val_loss: 1.5256e-05 - val_my_r2: 0.9952\n",
      "Epoch 1014/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3193e-04 - my_r2: 0.9320 - val_loss: 1.3946e-05 - val_my_r2: 0.9956\n",
      "Epoch 1015/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1376e-04 - my_r2: 0.8945 - val_loss: 1.4950e-05 - val_my_r2: 0.9952\n",
      "Epoch 1016/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8938e-04 - my_r2: 0.9273 - val_loss: 1.7066e-05 - val_my_r2: 0.9940\n",
      "Epoch 1017/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9652e-04 - my_r2: 0.9504 - val_loss: 1.8212e-05 - val_my_r2: 0.9935\n",
      "Epoch 1018/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4745e-04 - my_r2: 0.9091 - val_loss: 1.7343e-05 - val_my_r2: 0.9941\n",
      "Epoch 1019/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7796e-04 - my_r2: 0.9315 - val_loss: 1.5779e-05 - val_my_r2: 0.9946\n",
      "Epoch 1020/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2352e-04 - my_r2: 0.9356 - val_loss: 1.7442e-05 - val_my_r2: 0.9937\n",
      "Epoch 1021/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5041e-04 - my_r2: 0.9283 - val_loss: 1.9560e-05 - val_my_r2: 0.9931\n",
      "Epoch 1022/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.2085e-04 - my_r2: 0.9538 - val_loss: 2.3915e-05 - val_my_r2: 0.9914\n",
      "Epoch 1023/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6504e-04 - my_r2: 0.9197 - val_loss: 2.0944e-05 - val_my_r2: 0.9924\n",
      "Epoch 1024/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2118e-04 - my_r2: 0.8716 - val_loss: 1.9829e-05 - val_my_r2: 0.9931\n",
      "Epoch 1025/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2956e-04 - my_r2: 0.9311 - val_loss: 1.9403e-05 - val_my_r2: 0.9935\n",
      "Epoch 1026/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0743e-04 - my_r2: 0.9545 - val_loss: 1.4885e-05 - val_my_r2: 0.9950\n",
      "Epoch 1027/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5771e-04 - my_r2: 0.9318 - val_loss: 1.5936e-05 - val_my_r2: 0.9944\n",
      "Epoch 1028/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2074e-04 - my_r2: 0.9543 - val_loss: 1.5370e-05 - val_my_r2: 0.9948\n",
      "Epoch 1029/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5554e-04 - my_r2: 0.9149 - val_loss: 2.0976e-05 - val_my_r2: 0.9927\n",
      "Epoch 1030/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7004e-04 - my_r2: 0.9222 - val_loss: 1.7441e-05 - val_my_r2: 0.9935\n",
      "Epoch 1031/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5183e-04 - my_r2: 0.9184 - val_loss: 1.5660e-05 - val_my_r2: 0.9939\n",
      "Epoch 1032/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6994e-04 - my_r2: 0.8893 - val_loss: 1.8829e-05 - val_my_r2: 0.9930\n",
      "Epoch 1033/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0589e-04 - my_r2: 0.9273 - val_loss: 1.8536e-05 - val_my_r2: 0.9931\n",
      "Epoch 1034/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6397e-04 - my_r2: 0.9497 - val_loss: 1.5254e-05 - val_my_r2: 0.9942\n",
      "Epoch 1035/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7005e-04 - my_r2: 0.9103 - val_loss: 1.6197e-05 - val_my_r2: 0.9941\n",
      "Epoch 1036/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5499e-04 - my_r2: 0.9421 - val_loss: 1.8703e-05 - val_my_r2: 0.9935\n",
      "Epoch 1037/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3327e-04 - my_r2: 0.9050 - val_loss: 1.7570e-05 - val_my_r2: 0.9939\n",
      "Epoch 1038/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5650e-04 - my_r2: 0.9163 - val_loss: 1.6850e-05 - val_my_r2: 0.9944\n",
      "Epoch 1039/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2716e-04 - my_r2: 0.9270 - val_loss: 1.9996e-05 - val_my_r2: 0.9932\n",
      "Epoch 1040/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0164e-04 - my_r2: 0.9144 - val_loss: 2.0610e-05 - val_my_r2: 0.9929\n",
      "Epoch 1041/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6307e-04 - my_r2: 0.9212 - val_loss: 1.9303e-05 - val_my_r2: 0.9932\n",
      "Epoch 1042/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2596e-04 - my_r2: 0.9319 - val_loss: 2.2081e-05 - val_my_r2: 0.9930\n",
      "Epoch 1043/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4439e-04 - my_r2: 0.9492 - val_loss: 2.1677e-05 - val_my_r2: 0.9932\n",
      "Epoch 1044/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4556e-04 - my_r2: 0.9261 - val_loss: 2.0516e-05 - val_my_r2: 0.9932\n",
      "Epoch 1045/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5994e-04 - my_r2: 0.9432 - val_loss: 1.8568e-05 - val_my_r2: 0.9938\n",
      "Epoch 1046/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6643e-04 - my_r2: 0.9052 - val_loss: 1.8969e-05 - val_my_r2: 0.9943\n",
      "Epoch 1047/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5031e-04 - my_r2: 0.9395 - val_loss: 1.9709e-05 - val_my_r2: 0.9943\n",
      "Epoch 1048/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9194e-04 - my_r2: 0.9266 - val_loss: 1.5296e-05 - val_my_r2: 0.9954\n",
      "Epoch 1049/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8439e-04 - my_r2: 0.9161 - val_loss: 1.5370e-05 - val_my_r2: 0.9952\n",
      "Epoch 1050/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0998e-04 - my_r2: 0.9211 - val_loss: 1.7088e-05 - val_my_r2: 0.9945\n",
      "Epoch 1051/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0013e-04 - my_r2: 0.9391 - val_loss: 2.1666e-05 - val_my_r2: 0.9930\n",
      "Epoch 1052/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7273e-04 - my_r2: 0.9229 - val_loss: 2.0879e-05 - val_my_r2: 0.9937\n",
      "Epoch 1053/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5061e-04 - my_r2: 0.9429 - val_loss: 1.9379e-05 - val_my_r2: 0.9934\n",
      "Epoch 1054/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8538e-04 - my_r2: 0.9353 - val_loss: 2.3026e-05 - val_my_r2: 0.9913\n",
      "Epoch 1055/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1766e-04 - my_r2: 0.9456 - val_loss: 2.0402e-05 - val_my_r2: 0.9922\n",
      "Epoch 1056/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5738e-04 - my_r2: 0.3389 - val_loss: 2.4973e-05 - val_my_r2: 0.9909\n",
      "Epoch 1057/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1704e-04 - my_r2: 0.9202 - val_loss: 2.8260e-05 - val_my_r2: 0.9895\n",
      "Epoch 1058/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6462e-04 - my_r2: 0.8715 - val_loss: 1.7767e-05 - val_my_r2: 0.9931\n",
      "Epoch 1059/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2890e-04 - my_r2: 0.9189 - val_loss: 1.5234e-05 - val_my_r2: 0.9940\n",
      "Epoch 1060/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8879e-04 - my_r2: 0.9035 - val_loss: 1.7686e-05 - val_my_r2: 0.9933\n",
      "Epoch 1061/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0028e-04 - my_r2: 0.9288 - val_loss: 1.5894e-05 - val_my_r2: 0.9937\n",
      "Epoch 1062/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7505e-04 - my_r2: 0.9379 - val_loss: 1.5881e-05 - val_my_r2: 0.9940\n",
      "Epoch 1063/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2540e-04 - my_r2: 0.9453 - val_loss: 1.7572e-05 - val_my_r2: 0.9936\n",
      "Epoch 1064/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0564e-04 - my_r2: 0.9399 - val_loss: 2.0658e-05 - val_my_r2: 0.9933\n",
      "Epoch 1065/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2662e-04 - my_r2: 0.9053 - val_loss: 2.1042e-05 - val_my_r2: 0.9931\n",
      "Epoch 1066/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8281e-04 - my_r2: 0.9128 - val_loss: 2.0425e-05 - val_my_r2: 0.9934\n",
      "Epoch 1067/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9226e-04 - my_r2: 0.9402 - val_loss: 1.9938e-05 - val_my_r2: 0.9936\n",
      "Epoch 1068/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6656e-04 - my_r2: 0.9321 - val_loss: 2.0689e-05 - val_my_r2: 0.9933\n",
      "Epoch 1069/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8189e-04 - my_r2: 0.9398 - val_loss: 1.7753e-05 - val_my_r2: 0.9944\n",
      "Epoch 1070/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9701e-04 - my_r2: 0.8877 - val_loss: 1.6016e-05 - val_my_r2: 0.9945\n",
      "Epoch 1071/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8213e-04 - my_r2: 0.9494 - val_loss: 1.6931e-05 - val_my_r2: 0.9942\n",
      "Epoch 1072/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6709e-04 - my_r2: 0.9395 - val_loss: 1.4903e-05 - val_my_r2: 0.9949\n",
      "Epoch 1073/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6636e-04 - my_r2: 0.9398 - val_loss: 1.4262e-05 - val_my_r2: 0.9951\n",
      "Epoch 1074/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4059e-04 - my_r2: 0.9407 - val_loss: 1.2443e-05 - val_my_r2: 0.9958\n",
      "Epoch 1075/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4709e-04 - my_r2: 0.9344 - val_loss: 1.2280e-05 - val_my_r2: 0.9961\n",
      "Epoch 1076/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9561e-04 - my_r2: 0.9066 - val_loss: 1.4739e-05 - val_my_r2: 0.9956\n",
      "Epoch 1077/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9071e-04 - my_r2: 0.9222 - val_loss: 2.0857e-05 - val_my_r2: 0.9935\n",
      "Epoch 1078/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0573e-04 - my_r2: 0.9300 - val_loss: 1.9840e-05 - val_my_r2: 0.9937\n",
      "Epoch 1079/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4679e-04 - my_r2: 0.9112 - val_loss: 1.3649e-05 - val_my_r2: 0.9954\n",
      "Epoch 1080/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2335e-04 - my_r2: 0.9489 - val_loss: 1.2812e-05 - val_my_r2: 0.9954\n",
      "Epoch 1081/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2047e-04 - my_r2: 0.9424 - val_loss: 1.1171e-05 - val_my_r2: 0.9962\n",
      "Epoch 1082/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0366e-04 - my_r2: 0.9084 - val_loss: 1.2866e-05 - val_my_r2: 0.9954\n",
      "Epoch 1083/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0240e-04 - my_r2: 0.9234 - val_loss: 1.3843e-05 - val_my_r2: 0.9948\n",
      "Epoch 1084/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3520e-04 - my_r2: 0.9138 - val_loss: 1.3258e-05 - val_my_r2: 0.9948\n",
      "Epoch 1085/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4090e-04 - my_r2: 0.9531 - val_loss: 1.5246e-05 - val_my_r2: 0.9943\n",
      "Epoch 1086/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6142e-04 - my_r2: 0.8657 - val_loss: 1.7486e-05 - val_my_r2: 0.9938\n",
      "Epoch 1087/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8966e-04 - my_r2: 0.8832 - val_loss: 1.5437e-05 - val_my_r2: 0.9943\n",
      "Epoch 1088/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4267e-04 - my_r2: 0.9261 - val_loss: 1.3270e-05 - val_my_r2: 0.9951\n",
      "Epoch 1089/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1004e-04 - my_r2: 0.9506 - val_loss: 1.3632e-05 - val_my_r2: 0.9946\n",
      "Epoch 1090/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3239e-04 - my_r2: 0.9065 - val_loss: 1.6220e-05 - val_my_r2: 0.9938\n",
      "Epoch 1091/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3854e-04 - my_r2: 0.9502 - val_loss: 1.4862e-05 - val_my_r2: 0.9944\n",
      "Epoch 1092/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1281e-04 - my_r2: 0.9229 - val_loss: 1.9257e-05 - val_my_r2: 0.9932\n",
      "Epoch 1093/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3799e-04 - my_r2: 0.9433 - val_loss: 2.6444e-05 - val_my_r2: 0.9912\n",
      "Epoch 1094/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7120e-04 - my_r2: 0.9145 - val_loss: 1.6872e-05 - val_my_r2: 0.9942\n",
      "Epoch 1095/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3222e-04 - my_r2: 0.8564 - val_loss: 1.8436e-05 - val_my_r2: 0.9931\n",
      "Epoch 1096/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6120e-04 - my_r2: 0.9349 - val_loss: 1.9647e-05 - val_my_r2: 0.9920\n",
      "Epoch 1097/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0123e-04 - my_r2: 0.9211 - val_loss: 1.6781e-05 - val_my_r2: 0.9940\n",
      "Epoch 1098/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3097e-04 - my_r2: 0.9247 - val_loss: 1.4595e-05 - val_my_r2: 0.9953\n",
      "Epoch 1099/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2171e-04 - my_r2: 0.9264 - val_loss: 1.4314e-05 - val_my_r2: 0.9952\n",
      "Epoch 1100/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3957e-04 - my_r2: 0.9333 - val_loss: 1.3786e-05 - val_my_r2: 0.9950\n",
      "Epoch 1101/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0955e-04 - my_r2: 0.9538 - val_loss: 1.5614e-05 - val_my_r2: 0.9945\n",
      "Epoch 1102/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3590e-04 - my_r2: 0.9483 - val_loss: 1.3529e-05 - val_my_r2: 0.9953\n",
      "Epoch 1103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3870e-04 - my_r2: 0.9059 - val_loss: 1.3206e-05 - val_my_r2: 0.9957\n",
      "Epoch 1104/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2038e-04 - my_r2: 0.9152 - val_loss: 1.4028e-05 - val_my_r2: 0.9955\n",
      "Epoch 1105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1321e-04 - my_r2: 0.9300 - val_loss: 1.7653e-05 - val_my_r2: 0.9944\n",
      "Epoch 1106/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6615e-04 - my_r2: 0.9055 - val_loss: 1.8590e-05 - val_my_r2: 0.9937\n",
      "Epoch 1107/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1543e-04 - my_r2: 0.8962 - val_loss: 1.7915e-05 - val_my_r2: 0.9935\n",
      "Epoch 1108/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9536e-04 - my_r2: 0.9310 - val_loss: 1.6763e-05 - val_my_r2: 0.9940\n",
      "Epoch 1109/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7697e-04 - my_r2: 0.9086 - val_loss: 1.4863e-05 - val_my_r2: 0.9943\n",
      "Epoch 1110/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9656e-04 - my_r2: 0.8917 - val_loss: 1.3173e-05 - val_my_r2: 0.9949\n",
      "Epoch 1111/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2787e-04 - my_r2: 0.9351 - val_loss: 1.6915e-05 - val_my_r2: 0.9936\n",
      "Epoch 1112/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5586e-04 - my_r2: 0.9509 - val_loss: 1.4073e-05 - val_my_r2: 0.9948\n",
      "Epoch 1113/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7441e-04 - my_r2: 0.9101 - val_loss: 1.2732e-05 - val_my_r2: 0.9955\n",
      "Epoch 1114/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2120e-04 - my_r2: 0.9346 - val_loss: 1.3706e-05 - val_my_r2: 0.9951\n",
      "Epoch 1115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2964e-04 - my_r2: 0.9438 - val_loss: 1.4502e-05 - val_my_r2: 0.9947\n",
      "Epoch 1116/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1608e-04 - my_r2: 0.9010 - val_loss: 1.7454e-05 - val_my_r2: 0.9939\n",
      "Epoch 1117/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3974e-04 - my_r2: 0.8621 - val_loss: 1.8631e-05 - val_my_r2: 0.9935\n",
      "Epoch 1118/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1802e-04 - my_r2: 0.9135 - val_loss: 1.6553e-05 - val_my_r2: 0.9943\n",
      "Epoch 1119/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9521e-04 - my_r2: 0.9267 - val_loss: 1.9211e-05 - val_my_r2: 0.9931\n",
      "Epoch 1120/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5707e-04 - my_r2: 0.8821 - val_loss: 1.1475e-05 - val_my_r2: 0.9958\n",
      "Epoch 1121/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5639e-04 - my_r2: 0.9389 - val_loss: 1.3030e-05 - val_my_r2: 0.9951\n",
      "Epoch 1122/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8289e-04 - my_r2: 0.8947 - val_loss: 1.3285e-05 - val_my_r2: 0.9949\n",
      "Epoch 1123/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2538e-04 - my_r2: 0.9125 - val_loss: 1.5889e-05 - val_my_r2: 0.9934\n",
      "Epoch 1124/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6788e-04 - my_r2: 0.9265 - val_loss: 1.5077e-05 - val_my_r2: 0.9938\n",
      "Epoch 1125/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5006e-04 - my_r2: 0.9317 - val_loss: 1.6294e-05 - val_my_r2: 0.9937\n",
      "Epoch 1126/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7617e-04 - my_r2: 0.9435 - val_loss: 1.5760e-05 - val_my_r2: 0.9942\n",
      "Epoch 1127/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3021e-04 - my_r2: 0.9476 - val_loss: 1.3348e-05 - val_my_r2: 0.9955\n",
      "Epoch 1128/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5409e-04 - my_r2: 0.9238 - val_loss: 1.5249e-05 - val_my_r2: 0.9945\n",
      "Epoch 1129/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5789e-04 - my_r2: 0.9366 - val_loss: 2.1308e-05 - val_my_r2: 0.9926\n",
      "Epoch 1130/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2836e-04 - my_r2: 0.9314 - val_loss: 1.9752e-05 - val_my_r2: 0.9935\n",
      "Epoch 1131/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3616e-04 - my_r2: 0.8623 - val_loss: 1.6204e-05 - val_my_r2: 0.9950\n",
      "Epoch 1132/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9934e-04 - my_r2: 0.9519 - val_loss: 1.5053e-05 - val_my_r2: 0.9949\n",
      "Epoch 1133/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1411e-04 - my_r2: 0.9467 - val_loss: 1.6142e-05 - val_my_r2: 0.9943\n",
      "Epoch 1134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8521e-04 - my_r2: 0.8887 - val_loss: 1.4966e-05 - val_my_r2: 0.9947\n",
      "Epoch 1135/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6653e-04 - my_r2: 0.9254 - val_loss: 1.5881e-05 - val_my_r2: 0.9947\n",
      "Epoch 1136/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9093e-04 - my_r2: 0.9050 - val_loss: 1.6980e-05 - val_my_r2: 0.9949\n",
      "Epoch 1137/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4251e-04 - my_r2: 0.9181 - val_loss: 1.5243e-05 - val_my_r2: 0.9949\n",
      "Epoch 1138/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6887e-04 - my_r2: 0.9179 - val_loss: 1.2568e-05 - val_my_r2: 0.9959\n",
      "Epoch 1139/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4991e-04 - my_r2: 0.9312 - val_loss: 1.3446e-05 - val_my_r2: 0.9952\n",
      "Epoch 1140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1443e-04 - my_r2: 0.9440 - val_loss: 1.2243e-05 - val_my_r2: 0.9957\n",
      "Epoch 1141/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8644e-04 - my_r2: 0.9623 - val_loss: 1.2892e-05 - val_my_r2: 0.9953\n",
      "Epoch 1142/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1700e-04 - my_r2: 0.9436 - val_loss: 1.2226e-05 - val_my_r2: 0.9951\n",
      "Epoch 1143/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7901e-04 - my_r2: 0.9209 - val_loss: 1.1489e-05 - val_my_r2: 0.9959\n",
      "Epoch 1144/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9917e-04 - my_r2: 0.9366 - val_loss: 1.7387e-05 - val_my_r2: 0.9941\n",
      "Epoch 1145/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0333e-04 - my_r2: 0.9454 - val_loss: 1.2880e-05 - val_my_r2: 0.9958\n",
      "Epoch 1146/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2538e-04 - my_r2: 0.9485 - val_loss: 1.2552e-05 - val_my_r2: 0.9960\n",
      "Epoch 1147/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8152e-04 - my_r2: 0.9338 - val_loss: 1.1349e-05 - val_my_r2: 0.9964\n",
      "Epoch 1148/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0010e-04 - my_r2: 0.9393 - val_loss: 1.6578e-05 - val_my_r2: 0.9942\n",
      "Epoch 1149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9862e-04 - my_r2: 0.9163 - val_loss: 2.6329e-05 - val_my_r2: 0.9909\n",
      "Epoch 1150/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5205e-04 - my_r2: 0.8878 - val_loss: 2.1110e-05 - val_my_r2: 0.9923\n",
      "Epoch 1151/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5246e-04 - my_r2: 0.9541 - val_loss: 1.8849e-05 - val_my_r2: 0.9932\n",
      "Epoch 1152/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2826e-04 - my_r2: 0.9303 - val_loss: 1.9411e-05 - val_my_r2: 0.9932\n",
      "Epoch 1153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6108e-04 - my_r2: 0.9164 - val_loss: 1.6049e-05 - val_my_r2: 0.9945\n",
      "Epoch 1154/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2606e-04 - my_r2: 0.9116 - val_loss: 1.3065e-05 - val_my_r2: 0.9959\n",
      "Epoch 1155/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8777e-04 - my_r2: 0.9087 - val_loss: 1.4833e-05 - val_my_r2: 0.9949\n",
      "Epoch 1156/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6227e-04 - my_r2: 0.9474 - val_loss: 1.6401e-05 - val_my_r2: 0.9943\n",
      "Epoch 1157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5251e-04 - my_r2: 0.9406 - val_loss: 1.0795e-05 - val_my_r2: 0.9964\n",
      "Epoch 1158/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1104e-04 - my_r2: 0.9586 - val_loss: 1.2082e-05 - val_my_r2: 0.9961\n",
      "Epoch 1159/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3643e-04 - my_r2: 0.9134 - val_loss: 1.0939e-05 - val_my_r2: 0.9961\n",
      "Epoch 1160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5698e-04 - my_r2: 0.9351 - val_loss: 1.1637e-05 - val_my_r2: 0.9961\n",
      "Epoch 1161/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5137e-04 - my_r2: 0.9259 - val_loss: 1.1125e-05 - val_my_r2: 0.9962\n",
      "Epoch 1162/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5111e-04 - my_r2: 0.9090 - val_loss: 1.8724e-05 - val_my_r2: 0.9936\n",
      "Epoch 1163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8472e-04 - my_r2: 0.9250 - val_loss: 1.5032e-05 - val_my_r2: 0.9949\n",
      "Epoch 1164/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7301e-04 - my_r2: 0.9183 - val_loss: 1.5934e-05 - val_my_r2: 0.9944\n",
      "Epoch 1165/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5004e-04 - my_r2: 0.8930 - val_loss: 1.3699e-05 - val_my_r2: 0.9954\n",
      "Epoch 1166/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0290e-04 - my_r2: 0.8876 - val_loss: 1.3716e-05 - val_my_r2: 0.9951\n",
      "Epoch 1167/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9348e-04 - my_r2: 0.8812 - val_loss: 1.7678e-05 - val_my_r2: 0.9939\n",
      "Epoch 1168/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5725e-04 - my_r2: 0.9229 - val_loss: 1.2579e-05 - val_my_r2: 0.9959\n",
      "Epoch 1169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8022e-04 - my_r2: 0.9019 - val_loss: 1.3549e-05 - val_my_r2: 0.9955\n",
      "Epoch 1170/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9469e-04 - my_r2: 0.9585 - val_loss: 1.2573e-05 - val_my_r2: 0.9959\n",
      "Epoch 1171/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4181e-04 - my_r2: 0.9009 - val_loss: 1.5633e-05 - val_my_r2: 0.9944\n",
      "Epoch 1172/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0291e-04 - my_r2: 0.8922 - val_loss: 1.6019e-05 - val_my_r2: 0.9946\n",
      "Epoch 1173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6757e-04 - my_r2: 0.9522 - val_loss: 2.0718e-05 - val_my_r2: 0.9934\n",
      "Epoch 1174/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4884e-04 - my_r2: 0.9112 - val_loss: 1.8349e-05 - val_my_r2: 0.9933\n",
      "Epoch 1175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6174e-04 - my_r2: 0.9197 - val_loss: 1.4448e-05 - val_my_r2: 0.9948\n",
      "Epoch 1176/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1209e-04 - my_r2: 0.9278 - val_loss: 1.7635e-05 - val_my_r2: 0.9938\n",
      "Epoch 1177/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2021e-04 - my_r2: 0.9502 - val_loss: 1.6180e-05 - val_my_r2: 0.9944\n",
      "Epoch 1178/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5247e-04 - my_r2: 0.9427 - val_loss: 1.3723e-05 - val_my_r2: 0.9952\n",
      "Epoch 1179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2068e-04 - my_r2: 0.9085 - val_loss: 1.4565e-05 - val_my_r2: 0.9949\n",
      "Epoch 1180/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6199e-04 - my_r2: 0.9367 - val_loss: 1.5605e-05 - val_my_r2: 0.9946\n",
      "Epoch 1181/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0454e-04 - my_r2: 0.9503 - val_loss: 1.5720e-05 - val_my_r2: 0.9947\n",
      "Epoch 1182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2725e-04 - my_r2: 0.8874 - val_loss: 1.2834e-05 - val_my_r2: 0.9954\n",
      "Epoch 1183/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4867e-04 - my_r2: 0.9370 - val_loss: 1.5249e-05 - val_my_r2: 0.9946\n",
      "Epoch 1184/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2927e-04 - my_r2: 0.9446 - val_loss: 1.6903e-05 - val_my_r2: 0.9943\n",
      "Epoch 1185/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6054e-04 - my_r2: 0.9315 - val_loss: 1.4175e-05 - val_my_r2: 0.9952\n",
      "Epoch 1186/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8569e-04 - my_r2: 0.9226 - val_loss: 1.4927e-05 - val_my_r2: 0.9948\n",
      "Epoch 1187/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3638e-04 - my_r2: 0.9200 - val_loss: 1.3812e-05 - val_my_r2: 0.9952\n",
      "Epoch 1188/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9475e-04 - my_r2: 0.9506 - val_loss: 1.6044e-05 - val_my_r2: 0.9944\n",
      "Epoch 1189/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8880e-04 - my_r2: 0.8611 - val_loss: 1.6984e-05 - val_my_r2: 0.9942\n",
      "Epoch 1190/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9526e-04 - my_r2: 0.9097 - val_loss: 1.3915e-05 - val_my_r2: 0.9952\n",
      "Epoch 1191/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9990e-04 - my_r2: 0.9077 - val_loss: 1.4924e-05 - val_my_r2: 0.9946\n",
      "Epoch 1192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6442e-04 - my_r2: 0.9146 - val_loss: 1.6614e-05 - val_my_r2: 0.9939\n",
      "Epoch 1193/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3403e-04 - my_r2: 0.9211 - val_loss: 1.8532e-05 - val_my_r2: 0.9933\n",
      "Epoch 1194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5224e-04 - my_r2: 0.8811 - val_loss: 2.0313e-05 - val_my_r2: 0.9924\n",
      "Epoch 1195/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8443e-04 - my_r2: 0.9052 - val_loss: 1.3578e-05 - val_my_r2: 0.9951\n",
      "Epoch 1196/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6347e-04 - my_r2: 0.8731 - val_loss: 1.2480e-05 - val_my_r2: 0.9954\n",
      "Epoch 1197/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6703e-04 - my_r2: 0.9460 - val_loss: 1.5746e-05 - val_my_r2: 0.9936\n",
      "Epoch 1198/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2946e-04 - my_r2: 0.9070 - val_loss: 1.5101e-05 - val_my_r2: 0.9947\n",
      "Epoch 1199/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6255e-04 - my_r2: 0.9104 - val_loss: 1.1841e-05 - val_my_r2: 0.9956\n",
      "Epoch 1200/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1591e-04 - my_r2: 0.9354 - val_loss: 1.2197e-05 - val_my_r2: 0.9954\n",
      "Epoch 1201/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9050e-04 - my_r2: 0.8920 - val_loss: 1.0862e-05 - val_my_r2: 0.9963\n",
      "Epoch 1202/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6162e-04 - my_r2: 0.8852 - val_loss: 1.4402e-05 - val_my_r2: 0.9953\n",
      "Epoch 1203/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3579e-04 - my_r2: 0.9343 - val_loss: 1.5345e-05 - val_my_r2: 0.9950\n",
      "Epoch 1204/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0099e-04 - my_r2: 0.9201 - val_loss: 1.3401e-05 - val_my_r2: 0.9955\n",
      "Epoch 1205/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0827e-04 - my_r2: 0.9493 - val_loss: 1.1221e-05 - val_my_r2: 0.9959\n",
      "Epoch 1206/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3697e-04 - my_r2: 0.9422 - val_loss: 1.1937e-05 - val_my_r2: 0.9958\n",
      "Epoch 1207/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5628e-04 - my_r2: 0.9320 - val_loss: 1.1857e-05 - val_my_r2: 0.9957\n",
      "Epoch 1208/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5625e-04 - my_r2: 0.7636 - val_loss: 9.6611e-06 - val_my_r2: 0.9965\n",
      "Epoch 1209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8971e-04 - my_r2: 0.9292 - val_loss: 1.4097e-05 - val_my_r2: 0.9947\n",
      "Epoch 1210/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2172e-04 - my_r2: 0.9463 - val_loss: 1.4042e-05 - val_my_r2: 0.9953\n",
      "Epoch 1211/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1747e-04 - my_r2: 0.9539 - val_loss: 9.5023e-06 - val_my_r2: 0.9965\n",
      "Epoch 1212/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7389e-04 - my_r2: 0.8818 - val_loss: 1.0034e-05 - val_my_r2: 0.9961\n",
      "Epoch 1213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4309e-04 - my_r2: 0.9233 - val_loss: 1.1134e-05 - val_my_r2: 0.9957\n",
      "Epoch 1214/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5345e-04 - my_r2: 0.9287 - val_loss: 9.1242e-06 - val_my_r2: 0.9968\n",
      "Epoch 1215/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6048e-04 - my_r2: 0.9229 - val_loss: 9.5034e-06 - val_my_r2: 0.9968\n",
      "Epoch 1216/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5462e-04 - my_r2: 0.9387 - val_loss: 1.1012e-05 - val_my_r2: 0.9965\n",
      "Epoch 1217/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4143e-04 - my_r2: 0.9501 - val_loss: 1.3287e-05 - val_my_r2: 0.9955\n",
      "Epoch 1218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3804e-04 - my_r2: 0.8408 - val_loss: 1.1820e-05 - val_my_r2: 0.9956\n",
      "Epoch 1219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0856e-04 - my_r2: 0.9363 - val_loss: 1.0569e-05 - val_my_r2: 0.9962\n",
      "Epoch 1220/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4377e-04 - my_r2: 0.9089 - val_loss: 1.4439e-05 - val_my_r2: 0.9947\n",
      "Epoch 1221/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1674e-04 - my_r2: 0.9481 - val_loss: 1.2498e-05 - val_my_r2: 0.9954\n",
      "Epoch 1222/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3505e-04 - my_r2: 0.9500 - val_loss: 1.1439e-05 - val_my_r2: 0.9957\n",
      "Epoch 1223/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6234e-04 - my_r2: 0.8994 - val_loss: 1.3393e-05 - val_my_r2: 0.9952\n",
      "Epoch 1224/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5048e-04 - my_r2: 0.9346 - val_loss: 1.5333e-05 - val_my_r2: 0.9946\n",
      "Epoch 1225/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8351e-04 - my_r2: 0.9279 - val_loss: 1.1321e-05 - val_my_r2: 0.9960\n",
      "Epoch 1226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5782e-04 - my_r2: 0.8915 - val_loss: 9.2645e-06 - val_my_r2: 0.9965\n",
      "Epoch 1227/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3825e-04 - my_r2: 0.9396 - val_loss: 1.1545e-05 - val_my_r2: 0.9955\n",
      "Epoch 1228/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1192e-04 - my_r2: 0.9094 - val_loss: 1.4313e-05 - val_my_r2: 0.9943\n",
      "Epoch 1229/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5806e-04 - my_r2: 0.9056 - val_loss: 1.1791e-05 - val_my_r2: 0.9952\n",
      "Epoch 1230/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1808e-04 - my_r2: 0.8264 - val_loss: 1.0894e-05 - val_my_r2: 0.9957\n",
      "Epoch 1231/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4927e-04 - my_r2: 0.8022 - val_loss: 1.1270e-05 - val_my_r2: 0.9954\n",
      "Epoch 1232/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4849e-04 - my_r2: 0.9314 - val_loss: 1.0477e-05 - val_my_r2: 0.9959\n",
      "Epoch 1233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4479e-04 - my_r2: 0.9143 - val_loss: 9.5253e-06 - val_my_r2: 0.9965\n",
      "Epoch 1234/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4297e-04 - my_r2: 0.9412 - val_loss: 1.3381e-05 - val_my_r2: 0.9952\n",
      "Epoch 1235/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3633e-04 - my_r2: 0.9332 - val_loss: 1.8806e-05 - val_my_r2: 0.9937\n",
      "Epoch 1236/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1304e-04 - my_r2: 0.9383 - val_loss: 1.0493e-05 - val_my_r2: 0.9964\n",
      "Epoch 1237/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3612e-04 - my_r2: 0.9166 - val_loss: 1.0362e-05 - val_my_r2: 0.9962\n",
      "Epoch 1238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7263e-04 - my_r2: 0.9506 - val_loss: 1.0987e-05 - val_my_r2: 0.9959\n",
      "Epoch 1239/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3883e-04 - my_r2: 0.9367 - val_loss: 1.1877e-05 - val_my_r2: 0.9957\n",
      "Epoch 1240/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3440e-04 - my_r2: 0.9350 - val_loss: 1.1533e-05 - val_my_r2: 0.9956\n",
      "Epoch 1241/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1371e-04 - my_r2: 0.9246 - val_loss: 1.1228e-05 - val_my_r2: 0.9957\n",
      "Epoch 1242/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3629e-04 - my_r2: 0.9442 - val_loss: 1.2644e-05 - val_my_r2: 0.9951\n",
      "Epoch 1243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7784e-04 - my_r2: 0.9220 - val_loss: 1.2041e-05 - val_my_r2: 0.9954\n",
      "Epoch 1244/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3852e-04 - my_r2: 0.9034 - val_loss: 1.3294e-05 - val_my_r2: 0.9947\n",
      "Epoch 1245/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8380e-04 - my_r2: 0.9341 - val_loss: 1.2160e-05 - val_my_r2: 0.9954\n",
      "Epoch 1246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6352e-04 - my_r2: 0.9277 - val_loss: 1.3570e-05 - val_my_r2: 0.9952\n",
      "Epoch 1247/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4960e-04 - my_r2: 0.9379 - val_loss: 1.5389e-05 - val_my_r2: 0.9945\n",
      "Epoch 1248/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6961e-04 - my_r2: 0.9216 - val_loss: 1.6762e-05 - val_my_r2: 0.9940\n",
      "Epoch 1249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9872e-04 - my_r2: 0.9257 - val_loss: 9.9060e-06 - val_my_r2: 0.9966\n",
      "Epoch 1250/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0692e-04 - my_r2: 0.9457 - val_loss: 1.0313e-05 - val_my_r2: 0.9963\n",
      "Epoch 1251/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5395e-04 - my_r2: 0.8946 - val_loss: 1.2605e-05 - val_my_r2: 0.9958\n",
      "Epoch 1252/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6226e-04 - my_r2: 0.9290 - val_loss: 1.1385e-05 - val_my_r2: 0.9966\n",
      "Epoch 1253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0179e-04 - my_r2: 0.9375 - val_loss: 1.1121e-05 - val_my_r2: 0.9966\n",
      "Epoch 1254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2165e-04 - my_r2: 0.9282 - val_loss: 1.1618e-05 - val_my_r2: 0.9966\n",
      "Epoch 1255/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0067e-04 - my_r2: 0.8916 - val_loss: 1.2433e-05 - val_my_r2: 0.9965\n",
      "Epoch 1256/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7578e-04 - my_r2: 0.8985 - val_loss: 1.4398e-05 - val_my_r2: 0.9959\n",
      "Epoch 1257/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2939e-04 - my_r2: 0.9385 - val_loss: 1.7408e-05 - val_my_r2: 0.9947\n",
      "Epoch 1258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3173e-04 - my_r2: 0.9442 - val_loss: 1.2406e-05 - val_my_r2: 0.9964\n",
      "Epoch 1259/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3684e-04 - my_r2: 0.9280 - val_loss: 9.4690e-06 - val_my_r2: 0.9973\n",
      "Epoch 1260/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2890e-04 - my_r2: 0.9358 - val_loss: 1.0187e-05 - val_my_r2: 0.9968\n",
      "Epoch 1261/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1541e-04 - my_r2: 0.9406 - val_loss: 8.6155e-06 - val_my_r2: 0.9971\n",
      "Epoch 1262/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0666e-04 - my_r2: 0.9175 - val_loss: 1.3578e-05 - val_my_r2: 0.9951\n",
      "Epoch 1263/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5133e-04 - my_r2: 0.9247 - val_loss: 1.1885e-05 - val_my_r2: 0.9959\n",
      "Epoch 1264/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7198e-04 - my_r2: 0.8627 - val_loss: 1.1397e-05 - val_my_r2: 0.9963\n",
      "Epoch 1265/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7958e-04 - my_r2: 0.8992 - val_loss: 1.3998e-05 - val_my_r2: 0.9951\n",
      "Epoch 1266/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2907e-04 - my_r2: 0.9262 - val_loss: 1.1941e-05 - val_my_r2: 0.9960\n",
      "Epoch 1267/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5882e-04 - my_r2: 0.8839 - val_loss: 1.5497e-05 - val_my_r2: 0.9943\n",
      "Epoch 1268/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9005e-04 - my_r2: 0.9005 - val_loss: 1.1931e-05 - val_my_r2: 0.9956\n",
      "Epoch 1269/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2822e-04 - my_r2: 0.9497 - val_loss: 1.0433e-05 - val_my_r2: 0.9963\n",
      "Epoch 1270/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2061e-04 - my_r2: 0.9306 - val_loss: 8.7880e-06 - val_my_r2: 0.9968\n",
      "Epoch 1271/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2927e-04 - my_r2: 0.9009 - val_loss: 1.0379e-05 - val_my_r2: 0.9964\n",
      "Epoch 1272/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5572e-04 - my_r2: 0.9034 - val_loss: 9.0538e-06 - val_my_r2: 0.9964\n",
      "Epoch 1273/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5590e-04 - my_r2: 0.9417 - val_loss: 9.4832e-06 - val_my_r2: 0.9963\n",
      "Epoch 1274/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2599e-04 - my_r2: 0.9001 - val_loss: 1.0568e-05 - val_my_r2: 0.9961\n",
      "Epoch 1275/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6055e-04 - my_r2: 0.9355 - val_loss: 1.0904e-05 - val_my_r2: 0.9961\n",
      "Epoch 1276/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4762e-04 - my_r2: 0.9251 - val_loss: 1.1929e-05 - val_my_r2: 0.9959\n",
      "Epoch 1277/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3826e-04 - my_r2: 0.8867 - val_loss: 1.3729e-05 - val_my_r2: 0.9951\n",
      "Epoch 1278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2622e-04 - my_r2: 0.9401 - val_loss: 1.3654e-05 - val_my_r2: 0.9951\n",
      "Epoch 1279/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2735e-04 - my_r2: 0.9403 - val_loss: 1.3215e-05 - val_my_r2: 0.9956\n",
      "Epoch 1280/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4408e-04 - my_r2: 0.9291 - val_loss: 1.3630e-05 - val_my_r2: 0.9950\n",
      "Epoch 1281/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1975e-04 - my_r2: 0.8709 - val_loss: 1.4349e-05 - val_my_r2: 0.9944\n",
      "Epoch 1282/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3231e-04 - my_r2: 0.9008 - val_loss: 1.3419e-05 - val_my_r2: 0.9949\n",
      "Epoch 1283/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.7197e-04 - my_r2: 0.9215 - val_loss: 1.6853e-05 - val_my_r2: 0.9938\n",
      "Epoch 1284/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7113e-04 - my_r2: 0.9144 - val_loss: 1.3860e-05 - val_my_r2: 0.9950\n",
      "Epoch 1285/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9200e-04 - my_r2: 0.9314 - val_loss: 1.3866e-05 - val_my_r2: 0.9948\n",
      "Epoch 1286/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1220e-04 - my_r2: 0.8937 - val_loss: 1.1778e-05 - val_my_r2: 0.9957\n",
      "Epoch 1287/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4669e-04 - my_r2: 0.9165 - val_loss: 1.1955e-05 - val_my_r2: 0.9955\n",
      "Epoch 1288/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3757e-04 - my_r2: 0.9111 - val_loss: 1.6252e-05 - val_my_r2: 0.9942\n",
      "Epoch 1289/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8063e-04 - my_r2: 0.8431 - val_loss: 1.2199e-05 - val_my_r2: 0.9958\n",
      "Epoch 1290/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1727e-04 - my_r2: 0.9165 - val_loss: 1.1654e-05 - val_my_r2: 0.9959\n",
      "Epoch 1291/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1458e-04 - my_r2: 0.9371 - val_loss: 1.0160e-05 - val_my_r2: 0.9962\n",
      "Epoch 1292/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6564e-04 - my_r2: 0.9095 - val_loss: 1.0161e-05 - val_my_r2: 0.9962\n",
      "Epoch 1293/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4932e-04 - my_r2: 0.9503 - val_loss: 9.8222e-06 - val_my_r2: 0.9965\n",
      "Epoch 1294/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1440e-04 - my_r2: 0.8914 - val_loss: 1.4595e-05 - val_my_r2: 0.9946\n",
      "Epoch 1295/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2287e-04 - my_r2: 0.9228 - val_loss: 1.2842e-05 - val_my_r2: 0.9952\n",
      "Epoch 1296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3617e-04 - my_r2: 0.9518 - val_loss: 1.3431e-05 - val_my_r2: 0.9943\n",
      "Epoch 1297/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.3372e-04 - my_r2: 0.9376 - val_loss: 1.4340e-05 - val_my_r2: 0.9941\n",
      "Epoch 1298/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3245e-04 - my_r2: 0.9336 - val_loss: 1.3124e-05 - val_my_r2: 0.9951\n",
      "Epoch 1299/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1368e-04 - my_r2: 0.9565 - val_loss: 1.1691e-05 - val_my_r2: 0.9960\n",
      "Epoch 1300/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5197e-04 - my_r2: 0.9402 - val_loss: 1.2939e-05 - val_my_r2: 0.9960\n",
      "Epoch 1301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8318e-04 - my_r2: 0.9110 - val_loss: 1.0279e-05 - val_my_r2: 0.9970\n",
      "Epoch 1302/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3529e-04 - my_r2: 0.9472 - val_loss: 1.2476e-05 - val_my_r2: 0.9959\n",
      "Epoch 1303/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1766e-04 - my_r2: 0.9414 - val_loss: 1.2522e-05 - val_my_r2: 0.9956\n",
      "Epoch 1304/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3911e-04 - my_r2: 0.9115 - val_loss: 1.6085e-05 - val_my_r2: 0.9948\n",
      "Epoch 1305/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9064e-04 - my_r2: 0.8714 - val_loss: 1.3628e-05 - val_my_r2: 0.9954\n",
      "Epoch 1306/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8261e-04 - my_r2: 0.9355 - val_loss: 1.0374e-05 - val_my_r2: 0.9967\n",
      "Epoch 1307/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3006e-04 - my_r2: 0.9168 - val_loss: 9.7979e-06 - val_my_r2: 0.9968\n",
      "Epoch 1308/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5333e-04 - my_r2: 0.9096 - val_loss: 9.0157e-06 - val_my_r2: 0.9971\n",
      "Epoch 1309/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7238e-04 - my_r2: 0.9099 - val_loss: 8.6674e-06 - val_my_r2: 0.9971\n",
      "Epoch 1310/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1676e-04 - my_r2: 0.9451 - val_loss: 9.1733e-06 - val_my_r2: 0.9966\n",
      "Epoch 1311/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6212e-04 - my_r2: 0.9440 - val_loss: 1.0987e-05 - val_my_r2: 0.9959\n",
      "Epoch 1312/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7549e-04 - my_r2: 0.8948 - val_loss: 1.2877e-05 - val_my_r2: 0.9955\n",
      "Epoch 1313/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5862e-04 - my_r2: 0.9253 - val_loss: 1.2678e-05 - val_my_r2: 0.9958\n",
      "Epoch 1314/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4058e-04 - my_r2: 0.9157 - val_loss: 1.1998e-05 - val_my_r2: 0.9957\n",
      "Epoch 1315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9540e-04 - my_r2: 0.9349 - val_loss: 9.4689e-06 - val_my_r2: 0.9964\n",
      "Epoch 1316/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1731e-04 - my_r2: 0.9040 - val_loss: 1.1282e-05 - val_my_r2: 0.9952\n",
      "Epoch 1317/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7722e-04 - my_r2: 0.8827 - val_loss: 1.3877e-05 - val_my_r2: 0.9943\n",
      "Epoch 1318/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7032e-04 - my_r2: 0.9180 - val_loss: 1.4847e-05 - val_my_r2: 0.9946\n",
      "Epoch 1319/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4449e-04 - my_r2: 0.9446 - val_loss: 1.2388e-05 - val_my_r2: 0.9956\n",
      "Epoch 1320/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0890e-04 - my_r2: 0.9408 - val_loss: 1.1786e-05 - val_my_r2: 0.9962\n",
      "Epoch 1321/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3980e-04 - my_r2: 0.9218 - val_loss: 1.1777e-05 - val_my_r2: 0.9963\n",
      "Epoch 1322/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3064e-04 - my_r2: 0.9406 - val_loss: 1.2644e-05 - val_my_r2: 0.9957\n",
      "Epoch 1323/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3643e-04 - my_r2: 0.9282 - val_loss: 9.8916e-06 - val_my_r2: 0.9964\n",
      "Epoch 1324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3655e-04 - my_r2: 0.8524 - val_loss: 1.1617e-05 - val_my_r2: 0.9961\n",
      "Epoch 1325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0513e-04 - my_r2: 0.9105 - val_loss: 8.6330e-06 - val_my_r2: 0.9970\n",
      "Epoch 1326/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7672e-04 - my_r2: 0.8461 - val_loss: 9.6560e-06 - val_my_r2: 0.9964\n",
      "Epoch 1327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2293e-04 - my_r2: 0.9388 - val_loss: 1.2665e-05 - val_my_r2: 0.9953\n",
      "Epoch 1328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6313e-04 - my_r2: 0.9276 - val_loss: 1.2839e-05 - val_my_r2: 0.9958\n",
      "Epoch 1329/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5493e-04 - my_r2: 0.9490 - val_loss: 1.1584e-05 - val_my_r2: 0.9961\n",
      "Epoch 1330/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1266e-04 - my_r2: 0.8728 - val_loss: 1.5021e-05 - val_my_r2: 0.9946\n",
      "Epoch 1331/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5209e-04 - my_r2: 0.9296 - val_loss: 1.2338e-05 - val_my_r2: 0.9953\n",
      "Epoch 1332/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5564e-04 - my_r2: 0.9452 - val_loss: 9.4657e-06 - val_my_r2: 0.9964\n",
      "Epoch 1333/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4397e-04 - my_r2: 0.9533 - val_loss: 1.1082e-05 - val_my_r2: 0.9958\n",
      "Epoch 1334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6004e-04 - my_r2: 0.9209 - val_loss: 1.3856e-05 - val_my_r2: 0.9951\n",
      "Epoch 1335/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7361e-04 - my_r2: 0.9039 - val_loss: 1.3026e-05 - val_my_r2: 0.9955\n",
      "Epoch 1336/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8412e-04 - my_r2: 0.9091 - val_loss: 1.2724e-05 - val_my_r2: 0.9953\n",
      "Epoch 1337/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4653e-04 - my_r2: 0.9093 - val_loss: 1.5056e-05 - val_my_r2: 0.9945\n",
      "Epoch 1338/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3410e-04 - my_r2: 0.9085 - val_loss: 1.4969e-05 - val_my_r2: 0.9945\n",
      "Epoch 1339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0685e-04 - my_r2: 0.9008 - val_loss: 1.4521e-05 - val_my_r2: 0.9950\n",
      "Epoch 1340/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5085e-04 - my_r2: 0.9301 - val_loss: 1.5930e-05 - val_my_r2: 0.9945\n",
      "Epoch 1341/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5590e-04 - my_r2: 0.9139 - val_loss: 1.3877e-05 - val_my_r2: 0.9954\n",
      "Epoch 1342/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2312e-04 - my_r2: 0.9519 - val_loss: 1.7769e-05 - val_my_r2: 0.9941\n",
      "Epoch 1343/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4549e-04 - my_r2: 0.9312 - val_loss: 1.1167e-05 - val_my_r2: 0.9961\n",
      "Epoch 1344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3024e-04 - my_r2: 0.9490 - val_loss: 1.2406e-05 - val_my_r2: 0.9955\n",
      "Epoch 1345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5115e-04 - my_r2: 0.9135 - val_loss: 1.1323e-05 - val_my_r2: 0.9956\n",
      "Epoch 1346/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0067e-04 - my_r2: 0.9154 - val_loss: 1.6659e-05 - val_my_r2: 0.9950\n",
      "Epoch 1347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1154e-04 - my_r2: 0.8886 - val_loss: 1.0867e-05 - val_my_r2: 0.9968\n",
      "Epoch 1348/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8428e-04 - my_r2: 0.9367 - val_loss: 9.1086e-06 - val_my_r2: 0.9970\n",
      "Epoch 1349/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0019e-04 - my_r2: 0.8799 - val_loss: 9.0971e-06 - val_my_r2: 0.9963\n",
      "Epoch 1350/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0818e-04 - my_r2: 0.9253 - val_loss: 1.1978e-05 - val_my_r2: 0.9953\n",
      "Epoch 1351/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8317e-04 - my_r2: 0.9239 - val_loss: 1.6070e-05 - val_my_r2: 0.9940\n",
      "Epoch 1352/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3168e-04 - my_r2: 0.9310 - val_loss: 1.3892e-05 - val_my_r2: 0.9948\n",
      "Epoch 1353/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6843e-04 - my_r2: 0.9301 - val_loss: 1.5350e-05 - val_my_r2: 0.9950\n",
      "Epoch 1354/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2059e-04 - my_r2: 0.9458 - val_loss: 1.6946e-05 - val_my_r2: 0.9944\n",
      "Epoch 1355/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2083e-04 - my_r2: 0.9434 - val_loss: 1.3490e-05 - val_my_r2: 0.9955\n",
      "Epoch 1356/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4358e-04 - my_r2: 0.9126 - val_loss: 1.6816e-05 - val_my_r2: 0.9946\n",
      "Epoch 1357/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6789e-04 - my_r2: 0.9452 - val_loss: 1.4260e-05 - val_my_r2: 0.9956\n",
      "Epoch 1358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2027e-04 - my_r2: 0.9480 - val_loss: 1.1656e-05 - val_my_r2: 0.9965\n",
      "Epoch 1359/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8961e-04 - my_r2: 0.9332 - val_loss: 1.1896e-05 - val_my_r2: 0.9962\n",
      "Epoch 1360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8273e-04 - my_r2: 0.9437 - val_loss: 1.2692e-05 - val_my_r2: 0.9963\n",
      "Epoch 1361/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5671e-04 - my_r2: 0.9395 - val_loss: 1.0328e-05 - val_my_r2: 0.9968\n",
      "Epoch 1362/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7819e-04 - my_r2: 0.9420 - val_loss: 1.1473e-05 - val_my_r2: 0.9965\n",
      "Epoch 1363/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5612e-04 - my_r2: 0.9325 - val_loss: 1.1973e-05 - val_my_r2: 0.9960\n",
      "Epoch 1364/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1081e-04 - my_r2: 0.8887 - val_loss: 1.2364e-05 - val_my_r2: 0.9958\n",
      "Epoch 1365/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8191e-04 - my_r2: 0.9383 - val_loss: 1.5973e-05 - val_my_r2: 0.9947\n",
      "Epoch 1366/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5846e-04 - my_r2: 0.9349 - val_loss: 1.3305e-05 - val_my_r2: 0.9954\n",
      "Epoch 1367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5937e-04 - my_r2: 0.8853 - val_loss: 1.9887e-05 - val_my_r2: 0.9935\n",
      "Epoch 1368/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2058e-04 - my_r2: 0.9523 - val_loss: 1.3373e-05 - val_my_r2: 0.9957\n",
      "Epoch 1369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9643e-04 - my_r2: 0.9189 - val_loss: 1.1979e-05 - val_my_r2: 0.9961\n",
      "Epoch 1370/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5117e-04 - my_r2: 0.9467 - val_loss: 9.7052e-06 - val_my_r2: 0.9969\n",
      "Epoch 1371/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7461e-04 - my_r2: 0.9199 - val_loss: 1.0804e-05 - val_my_r2: 0.9964\n",
      "Epoch 1372/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4853e-04 - my_r2: 0.9267 - val_loss: 1.2340e-05 - val_my_r2: 0.9955\n",
      "Epoch 1373/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6689e-04 - my_r2: 0.9328 - val_loss: 1.2802e-05 - val_my_r2: 0.9952\n",
      "Epoch 1374/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3461e-04 - my_r2: 0.9502 - val_loss: 1.2819e-05 - val_my_r2: 0.9953\n",
      "Epoch 1375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6994e-04 - my_r2: 0.9312 - val_loss: 1.1086e-05 - val_my_r2: 0.9958\n",
      "Epoch 1376/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9723e-04 - my_r2: 0.9196 - val_loss: 1.2671e-05 - val_my_r2: 0.9955\n",
      "Epoch 1377/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3916e-04 - my_r2: 0.9055 - val_loss: 1.3800e-05 - val_my_r2: 0.9952\n",
      "Epoch 1378/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9369e-04 - my_r2: 0.9391 - val_loss: 9.7252e-06 - val_my_r2: 0.9964\n",
      "Epoch 1379/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9038e-04 - my_r2: 0.9449 - val_loss: 9.5401e-06 - val_my_r2: 0.9965\n",
      "Epoch 1380/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6320e-04 - my_r2: 0.9415 - val_loss: 1.2085e-05 - val_my_r2: 0.9958\n",
      "Epoch 1381/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3788e-04 - my_r2: 0.9355 - val_loss: 1.0267e-05 - val_my_r2: 0.9964\n",
      "Epoch 1382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1329e-04 - my_r2: 0.9398 - val_loss: 1.2130e-05 - val_my_r2: 0.9960\n",
      "Epoch 1383/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0727e-04 - my_r2: 0.9538 - val_loss: 1.2256e-05 - val_my_r2: 0.9963\n",
      "Epoch 1384/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5843e-04 - my_r2: 0.9379 - val_loss: 1.0763e-05 - val_my_r2: 0.9964\n",
      "Epoch 1385/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6035e-04 - my_r2: 0.9235 - val_loss: 1.2119e-05 - val_my_r2: 0.9957\n",
      "Epoch 1386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7622e-04 - my_r2: 0.9248 - val_loss: 1.1631e-05 - val_my_r2: 0.9963\n",
      "Epoch 1387/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1712e-04 - my_r2: 0.9527 - val_loss: 9.5303e-06 - val_my_r2: 0.9966\n",
      "Epoch 1388/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5547e-04 - my_r2: 0.9429 - val_loss: 9.7506e-06 - val_my_r2: 0.9964\n",
      "Epoch 1389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6120e-04 - my_r2: 0.9225 - val_loss: 9.1764e-06 - val_my_r2: 0.9966\n",
      "Epoch 1390/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7586e-04 - my_r2: 0.9044 - val_loss: 1.0817e-05 - val_my_r2: 0.9959\n",
      "Epoch 1391/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7653e-04 - my_r2: 0.9601 - val_loss: 1.6496e-05 - val_my_r2: 0.9940\n",
      "Epoch 1392/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7687e-04 - my_r2: 0.7880 - val_loss: 1.9930e-05 - val_my_r2: 0.9926\n",
      "Epoch 1393/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3539e-04 - my_r2: 0.8736 - val_loss: 1.5493e-05 - val_my_r2: 0.9941\n",
      "Epoch 1394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6954e-04 - my_r2: 0.9158 - val_loss: 1.2357e-05 - val_my_r2: 0.9952\n",
      "Epoch 1395/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5158e-04 - my_r2: 0.9042 - val_loss: 1.4227e-05 - val_my_r2: 0.9948\n",
      "Epoch 1396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7976e-04 - my_r2: 0.9317 - val_loss: 1.7525e-05 - val_my_r2: 0.9942\n",
      "Epoch 1397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0331e-04 - my_r2: 0.9479 - val_loss: 1.0991e-05 - val_my_r2: 0.9962\n",
      "Epoch 1398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1896e-04 - my_r2: 0.9504 - val_loss: 1.2135e-05 - val_my_r2: 0.9955\n",
      "Epoch 1399/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2925e-04 - my_r2: 0.9480 - val_loss: 1.2716e-05 - val_my_r2: 0.9951\n",
      "Epoch 1400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6217e-04 - my_r2: 0.9290 - val_loss: 1.2375e-05 - val_my_r2: 0.9951\n",
      "Epoch 1401/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1909e-04 - my_r2: 0.9235 - val_loss: 1.2632e-05 - val_my_r2: 0.9956\n",
      "Epoch 1402/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8264e-04 - my_r2: 0.9107 - val_loss: 1.3322e-05 - val_my_r2: 0.9953\n",
      "Epoch 1403/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6872e-04 - my_r2: 0.9351 - val_loss: 2.0239e-05 - val_my_r2: 0.9929\n",
      "Epoch 1404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8135e-04 - my_r2: 0.8875 - val_loss: 1.6634e-05 - val_my_r2: 0.9943\n",
      "Epoch 1405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5208e-04 - my_r2: 0.9429 - val_loss: 9.9856e-06 - val_my_r2: 0.9964\n",
      "Epoch 1406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4552e-04 - my_r2: 0.9426 - val_loss: 1.1333e-05 - val_my_r2: 0.9961\n",
      "Epoch 1407/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5588e-04 - my_r2: 0.9520 - val_loss: 1.1754e-05 - val_my_r2: 0.9963\n",
      "Epoch 1408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2433e-04 - my_r2: 0.9421 - val_loss: 1.2878e-05 - val_my_r2: 0.9961\n",
      "Epoch 1409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3685e-04 - my_r2: 0.9154 - val_loss: 1.1609e-05 - val_my_r2: 0.9962\n",
      "Epoch 1410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2135e-04 - my_r2: 0.9377 - val_loss: 1.1345e-05 - val_my_r2: 0.9962\n",
      "Epoch 1411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0137e-04 - my_r2: 0.9328 - val_loss: 1.2683e-05 - val_my_r2: 0.9958\n",
      "Epoch 1412/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2150e-04 - my_r2: 0.9424 - val_loss: 1.3844e-05 - val_my_r2: 0.9952\n",
      "Epoch 1413/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9900e-04 - my_r2: 0.9055 - val_loss: 1.0553e-05 - val_my_r2: 0.9964\n",
      "Epoch 1414/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2558e-04 - my_r2: 0.9400 - val_loss: 1.0453e-05 - val_my_r2: 0.9968\n",
      "Epoch 1415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3271e-04 - my_r2: 0.9492 - val_loss: 9.4778e-06 - val_my_r2: 0.9970\n",
      "Epoch 1416/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4777e-04 - my_r2: 0.9404 - val_loss: 1.2747e-05 - val_my_r2: 0.9958\n",
      "Epoch 1417/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7223e-04 - my_r2: 0.9319 - val_loss: 1.0916e-05 - val_my_r2: 0.9962\n",
      "Epoch 1418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5918e-04 - my_r2: 0.9393 - val_loss: 9.4649e-06 - val_my_r2: 0.9968\n",
      "Epoch 1419/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6932e-04 - my_r2: 0.8400 - val_loss: 9.7593e-06 - val_my_r2: 0.9964\n",
      "Epoch 1420/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2668e-04 - my_r2: 0.9479 - val_loss: 2.0331e-05 - val_my_r2: 0.9928\n",
      "Epoch 1421/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4979e-04 - my_r2: 0.9326 - val_loss: 1.4729e-05 - val_my_r2: 0.9942\n",
      "Epoch 1422/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5442e-04 - my_r2: 0.9351 - val_loss: 1.3708e-05 - val_my_r2: 0.9948\n",
      "Epoch 1423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7261e-04 - my_r2: 0.9182 - val_loss: 1.2390e-05 - val_my_r2: 0.9957\n",
      "Epoch 1424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8082e-04 - my_r2: 0.7787 - val_loss: 9.7370e-06 - val_my_r2: 0.9966\n",
      "Epoch 1425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5022e-04 - my_r2: 0.9372 - val_loss: 1.0060e-05 - val_my_r2: 0.9967\n",
      "Epoch 1426/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0463e-04 - my_r2: 0.9548 - val_loss: 1.0198e-05 - val_my_r2: 0.9968\n",
      "Epoch 1427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1157e-04 - my_r2: 0.9416 - val_loss: 9.5068e-06 - val_my_r2: 0.9968\n",
      "Epoch 1428/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4278e-04 - my_r2: 0.9424 - val_loss: 1.1203e-05 - val_my_r2: 0.9960\n",
      "Epoch 1429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2406e-04 - my_r2: 0.8077 - val_loss: 1.3521e-05 - val_my_r2: 0.9950\n",
      "Epoch 1430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1693e-04 - my_r2: 0.9301 - val_loss: 1.3699e-05 - val_my_r2: 0.9951\n",
      "Epoch 1431/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0071e-04 - my_r2: 0.9155 - val_loss: 1.0082e-05 - val_my_r2: 0.9964\n",
      "Epoch 1432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9182e-04 - my_r2: 0.9434 - val_loss: 1.0827e-05 - val_my_r2: 0.9962\n",
      "Epoch 1433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9518e-04 - my_r2: 0.9309 - val_loss: 1.1442e-05 - val_my_r2: 0.9959\n",
      "Epoch 1434/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8388e-04 - my_r2: 0.8855 - val_loss: 1.2666e-05 - val_my_r2: 0.9962\n",
      "Epoch 1435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2421e-04 - my_r2: 0.9347 - val_loss: 1.1171e-05 - val_my_r2: 0.9966\n",
      "Epoch 1436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9686e-04 - my_r2: 0.9608 - val_loss: 9.8531e-06 - val_my_r2: 0.9970\n",
      "Epoch 1437/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7281e-04 - my_r2: 0.9196 - val_loss: 8.8482e-06 - val_my_r2: 0.9972\n",
      "Epoch 1438/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2964e-04 - my_r2: 0.9441 - val_loss: 9.1655e-06 - val_my_r2: 0.9966\n",
      "Epoch 1439/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6439e-04 - my_r2: 0.9384 - val_loss: 1.1148e-05 - val_my_r2: 0.9954\n",
      "Epoch 1440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8534e-04 - my_r2: 0.9444 - val_loss: 1.3118e-05 - val_my_r2: 0.9946\n",
      "Epoch 1441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2949e-04 - my_r2: 0.9464 - val_loss: 1.2458e-05 - val_my_r2: 0.9949\n",
      "Epoch 1442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7832e-04 - my_r2: 0.9278 - val_loss: 1.2027e-05 - val_my_r2: 0.9957\n",
      "Epoch 1443/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9561e-04 - my_r2: 0.9424 - val_loss: 1.2012e-05 - val_my_r2: 0.9958\n",
      "Epoch 1444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2976e-04 - my_r2: 0.9488 - val_loss: 1.2270e-05 - val_my_r2: 0.9964\n",
      "Epoch 1445/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7487e-04 - my_r2: 0.9304 - val_loss: 1.1002e-05 - val_my_r2: 0.9967\n",
      "Epoch 1446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9197e-04 - my_r2: 0.9460 - val_loss: 1.3454e-05 - val_my_r2: 0.9954\n",
      "Epoch 1447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1123e-04 - my_r2: 0.8671 - val_loss: 1.2639e-05 - val_my_r2: 0.9951\n",
      "Epoch 1448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6645e-04 - my_r2: 0.9274 - val_loss: 1.3804e-05 - val_my_r2: 0.9946\n",
      "Epoch 1449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6441e-04 - my_r2: 0.9379 - val_loss: 1.1470e-05 - val_my_r2: 0.9954\n",
      "Epoch 1450/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1820e-04 - my_r2: 0.9418 - val_loss: 1.0556e-05 - val_my_r2: 0.9960\n",
      "Epoch 1451/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2222e-04 - my_r2: 0.9152 - val_loss: 1.0266e-05 - val_my_r2: 0.9961\n",
      "Epoch 1452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7275e-04 - my_r2: 0.9336 - val_loss: 1.3428e-05 - val_my_r2: 0.9953\n",
      "Epoch 1453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2883e-04 - my_r2: 0.9472 - val_loss: 1.2994e-05 - val_my_r2: 0.9952\n",
      "Epoch 1454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8155e-04 - my_r2: 0.9372 - val_loss: 9.4010e-06 - val_my_r2: 0.9965\n",
      "Epoch 1455/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3329e-04 - my_r2: 0.9330 - val_loss: 1.0811e-05 - val_my_r2: 0.9963\n",
      "Epoch 1456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3925e-04 - my_r2: 0.9387 - val_loss: 1.3094e-05 - val_my_r2: 0.9956\n",
      "Epoch 1457/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3523e-04 - my_r2: 0.9492 - val_loss: 1.1980e-05 - val_my_r2: 0.9957\n",
      "Epoch 1458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6173e-04 - my_r2: 0.8539 - val_loss: 1.1374e-05 - val_my_r2: 0.9960\n",
      "Epoch 1459/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3271e-04 - my_r2: 0.9414 - val_loss: 1.1450e-05 - val_my_r2: 0.9962\n",
      "Epoch 1460/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1678e-04 - my_r2: 0.9465 - val_loss: 1.2833e-05 - val_my_r2: 0.9956\n",
      "Epoch 1461/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1296e-04 - my_r2: 0.9211 - val_loss: 1.2954e-05 - val_my_r2: 0.9956\n",
      "Epoch 1462/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6835e-04 - my_r2: 0.9369 - val_loss: 1.2270e-05 - val_my_r2: 0.9961\n",
      "Epoch 1463/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6620e-04 - my_r2: 0.9221 - val_loss: 1.5232e-05 - val_my_r2: 0.9946\n",
      "Epoch 1464/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3606e-04 - my_r2: 0.9371 - val_loss: 1.2356e-05 - val_my_r2: 0.9953\n",
      "Epoch 1465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4373e-04 - my_r2: 0.9466 - val_loss: 1.0415e-05 - val_my_r2: 0.9964\n",
      "Epoch 1466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1866e-04 - my_r2: 0.9331 - val_loss: 9.6269e-06 - val_my_r2: 0.9967\n",
      "Epoch 1467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4898e-04 - my_r2: 0.9368 - val_loss: 1.0523e-05 - val_my_r2: 0.9963\n",
      "Epoch 1468/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8343e-04 - my_r2: 0.9388 - val_loss: 1.3636e-05 - val_my_r2: 0.9950\n",
      "Epoch 1469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6256e-04 - my_r2: 0.9374 - val_loss: 1.2862e-05 - val_my_r2: 0.9956\n",
      "Epoch 1470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2825e-04 - my_r2: 0.9253 - val_loss: 1.5411e-05 - val_my_r2: 0.9948\n",
      "Epoch 1471/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4549e-04 - my_r2: 0.9112 - val_loss: 1.0735e-05 - val_my_r2: 0.9965\n",
      "Epoch 1472/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4740e-04 - my_r2: 0.9423 - val_loss: 1.1084e-05 - val_my_r2: 0.9965\n",
      "Epoch 1473/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3277e-04 - my_r2: 0.9378 - val_loss: 1.5023e-05 - val_my_r2: 0.9952\n",
      "Epoch 1474/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2658e-04 - my_r2: 0.9389 - val_loss: 1.3718e-05 - val_my_r2: 0.9958\n",
      "Epoch 1475/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2783e-04 - my_r2: 0.9459 - val_loss: 1.4497e-05 - val_my_r2: 0.9956\n",
      "Epoch 1476/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3376e-04 - my_r2: 0.9568 - val_loss: 1.1700e-05 - val_my_r2: 0.9961\n",
      "Epoch 1477/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6595e-04 - my_r2: 0.9360 - val_loss: 1.5322e-05 - val_my_r2: 0.9946\n",
      "Epoch 1478/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5198e-04 - my_r2: 0.9342 - val_loss: 1.1538e-05 - val_my_r2: 0.9960\n",
      "Epoch 1479/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9193e-04 - my_r2: 0.9271 - val_loss: 1.0983e-05 - val_my_r2: 0.9962\n",
      "Epoch 1480/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9190e-04 - my_r2: 0.9288 - val_loss: 9.8967e-06 - val_my_r2: 0.9967\n",
      "Epoch 1481/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6481e-04 - my_r2: 0.9376 - val_loss: 1.3206e-05 - val_my_r2: 0.9955\n",
      "Epoch 1482/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9374e-04 - my_r2: 0.9581 - val_loss: 1.1530e-05 - val_my_r2: 0.9960\n",
      "Epoch 1483/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3837e-04 - my_r2: 0.9317 - val_loss: 1.0085e-05 - val_my_r2: 0.9966\n",
      "Epoch 1484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0637e-04 - my_r2: 0.9452 - val_loss: 9.8299e-06 - val_my_r2: 0.9967\n",
      "Epoch 1485/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4890e-04 - my_r2: 0.8947 - val_loss: 9.2033e-06 - val_my_r2: 0.9967\n",
      "Epoch 1486/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4509e-04 - my_r2: 0.9429 - val_loss: 9.3343e-06 - val_my_r2: 0.9967\n",
      "Epoch 1487/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1047e-04 - my_r2: 0.9349 - val_loss: 8.7496e-06 - val_my_r2: 0.9970\n",
      "Epoch 1488/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6655e-04 - my_r2: 0.9404 - val_loss: 1.0684e-05 - val_my_r2: 0.9963\n",
      "Epoch 1489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8906e-04 - my_r2: 0.9578 - val_loss: 1.2551e-05 - val_my_r2: 0.9952\n",
      "Epoch 1490/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2159e-04 - my_r2: 0.9291 - val_loss: 1.1121e-05 - val_my_r2: 0.9958\n",
      "Epoch 1491/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1446e-04 - my_r2: 0.9309 - val_loss: 1.0183e-05 - val_my_r2: 0.9969\n",
      "Epoch 1492/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0148e-04 - my_r2: 0.9279 - val_loss: 1.1754e-05 - val_my_r2: 0.9965\n",
      "Epoch 1493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7535e-04 - my_r2: 0.9396 - val_loss: 1.1819e-05 - val_my_r2: 0.9958\n",
      "Epoch 1494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9161e-04 - my_r2: 0.9254 - val_loss: 1.0624e-05 - val_my_r2: 0.9960\n",
      "Epoch 1495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4912e-04 - my_r2: 0.9340 - val_loss: 1.1330e-05 - val_my_r2: 0.9958\n",
      "Epoch 1496/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5610e-04 - my_r2: 0.9506 - val_loss: 1.0311e-05 - val_my_r2: 0.9964\n",
      "Epoch 1497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9846e-04 - my_r2: 0.9476 - val_loss: 1.2117e-05 - val_my_r2: 0.9960\n",
      "Epoch 1498/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5578e-04 - my_r2: 0.9211 - val_loss: 1.1464e-05 - val_my_r2: 0.9965\n",
      "Epoch 1499/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4478e-04 - my_r2: 0.9327 - val_loss: 1.6126e-05 - val_my_r2: 0.9948\n",
      "Epoch 1500/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4726e-04 - my_r2: 0.9336 - val_loss: 1.0038e-05 - val_my_r2: 0.9966\n",
      "Epoch 1501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5405e-04 - my_r2: 0.9524 - val_loss: 1.1082e-05 - val_my_r2: 0.9961\n",
      "Epoch 1502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3567e-04 - my_r2: 0.9288 - val_loss: 1.0793e-05 - val_my_r2: 0.9965\n",
      "Epoch 1503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2662e-04 - my_r2: 0.8633 - val_loss: 1.1748e-05 - val_my_r2: 0.9963\n",
      "Epoch 1504/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7478e-04 - my_r2: 0.9437 - val_loss: 1.5423e-05 - val_my_r2: 0.9954\n",
      "Epoch 1505/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6360e-04 - my_r2: 0.9085 - val_loss: 1.3621e-05 - val_my_r2: 0.9958\n",
      "Epoch 1506/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1256e-04 - my_r2: 0.8765 - val_loss: 1.9399e-05 - val_my_r2: 0.9935\n",
      "Epoch 1507/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6494e-04 - my_r2: 0.9463 - val_loss: 2.3573e-05 - val_my_r2: 0.9920\n",
      "Epoch 1508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0353e-04 - my_r2: 0.9522 - val_loss: 2.2404e-05 - val_my_r2: 0.9925\n",
      "Epoch 1509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6033e-04 - my_r2: 0.9479 - val_loss: 1.7956e-05 - val_my_r2: 0.9941\n",
      "Epoch 1510/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6522e-04 - my_r2: 0.9060 - val_loss: 1.1411e-05 - val_my_r2: 0.9962\n",
      "Epoch 1511/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2077e-04 - my_r2: 0.9542 - val_loss: 1.1636e-05 - val_my_r2: 0.9957\n",
      "Epoch 1512/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1009e-04 - my_r2: 0.9424 - val_loss: 1.4087e-05 - val_my_r2: 0.9945\n",
      "Epoch 1513/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3862e-04 - my_r2: 0.9458 - val_loss: 1.4073e-05 - val_my_r2: 0.9944\n",
      "Epoch 1514/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5484e-04 - my_r2: 0.9150 - val_loss: 1.0323e-05 - val_my_r2: 0.9964\n",
      "Epoch 1515/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1442e-04 - my_r2: 0.9429 - val_loss: 1.4351e-05 - val_my_r2: 0.9953\n",
      "Epoch 1516/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6053e-04 - my_r2: 0.9134 - val_loss: 9.3317e-06 - val_my_r2: 0.9970\n",
      "Epoch 1517/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8392e-04 - my_r2: 0.9084 - val_loss: 1.0492e-05 - val_my_r2: 0.9963\n",
      "Epoch 1518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1888e-04 - my_r2: 0.9311 - val_loss: 1.0385e-05 - val_my_r2: 0.9966\n",
      "Epoch 1519/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8822e-04 - my_r2: 0.9124 - val_loss: 1.1050e-05 - val_my_r2: 0.9965\n",
      "Epoch 1520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3594e-04 - my_r2: 0.9314 - val_loss: 1.1296e-05 - val_my_r2: 0.9967\n",
      "Epoch 1521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9832e-04 - my_r2: 0.9292 - val_loss: 1.0640e-05 - val_my_r2: 0.9970\n",
      "Epoch 1522/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2149e-04 - my_r2: 0.9440 - val_loss: 9.1659e-06 - val_my_r2: 0.9972\n",
      "Epoch 1523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1070e-04 - my_r2: 0.9534 - val_loss: 8.9060e-06 - val_my_r2: 0.9969\n",
      "Epoch 1524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2271e-04 - my_r2: 0.9469 - val_loss: 1.1339e-05 - val_my_r2: 0.9958\n",
      "Epoch 1525/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2226e-04 - my_r2: 0.9465 - val_loss: 1.3973e-05 - val_my_r2: 0.9946\n",
      "Epoch 1526/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8159e-04 - my_r2: 0.9306 - val_loss: 1.1299e-05 - val_my_r2: 0.9959\n",
      "Epoch 1527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2429e-04 - my_r2: 0.9437 - val_loss: 1.1656e-05 - val_my_r2: 0.9961\n",
      "Epoch 1528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5434e-04 - my_r2: 0.9432 - val_loss: 9.6680e-06 - val_my_r2: 0.9967\n",
      "Epoch 1529/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4843e-04 - my_r2: 0.9323 - val_loss: 9.0343e-06 - val_my_r2: 0.9968\n",
      "Epoch 1530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0417e-04 - my_r2: 0.9231 - val_loss: 9.3724e-06 - val_my_r2: 0.9969\n",
      "Epoch 1531/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0119e-04 - my_r2: 0.9380 - val_loss: 9.3165e-06 - val_my_r2: 0.9968\n",
      "Epoch 1532/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2013e-04 - my_r2: 0.9218 - val_loss: 8.2187e-06 - val_my_r2: 0.9972\n",
      "Epoch 1533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4946e-04 - my_r2: 0.9178 - val_loss: 9.4744e-06 - val_my_r2: 0.9966\n",
      "Epoch 1534/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1233e-04 - my_r2: 0.9365 - val_loss: 9.8611e-06 - val_my_r2: 0.9966\n",
      "Epoch 1535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0302e-04 - my_r2: 0.8999 - val_loss: 1.2627e-05 - val_my_r2: 0.9955\n",
      "Epoch 1536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3042e-04 - my_r2: 0.9321 - val_loss: 1.6016e-05 - val_my_r2: 0.9940\n",
      "Epoch 1537/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6929e-04 - my_r2: 0.9400 - val_loss: 1.0304e-05 - val_my_r2: 0.9964\n",
      "Epoch 1538/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1138e-04 - my_r2: 0.9257 - val_loss: 1.3010e-05 - val_my_r2: 0.9955\n",
      "Epoch 1539/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5139e-04 - my_r2: 0.9145 - val_loss: 1.2085e-05 - val_my_r2: 0.9960\n",
      "Epoch 1540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7850e-04 - my_r2: 0.8600 - val_loss: 8.7770e-06 - val_my_r2: 0.9970\n",
      "Epoch 1541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8058e-04 - my_r2: 0.9036 - val_loss: 7.4569e-06 - val_my_r2: 0.9976\n",
      "Epoch 1542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4242e-04 - my_r2: 0.9462 - val_loss: 1.0182e-05 - val_my_r2: 0.9967\n",
      "Epoch 1543/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2807e-04 - my_r2: 0.9167 - val_loss: 1.0025e-05 - val_my_r2: 0.9966\n",
      "Epoch 1544/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6800e-04 - my_r2: 0.9206 - val_loss: 1.0432e-05 - val_my_r2: 0.9967\n",
      "Epoch 1545/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8787e-04 - my_r2: 0.9352 - val_loss: 1.1225e-05 - val_my_r2: 0.9961\n",
      "Epoch 1546/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1675e-04 - my_r2: 0.9085 - val_loss: 1.0290e-05 - val_my_r2: 0.9966\n",
      "Epoch 1547/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8270e-04 - my_r2: 0.9308 - val_loss: 1.0817e-05 - val_my_r2: 0.9964\n",
      "Epoch 1548/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9362e-04 - my_r2: 0.9393 - val_loss: 9.7335e-06 - val_my_r2: 0.9963\n",
      "Epoch 1549/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1893e-04 - my_r2: 0.9596 - val_loss: 1.2459e-05 - val_my_r2: 0.9950\n",
      "Epoch 1550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2586e-04 - my_r2: 0.9247 - val_loss: 9.5313e-06 - val_my_r2: 0.9962\n",
      "Epoch 1551/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3402e-04 - my_r2: 0.9319 - val_loss: 1.3432e-05 - val_my_r2: 0.9954\n",
      "Epoch 1552/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.6497e-04 - my_r2: 0.9091 - val_loss: 1.2058e-05 - val_my_r2: 0.9961\n",
      "Epoch 1553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3960e-04 - my_r2: 0.9077 - val_loss: 1.0820e-05 - val_my_r2: 0.9962\n",
      "Epoch 1554/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7854e-04 - my_r2: 0.9193 - val_loss: 1.0318e-05 - val_my_r2: 0.9967\n",
      "Epoch 1555/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3003e-04 - my_r2: 0.9234 - val_loss: 1.7210e-05 - val_my_r2: 0.9945\n",
      "Epoch 1556/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4697e-04 - my_r2: 0.9403 - val_loss: 1.3014e-05 - val_my_r2: 0.9955\n",
      "Epoch 1557/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6334e-04 - my_r2: 0.9309 - val_loss: 1.1712e-05 - val_my_r2: 0.9956\n",
      "Epoch 1558/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8525e-04 - my_r2: 0.9062 - val_loss: 1.0927e-05 - val_my_r2: 0.9960\n",
      "Epoch 1559/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1068e-04 - my_r2: 0.9124 - val_loss: 1.2543e-05 - val_my_r2: 0.9952\n",
      "Epoch 1560/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4388e-04 - my_r2: 0.9268 - val_loss: 1.4498e-05 - val_my_r2: 0.9943\n",
      "Epoch 1561/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2884e-04 - my_r2: 0.9385 - val_loss: 1.4024e-05 - val_my_r2: 0.9947\n",
      "Epoch 1562/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4774e-04 - my_r2: 0.9359 - val_loss: 1.0250e-05 - val_my_r2: 0.9964\n",
      "Epoch 1563/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5504e-04 - my_r2: 0.9169 - val_loss: 6.7455e-06 - val_my_r2: 0.9977\n",
      "Epoch 1564/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8282e-04 - my_r2: 0.9474 - val_loss: 8.1949e-06 - val_my_r2: 0.9969\n",
      "Epoch 1565/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3045e-04 - my_r2: 0.9420 - val_loss: 9.6170e-06 - val_my_r2: 0.9962\n",
      "Epoch 1566/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6786e-04 - my_r2: 0.8639 - val_loss: 1.0298e-05 - val_my_r2: 0.9956\n",
      "Epoch 1567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8462e-04 - my_r2: 0.9296 - val_loss: 1.3087e-05 - val_my_r2: 0.9939\n",
      "Epoch 1568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2865e-04 - my_r2: 0.9230 - val_loss: 1.6389e-05 - val_my_r2: 0.9930\n",
      "Epoch 1569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4296e-04 - my_r2: 0.9254 - val_loss: 1.2741e-05 - val_my_r2: 0.9954\n",
      "Epoch 1570/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3427e-04 - my_r2: 0.8914 - val_loss: 1.4990e-05 - val_my_r2: 0.9947\n",
      "Epoch 1571/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1038e-04 - my_r2: 0.9455 - val_loss: 1.5741e-05 - val_my_r2: 0.9946\n",
      "Epoch 1572/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1722e-04 - my_r2: 0.9008 - val_loss: 1.6511e-05 - val_my_r2: 0.9945\n",
      "Epoch 1573/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9824e-04 - my_r2: 0.9353 - val_loss: 1.5110e-05 - val_my_r2: 0.9953\n",
      "Epoch 1574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8482e-04 - my_r2: 0.9241 - val_loss: 1.4331e-05 - val_my_r2: 0.9952\n",
      "Epoch 1575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9208e-04 - my_r2: 0.8987 - val_loss: 1.4893e-05 - val_my_r2: 0.9947\n",
      "Epoch 1576/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0745e-04 - my_r2: 0.9144 - val_loss: 1.3995e-05 - val_my_r2: 0.9950\n",
      "Epoch 1577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4660e-04 - my_r2: 0.9383 - val_loss: 1.4332e-05 - val_my_r2: 0.9952\n",
      "Epoch 1578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0533e-04 - my_r2: 0.9139 - val_loss: 1.1093e-05 - val_my_r2: 0.9960\n",
      "Epoch 1579/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5722e-04 - my_r2: 0.9441 - val_loss: 9.8040e-06 - val_my_r2: 0.9964\n",
      "Epoch 1580/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4282e-04 - my_r2: 0.9361 - val_loss: 1.1378e-05 - val_my_r2: 0.9961\n",
      "Epoch 1581/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5085e-04 - my_r2: 0.9300 - val_loss: 1.1785e-05 - val_my_r2: 0.9960\n",
      "Epoch 1582/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8736e-04 - my_r2: 0.9277 - val_loss: 1.4985e-05 - val_my_r2: 0.9955\n",
      "Epoch 1583/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1268e-04 - my_r2: 0.9303 - val_loss: 1.3905e-05 - val_my_r2: 0.9959\n",
      "Epoch 1584/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2330e-04 - my_r2: 0.9392 - val_loss: 1.1005e-05 - val_my_r2: 0.9967\n",
      "Epoch 1585/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8236e-04 - my_r2: 0.9184 - val_loss: 1.0371e-05 - val_my_r2: 0.9969\n",
      "Epoch 1586/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2328e-04 - my_r2: 0.9327 - val_loss: 1.0139e-05 - val_my_r2: 0.9969\n",
      "Epoch 1587/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1657e-04 - my_r2: 0.9371 - val_loss: 1.0145e-05 - val_my_r2: 0.9967\n",
      "Epoch 1588/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.8199e-04 - my_r2: 0.8081 - val_loss: 9.8838e-06 - val_my_r2: 0.9968\n",
      "Epoch 1589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3040e-04 - my_r2: 0.9200 - val_loss: 1.2099e-05 - val_my_r2: 0.9963\n",
      "Epoch 1590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9285e-04 - my_r2: 0.8800 - val_loss: 1.3055e-05 - val_my_r2: 0.9960\n",
      "Epoch 1591/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5587e-04 - my_r2: 0.9265 - val_loss: 1.2057e-05 - val_my_r2: 0.9960\n",
      "Epoch 1592/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0884e-04 - my_r2: 0.9517 - val_loss: 1.4388e-05 - val_my_r2: 0.9952\n",
      "Epoch 1593/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4819e-04 - my_r2: 0.9267 - val_loss: 1.2832e-05 - val_my_r2: 0.9959\n",
      "Epoch 1594/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4781e-04 - my_r2: 0.9303 - val_loss: 1.3362e-05 - val_my_r2: 0.9960\n",
      "Epoch 1595/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1266e-04 - my_r2: 0.8343 - val_loss: 1.5087e-05 - val_my_r2: 0.9957\n",
      "Epoch 1596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3951e-04 - my_r2: 0.9466 - val_loss: 1.8252e-05 - val_my_r2: 0.9946\n",
      "Epoch 1597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4556e-04 - my_r2: 0.9366 - val_loss: 1.5715e-05 - val_my_r2: 0.9950\n",
      "Epoch 1598/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7198e-04 - my_r2: 0.9053 - val_loss: 1.1566e-05 - val_my_r2: 0.9957\n",
      "Epoch 1599/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4387e-04 - my_r2: 0.9319 - val_loss: 1.3073e-05 - val_my_r2: 0.9961\n",
      "Epoch 1600/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2095e-04 - my_r2: 0.9559 - val_loss: 1.2684e-05 - val_my_r2: 0.9960\n",
      "Epoch 1601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0911e-04 - my_r2: 0.9391 - val_loss: 1.3406e-05 - val_my_r2: 0.9955\n",
      "Epoch 1602/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8659e-04 - my_r2: 0.9073 - val_loss: 9.3164e-06 - val_my_r2: 0.9967\n",
      "Epoch 1603/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8585e-04 - my_r2: 0.9234 - val_loss: 7.9090e-06 - val_my_r2: 0.9973\n",
      "Epoch 1604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1928e-04 - my_r2: 0.8907 - val_loss: 1.0271e-05 - val_my_r2: 0.9961\n",
      "Epoch 1605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7344e-04 - my_r2: 0.9269 - val_loss: 2.4013e-05 - val_my_r2: 0.9903\n",
      "Epoch 1606/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5542e-04 - my_r2: 0.9293 - val_loss: 1.3799e-05 - val_my_r2: 0.9945\n",
      "Epoch 1607/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4508e-04 - my_r2: 0.9354 - val_loss: 8.3531e-06 - val_my_r2: 0.9973\n",
      "Epoch 1608/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2442e-04 - my_r2: 0.8951 - val_loss: 8.8512e-06 - val_my_r2: 0.9973\n",
      "Epoch 1609/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5536e-04 - my_r2: 0.9233 - val_loss: 1.4502e-05 - val_my_r2: 0.9940\n",
      "Epoch 1610/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1679e-04 - my_r2: 0.9287 - val_loss: 1.1620e-05 - val_my_r2: 0.9957\n",
      "Epoch 1611/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3753e-04 - my_r2: 0.9308 - val_loss: 1.3431e-05 - val_my_r2: 0.9951\n",
      "Epoch 1612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8837e-04 - my_r2: 0.9486 - val_loss: 8.6200e-06 - val_my_r2: 0.9970\n",
      "Epoch 1613/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6577e-04 - my_r2: 0.9108 - val_loss: 1.0873e-05 - val_my_r2: 0.9963\n",
      "Epoch 1614/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0571e-04 - my_r2: 0.9573 - val_loss: 1.4825e-05 - val_my_r2: 0.9944\n",
      "Epoch 1615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8497e-04 - my_r2: 0.9226 - val_loss: 1.2179e-05 - val_my_r2: 0.9951\n",
      "Epoch 1616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3482e-04 - my_r2: 0.9267 - val_loss: 1.1416e-05 - val_my_r2: 0.9955\n",
      "Epoch 1617/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5173e-04 - my_r2: 0.9307 - val_loss: 1.2153e-05 - val_my_r2: 0.9949\n",
      "Epoch 1618/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7802e-04 - my_r2: 0.9329 - val_loss: 1.0327e-05 - val_my_r2: 0.9961\n",
      "Epoch 1619/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8919e-04 - my_r2: 0.9375 - val_loss: 1.0413e-05 - val_my_r2: 0.9964\n",
      "Epoch 1620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0503e-04 - my_r2: 0.9527 - val_loss: 2.4854e-05 - val_my_r2: 0.9910\n",
      "Epoch 1621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3325e-04 - my_r2: 0.9273 - val_loss: 1.2220e-05 - val_my_r2: 0.9952\n",
      "Epoch 1622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1563e-04 - my_r2: 0.9194 - val_loss: 1.1059e-05 - val_my_r2: 0.9957\n",
      "Epoch 1623/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8473e-04 - my_r2: 0.9270 - val_loss: 2.2996e-05 - val_my_r2: 0.9914\n",
      "Epoch 1624/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3559e-04 - my_r2: 0.9467 - val_loss: 1.0645e-05 - val_my_r2: 0.9959\n",
      "Epoch 1625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6607e-04 - my_r2: 0.9403 - val_loss: 1.1124e-05 - val_my_r2: 0.9961\n",
      "Epoch 1626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2141e-04 - my_r2: 0.9447 - val_loss: 1.3063e-05 - val_my_r2: 0.9960\n",
      "Epoch 1627/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8507e-04 - my_r2: 0.9215 - val_loss: 1.2565e-05 - val_my_r2: 0.9956\n",
      "Epoch 1628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3524e-04 - my_r2: 0.9343 - val_loss: 8.2490e-06 - val_my_r2: 0.9966\n",
      "Epoch 1629/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4880e-04 - my_r2: 0.9123 - val_loss: 9.8854e-06 - val_my_r2: 0.9960\n",
      "Epoch 1630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0200e-04 - my_r2: 0.9387 - val_loss: 1.2031e-05 - val_my_r2: 0.9953\n",
      "Epoch 1631/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5953e-04 - my_r2: 0.9256 - val_loss: 1.1076e-05 - val_my_r2: 0.9961\n",
      "Epoch 1632/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3340e-04 - my_r2: 0.9347 - val_loss: 1.1230e-05 - val_my_r2: 0.9963\n",
      "Epoch 1633/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6002e-04 - my_r2: 0.8767 - val_loss: 1.1996e-05 - val_my_r2: 0.9961\n",
      "Epoch 1634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3517e-04 - my_r2: 0.8740 - val_loss: 1.5351e-05 - val_my_r2: 0.9950\n",
      "Epoch 1635/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5811e-04 - my_r2: 0.9233 - val_loss: 1.0967e-05 - val_my_r2: 0.9963\n",
      "Epoch 1636/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5977e-04 - my_r2: 0.9063 - val_loss: 1.0086e-05 - val_my_r2: 0.9968\n",
      "Epoch 1637/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7677e-04 - my_r2: 0.9182 - val_loss: 1.4806e-05 - val_my_r2: 0.9947\n",
      "Epoch 1638/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8460e-04 - my_r2: 0.9411 - val_loss: 1.5327e-05 - val_my_r2: 0.9945\n",
      "Epoch 1639/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7178e-04 - my_r2: 0.9259 - val_loss: 1.5791e-05 - val_my_r2: 0.9940\n",
      "Epoch 1640/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8046e-04 - my_r2: 0.9318 - val_loss: 1.4875e-05 - val_my_r2: 0.9943\n",
      "Epoch 1641/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4733e-04 - my_r2: 0.9488 - val_loss: 2.0489e-05 - val_my_r2: 0.9928\n",
      "Epoch 1642/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2726e-04 - my_r2: 0.8941 - val_loss: 7.4814e-06 - val_my_r2: 0.9973\n",
      "Epoch 1643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1124e-04 - my_r2: 0.9285 - val_loss: 7.3718e-06 - val_my_r2: 0.9971\n",
      "Epoch 1644/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4123e-04 - my_r2: 0.9361 - val_loss: 7.9462e-06 - val_my_r2: 0.9968\n",
      "Epoch 1645/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3282e-04 - my_r2: 0.9378 - val_loss: 1.1323e-05 - val_my_r2: 0.9956\n",
      "Epoch 1646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1856e-04 - my_r2: 0.9420 - val_loss: 1.1043e-05 - val_my_r2: 0.9955\n",
      "Epoch 1647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4626e-04 - my_r2: 0.9352 - val_loss: 1.4864e-05 - val_my_r2: 0.9939\n",
      "Epoch 1648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3344e-04 - my_r2: 0.9448 - val_loss: 1.1909e-05 - val_my_r2: 0.9950\n",
      "Epoch 1649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6838e-04 - my_r2: 0.9249 - val_loss: 1.2701e-05 - val_my_r2: 0.9944\n",
      "Epoch 1650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7205e-04 - my_r2: 0.9130 - val_loss: 1.1344e-05 - val_my_r2: 0.9952\n",
      "Epoch 1651/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4918e-04 - my_r2: 0.8521 - val_loss: 8.6671e-06 - val_my_r2: 0.9969\n",
      "Epoch 1652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6166e-04 - my_r2: 0.9135 - val_loss: 6.1519e-06 - val_my_r2: 0.9978\n",
      "Epoch 1653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1664e-04 - my_r2: 0.9481 - val_loss: 7.2508e-06 - val_my_r2: 0.9975\n",
      "Epoch 1654/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9570e-04 - my_r2: 0.9502 - val_loss: 8.2936e-06 - val_my_r2: 0.9974\n",
      "Epoch 1655/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5954e-04 - my_r2: 0.8810 - val_loss: 1.1634e-05 - val_my_r2: 0.9957\n",
      "Epoch 1656/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2687e-04 - my_r2: 0.9038 - val_loss: 8.0037e-06 - val_my_r2: 0.9975\n",
      "Epoch 1657/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5873e-04 - my_r2: 0.9382 - val_loss: 8.1176e-06 - val_my_r2: 0.9975\n",
      "Epoch 1658/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3608e-04 - my_r2: 0.9362 - val_loss: 8.9919e-06 - val_my_r2: 0.9972\n",
      "Epoch 1659/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7450e-04 - my_r2: 0.9351 - val_loss: 1.1352e-05 - val_my_r2: 0.9962\n",
      "Epoch 1660/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4064e-04 - my_r2: 0.9135 - val_loss: 1.1691e-05 - val_my_r2: 0.9960\n",
      "Epoch 1661/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0221e-04 - my_r2: 0.9433 - val_loss: 1.1917e-05 - val_my_r2: 0.9955\n",
      "Epoch 1662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1399e-04 - my_r2: 0.9565 - val_loss: 7.9462e-06 - val_my_r2: 0.9972\n",
      "Epoch 1663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1986e-04 - my_r2: 0.9061 - val_loss: 9.7778e-06 - val_my_r2: 0.9968\n",
      "Epoch 1664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1185e-04 - my_r2: 0.9324 - val_loss: 9.6662e-06 - val_my_r2: 0.9968\n",
      "Epoch 1665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4347e-04 - my_r2: 0.9353 - val_loss: 1.0397e-05 - val_my_r2: 0.9961\n",
      "Epoch 1666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3564e-04 - my_r2: 0.8735 - val_loss: 1.3414e-05 - val_my_r2: 0.9952\n",
      "Epoch 1667/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1800e-04 - my_r2: 0.9473 - val_loss: 1.5231e-05 - val_my_r2: 0.9950\n",
      "Epoch 1668/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2237e-04 - my_r2: 0.8897 - val_loss: 1.4867e-05 - val_my_r2: 0.9955\n",
      "Epoch 1669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5785e-04 - my_r2: 0.9361 - val_loss: 9.2491e-06 - val_my_r2: 0.9970\n",
      "Epoch 1670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5553e-04 - my_r2: 0.9278 - val_loss: 8.1470e-06 - val_my_r2: 0.9975\n",
      "Epoch 1671/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2580e-04 - my_r2: 0.9575 - val_loss: 1.1825e-05 - val_my_r2: 0.9960\n",
      "Epoch 1672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7631e-04 - my_r2: 0.9424 - val_loss: 7.7561e-06 - val_my_r2: 0.9977\n",
      "Epoch 1673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5283e-04 - my_r2: 0.9379 - val_loss: 7.3527e-06 - val_my_r2: 0.9979\n",
      "Epoch 1674/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1091e-04 - my_r2: 0.9210 - val_loss: 9.2113e-06 - val_my_r2: 0.9970\n",
      "Epoch 1675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8061e-04 - my_r2: 0.9134 - val_loss: 1.0709e-05 - val_my_r2: 0.9964\n",
      "Epoch 1676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3059e-04 - my_r2: 0.9337 - val_loss: 1.0722e-05 - val_my_r2: 0.9959\n",
      "Epoch 1677/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6455e-04 - my_r2: 0.9266 - val_loss: 1.1858e-05 - val_my_r2: 0.9955\n",
      "Epoch 1678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2852e-04 - my_r2: 0.9455 - val_loss: 1.1682e-05 - val_my_r2: 0.9956\n",
      "Epoch 1679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6370e-04 - my_r2: 0.9329 - val_loss: 9.3317e-06 - val_my_r2: 0.9966\n",
      "Epoch 1680/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5453e-04 - my_r2: 0.9400 - val_loss: 8.8627e-06 - val_my_r2: 0.9968\n",
      "Epoch 1681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8398e-04 - my_r2: 0.9163 - val_loss: 7.9820e-06 - val_my_r2: 0.9970\n",
      "Epoch 1682/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6988e-04 - my_r2: 0.8943 - val_loss: 8.9435e-06 - val_my_r2: 0.9968\n",
      "Epoch 1683/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4171e-04 - my_r2: 0.9451 - val_loss: 8.8087e-06 - val_my_r2: 0.9971\n",
      "Epoch 1684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3736e-04 - my_r2: 0.9485 - val_loss: 7.8879e-06 - val_my_r2: 0.9974\n",
      "Epoch 1685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3278e-04 - my_r2: 0.9166 - val_loss: 8.1886e-06 - val_my_r2: 0.9974\n",
      "Epoch 1686/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9330e-04 - my_r2: 0.8937 - val_loss: 7.4900e-06 - val_my_r2: 0.9977\n",
      "Epoch 1687/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6771e-04 - my_r2: 0.9048 - val_loss: 8.0501e-06 - val_my_r2: 0.9972\n",
      "Epoch 1688/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2178e-04 - my_r2: 0.9507 - val_loss: 1.0263e-05 - val_my_r2: 0.9961\n",
      "Epoch 1689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5455e-04 - my_r2: 0.9458 - val_loss: 1.3512e-05 - val_my_r2: 0.9956\n",
      "Epoch 1690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4168e-04 - my_r2: 0.9065 - val_loss: 1.2054e-05 - val_my_r2: 0.9961\n",
      "Epoch 1691/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6126e-04 - my_r2: 0.9287 - val_loss: 1.0546e-05 - val_my_r2: 0.9963\n",
      "Epoch 1692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1959e-04 - my_r2: 0.9533 - val_loss: 1.0743e-05 - val_my_r2: 0.9962\n",
      "Epoch 1693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7824e-04 - my_r2: 0.8876 - val_loss: 9.2782e-06 - val_my_r2: 0.9967\n",
      "Epoch 1694/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0915e-04 - my_r2: 0.9030 - val_loss: 1.0083e-05 - val_my_r2: 0.9961\n",
      "Epoch 1695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5763e-04 - my_r2: 0.9134 - val_loss: 8.2725e-06 - val_my_r2: 0.9967\n",
      "Epoch 1696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4529e-04 - my_r2: 0.9235 - val_loss: 8.2630e-06 - val_my_r2: 0.9971\n",
      "Epoch 1697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0545e-04 - my_r2: 0.9301 - val_loss: 9.1665e-06 - val_my_r2: 0.9971\n",
      "Epoch 1698/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0838e-04 - my_r2: 0.9451 - val_loss: 1.3851e-05 - val_my_r2: 0.9954\n",
      "Epoch 1699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9224e-04 - my_r2: 0.9404 - val_loss: 1.4830e-05 - val_my_r2: 0.9952\n",
      "Epoch 1700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1576e-04 - my_r2: 0.9479 - val_loss: 1.2257e-05 - val_my_r2: 0.9961\n",
      "Epoch 1701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3142e-04 - my_r2: 0.9316 - val_loss: 9.9906e-06 - val_my_r2: 0.9967\n",
      "Epoch 1702/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1447e-04 - my_r2: 0.9320 - val_loss: 1.4527e-05 - val_my_r2: 0.9953\n",
      "Epoch 1703/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6459e-04 - my_r2: 0.8985 - val_loss: 8.8696e-06 - val_my_r2: 0.9968\n",
      "Epoch 1704/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7581e-04 - my_r2: 0.8910 - val_loss: 7.9400e-06 - val_my_r2: 0.9971\n",
      "Epoch 1705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1385e-04 - my_r2: 0.9147 - val_loss: 8.8602e-06 - val_my_r2: 0.9967\n",
      "Epoch 1706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8231e-04 - my_r2: 0.9247 - val_loss: 7.5348e-06 - val_my_r2: 0.9972\n",
      "Epoch 1707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5007e-04 - my_r2: 0.9385 - val_loss: 7.0910e-06 - val_my_r2: 0.9975\n",
      "Epoch 1708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2357e-04 - my_r2: 0.9231 - val_loss: 7.9635e-06 - val_my_r2: 0.9968\n",
      "Epoch 1709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5881e-04 - my_r2: 0.9205 - val_loss: 9.8969e-06 - val_my_r2: 0.9966\n",
      "Epoch 1710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9090e-04 - my_r2: 0.9292 - val_loss: 1.1938e-05 - val_my_r2: 0.9956\n",
      "Epoch 1711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2327e-04 - my_r2: 0.9190 - val_loss: 1.0564e-05 - val_my_r2: 0.9965\n",
      "Epoch 1712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0553e-04 - my_r2: 0.9496 - val_loss: 7.8842e-06 - val_my_r2: 0.9976\n",
      "Epoch 1713/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0172e-04 - my_r2: 0.9373 - val_loss: 9.6963e-06 - val_my_r2: 0.9966\n",
      "Epoch 1714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0533e-04 - my_r2: 0.9526 - val_loss: 1.0377e-05 - val_my_r2: 0.9966\n",
      "Epoch 1715/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3052e-04 - my_r2: 0.9196 - val_loss: 1.6224e-05 - val_my_r2: 0.9941\n",
      "Epoch 1716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0728e-04 - my_r2: 0.9480 - val_loss: 1.4579e-05 - val_my_r2: 0.9944\n",
      "Epoch 1717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3825e-04 - my_r2: 0.8900 - val_loss: 1.3071e-05 - val_my_r2: 0.9948\n",
      "Epoch 1718/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1993e-04 - my_r2: 0.9371 - val_loss: 1.0874e-05 - val_my_r2: 0.9956\n",
      "Epoch 1719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8049e-04 - my_r2: 0.9250 - val_loss: 8.9564e-06 - val_my_r2: 0.9966\n",
      "Epoch 1720/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2100e-04 - my_r2: 0.9245 - val_loss: 8.1149e-06 - val_my_r2: 0.9971\n",
      "Epoch 1721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4077e-04 - my_r2: 0.9350 - val_loss: 1.1813e-05 - val_my_r2: 0.9964\n",
      "Epoch 1722/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7158e-04 - my_r2: 0.9220 - val_loss: 9.1924e-06 - val_my_r2: 0.9971\n",
      "Epoch 1723/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2529e-04 - my_r2: 0.9028 - val_loss: 7.1489e-06 - val_my_r2: 0.9976\n",
      "Epoch 1724/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1194e-04 - my_r2: 0.9307 - val_loss: 1.4160e-05 - val_my_r2: 0.9948\n",
      "Epoch 1725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1885e-04 - my_r2: 0.9061 - val_loss: 1.4296e-05 - val_my_r2: 0.9946\n",
      "Epoch 1726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3552e-04 - my_r2: 0.9361 - val_loss: 8.6481e-06 - val_my_r2: 0.9973\n",
      "Epoch 1727/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3839e-04 - my_r2: 0.8983 - val_loss: 1.1012e-05 - val_my_r2: 0.9967\n",
      "Epoch 1728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0011e-04 - my_r2: 0.9376 - val_loss: 9.6789e-06 - val_my_r2: 0.9968\n",
      "Epoch 1729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0675e-04 - my_r2: 0.9027 - val_loss: 8.8162e-06 - val_my_r2: 0.9971\n",
      "Epoch 1730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1556e-04 - my_r2: 0.9470 - val_loss: 1.0282e-05 - val_my_r2: 0.9969\n",
      "Epoch 1731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5318e-04 - my_r2: 0.9065 - val_loss: 8.9621e-06 - val_my_r2: 0.9974\n",
      "Epoch 1732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5741e-04 - my_r2: 0.9373 - val_loss: 1.0894e-05 - val_my_r2: 0.9964\n",
      "Epoch 1733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3724e-04 - my_r2: 0.9341 - val_loss: 9.9609e-06 - val_my_r2: 0.9967\n",
      "Epoch 1734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4312e-04 - my_r2: 0.9430 - val_loss: 8.4823e-06 - val_my_r2: 0.9973\n",
      "Epoch 1735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4481e-04 - my_r2: 0.9409 - val_loss: 1.0009e-05 - val_my_r2: 0.9967\n",
      "Epoch 1736/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4621e-04 - my_r2: 0.9192 - val_loss: 1.0167e-05 - val_my_r2: 0.9968\n",
      "Epoch 1737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6084e-04 - my_r2: 0.9311 - val_loss: 1.2207e-05 - val_my_r2: 0.9964\n",
      "Epoch 1738/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4492e-04 - my_r2: 0.9259 - val_loss: 1.1368e-05 - val_my_r2: 0.9968\n",
      "Epoch 1739/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9245e-04 - my_r2: 0.9297 - val_loss: 1.0015e-05 - val_my_r2: 0.9971\n",
      "Epoch 1740/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1702e-04 - my_r2: 0.9582 - val_loss: 1.3158e-05 - val_my_r2: 0.9956\n",
      "Epoch 1741/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2479e-04 - my_r2: 0.7429 - val_loss: 1.1121e-05 - val_my_r2: 0.9969\n",
      "Epoch 1742/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2546e-04 - my_r2: 0.9162 - val_loss: 9.6597e-06 - val_my_r2: 0.9972\n",
      "Epoch 1743/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0496e-04 - my_r2: 0.9197 - val_loss: 1.7822e-05 - val_my_r2: 0.9936\n",
      "Epoch 1744/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1994e-04 - my_r2: 0.9490 - val_loss: 1.2911e-05 - val_my_r2: 0.9957\n",
      "Epoch 1745/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1659e-04 - my_r2: 0.8338 - val_loss: 1.1216e-05 - val_my_r2: 0.9964\n",
      "Epoch 1746/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5100e-04 - my_r2: 0.9158 - val_loss: 9.8907e-06 - val_my_r2: 0.9971\n",
      "Epoch 1747/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6724e-04 - my_r2: 0.9411 - val_loss: 8.5914e-06 - val_my_r2: 0.9974\n",
      "Epoch 1748/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7096e-04 - my_r2: 0.8942 - val_loss: 8.2406e-06 - val_my_r2: 0.9975\n",
      "Epoch 1749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0958e-04 - my_r2: 0.9346 - val_loss: 9.0429e-06 - val_my_r2: 0.9971\n",
      "Epoch 1750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4410e-04 - my_r2: 0.9327 - val_loss: 1.2198e-05 - val_my_r2: 0.9956\n",
      "Epoch 1751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2620e-04 - my_r2: 0.9533 - val_loss: 1.1392e-05 - val_my_r2: 0.9960\n",
      "Epoch 1752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2518e-04 - my_r2: 0.9506 - val_loss: 9.5672e-06 - val_my_r2: 0.9967\n",
      "Epoch 1753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3123e-04 - my_r2: 0.9497 - val_loss: 9.4945e-06 - val_my_r2: 0.9965\n",
      "Epoch 1754/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8876e-04 - my_r2: 0.9447 - val_loss: 9.2383e-06 - val_my_r2: 0.9963\n",
      "Epoch 1755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6913e-04 - my_r2: 0.9085 - val_loss: 8.0268e-06 - val_my_r2: 0.9969\n",
      "Epoch 1756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4944e-04 - my_r2: 0.8620 - val_loss: 8.8385e-06 - val_my_r2: 0.9967\n",
      "Epoch 1757/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8704e-04 - my_r2: 0.9010 - val_loss: 9.8263e-06 - val_my_r2: 0.9962\n",
      "Epoch 1758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1286e-04 - my_r2: 0.9535 - val_loss: 1.2599e-05 - val_my_r2: 0.9950\n",
      "Epoch 1759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2752e-04 - my_r2: 0.9547 - val_loss: 1.0540e-05 - val_my_r2: 0.9958\n",
      "Epoch 1760/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8163e-04 - my_r2: 0.9474 - val_loss: 9.9854e-06 - val_my_r2: 0.9960\n",
      "Epoch 1761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2986e-04 - my_r2: 0.9299 - val_loss: 9.4165e-06 - val_my_r2: 0.9962\n",
      "Epoch 1762/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4930e-04 - my_r2: 0.8299 - val_loss: 1.0208e-05 - val_my_r2: 0.9959\n",
      "Epoch 1763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8539e-04 - my_r2: 0.9281 - val_loss: 1.1100e-05 - val_my_r2: 0.9958\n",
      "Epoch 1764/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8523e-04 - my_r2: 0.8616 - val_loss: 1.1217e-05 - val_my_r2: 0.9961\n",
      "Epoch 1765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3145e-04 - my_r2: 0.9181 - val_loss: 1.4340e-05 - val_my_r2: 0.9946\n",
      "Epoch 1766/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4276e-04 - my_r2: 0.9192 - val_loss: 1.2398e-05 - val_my_r2: 0.9955\n",
      "Epoch 1767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4556e-04 - my_r2: 0.9247 - val_loss: 1.1472e-05 - val_my_r2: 0.9959\n",
      "Epoch 1768/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0163e-04 - my_r2: 0.9288 - val_loss: 1.0608e-05 - val_my_r2: 0.9960\n",
      "Epoch 1769/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.6385e-04 - my_r2: 0.9272 - val_loss: 8.0022e-06 - val_my_r2: 0.9971\n",
      "Epoch 1770/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2678e-04 - my_r2: 0.9350 - val_loss: 8.3867e-06 - val_my_r2: 0.9970\n",
      "Epoch 1771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3335e-04 - my_r2: 0.9194 - val_loss: 9.6180e-06 - val_my_r2: 0.9968\n",
      "Epoch 1772/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5248e-04 - my_r2: 0.9497 - val_loss: 8.8018e-06 - val_my_r2: 0.9968\n",
      "Epoch 1773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6168e-04 - my_r2: 0.9284 - val_loss: 1.0668e-05 - val_my_r2: 0.9960\n",
      "Epoch 1774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5098e-04 - my_r2: 0.9404 - val_loss: 1.0350e-05 - val_my_r2: 0.9961\n",
      "Epoch 1775/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8388e-04 - my_r2: 0.9105 - val_loss: 1.0449e-05 - val_my_r2: 0.9966\n",
      "Epoch 1776/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7004e-04 - my_r2: 0.9417 - val_loss: 7.8978e-06 - val_my_r2: 0.9975\n",
      "Epoch 1777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5058e-04 - my_r2: 0.9322 - val_loss: 9.3466e-06 - val_my_r2: 0.9971\n",
      "Epoch 1778/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0536e-04 - my_r2: 0.9219 - val_loss: 9.5164e-06 - val_my_r2: 0.9972\n",
      "Epoch 1779/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3996e-04 - my_r2: 0.9335 - val_loss: 1.1668e-05 - val_my_r2: 0.9963\n",
      "Epoch 1780/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0371e-04 - my_r2: 0.9570 - val_loss: 1.1774e-05 - val_my_r2: 0.9966\n",
      "Epoch 1781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3961e-04 - my_r2: 0.9413 - val_loss: 1.2715e-05 - val_my_r2: 0.9963\n",
      "Epoch 1782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7973e-04 - my_r2: 0.8571 - val_loss: 1.4308e-05 - val_my_r2: 0.9957\n",
      "Epoch 1783/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0456e-04 - my_r2: 0.9336 - val_loss: 1.2690e-05 - val_my_r2: 0.9961\n",
      "Epoch 1784/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0291e-04 - my_r2: 0.9329 - val_loss: 1.4495e-05 - val_my_r2: 0.9955\n",
      "Epoch 1785/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4904e-04 - my_r2: 0.9381 - val_loss: 1.3876e-05 - val_my_r2: 0.9957\n",
      "Epoch 1786/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6998e-04 - my_r2: 0.9194 - val_loss: 1.0451e-05 - val_my_r2: 0.9968\n",
      "Epoch 1787/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7547e-04 - my_r2: 0.9108 - val_loss: 1.2426e-05 - val_my_r2: 0.9963\n",
      "Epoch 1788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4632e-04 - my_r2: 0.9404 - val_loss: 1.3105e-05 - val_my_r2: 0.9954\n",
      "Epoch 1789/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7932e-04 - my_r2: 0.9613 - val_loss: 1.3188e-05 - val_my_r2: 0.9953\n",
      "Epoch 1790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0587e-04 - my_r2: 0.9477 - val_loss: 1.2394e-05 - val_my_r2: 0.9958\n",
      "Epoch 1791/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3129e-04 - my_r2: 0.9320 - val_loss: 1.2370e-05 - val_my_r2: 0.9959\n",
      "Epoch 1792/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4482e-04 - my_r2: 0.9363 - val_loss: 9.2788e-06 - val_my_r2: 0.9967\n",
      "Epoch 1793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0636e-04 - my_r2: 0.9340 - val_loss: 8.6684e-06 - val_my_r2: 0.9972\n",
      "Epoch 1794/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4815e-04 - my_r2: 0.9168 - val_loss: 1.0075e-05 - val_my_r2: 0.9965\n",
      "Epoch 1795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9753e-04 - my_r2: 0.9250 - val_loss: 9.5685e-06 - val_my_r2: 0.9966\n",
      "Epoch 1796/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8587e-04 - my_r2: 0.8595 - val_loss: 8.8187e-06 - val_my_r2: 0.9969\n",
      "Epoch 1797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6591e-04 - my_r2: 0.9282 - val_loss: 9.6624e-06 - val_my_r2: 0.9969\n",
      "Epoch 1798/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2157e-04 - my_r2: 0.9534 - val_loss: 1.1805e-05 - val_my_r2: 0.9963\n",
      "Epoch 1799/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4287e-04 - my_r2: 0.8994 - val_loss: 1.1597e-05 - val_my_r2: 0.9963\n",
      "Epoch 1800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1075e-04 - my_r2: 0.9170 - val_loss: 1.2759e-05 - val_my_r2: 0.9956\n",
      "Epoch 1801/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1311e-04 - my_r2: 0.9550 - val_loss: 1.3604e-05 - val_my_r2: 0.9957\n",
      "Epoch 1802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6904e-04 - my_r2: 0.9162 - val_loss: 1.4469e-05 - val_my_r2: 0.9956\n",
      "Epoch 1803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6142e-04 - my_r2: 0.9343 - val_loss: 1.1785e-05 - val_my_r2: 0.9964\n",
      "Epoch 1804/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3383e-04 - my_r2: 0.9130 - val_loss: 9.4847e-06 - val_my_r2: 0.9971\n",
      "Epoch 1805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5074e-04 - my_r2: 0.9101 - val_loss: 1.0656e-05 - val_my_r2: 0.9968\n",
      "Epoch 1806/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5481e-04 - my_r2: 0.8781 - val_loss: 1.1131e-05 - val_my_r2: 0.9967\n",
      "Epoch 1807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3501e-04 - my_r2: 0.9404 - val_loss: 9.9787e-06 - val_my_r2: 0.9970\n",
      "Epoch 1808/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0376e-04 - my_r2: 0.9398 - val_loss: 1.0468e-05 - val_my_r2: 0.9968\n",
      "Epoch 1809/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4669e-04 - my_r2: 0.9357 - val_loss: 1.0611e-05 - val_my_r2: 0.9965\n",
      "Epoch 1810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1426e-04 - my_r2: 0.9101 - val_loss: 9.0550e-06 - val_my_r2: 0.9969\n",
      "Epoch 1811/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3842e-04 - my_r2: 0.9362 - val_loss: 7.6865e-06 - val_my_r2: 0.9973\n",
      "Epoch 1812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0274e-04 - my_r2: 0.9221 - val_loss: 8.8325e-06 - val_my_r2: 0.9968\n",
      "Epoch 1813/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5875e-04 - my_r2: 0.9217 - val_loss: 1.1257e-05 - val_my_r2: 0.9960\n",
      "Epoch 1814/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2415e-04 - my_r2: 0.9240 - val_loss: 7.8374e-06 - val_my_r2: 0.9973\n",
      "Epoch 1815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9781e-04 - my_r2: 0.9247 - val_loss: 1.0073e-05 - val_my_r2: 0.9964\n",
      "Epoch 1816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4067e-04 - my_r2: 0.9300 - val_loss: 6.9687e-06 - val_my_r2: 0.9978\n",
      "Epoch 1817/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2850e-04 - my_r2: 0.9320 - val_loss: 6.6548e-06 - val_my_r2: 0.9979\n",
      "Epoch 1818/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9595e-04 - my_r2: 0.9580 - val_loss: 8.3213e-06 - val_my_r2: 0.9975\n",
      "Epoch 1819/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5914e-04 - my_r2: 0.9264 - val_loss: 7.0631e-06 - val_my_r2: 0.9978\n",
      "Epoch 1820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3733e-04 - my_r2: 0.9429 - val_loss: 9.6745e-06 - val_my_r2: 0.9969\n",
      "Epoch 1821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8441e-04 - my_r2: 0.9394 - val_loss: 9.5945e-06 - val_my_r2: 0.9968\n",
      "Epoch 1822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1447e-04 - my_r2: 0.9204 - val_loss: 7.9431e-06 - val_my_r2: 0.9972\n",
      "Epoch 1823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1876e-04 - my_r2: 0.9133 - val_loss: 8.4575e-06 - val_my_r2: 0.9972\n",
      "Epoch 1824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4006e-04 - my_r2: 0.9337 - val_loss: 7.2396e-06 - val_my_r2: 0.9979\n",
      "Epoch 1825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1555e-04 - my_r2: 0.9384 - val_loss: 8.0899e-06 - val_my_r2: 0.9977\n",
      "Epoch 1826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9782e-04 - my_r2: 0.9107 - val_loss: 1.0361e-05 - val_my_r2: 0.9967\n",
      "Epoch 1827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7138e-04 - my_r2: 0.9426 - val_loss: 8.3682e-06 - val_my_r2: 0.9974\n",
      "Epoch 1828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5660e-04 - my_r2: 0.9248 - val_loss: 1.0593e-05 - val_my_r2: 0.9963\n",
      "Epoch 1829/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0620e-04 - my_r2: 0.9499 - val_loss: 8.1499e-06 - val_my_r2: 0.9973\n",
      "Epoch 1830/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3499e-04 - my_r2: 0.9348 - val_loss: 6.5961e-06 - val_my_r2: 0.9981\n",
      "Epoch 1831/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5857e-04 - my_r2: 0.9328 - val_loss: 7.5101e-06 - val_my_r2: 0.9977\n",
      "Epoch 1832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6689e-04 - my_r2: 0.9186 - val_loss: 1.7954e-05 - val_my_r2: 0.9940\n",
      "Epoch 1833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5791e-04 - my_r2: 0.9427 - val_loss: 1.4469e-05 - val_my_r2: 0.9956\n",
      "Epoch 1834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1266e-04 - my_r2: 0.9359 - val_loss: 9.4547e-06 - val_my_r2: 0.9972\n",
      "Epoch 1835/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3298e-04 - my_r2: 0.9405 - val_loss: 8.6581e-06 - val_my_r2: 0.9972\n",
      "Epoch 1836/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4084e-04 - my_r2: 0.9326 - val_loss: 8.5832e-06 - val_my_r2: 0.9970\n",
      "Epoch 1837/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4791e-04 - my_r2: 0.9396 - val_loss: 9.7170e-06 - val_my_r2: 0.9965\n",
      "Epoch 1838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7732e-04 - my_r2: 0.9435 - val_loss: 7.6880e-06 - val_my_r2: 0.9972\n",
      "Epoch 1839/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3372e-04 - my_r2: 0.9243 - val_loss: 8.2184e-06 - val_my_r2: 0.9970\n",
      "Epoch 1840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3899e-04 - my_r2: 0.9507 - val_loss: 9.9624e-06 - val_my_r2: 0.9968\n",
      "Epoch 1841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2050e-04 - my_r2: 0.9302 - val_loss: 8.8296e-06 - val_my_r2: 0.9972\n",
      "Epoch 1842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4041e-04 - my_r2: 0.9251 - val_loss: 9.0337e-06 - val_my_r2: 0.9970\n",
      "Epoch 1843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0306e-04 - my_r2: 0.9530 - val_loss: 1.4114e-05 - val_my_r2: 0.9949\n",
      "Epoch 1844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7684e-04 - my_r2: 0.9188 - val_loss: 8.0825e-06 - val_my_r2: 0.9972\n",
      "Epoch 1845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2989e-04 - my_r2: 0.9135 - val_loss: 7.4514e-06 - val_my_r2: 0.9973\n",
      "Epoch 1846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6081e-04 - my_r2: 0.9328 - val_loss: 7.8051e-06 - val_my_r2: 0.9972\n",
      "Epoch 1847/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2819e-04 - my_r2: 0.9298 - val_loss: 8.6124e-06 - val_my_r2: 0.9969\n",
      "Epoch 1848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7971e-04 - my_r2: 0.9310 - val_loss: 1.0869e-05 - val_my_r2: 0.9963\n",
      "Epoch 1849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9836e-04 - my_r2: 0.9258 - val_loss: 9.1080e-06 - val_my_r2: 0.9970\n",
      "Epoch 1850/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5122e-04 - my_r2: 0.9348 - val_loss: 7.8840e-06 - val_my_r2: 0.9976\n",
      "Epoch 1851/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9315e-04 - my_r2: 0.9129 - val_loss: 9.9996e-06 - val_my_r2: 0.9969\n",
      "Epoch 1852/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4124e-04 - my_r2: 0.9235 - val_loss: 9.1318e-06 - val_my_r2: 0.9970\n",
      "Epoch 1853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6370e-04 - my_r2: 0.9406 - val_loss: 8.7519e-06 - val_my_r2: 0.9967\n",
      "Epoch 1854/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1981e-04 - my_r2: 0.9517 - val_loss: 8.8238e-06 - val_my_r2: 0.9967\n",
      "Epoch 1855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3201e-04 - my_r2: 0.9209 - val_loss: 8.1315e-06 - val_my_r2: 0.9967\n",
      "Epoch 1856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2296e-04 - my_r2: 0.9253 - val_loss: 8.1375e-06 - val_my_r2: 0.9969\n",
      "Epoch 1857/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3866e-04 - my_r2: 0.9492 - val_loss: 9.3382e-06 - val_my_r2: 0.9965\n",
      "Epoch 1858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7637e-04 - my_r2: 0.8483 - val_loss: 8.6833e-06 - val_my_r2: 0.9966\n",
      "Epoch 1859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4690e-04 - my_r2: 0.9047 - val_loss: 8.5827e-06 - val_my_r2: 0.9968\n",
      "Epoch 1860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9399e-04 - my_r2: 0.9237 - val_loss: 1.0838e-05 - val_my_r2: 0.9964\n",
      "Epoch 1861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7933e-04 - my_r2: 0.9247 - val_loss: 1.0476e-05 - val_my_r2: 0.9966\n",
      "Epoch 1862/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2980e-04 - my_r2: 0.9304 - val_loss: 9.2730e-06 - val_my_r2: 0.9972\n",
      "Epoch 1863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7455e-04 - my_r2: 0.9124 - val_loss: 1.0783e-05 - val_my_r2: 0.9966\n",
      "Epoch 1864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6504e-04 - my_r2: 0.8870 - val_loss: 1.0807e-05 - val_my_r2: 0.9967\n",
      "Epoch 1865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1565e-04 - my_r2: 0.9019 - val_loss: 8.0888e-06 - val_my_r2: 0.9973\n",
      "Epoch 1866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4759e-04 - my_r2: 0.9346 - val_loss: 6.0114e-06 - val_my_r2: 0.9981\n",
      "Epoch 1867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7243e-04 - my_r2: 0.9183 - val_loss: 7.1883e-06 - val_my_r2: 0.9974\n",
      "Epoch 1868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5759e-04 - my_r2: 0.9446 - val_loss: 1.1492e-05 - val_my_r2: 0.9949\n",
      "Epoch 1869/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9225e-04 - my_r2: 0.9175 - val_loss: 1.4010e-05 - val_my_r2: 0.9941\n",
      "Epoch 1870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4957e-04 - my_r2: 0.9212 - val_loss: 1.2012e-05 - val_my_r2: 0.9953\n",
      "Epoch 1871/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2236e-04 - my_r2: 0.9009 - val_loss: 9.1489e-06 - val_my_r2: 0.9968\n",
      "Epoch 1872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1752e-04 - my_r2: 0.9403 - val_loss: 1.0184e-05 - val_my_r2: 0.9968\n",
      "Epoch 1873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6146e-04 - my_r2: 0.9437 - val_loss: 1.0318e-05 - val_my_r2: 0.9969\n",
      "Epoch 1874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5245e-04 - my_r2: 0.9203 - val_loss: 1.0377e-05 - val_my_r2: 0.9970\n",
      "Epoch 1875/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2923e-04 - my_r2: 0.9468 - val_loss: 7.1393e-06 - val_my_r2: 0.9977\n",
      "Epoch 1876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5521e-04 - my_r2: 0.9271 - val_loss: 1.1287e-05 - val_my_r2: 0.9964\n",
      "Epoch 1877/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6342e-04 - my_r2: 0.9309 - val_loss: 1.1707e-05 - val_my_r2: 0.9958\n",
      "Epoch 1878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0865e-04 - my_r2: 0.9211 - val_loss: 1.1677e-05 - val_my_r2: 0.9958\n",
      "Epoch 1879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4005e-04 - my_r2: 0.8683 - val_loss: 1.5888e-05 - val_my_r2: 0.9944\n",
      "Epoch 1880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0959e-04 - my_r2: 0.9415 - val_loss: 1.7568e-05 - val_my_r2: 0.9938\n",
      "Epoch 1881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0739e-04 - my_r2: 0.9328 - val_loss: 1.3110e-05 - val_my_r2: 0.9950\n",
      "Epoch 1882/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7639e-04 - my_r2: 0.9325 - val_loss: 1.0664e-05 - val_my_r2: 0.9966\n",
      "Epoch 1883/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3369e-04 - my_r2: 0.9326 - val_loss: 1.3019e-05 - val_my_r2: 0.9959\n",
      "Epoch 1884/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6248e-04 - my_r2: 0.9380 - val_loss: 1.0125e-05 - val_my_r2: 0.9969\n",
      "Epoch 1885/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5390e-04 - my_r2: 0.9403 - val_loss: 9.0896e-06 - val_my_r2: 0.9973\n",
      "Epoch 1886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3755e-04 - my_r2: 0.9521 - val_loss: 8.5214e-06 - val_my_r2: 0.9973\n",
      "Epoch 1887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4072e-04 - my_r2: 0.9420 - val_loss: 8.4644e-06 - val_my_r2: 0.9969\n",
      "Epoch 1888/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4955e-04 - my_r2: 0.9308 - val_loss: 8.8427e-06 - val_my_r2: 0.9966\n",
      "Epoch 1889/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6869e-04 - my_r2: 0.8971 - val_loss: 6.4164e-06 - val_my_r2: 0.9980\n",
      "Epoch 1890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4479e-04 - my_r2: 0.9280 - val_loss: 7.8195e-06 - val_my_r2: 0.9976\n",
      "Epoch 1891/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8145e-04 - my_r2: 0.9608 - val_loss: 7.6918e-06 - val_my_r2: 0.9978\n",
      "Epoch 1892/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7164e-04 - my_r2: 0.9066 - val_loss: 6.9258e-06 - val_my_r2: 0.9981\n",
      "Epoch 1893/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2284e-04 - my_r2: 0.9433 - val_loss: 7.0989e-06 - val_my_r2: 0.9978\n",
      "Epoch 1894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6024e-04 - my_r2: 0.9300 - val_loss: 7.1561e-06 - val_my_r2: 0.9978\n",
      "Epoch 1895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2403e-04 - my_r2: 0.9426 - val_loss: 7.6925e-06 - val_my_r2: 0.9975\n",
      "Epoch 1896/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9671e-04 - my_r2: 0.8385 - val_loss: 7.6504e-06 - val_my_r2: 0.9974\n",
      "Epoch 1897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5645e-04 - my_r2: 0.8727 - val_loss: 9.8846e-06 - val_my_r2: 0.9968\n",
      "Epoch 1898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9798e-04 - my_r2: 0.9413 - val_loss: 1.4293e-05 - val_my_r2: 0.9954\n",
      "Epoch 1899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1502e-04 - my_r2: 0.9501 - val_loss: 1.4213e-05 - val_my_r2: 0.9954\n",
      "Epoch 1900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4114e-04 - my_r2: 0.9238 - val_loss: 1.0670e-05 - val_my_r2: 0.9965\n",
      "Epoch 1901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1809e-04 - my_r2: 0.9615 - val_loss: 1.0032e-05 - val_my_r2: 0.9970\n",
      "Epoch 1902/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6774e-04 - my_r2: 0.9334 - val_loss: 9.7163e-06 - val_my_r2: 0.9970\n",
      "Epoch 1903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6853e-04 - my_r2: 0.9102 - val_loss: 7.8715e-06 - val_my_r2: 0.9976\n",
      "Epoch 1904/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7784e-04 - my_r2: 0.9059 - val_loss: 8.6419e-06 - val_my_r2: 0.9976\n",
      "Epoch 1905/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5941e-04 - my_r2: 0.9116 - val_loss: 7.9359e-06 - val_my_r2: 0.9977\n",
      "Epoch 1906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0293e-04 - my_r2: 0.9621 - val_loss: 8.4296e-06 - val_my_r2: 0.9968\n",
      "Epoch 1907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1448e-04 - my_r2: 0.9489 - val_loss: 7.0543e-06 - val_my_r2: 0.9975\n",
      "Epoch 1908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4955e-04 - my_r2: 0.9128 - val_loss: 6.2741e-06 - val_my_r2: 0.9982\n",
      "Epoch 1909/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6111e-04 - my_r2: 0.9314 - val_loss: 7.6048e-06 - val_my_r2: 0.9979\n",
      "Epoch 1910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5132e-04 - my_r2: 0.9343 - val_loss: 7.1322e-06 - val_my_r2: 0.9974\n",
      "Epoch 1911/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3836e-04 - my_r2: 0.9478 - val_loss: 1.1948e-05 - val_my_r2: 0.9953\n",
      "Epoch 1912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7357e-04 - my_r2: 0.9480 - val_loss: 6.8844e-06 - val_my_r2: 0.9977\n",
      "Epoch 1913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6801e-04 - my_r2: 0.9143 - val_loss: 1.2873e-05 - val_my_r2: 0.9957\n",
      "Epoch 1914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8905e-04 - my_r2: 0.9026 - val_loss: 1.0864e-05 - val_my_r2: 0.9964\n",
      "Epoch 1915/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7357e-04 - my_r2: 0.9441 - val_loss: 1.1267e-05 - val_my_r2: 0.9961\n",
      "Epoch 1916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4322e-04 - my_r2: 0.9229 - val_loss: 1.0771e-05 - val_my_r2: 0.9962\n",
      "Epoch 1917/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0454e-04 - my_r2: 0.9517 - val_loss: 1.0824e-05 - val_my_r2: 0.9962\n",
      "Epoch 1918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6759e-04 - my_r2: 0.9116 - val_loss: 7.7078e-06 - val_my_r2: 0.9975\n",
      "Epoch 1919/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4779e-04 - my_r2: 0.8772 - val_loss: 7.8492e-06 - val_my_r2: 0.9974\n",
      "Epoch 1920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0016e-04 - my_r2: 0.9430 - val_loss: 7.7444e-06 - val_my_r2: 0.9975\n",
      "Epoch 1921/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5289e-04 - my_r2: 0.9289 - val_loss: 8.1129e-06 - val_my_r2: 0.9976\n",
      "Epoch 1922/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7740e-04 - my_r2: 0.8833 - val_loss: 7.4331e-06 - val_my_r2: 0.9978\n",
      "Epoch 1923/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4117e-04 - my_r2: 0.9356 - val_loss: 7.7860e-06 - val_my_r2: 0.9975\n",
      "Epoch 1924/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8608e-04 - my_r2: 0.7935 - val_loss: 9.3154e-06 - val_my_r2: 0.9969\n",
      "Epoch 1925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5826e-04 - my_r2: 0.8985 - val_loss: 9.7162e-06 - val_my_r2: 0.9970\n",
      "Epoch 1926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6935e-04 - my_r2: 0.8830 - val_loss: 2.1622e-05 - val_my_r2: 0.9932\n",
      "Epoch 1927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4007e-04 - my_r2: 0.9025 - val_loss: 9.6312e-06 - val_my_r2: 0.9971\n",
      "Epoch 1928/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0908e-04 - my_r2: 0.9480 - val_loss: 8.0918e-06 - val_my_r2: 0.9975\n",
      "Epoch 1929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4721e-04 - my_r2: 0.9346 - val_loss: 8.1321e-06 - val_my_r2: 0.9974\n",
      "Epoch 1930/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6951e-04 - my_r2: 0.9285 - val_loss: 7.1473e-06 - val_my_r2: 0.9980\n",
      "Epoch 1931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0946e-04 - my_r2: 0.9423 - val_loss: 6.9694e-06 - val_my_r2: 0.9981\n",
      "Epoch 1932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7062e-04 - my_r2: 0.9095 - val_loss: 7.0135e-06 - val_my_r2: 0.9980\n",
      "Epoch 1933/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0385e-04 - my_r2: 0.8973 - val_loss: 1.1622e-05 - val_my_r2: 0.9964\n",
      "Epoch 1934/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1624e-04 - my_r2: 0.9552 - val_loss: 9.4965e-06 - val_my_r2: 0.9971\n",
      "Epoch 1935/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0122e-04 - my_r2: 0.9571 - val_loss: 7.4341e-06 - val_my_r2: 0.9978\n",
      "Epoch 1936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1712e-04 - my_r2: 0.9045 - val_loss: 9.1960e-06 - val_my_r2: 0.9973\n",
      "Epoch 1937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6063e-04 - my_r2: 0.9439 - val_loss: 9.0835e-06 - val_my_r2: 0.9973\n",
      "Epoch 1938/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4892e-04 - my_r2: 0.8576 - val_loss: 7.8633e-06 - val_my_r2: 0.9975\n",
      "Epoch 1939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0098e-04 - my_r2: 0.9206 - val_loss: 7.4340e-06 - val_my_r2: 0.9977\n",
      "Epoch 1940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3760e-04 - my_r2: 0.9421 - val_loss: 1.0162e-05 - val_my_r2: 0.9969\n",
      "Epoch 1941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1166e-04 - my_r2: 0.9574 - val_loss: 1.0717e-05 - val_my_r2: 0.9967\n",
      "Epoch 1942/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9623e-04 - my_r2: 0.9488 - val_loss: 1.5629e-05 - val_my_r2: 0.9952\n",
      "Epoch 1943/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3646e-04 - my_r2: 0.9341 - val_loss: 1.4019e-05 - val_my_r2: 0.9957\n",
      "Epoch 1944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0004e-04 - my_r2: 0.9599 - val_loss: 1.2620e-05 - val_my_r2: 0.9963\n",
      "Epoch 1945/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5844e-04 - my_r2: 0.9371 - val_loss: 1.1227e-05 - val_my_r2: 0.9964\n",
      "Epoch 1946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4867e-04 - my_r2: 0.9493 - val_loss: 1.0110e-05 - val_my_r2: 0.9969\n",
      "Epoch 1947/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2950e-04 - my_r2: 0.9233 - val_loss: 9.7107e-06 - val_my_r2: 0.9969\n",
      "Epoch 1948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8618e-04 - my_r2: 0.9213 - val_loss: 1.1496e-05 - val_my_r2: 0.9962\n",
      "Epoch 1949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2668e-04 - my_r2: 0.9414 - val_loss: 7.6503e-06 - val_my_r2: 0.9974\n",
      "Epoch 1950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2382e-04 - my_r2: 0.9190 - val_loss: 7.5645e-06 - val_my_r2: 0.9975\n",
      "Epoch 1951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8579e-04 - my_r2: 0.9241 - val_loss: 1.0219e-05 - val_my_r2: 0.9967\n",
      "Epoch 1952/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7121e-04 - my_r2: 0.9146 - val_loss: 8.9852e-06 - val_my_r2: 0.9970\n",
      "Epoch 1953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4070e-04 - my_r2: 0.9202 - val_loss: 1.0456e-05 - val_my_r2: 0.9968\n",
      "Epoch 1954/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8605e-04 - my_r2: 0.9195 - val_loss: 9.5580e-06 - val_my_r2: 0.9970\n",
      "Epoch 1955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5636e-04 - my_r2: 0.9254 - val_loss: 7.3845e-06 - val_my_r2: 0.9977\n",
      "Epoch 1956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1075e-04 - my_r2: 0.9311 - val_loss: 7.6264e-06 - val_my_r2: 0.9977\n",
      "Epoch 1957/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2966e-04 - my_r2: 0.9378 - val_loss: 8.3974e-06 - val_my_r2: 0.9974\n",
      "Epoch 1958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4221e-04 - my_r2: 0.9054 - val_loss: 1.1259e-05 - val_my_r2: 0.9966\n",
      "Epoch 1959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3023e-04 - my_r2: 0.9303 - val_loss: 9.5155e-06 - val_my_r2: 0.9973\n",
      "Epoch 1960/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6222e-04 - my_r2: 0.9268 - val_loss: 1.1441e-05 - val_my_r2: 0.9966\n",
      "Epoch 1961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3675e-04 - my_r2: 0.9094 - val_loss: 1.7387e-05 - val_my_r2: 0.9946\n",
      "Epoch 1962/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7414e-04 - my_r2: 0.9458 - val_loss: 1.1166e-05 - val_my_r2: 0.9966\n",
      "Epoch 1963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0468e-04 - my_r2: 0.9470 - val_loss: 1.1955e-05 - val_my_r2: 0.9964\n",
      "Epoch 1964/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2627e-04 - my_r2: 0.9291 - val_loss: 1.1193e-05 - val_my_r2: 0.9965\n",
      "Epoch 1965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1472e-04 - my_r2: 0.9445 - val_loss: 9.3163e-06 - val_my_r2: 0.9971\n",
      "Epoch 1966/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8187e-04 - my_r2: 0.9478 - val_loss: 7.9745e-06 - val_my_r2: 0.9975\n",
      "Epoch 1967/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4495e-04 - my_r2: 0.9354 - val_loss: 1.3967e-05 - val_my_r2: 0.9949\n",
      "Epoch 1968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7104e-04 - my_r2: 0.9368 - val_loss: 8.4340e-06 - val_my_r2: 0.9971\n",
      "Epoch 1969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3031e-04 - my_r2: 0.9483 - val_loss: 8.8043e-06 - val_my_r2: 0.9970\n",
      "Epoch 1970/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0985e-04 - my_r2: 0.9529 - val_loss: 9.4002e-06 - val_my_r2: 0.9969\n",
      "Epoch 1971/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8519e-04 - my_r2: 0.9140 - val_loss: 9.3580e-06 - val_my_r2: 0.9968\n",
      "Epoch 1972/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7045e-04 - my_r2: 0.9235 - val_loss: 9.5405e-06 - val_my_r2: 0.9968\n",
      "Epoch 1973/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9694e-04 - my_r2: 0.9358 - val_loss: 8.9151e-06 - val_my_r2: 0.9968\n",
      "Epoch 1974/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1497e-04 - my_r2: 0.9267 - val_loss: 1.0689e-05 - val_my_r2: 0.9968\n",
      "Epoch 1975/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1436e-04 - my_r2: 0.9359 - val_loss: 8.2347e-06 - val_my_r2: 0.9975\n",
      "Epoch 1976/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6918e-04 - my_r2: 0.9161 - val_loss: 8.7031e-06 - val_my_r2: 0.9970\n",
      "Epoch 1977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3190e-04 - my_r2: 0.9195 - val_loss: 7.1447e-06 - val_my_r2: 0.9979\n",
      "Epoch 1978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7325e-04 - my_r2: 0.9341 - val_loss: 8.5442e-06 - val_my_r2: 0.9974\n",
      "Epoch 1979/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0780e-04 - my_r2: 0.9158 - val_loss: 8.0262e-06 - val_my_r2: 0.9978\n",
      "Epoch 1980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9350e-04 - my_r2: 0.9521 - val_loss: 7.7110e-06 - val_my_r2: 0.9979\n",
      "Epoch 1981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3226e-04 - my_r2: 0.9316 - val_loss: 1.0980e-05 - val_my_r2: 0.9967\n",
      "Epoch 1982/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8339e-04 - my_r2: 0.8793 - val_loss: 1.1262e-05 - val_my_r2: 0.9968\n",
      "Epoch 1983/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1639e-04 - my_r2: 0.9255 - val_loss: 1.3607e-05 - val_my_r2: 0.9964\n",
      "Epoch 1984/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6636e-04 - my_r2: 0.8080 - val_loss: 9.7945e-06 - val_my_r2: 0.9976\n",
      "Epoch 1985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5151e-04 - my_r2: 0.9208 - val_loss: 1.2726e-05 - val_my_r2: 0.9966\n",
      "Epoch 1986/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2818e-04 - my_r2: 0.9297 - val_loss: 1.4363e-05 - val_my_r2: 0.9961\n",
      "Epoch 1987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7608e-04 - my_r2: 0.9119 - val_loss: 1.2868e-05 - val_my_r2: 0.9963\n",
      "Epoch 1988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0790e-04 - my_r2: 0.9349 - val_loss: 1.2009e-05 - val_my_r2: 0.9963\n",
      "Epoch 1989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3637e-04 - my_r2: 0.9277 - val_loss: 1.3119e-05 - val_my_r2: 0.9958\n",
      "Epoch 1990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4148e-04 - my_r2: 0.9339 - val_loss: 7.0469e-06 - val_my_r2: 0.9980\n",
      "Epoch 1991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1857e-04 - my_r2: 0.9474 - val_loss: 9.0165e-06 - val_my_r2: 0.9972\n",
      "Epoch 1992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0181e-04 - my_r2: 0.9182 - val_loss: 9.2683e-06 - val_my_r2: 0.9965\n",
      "Epoch 1993/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5159e-04 - my_r2: 0.9132 - val_loss: 1.1053e-05 - val_my_r2: 0.9960\n",
      "Epoch 1994/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9127e-04 - my_r2: 0.9361 - val_loss: 1.1927e-05 - val_my_r2: 0.9954\n",
      "Epoch 1995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2320e-04 - my_r2: 0.9339 - val_loss: 1.2261e-05 - val_my_r2: 0.9954\n",
      "Epoch 1996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1516e-04 - my_r2: 0.9192 - val_loss: 1.3303e-05 - val_my_r2: 0.9951\n",
      "Epoch 1997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7737e-04 - my_r2: 0.9259 - val_loss: 5.7499e-06 - val_my_r2: 0.9980\n",
      "Epoch 1998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4141e-04 - my_r2: 0.9362 - val_loss: 7.1092e-06 - val_my_r2: 0.9976\n",
      "Epoch 1999/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8918e-04 - my_r2: 0.9155 - val_loss: 1.3555e-05 - val_my_r2: 0.9954\n",
      "Epoch 2000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3083e-04 - my_r2: 0.9293 - val_loss: 6.9059e-06 - val_my_r2: 0.9973\n",
      "---------- 1\n",
      "---------- 1\n",
      "train = 1.00 test = 1.00 loss-train = -1.000000 loss-test = -1.000000 iter=0\n",
      "Stats for iML1515_ec6_UB_AMN_QP CPU-time 1405.3648\n",
      "R2 = 0.9990 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Q2 = 0.9990 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Iter 1 Collated Q2 0.9990288113312235\n",
      "number of reactions:  1186 1186\n",
      "number of metabolites:  2084\n",
      "filtered measurements size:  1\n",
      "RC reservoir file: ./Reservoir/iML1515_ec6_UB_AMN_QP\n",
      "RC model type: RC\n",
      "RC scaler: 0.0\n",
      "RC model input dim: 38\n",
      "RC model output dim: 1\n",
      "RC model medium bound: UB\n",
      "training set size (110, 38) (110, 1)\n",
      "reservoir S, Pin, Pout matrices (2084, 1186) (38, 1186) (1, 1186)\n",
      "RC training epochs: 2000\n",
      "RC training regression: True\n",
      "RC training learn rate: 0.0001\n",
      "RC training dropout: 0.25\n",
      "RC training batch size: 5\n",
      "RC training validation iter: 0\n",
      "RC training xfold: 0\n",
      "RC training early stopping: False\n",
      "--------prior network --------\n",
      "training file: None\n",
      "model type: ANN_Dense\n",
      "model scaler: 0.0\n",
      "model input dim: 10\n",
      "model output dim: 10\n",
      "model medium bound: \n",
      "timestep: 0\n",
      "no training set provided\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "--------reservoir network-----\n",
      "training file: ./Dataset_model/iML1515_ec6_UB\n",
      "model type: AMN_QP\n",
      "model scaler: 7.95\n",
      "model input dim: 38\n",
      "model output dim: 2376\n",
      "model medium bound: UB\n",
      "timestep: 4\n",
      "training set size (11000, 38) (11000, 1)\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "gradient learn rate: 0.001\n",
      "gradient decay rate: 0.9\n",
      "training epochs: 20\n",
      "training regression: True\n",
      "training learn rate: 0.001\n",
      "training dropout: 0.25\n",
      "training batch size: 100\n",
      "training validation iter: 0\n",
      "training xfold: 5\n",
      "training early stopping: False\n",
      "AMN scaler 0.0\n",
      "RC input shape (110, 38) (110, 1)\n",
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "Physical devices cannot be modified after being initialized\n",
      "----------------------------------- RC\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 10 relu True\n",
      "Prior inputs and outputs (None, 10) (None, 10)\n",
      "Res inputs added to Prior_outputs 28\n",
      "Res inputs (final) (None, 38)\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 1186 relu False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 13:43:55.306984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMN output shapes for PoutV, SV, PinV, Vpos, V, outputs (None, 1) (None, 1) (None, 1) (None, 1) (None, 1186) (None, 2376)\n",
      "=======================\n",
      "PoutV: (None, 1)\n",
      "SV: (None, 1)\n",
      "PinV: (None, 1)\n",
      "Vpos: (None, 1)\n",
      "V: (None, 1186)\n",
      "V0: KerasTensor(type_spec=TensorSpec(shape=(None, 1186), dtype=tf.float32, name=None), name='tf.__operators__.add_29/AddV2:0', description=\"created by layer 'tf.__operators__.add_29'\")\n",
      "Vin: KerasTensor(type_spec=TensorSpec(shape=(None, 38), dtype=tf.float32, name=None), name='tf.math.truediv_64/truediv:0', description=\"created by layer 'tf.math.truediv_64'\")\n",
      "Vout: tf.Tensor([], shape=(0, 0), dtype=float32)\n",
      "Res_outputs-------------------- (None, 2376)\n",
      "SV, PinV, Vpos, V-------------- (None, 1) (None, 1) (None, 1) (None, 1186)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_15 (Lambda)             (None, 10)           0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 500)          5500        ['lambda_15[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 500)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_14 (Lambda)             (None, 28)           0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 10)           5010        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 38)           0           ['lambda_14[0][0]',              \n",
      "                                                                  'dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_28 (TFOp  (None, 38)          0           ['input_3[0][0]',                \n",
      " Lambda)                                                          'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_56 (TFOpLambd  (None, 38)          0           ['concatenate_6[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_28[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_64 (TFOpLambda  (None, 38)          0           ['tf.math.multiply_56[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 500)          19500       ['tf.math.truediv_64[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 500)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1186)         594186      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_40 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_64[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_22 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_40[0][0]',    \n",
      " a)                                                               'dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.relu_22 (TFOpLambda)     (None, 1186)         0           ['tf.math.subtract_22[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_29 (TFOp  (None, 1186)        0           ['tf.nn.relu_22[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_22[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract_23 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_29[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_57 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_29[0][0]'\n",
      " a)                                                              , 'dense_11[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply_58 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_23[0][0]',    \n",
      " a)                                                               'tf.linalg.matmul_40[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_59 (TFOpLambd  (None, 1186)        0           ['dense_11[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 1186)        0           ['tf.math.multiply_57[0][0]',    \n",
      " ambda)                                                           'tf.math.multiply_58[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 1186)        0           ['tf.math.multiply_59[0][0]',    \n",
      " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
      "                                                                                                  \n",
      " tf.linalg.matmul_43 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_29[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_24 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_43[0][0]',    \n",
      " a)                                                               'tf.math.truediv_64[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_23 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_24[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_18 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_29[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_41 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_29[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_30 (TFOp  (None, 38)          0           ['tf.nn.relu_23[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_23[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_24 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_18[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_42 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_41[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_61 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_23[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_30[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_31 (TFOp  (None, 1186)        0           ['tf.nn.relu_24[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_24[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_66 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_42[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_44 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_61[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_19 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_31[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_67 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_66[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_69 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_44[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_62 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_24[0][0]',          \n",
      " a)                                                               'tf.math.negative_19[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 1186)        0           ['tf.math.truediv_67[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_69[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_71 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_62[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_30[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_71[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_60 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_29[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_63 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_31[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_64 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_60[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_65 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_63[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_25 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_64[0][0]',    \n",
      " a)                                                               'tf.math.multiply_65[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_29[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_25[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_47 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_32[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_26 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_47[0][0]',    \n",
      " a)                                                               'tf.math.truediv_64[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_25 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_26[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_20 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_32[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_45 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_32[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_33 (TFOp  (None, 38)          0           ['tf.nn.relu_25[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_25[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_26 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_20[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_46 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_45[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_66 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_25[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_33[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_34 (TFOp  (None, 1186)        0           ['tf.nn.relu_26[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_26[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_73 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_46[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_48 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_66[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_21 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_34[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_74 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_73[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_76 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_48[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_67 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_26[0][0]',          \n",
      " a)                                                               'tf.math.negative_21[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 1186)        0           ['tf.math.truediv_74[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_76[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_78 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_67[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_33[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_78[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_68 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_34[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_69 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_25[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_70 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_68[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_27 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_69[0][0]',    \n",
      " a)                                                               'tf.math.multiply_70[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_32[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_27[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_51 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_35[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_28 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_51[0][0]',    \n",
      " a)                                                               'tf.math.truediv_64[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_27 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_28[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_22 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_35[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_49 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_35[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_36 (TFOp  (None, 38)          0           ['tf.nn.relu_27[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_27[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_28 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_22[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_50 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_49[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_71 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_27[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_36[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_37 (TFOp  (None, 1186)        0           ['tf.nn.relu_28[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_28[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_80 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_50[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_52 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_71[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_23 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_37[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_81 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_80[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_83 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_52[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_72 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_28[0][0]',          \n",
      " a)                                                               'tf.math.negative_23[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 1186)        0           ['tf.math.truediv_81[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_83[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_85 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_72[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_36[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_85[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_73 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_37[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_74 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_27[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_75 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_73[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_29 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_74[0][0]',    \n",
      " a)                                                               'tf.math.multiply_75[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_35[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_29[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_55 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_38[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_30 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_55[0][0]',    \n",
      " a)                                                               'tf.math.truediv_64[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_29 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_30[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_24 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_38[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_53 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_38[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_39 (TFOp  (None, 38)          0           ['tf.nn.relu_29[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_29[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_30 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_24[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_54 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_53[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_76 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_29[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_39[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_40 (TFOp  (None, 1186)        0           ['tf.nn.relu_30[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_30[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_87 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_54[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_56 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_76[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_25 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_40[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_88 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_87[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_90 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_56[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_77 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_30[0][0]',          \n",
      " a)                                                               'tf.math.negative_25[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 1186)        0           ['tf.math.truediv_88[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_90[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv_92 (TFOpLambda  (None, 1186)        0           ['tf.math.multiply_77[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_40 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_39[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_92[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_78 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_40[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_79 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_29[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_80 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_78[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_31 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_79[0][0]',    \n",
      " a)                                                               'tf.math.multiply_80[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_41 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_38[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_31[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_59 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_41[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_32 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_59[0][0]',    \n",
      " a)                                                               'tf.math.truediv_64[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.negative_26 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_41[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_58 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_41[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.nn.relu_31 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_32[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_32 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_26[0][0]']    \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_42 (TFOpLamb  (None, 1)           0           ['tf.linalg.matmul_58[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_43 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_31[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_44 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_32[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_57 (TFOpLambd  (None, 1)           0           ['tf.__operators__.add_41[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_93 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_42[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_94 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_43[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_95 (TFOpLambda  (None, 1)           0           ['tf.compat.v1.norm_44[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 2376)         0           ['tf.linalg.matmul_57[0][0]',    \n",
      "                                                                  'tf.math.truediv_93[0][0]',     \n",
      "                                                                  'tf.math.truediv_94[0][0]',     \n",
      "                                                                  'tf.math.truediv_95[0][0]',     \n",
      "                                                                  'tf.__operators__.add_41[0][0]',\n",
      "                                                                  'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      " lambda_16 (Lambda)             (None, 1)            0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_17 (Lambda)             (None, 1)            0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_18 (Lambda)             (None, 1)            0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_19 (Lambda)             (None, 1)            0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_20 (Lambda)             (None, 1186)         0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 1228)         0           ['lambda_16[0][0]',              \n",
      "                                                                  'lambda_17[0][0]',              \n",
      "                                                                  'lambda_18[0][0]',              \n",
      "                                                                  'lambda_19[0][0]',              \n",
      "                                                                  'lambda_20[0][0]',              \n",
      "                                                                  'tf.math.truediv_64[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 624,196\n",
      "Trainable params: 10,510\n",
      "Non-trainable params: 613,686\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "nbr parameters: 624196\n",
      "Epoch 1/2000\n",
      "22/22 [==============================] - 2s 47ms/step - loss: 0.0364 - my_r2: -6.8922 - val_loss: 0.0333 - val_my_r2: -9.2534\n",
      "Epoch 2/2000\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.0316 - my_r2: -10.5109 - val_loss: 0.0289 - val_my_r2: -7.9115\n",
      "Epoch 3/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0272 - my_r2: -6.2484 - val_loss: 0.0244 - val_my_r2: -6.5451\n",
      "Epoch 4/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0230 - my_r2: -4.2007 - val_loss: 0.0202 - val_my_r2: -5.2590\n",
      "Epoch 5/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0190 - my_r2: -3.2742 - val_loss: 0.0168 - val_my_r2: -4.2067\n",
      "Epoch 6/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.0157 - my_r2: -3.4888 - val_loss: 0.0140 - val_my_r2: -3.3369\n",
      "Epoch 7/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0137 - my_r2: -1.8040 - val_loss: 0.0117 - val_my_r2: -2.6280\n",
      "Epoch 8/2000\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0119 - my_r2: -1.9216 - val_loss: 0.0099 - val_my_r2: -2.0723\n",
      "Epoch 9/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.0093 - my_r2: -2.9969 - val_loss: 0.0084 - val_my_r2: -1.6192\n",
      "Epoch 10/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0083 - my_r2: -1.4845 - val_loss: 0.0073 - val_my_r2: -1.2573\n",
      "Epoch 11/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0069 - my_r2: -0.7169 - val_loss: 0.0064 - val_my_r2: -0.9921\n",
      "Epoch 12/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 0.0066 - my_r2: -0.3157 - val_loss: 0.0057 - val_my_r2: -0.7717\n",
      "Epoch 13/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0058 - my_r2: -1.5178 - val_loss: 0.0052 - val_my_r2: -0.6145\n",
      "Epoch 14/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0053 - my_r2: -3.0981 - val_loss: 0.0047 - val_my_r2: -0.4726\n",
      "Epoch 15/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0045 - my_r2: -0.1376 - val_loss: 0.0043 - val_my_r2: -0.3456\n",
      "Epoch 16/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0044 - my_r2: -0.2078 - val_loss: 0.0040 - val_my_r2: -0.2612\n",
      "Epoch 17/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0046 - my_r2: -0.4136 - val_loss: 0.0038 - val_my_r2: -0.1824\n",
      "Epoch 18/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0039 - my_r2: -0.1284 - val_loss: 0.0036 - val_my_r2: -0.1111\n",
      "Epoch 19/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0038 - my_r2: 0.0274 - val_loss: 0.0034 - val_my_r2: -0.0522\n",
      "Epoch 20/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0036 - my_r2: 0.1686 - val_loss: 0.0032 - val_my_r2: -4.9087e-04\n",
      "Epoch 21/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0035 - my_r2: 0.1694 - val_loss: 0.0030 - val_my_r2: 0.0471\n",
      "Epoch 22/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0031 - my_r2: 0.3117 - val_loss: 0.0029 - val_my_r2: 0.0861\n",
      "Epoch 23/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0031 - my_r2: 0.3680 - val_loss: 0.0028 - val_my_r2: 0.1245\n",
      "Epoch 24/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0029 - my_r2: 0.1690 - val_loss: 0.0027 - val_my_r2: 0.1577\n",
      "Epoch 25/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0030 - my_r2: -0.2184 - val_loss: 0.0026 - val_my_r2: 0.1879\n",
      "Epoch 26/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0028 - my_r2: 0.3531 - val_loss: 0.0024 - val_my_r2: 0.2180\n",
      "Epoch 27/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0028 - my_r2: 0.3738 - val_loss: 0.0024 - val_my_r2: 0.2471\n",
      "Epoch 28/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0026 - my_r2: 0.3932 - val_loss: 0.0023 - val_my_r2: 0.2712\n",
      "Epoch 29/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0027 - my_r2: -0.1909 - val_loss: 0.0022 - val_my_r2: 0.2977\n",
      "Epoch 30/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0024 - my_r2: 0.3989 - val_loss: 0.0021 - val_my_r2: 0.3171\n",
      "Epoch 31/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0022 - my_r2: 0.2578 - val_loss: 0.0020 - val_my_r2: 0.3402\n",
      "Epoch 32/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0024 - my_r2: 0.4010 - val_loss: 0.0020 - val_my_r2: 0.3597\n",
      "Epoch 33/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0023 - my_r2: 0.3258 - val_loss: 0.0019 - val_my_r2: 0.3776\n",
      "Epoch 34/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0021 - my_r2: 0.3610 - val_loss: 0.0019 - val_my_r2: 0.3928\n",
      "Epoch 35/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: 0.4958 - val_loss: 0.0018 - val_my_r2: 0.4082\n",
      "Epoch 36/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: 0.4666 - val_loss: 0.0018 - val_my_r2: 0.4222\n",
      "Epoch 37/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: 0.4975 - val_loss: 0.0017 - val_my_r2: 0.4384\n",
      "Epoch 38/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0020 - my_r2: 0.3983 - val_loss: 0.0017 - val_my_r2: 0.4517\n",
      "Epoch 39/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0019 - my_r2: 0.3224 - val_loss: 0.0016 - val_my_r2: 0.4659\n",
      "Epoch 40/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0017 - my_r2: 0.5741 - val_loss: 0.0016 - val_my_r2: 0.4797\n",
      "Epoch 41/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0018 - my_r2: 0.6373 - val_loss: 0.0015 - val_my_r2: 0.4922\n",
      "Epoch 42/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: 0.3661 - val_loss: 0.0015 - val_my_r2: 0.5053\n",
      "Epoch 43/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.6182 - val_loss: 0.0015 - val_my_r2: 0.5158\n",
      "Epoch 44/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0018 - my_r2: 0.5275 - val_loss: 0.0014 - val_my_r2: 0.5262\n",
      "Epoch 45/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0016 - my_r2: 0.4881 - val_loss: 0.0014 - val_my_r2: 0.5378\n",
      "Epoch 46/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0016 - my_r2: 0.5190 - val_loss: 0.0014 - val_my_r2: 0.5464\n",
      "Epoch 47/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.5993 - val_loss: 0.0013 - val_my_r2: 0.5556\n",
      "Epoch 48/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0016 - my_r2: 0.5982 - val_loss: 0.0013 - val_my_r2: 0.5634\n",
      "Epoch 49/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0015 - my_r2: 0.7273 - val_loss: 0.0013 - val_my_r2: 0.5710\n",
      "Epoch 50/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.6869 - val_loss: 0.0013 - val_my_r2: 0.5792\n",
      "Epoch 51/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0015 - my_r2: 0.6130 - val_loss: 0.0013 - val_my_r2: 0.5860\n",
      "Epoch 52/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.6308 - val_loss: 0.0012 - val_my_r2: 0.5928\n",
      "Epoch 53/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.6498 - val_loss: 0.0012 - val_my_r2: 0.5989\n",
      "Epoch 54/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0016 - my_r2: 0.6225 - val_loss: 0.0012 - val_my_r2: 0.6060\n",
      "Epoch 55/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.5933 - val_loss: 0.0012 - val_my_r2: 0.6151\n",
      "Epoch 56/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0017 - my_r2: 0.6517 - val_loss: 0.0011 - val_my_r2: 0.6227\n",
      "Epoch 57/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0013 - my_r2: 0.5993 - val_loss: 0.0011 - val_my_r2: 0.6290\n",
      "Epoch 58/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0014 - my_r2: 0.6816 - val_loss: 0.0011 - val_my_r2: 0.6331\n",
      "Epoch 59/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.5519 - val_loss: 0.0011 - val_my_r2: 0.6420\n",
      "Epoch 60/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.6393 - val_loss: 0.0011 - val_my_r2: 0.6466\n",
      "Epoch 61/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.5013 - val_loss: 0.0011 - val_my_r2: 0.6522\n",
      "Epoch 62/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.6705 - val_loss: 0.0010 - val_my_r2: 0.6573\n",
      "Epoch 63/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6507 - val_loss: 0.0010 - val_my_r2: 0.6622\n",
      "Epoch 64/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6149 - val_loss: 0.0010 - val_my_r2: 0.6658\n",
      "Epoch 65/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.6596 - val_loss: 9.8766e-04 - val_my_r2: 0.6709\n",
      "Epoch 66/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.6420 - val_loss: 9.7396e-04 - val_my_r2: 0.6751\n",
      "Epoch 67/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0010 - my_r2: 0.7341 - val_loss: 9.6198e-04 - val_my_r2: 0.6781\n",
      "Epoch 68/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.6421 - val_loss: 9.4868e-04 - val_my_r2: 0.6850\n",
      "Epoch 69/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.6262 - val_loss: 9.3373e-04 - val_my_r2: 0.6897\n",
      "Epoch 70/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.4664 - val_loss: 9.2406e-04 - val_my_r2: 0.6935\n",
      "Epoch 71/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7310 - val_loss: 9.0754e-04 - val_my_r2: 0.6965\n",
      "Epoch 72/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.7425 - val_loss: 8.9480e-04 - val_my_r2: 0.6998\n",
      "Epoch 73/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6845 - val_loss: 8.8390e-04 - val_my_r2: 0.7025\n",
      "Epoch 74/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7247 - val_loss: 8.7462e-04 - val_my_r2: 0.7054\n",
      "Epoch 75/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.8754e-04 - my_r2: 0.7015 - val_loss: 8.6173e-04 - val_my_r2: 0.7086\n",
      "Epoch 76/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.6817 - val_loss: 8.5130e-04 - val_my_r2: 0.7118\n",
      "Epoch 77/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7457 - val_loss: 8.3739e-04 - val_my_r2: 0.7165\n",
      "Epoch 78/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.8010 - val_loss: 8.2689e-04 - val_my_r2: 0.7194\n",
      "Epoch 79/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.3934 - val_loss: 8.1589e-04 - val_my_r2: 0.7214\n",
      "Epoch 80/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6901 - val_loss: 8.0769e-04 - val_my_r2: 0.7261\n",
      "Epoch 81/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7368 - val_loss: 7.9878e-04 - val_my_r2: 0.7288\n",
      "Epoch 82/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7465 - val_loss: 7.8908e-04 - val_my_r2: 0.7304\n",
      "Epoch 83/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7491 - val_loss: 7.7929e-04 - val_my_r2: 0.7339\n",
      "Epoch 84/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.8883e-04 - my_r2: 0.6941 - val_loss: 7.6762e-04 - val_my_r2: 0.7383\n",
      "Epoch 85/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.0541e-04 - my_r2: 0.6836 - val_loss: 7.5949e-04 - val_my_r2: 0.7416\n",
      "Epoch 86/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.4465 - val_loss: 7.5052e-04 - val_my_r2: 0.7431\n",
      "Epoch 87/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.8493e-04 - my_r2: 0.7428 - val_loss: 7.4246e-04 - val_my_r2: 0.7448\n",
      "Epoch 88/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6685 - val_loss: 7.3413e-04 - val_my_r2: 0.7468\n",
      "Epoch 89/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.6594 - val_loss: 7.2476e-04 - val_my_r2: 0.7513\n",
      "Epoch 90/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7625 - val_loss: 7.1513e-04 - val_my_r2: 0.7554\n",
      "Epoch 91/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0010 - my_r2: 0.7416 - val_loss: 7.0568e-04 - val_my_r2: 0.7588\n",
      "Epoch 92/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0010 - my_r2: 0.6892 - val_loss: 6.9847e-04 - val_my_r2: 0.7606\n",
      "Epoch 93/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.3886e-04 - my_r2: 0.6738 - val_loss: 6.9045e-04 - val_my_r2: 0.7621\n",
      "Epoch 94/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.9484e-04 - my_r2: 0.5909 - val_loss: 6.8303e-04 - val_my_r2: 0.7658\n",
      "Epoch 95/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.7196e-04 - my_r2: 0.6353 - val_loss: 6.7525e-04 - val_my_r2: 0.7685\n",
      "Epoch 96/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7629 - val_loss: 6.6917e-04 - val_my_r2: 0.7725\n",
      "Epoch 97/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7107 - val_loss: 6.6278e-04 - val_my_r2: 0.7755\n",
      "Epoch 98/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.3652e-04 - my_r2: 0.7745 - val_loss: 6.5479e-04 - val_my_r2: 0.7777\n",
      "Epoch 99/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.7103e-04 - my_r2: 0.8108 - val_loss: 6.4582e-04 - val_my_r2: 0.7799\n",
      "Epoch 100/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.4019e-04 - my_r2: 0.6817 - val_loss: 6.3877e-04 - val_my_r2: 0.7818\n",
      "Epoch 101/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.7980e-04 - my_r2: 0.7843 - val_loss: 6.3144e-04 - val_my_r2: 0.7837\n",
      "Epoch 102/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.1390e-04 - my_r2: 0.7172 - val_loss: 6.2677e-04 - val_my_r2: 0.7847\n",
      "Epoch 103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.4307e-04 - my_r2: 0.6188 - val_loss: 6.1927e-04 - val_my_r2: 0.7867\n",
      "Epoch 104/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.9494e-04 - my_r2: 0.7941 - val_loss: 6.1135e-04 - val_my_r2: 0.7894\n",
      "Epoch 105/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.0949e-04 - my_r2: 0.7645 - val_loss: 6.0657e-04 - val_my_r2: 0.7909\n",
      "Epoch 106/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 7.8357e-04 - my_r2: 0.8016 - val_loss: 6.0017e-04 - val_my_r2: 0.7923\n",
      "Epoch 107/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.7431e-04 - my_r2: 0.8004 - val_loss: 5.9388e-04 - val_my_r2: 0.7948\n",
      "Epoch 108/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.6333e-04 - my_r2: 0.7967 - val_loss: 5.8803e-04 - val_my_r2: 0.7974\n",
      "Epoch 109/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.3112e-04 - my_r2: 0.6766 - val_loss: 5.8212e-04 - val_my_r2: 0.8010\n",
      "Epoch 110/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.8883e-04 - my_r2: 0.8482 - val_loss: 5.7659e-04 - val_my_r2: 0.8026\n",
      "Epoch 111/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8146e-04 - my_r2: 0.7600 - val_loss: 5.7051e-04 - val_my_r2: 0.8038\n",
      "Epoch 112/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.1241e-04 - my_r2: 0.7334 - val_loss: 5.6497e-04 - val_my_r2: 0.8056\n",
      "Epoch 113/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.5554e-04 - my_r2: 0.8138 - val_loss: 5.5941e-04 - val_my_r2: 0.8073\n",
      "Epoch 114/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.8002e-04 - my_r2: 0.7822 - val_loss: 5.5511e-04 - val_my_r2: 0.8080\n",
      "Epoch 115/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.1258e-04 - my_r2: 0.7908 - val_loss: 5.5046e-04 - val_my_r2: 0.8084\n",
      "Epoch 116/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.6604e-04 - my_r2: 0.7936 - val_loss: 5.4349e-04 - val_my_r2: 0.8120\n",
      "Epoch 117/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.6364e-04 - my_r2: 0.7630 - val_loss: 5.3951e-04 - val_my_r2: 0.8118\n",
      "Epoch 118/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.5865e-04 - my_r2: 0.7658 - val_loss: 5.3472e-04 - val_my_r2: 0.8161\n",
      "Epoch 119/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.1678e-04 - my_r2: 0.7594 - val_loss: 5.2951e-04 - val_my_r2: 0.8176\n",
      "Epoch 120/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.2889e-04 - my_r2: 0.8143 - val_loss: 5.2317e-04 - val_my_r2: 0.8190\n",
      "Epoch 121/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.4046e-04 - my_r2: 0.8608 - val_loss: 5.1950e-04 - val_my_r2: 0.8213\n",
      "Epoch 122/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.3348e-04 - my_r2: 0.7191 - val_loss: 5.1443e-04 - val_my_r2: 0.8227\n",
      "Epoch 123/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.6453e-04 - my_r2: 0.8685 - val_loss: 5.0959e-04 - val_my_r2: 0.8244\n",
      "Epoch 124/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.9288e-04 - my_r2: 0.5836 - val_loss: 5.0751e-04 - val_my_r2: 0.8265\n",
      "Epoch 125/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.0763e-04 - my_r2: 0.6035 - val_loss: 5.0311e-04 - val_my_r2: 0.8275\n",
      "Epoch 126/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.8880e-04 - my_r2: 0.7599 - val_loss: 4.9748e-04 - val_my_r2: 0.8282\n",
      "Epoch 127/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1766e-04 - my_r2: 0.7918 - val_loss: 4.9275e-04 - val_my_r2: 0.8296\n",
      "Epoch 128/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.0654e-04 - my_r2: 0.7791 - val_loss: 4.8779e-04 - val_my_r2: 0.8321\n",
      "Epoch 129/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6734e-04 - my_r2: 0.8531 - val_loss: 4.8493e-04 - val_my_r2: 0.8314\n",
      "Epoch 130/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.6838e-04 - my_r2: 0.7662 - val_loss: 4.7960e-04 - val_my_r2: 0.8342\n",
      "Epoch 131/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3984e-04 - my_r2: 0.7805 - val_loss: 4.7502e-04 - val_my_r2: 0.8359\n",
      "Epoch 132/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.9508e-04 - my_r2: 0.6929 - val_loss: 4.7143e-04 - val_my_r2: 0.8362\n",
      "Epoch 133/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8460e-04 - my_r2: 0.8307 - val_loss: 4.6846e-04 - val_my_r2: 0.8367\n",
      "Epoch 134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0551e-04 - my_r2: 0.7904 - val_loss: 4.6528e-04 - val_my_r2: 0.8379\n",
      "Epoch 135/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.8078e-04 - my_r2: 0.7769 - val_loss: 4.6178e-04 - val_my_r2: 0.8392\n",
      "Epoch 136/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.6522e-04 - my_r2: 0.6846 - val_loss: 4.6082e-04 - val_my_r2: 0.8409\n",
      "Epoch 137/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.0276e-04 - my_r2: 0.8515 - val_loss: 4.5663e-04 - val_my_r2: 0.8416\n",
      "Epoch 138/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.9724e-04 - my_r2: 0.6819 - val_loss: 4.5458e-04 - val_my_r2: 0.8437\n",
      "Epoch 139/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3355e-04 - my_r2: 0.8376 - val_loss: 4.4937e-04 - val_my_r2: 0.8458\n",
      "Epoch 140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8626e-04 - my_r2: 0.8731 - val_loss: 4.4584e-04 - val_my_r2: 0.8474\n",
      "Epoch 141/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.5816e-04 - my_r2: 0.8148 - val_loss: 4.4287e-04 - val_my_r2: 0.8479\n",
      "Epoch 142/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.5001e-04 - my_r2: 0.8343 - val_loss: 4.3621e-04 - val_my_r2: 0.8509\n",
      "Epoch 143/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3428e-04 - my_r2: 0.8183 - val_loss: 4.3185e-04 - val_my_r2: 0.8523\n",
      "Epoch 144/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4002e-04 - my_r2: 0.8332 - val_loss: 4.2828e-04 - val_my_r2: 0.8529\n",
      "Epoch 145/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1547e-04 - my_r2: 0.8603 - val_loss: 4.2590e-04 - val_my_r2: 0.8526\n",
      "Epoch 146/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.4780e-04 - my_r2: 0.7202 - val_loss: 4.2496e-04 - val_my_r2: 0.8520\n",
      "Epoch 147/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.3180e-04 - my_r2: 0.8034 - val_loss: 4.2094e-04 - val_my_r2: 0.8538\n",
      "Epoch 148/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3424e-04 - my_r2: 0.8156 - val_loss: 4.1681e-04 - val_my_r2: 0.8549\n",
      "Epoch 149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3396e-04 - my_r2: 0.7571 - val_loss: 4.1550e-04 - val_my_r2: 0.8564\n",
      "Epoch 150/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7253e-04 - my_r2: 0.8120 - val_loss: 4.1249e-04 - val_my_r2: 0.8558\n",
      "Epoch 151/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.5264e-04 - my_r2: 0.8323 - val_loss: 4.0879e-04 - val_my_r2: 0.8578\n",
      "Epoch 152/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.1520e-04 - my_r2: 0.8202 - val_loss: 4.0479e-04 - val_my_r2: 0.8600\n",
      "Epoch 153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0082e-04 - my_r2: 0.7858 - val_loss: 4.0297e-04 - val_my_r2: 0.8603\n",
      "Epoch 154/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.1870e-04 - my_r2: 0.8067 - val_loss: 4.0081e-04 - val_my_r2: 0.8605\n",
      "Epoch 155/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.4432e-04 - my_r2: 0.8270 - val_loss: 3.9671e-04 - val_my_r2: 0.8634\n",
      "Epoch 156/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.9266e-04 - my_r2: 0.7976 - val_loss: 3.9489e-04 - val_my_r2: 0.8639\n",
      "Epoch 157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0877e-04 - my_r2: 0.8511 - val_loss: 3.9031e-04 - val_my_r2: 0.8666\n",
      "Epoch 158/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.5864e-04 - my_r2: 0.8437 - val_loss: 3.8880e-04 - val_my_r2: 0.8668\n",
      "Epoch 159/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2456e-04 - my_r2: 0.8513 - val_loss: 3.8479e-04 - val_my_r2: 0.8690\n",
      "Epoch 160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7475e-04 - my_r2: 0.8587 - val_loss: 3.8135e-04 - val_my_r2: 0.8706\n",
      "Epoch 161/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1850e-04 - my_r2: 0.8350 - val_loss: 3.7796e-04 - val_my_r2: 0.8720\n",
      "Epoch 162/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.9757e-04 - my_r2: 0.8321 - val_loss: 3.7591e-04 - val_my_r2: 0.8728\n",
      "Epoch 163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.4594e-04 - my_r2: 0.8699 - val_loss: 3.7296e-04 - val_my_r2: 0.8739\n",
      "Epoch 164/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.0844e-04 - my_r2: 0.7966 - val_loss: 3.7041e-04 - val_my_r2: 0.8743\n",
      "Epoch 165/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.7932e-04 - my_r2: 0.7904 - val_loss: 3.7320e-04 - val_my_r2: 0.8739\n",
      "Epoch 166/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1746e-04 - my_r2: 0.8314 - val_loss: 3.6795e-04 - val_my_r2: 0.8749\n",
      "Epoch 167/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.2273e-04 - my_r2: 0.8314 - val_loss: 3.6351e-04 - val_my_r2: 0.8757\n",
      "Epoch 168/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.8298e-04 - my_r2: 0.8281 - val_loss: 3.6030e-04 - val_my_r2: 0.8762\n",
      "Epoch 169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3824e-04 - my_r2: 0.7920 - val_loss: 3.5810e-04 - val_my_r2: 0.8769\n",
      "Epoch 170/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.6654e-04 - my_r2: 0.8643 - val_loss: 3.5580e-04 - val_my_r2: 0.8791\n",
      "Epoch 171/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.4361e-04 - my_r2: 0.8992 - val_loss: 3.5378e-04 - val_my_r2: 0.8790\n",
      "Epoch 172/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.6406e-04 - my_r2: 0.8582 - val_loss: 3.5108e-04 - val_my_r2: 0.8806\n",
      "Epoch 173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2370e-04 - my_r2: 0.8850 - val_loss: 3.4742e-04 - val_my_r2: 0.8811\n",
      "Epoch 174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5010e-04 - my_r2: 0.8694 - val_loss: 3.4377e-04 - val_my_r2: 0.8812\n",
      "Epoch 175/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 6.2469e-04 - my_r2: 0.7805 - val_loss: 3.4658e-04 - val_my_r2: 0.8804\n",
      "Epoch 176/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.4973e-04 - my_r2: 0.7416 - val_loss: 3.4722e-04 - val_my_r2: 0.8809\n",
      "Epoch 177/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.5466e-04 - my_r2: 0.8945 - val_loss: 3.4241e-04 - val_my_r2: 0.8830\n",
      "Epoch 178/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2514e-04 - my_r2: 0.8873 - val_loss: 3.3724e-04 - val_my_r2: 0.8839\n",
      "Epoch 179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4673e-04 - my_r2: 0.7987 - val_loss: 3.3757e-04 - val_my_r2: 0.8830\n",
      "Epoch 180/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5649e-04 - my_r2: 0.8715 - val_loss: 3.3695e-04 - val_my_r2: 0.8842\n",
      "Epoch 181/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.2202e-04 - my_r2: 0.8433 - val_loss: 3.3149e-04 - val_my_r2: 0.8848\n",
      "Epoch 182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5457e-04 - my_r2: 0.8336 - val_loss: 3.2943e-04 - val_my_r2: 0.8870\n",
      "Epoch 183/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7871e-04 - my_r2: 0.8045 - val_loss: 3.2661e-04 - val_my_r2: 0.8873\n",
      "Epoch 184/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9628e-04 - my_r2: 0.8424 - val_loss: 3.2304e-04 - val_my_r2: 0.8888\n",
      "Epoch 185/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.1048e-04 - my_r2: 0.8832 - val_loss: 3.2215e-04 - val_my_r2: 0.8894\n",
      "Epoch 186/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0171e-04 - my_r2: 0.8244 - val_loss: 3.1870e-04 - val_my_r2: 0.8897\n",
      "Epoch 187/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.4678e-04 - my_r2: 0.8757 - val_loss: 3.2311e-04 - val_my_r2: 0.8861\n",
      "Epoch 188/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6465e-04 - my_r2: 0.8249 - val_loss: 3.1962e-04 - val_my_r2: 0.8867\n",
      "Epoch 189/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0287e-04 - my_r2: 0.8000 - val_loss: 3.1269e-04 - val_my_r2: 0.8899\n",
      "Epoch 190/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.5204e-04 - my_r2: 0.8986 - val_loss: 3.1031e-04 - val_my_r2: 0.8918\n",
      "Epoch 191/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.9305e-04 - my_r2: 0.7789 - val_loss: 3.0907e-04 - val_my_r2: 0.8924\n",
      "Epoch 192/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.3646e-04 - my_r2: 0.7072 - val_loss: 3.0688e-04 - val_my_r2: 0.8935\n",
      "Epoch 193/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.7952e-04 - my_r2: 0.8604 - val_loss: 3.0367e-04 - val_my_r2: 0.8947\n",
      "Epoch 194/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1110e-04 - my_r2: 0.8827 - val_loss: 3.0056e-04 - val_my_r2: 0.8957\n",
      "Epoch 195/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.8940e-04 - my_r2: 0.8389 - val_loss: 3.0060e-04 - val_my_r2: 0.8956\n",
      "Epoch 196/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.7006e-04 - my_r2: 0.7502 - val_loss: 2.9909e-04 - val_my_r2: 0.8966\n",
      "Epoch 197/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1166e-04 - my_r2: 0.7239 - val_loss: 2.9712e-04 - val_my_r2: 0.8975\n",
      "Epoch 198/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1756e-04 - my_r2: 0.8687 - val_loss: 2.9348e-04 - val_my_r2: 0.8984\n",
      "Epoch 199/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8609e-04 - my_r2: 0.8875 - val_loss: 2.9184e-04 - val_my_r2: 0.8988\n",
      "Epoch 200/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 4.3513e-04 - my_r2: 0.9119 - val_loss: 2.9265e-04 - val_my_r2: 0.8994\n",
      "Epoch 201/2000\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 6.0226e-04 - my_r2: 0.8686 - val_loss: 2.9504e-04 - val_my_r2: 0.8986\n",
      "Epoch 202/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.0552e-04 - my_r2: 0.8899 - val_loss: 3.0160e-04 - val_my_r2: 0.8971\n",
      "Epoch 203/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1829e-04 - my_r2: 0.8308 - val_loss: 2.8980e-04 - val_my_r2: 0.9016\n",
      "Epoch 204/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1297e-04 - my_r2: 0.5969 - val_loss: 2.8509e-04 - val_my_r2: 0.9016\n",
      "Epoch 205/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5324e-04 - my_r2: 0.8585 - val_loss: 2.8441e-04 - val_my_r2: 0.9015\n",
      "Epoch 206/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4679e-04 - my_r2: 0.8954 - val_loss: 2.8502e-04 - val_my_r2: 0.9007\n",
      "Epoch 207/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6807e-04 - my_r2: 0.8810 - val_loss: 2.7889e-04 - val_my_r2: 0.9023\n",
      "Epoch 208/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9170e-04 - my_r2: 0.8745 - val_loss: 2.7552e-04 - val_my_r2: 0.9043\n",
      "Epoch 209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9171e-04 - my_r2: 0.8417 - val_loss: 2.7888e-04 - val_my_r2: 0.9046\n",
      "Epoch 210/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.7742e-04 - my_r2: 0.8101 - val_loss: 2.7472e-04 - val_my_r2: 0.9063\n",
      "Epoch 211/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2973e-04 - my_r2: 0.8731 - val_loss: 2.7306e-04 - val_my_r2: 0.9058\n",
      "Epoch 212/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7797e-04 - my_r2: 0.9037 - val_loss: 2.7065e-04 - val_my_r2: 0.9065\n",
      "Epoch 213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6394e-04 - my_r2: 0.8832 - val_loss: 2.6994e-04 - val_my_r2: 0.9066\n",
      "Epoch 214/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2311e-04 - my_r2: 0.8750 - val_loss: 2.7398e-04 - val_my_r2: 0.9051\n",
      "Epoch 215/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6519e-04 - my_r2: 0.8663 - val_loss: 2.6807e-04 - val_my_r2: 0.9071\n",
      "Epoch 216/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6312e-04 - my_r2: 0.7630 - val_loss: 2.6804e-04 - val_my_r2: 0.9063\n",
      "Epoch 217/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4916e-04 - my_r2: 0.9076 - val_loss: 2.6460e-04 - val_my_r2: 0.9082\n",
      "Epoch 218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1332e-04 - my_r2: 0.8676 - val_loss: 2.6210e-04 - val_my_r2: 0.9098\n",
      "Epoch 219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1937e-04 - my_r2: 0.8322 - val_loss: 2.5884e-04 - val_my_r2: 0.9107\n",
      "Epoch 220/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1679e-04 - my_r2: 0.8164 - val_loss: 2.5775e-04 - val_my_r2: 0.9122\n",
      "Epoch 221/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9829e-04 - my_r2: 0.8977 - val_loss: 2.5808e-04 - val_my_r2: 0.9125\n",
      "Epoch 222/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5057e-04 - my_r2: 0.8767 - val_loss: 2.5536e-04 - val_my_r2: 0.9126\n",
      "Epoch 223/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2009e-04 - my_r2: 0.8843 - val_loss: 2.5317e-04 - val_my_r2: 0.9133\n",
      "Epoch 224/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8036e-04 - my_r2: 0.8962 - val_loss: 2.5212e-04 - val_my_r2: 0.9147\n",
      "Epoch 225/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3574e-04 - my_r2: 0.8441 - val_loss: 2.4897e-04 - val_my_r2: 0.9158\n",
      "Epoch 226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9440e-04 - my_r2: 0.8552 - val_loss: 2.5307e-04 - val_my_r2: 0.9145\n",
      "Epoch 227/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8021e-04 - my_r2: 0.8774 - val_loss: 2.4995e-04 - val_my_r2: 0.9161\n",
      "Epoch 228/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3757e-04 - my_r2: 0.8737 - val_loss: 2.4939e-04 - val_my_r2: 0.9167\n",
      "Epoch 229/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9259e-04 - my_r2: 0.8800 - val_loss: 2.5263e-04 - val_my_r2: 0.9156\n",
      "Epoch 230/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5193e-04 - my_r2: 0.8952 - val_loss: 2.4236e-04 - val_my_r2: 0.9177\n",
      "Epoch 231/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0181e-04 - my_r2: 0.8538 - val_loss: 2.4025e-04 - val_my_r2: 0.9182\n",
      "Epoch 232/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1451e-04 - my_r2: 0.8551 - val_loss: 2.3979e-04 - val_my_r2: 0.9184\n",
      "Epoch 233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9768e-04 - my_r2: 0.7563 - val_loss: 2.3954e-04 - val_my_r2: 0.9188\n",
      "Epoch 234/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6352e-04 - my_r2: 0.8815 - val_loss: 2.3719e-04 - val_my_r2: 0.9194\n",
      "Epoch 235/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.3715e-04 - my_r2: 0.8652 - val_loss: 2.3557e-04 - val_my_r2: 0.9193\n",
      "Epoch 236/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9631e-04 - my_r2: 0.8287 - val_loss: 2.3539e-04 - val_my_r2: 0.9185\n",
      "Epoch 237/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0112e-04 - my_r2: 0.8326 - val_loss: 2.3213e-04 - val_my_r2: 0.9204\n",
      "Epoch 238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5086e-04 - my_r2: 0.8574 - val_loss: 2.3031e-04 - val_my_r2: 0.9207\n",
      "Epoch 239/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4562e-04 - my_r2: 0.9121 - val_loss: 2.2919e-04 - val_my_r2: 0.9207\n",
      "Epoch 240/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9922e-04 - my_r2: 0.8586 - val_loss: 2.2695e-04 - val_my_r2: 0.9215\n",
      "Epoch 241/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1811e-04 - my_r2: 0.8898 - val_loss: 2.2545e-04 - val_my_r2: 0.9215\n",
      "Epoch 242/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1048e-04 - my_r2: 0.8368 - val_loss: 2.2699e-04 - val_my_r2: 0.9215\n",
      "Epoch 243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5803e-04 - my_r2: 0.8835 - val_loss: 2.3053e-04 - val_my_r2: 0.9212\n",
      "Epoch 244/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6270e-04 - my_r2: 0.8738 - val_loss: 2.2841e-04 - val_my_r2: 0.9221\n",
      "Epoch 245/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4742e-04 - my_r2: 0.8812 - val_loss: 2.2194e-04 - val_my_r2: 0.9237\n",
      "Epoch 246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4040e-04 - my_r2: 0.8767 - val_loss: 2.1828e-04 - val_my_r2: 0.9241\n",
      "Epoch 247/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8930e-04 - my_r2: 0.6743 - val_loss: 2.1793e-04 - val_my_r2: 0.9243\n",
      "Epoch 248/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0680e-04 - my_r2: 0.8791 - val_loss: 2.1956e-04 - val_my_r2: 0.9230\n",
      "Epoch 249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1017e-04 - my_r2: 0.8718 - val_loss: 2.2001e-04 - val_my_r2: 0.9230\n",
      "Epoch 250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4757e-04 - my_r2: 0.8887 - val_loss: 2.1372e-04 - val_my_r2: 0.9275\n",
      "Epoch 251/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2224e-04 - my_r2: 0.7948 - val_loss: 2.1324e-04 - val_my_r2: 0.9275\n",
      "Epoch 252/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0238e-04 - my_r2: 0.8979 - val_loss: 2.1167e-04 - val_my_r2: 0.9280\n",
      "Epoch 253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3167e-04 - my_r2: 0.8999 - val_loss: 2.1287e-04 - val_my_r2: 0.9282\n",
      "Epoch 254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3774e-04 - my_r2: 0.6431 - val_loss: 2.0934e-04 - val_my_r2: 0.9285\n",
      "Epoch 255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7069e-04 - my_r2: 0.8599 - val_loss: 2.0909e-04 - val_my_r2: 0.9285\n",
      "Epoch 256/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5135e-04 - my_r2: 0.8586 - val_loss: 2.0722e-04 - val_my_r2: 0.9283\n",
      "Epoch 257/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1944e-04 - my_r2: 0.8719 - val_loss: 2.0417e-04 - val_my_r2: 0.9290\n",
      "Epoch 258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6039e-04 - my_r2: 0.8845 - val_loss: 2.0263e-04 - val_my_r2: 0.9297\n",
      "Epoch 259/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5038e-04 - my_r2: 0.8511 - val_loss: 2.0236e-04 - val_my_r2: 0.9290\n",
      "Epoch 260/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3992e-04 - my_r2: 0.8986 - val_loss: 2.0250e-04 - val_my_r2: 0.9288\n",
      "Epoch 261/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3450e-04 - my_r2: 0.8973 - val_loss: 2.0195e-04 - val_my_r2: 0.9290\n",
      "Epoch 262/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4588e-04 - my_r2: 0.8883 - val_loss: 2.0148e-04 - val_my_r2: 0.9291\n",
      "Epoch 263/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6352e-04 - my_r2: 0.8791 - val_loss: 1.9838e-04 - val_my_r2: 0.9303\n",
      "Epoch 264/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.5329e-04 - my_r2: 0.4264 - val_loss: 1.9911e-04 - val_my_r2: 0.9304\n",
      "Epoch 265/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0482e-04 - my_r2: 0.8417 - val_loss: 1.9477e-04 - val_my_r2: 0.9316\n",
      "Epoch 266/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9104e-04 - my_r2: 0.8902 - val_loss: 1.9403e-04 - val_my_r2: 0.9324\n",
      "Epoch 267/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1738e-04 - my_r2: 0.8533 - val_loss: 1.9453e-04 - val_my_r2: 0.9331\n",
      "Epoch 268/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2121e-04 - my_r2: 0.7985 - val_loss: 1.9512e-04 - val_my_r2: 0.9337\n",
      "Epoch 269/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4487e-04 - my_r2: 0.8096 - val_loss: 1.9102e-04 - val_my_r2: 0.9343\n",
      "Epoch 270/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2315e-04 - my_r2: 0.8250 - val_loss: 1.9018e-04 - val_my_r2: 0.9336\n",
      "Epoch 271/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0917e-04 - my_r2: 0.8682 - val_loss: 1.9298e-04 - val_my_r2: 0.9317\n",
      "Epoch 272/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 4.3514e-04 - my_r2: 0.8882 - val_loss: 1.9808e-04 - val_my_r2: 0.9310\n",
      "Epoch 273/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7042e-04 - my_r2: 0.8099 - val_loss: 1.9243e-04 - val_my_r2: 0.9348\n",
      "Epoch 274/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3398e-04 - my_r2: 0.8819 - val_loss: 1.8364e-04 - val_my_r2: 0.9372\n",
      "Epoch 275/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8759e-04 - my_r2: 0.8567 - val_loss: 1.8337e-04 - val_my_r2: 0.9374\n",
      "Epoch 276/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8262e-04 - my_r2: 0.8480 - val_loss: 1.8592e-04 - val_my_r2: 0.9371\n",
      "Epoch 277/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2983e-04 - my_r2: 0.9091 - val_loss: 1.8034e-04 - val_my_r2: 0.9385\n",
      "Epoch 278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3310e-04 - my_r2: 0.8709 - val_loss: 1.7920e-04 - val_my_r2: 0.9392\n",
      "Epoch 279/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4447e-04 - my_r2: 0.8853 - val_loss: 1.7839e-04 - val_my_r2: 0.9391\n",
      "Epoch 280/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1364e-04 - my_r2: 0.8892 - val_loss: 1.7693e-04 - val_my_r2: 0.9394\n",
      "Epoch 281/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2503e-04 - my_r2: 0.8609 - val_loss: 1.8107e-04 - val_my_r2: 0.9383\n",
      "Epoch 282/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0422e-04 - my_r2: 0.8762 - val_loss: 1.7843e-04 - val_my_r2: 0.9403\n",
      "Epoch 283/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6297e-04 - my_r2: 0.8934 - val_loss: 1.7458e-04 - val_my_r2: 0.9414\n",
      "Epoch 284/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2512e-04 - my_r2: 0.8126 - val_loss: 1.7480e-04 - val_my_r2: 0.9406\n",
      "Epoch 285/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9620e-04 - my_r2: 0.8820 - val_loss: 1.7082e-04 - val_my_r2: 0.9424\n",
      "Epoch 286/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 4.0082e-04 - my_r2: 0.9015 - val_loss: 1.7137e-04 - val_my_r2: 0.9415\n",
      "Epoch 287/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 4.4922e-04 - my_r2: 0.8487 - val_loss: 1.7303e-04 - val_my_r2: 0.9418\n",
      "Epoch 288/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0309e-04 - my_r2: 0.8356 - val_loss: 1.6855e-04 - val_my_r2: 0.9421\n",
      "Epoch 289/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4842e-04 - my_r2: 0.8690 - val_loss: 1.6954e-04 - val_my_r2: 0.9412\n",
      "Epoch 290/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6830e-04 - my_r2: 0.8888 - val_loss: 1.7334e-04 - val_my_r2: 0.9407\n",
      "Epoch 291/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2448e-04 - my_r2: 0.8648 - val_loss: 1.6915e-04 - val_my_r2: 0.9416\n",
      "Epoch 292/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2624e-04 - my_r2: 0.8771 - val_loss: 1.7063e-04 - val_my_r2: 0.9420\n",
      "Epoch 293/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7030e-04 - my_r2: 0.8742 - val_loss: 1.6815e-04 - val_my_r2: 0.9423\n",
      "Epoch 294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5830e-04 - my_r2: 0.8910 - val_loss: 1.6643e-04 - val_my_r2: 0.9430\n",
      "Epoch 295/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2268e-04 - my_r2: 0.9350 - val_loss: 1.6314e-04 - val_my_r2: 0.9438\n",
      "Epoch 296/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8584e-04 - my_r2: 0.9074 - val_loss: 1.6023e-04 - val_my_r2: 0.9441\n",
      "Epoch 297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6349e-04 - my_r2: 0.8661 - val_loss: 1.5917e-04 - val_my_r2: 0.9448\n",
      "Epoch 298/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6690e-04 - my_r2: 0.9038 - val_loss: 1.5902e-04 - val_my_r2: 0.9448\n",
      "Epoch 299/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2605e-04 - my_r2: 0.8289 - val_loss: 1.5772e-04 - val_my_r2: 0.9453\n",
      "Epoch 300/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7289e-04 - my_r2: 0.9245 - val_loss: 1.5631e-04 - val_my_r2: 0.9459\n",
      "Epoch 301/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9067e-04 - my_r2: 0.8683 - val_loss: 1.5749e-04 - val_my_r2: 0.9452\n",
      "Epoch 302/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6468e-04 - my_r2: 0.8716 - val_loss: 1.5643e-04 - val_my_r2: 0.9460\n",
      "Epoch 303/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7112e-04 - my_r2: 0.8054 - val_loss: 1.5745e-04 - val_my_r2: 0.9461\n",
      "Epoch 304/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0513e-04 - my_r2: 0.9102 - val_loss: 1.5512e-04 - val_my_r2: 0.9471\n",
      "Epoch 305/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5619e-04 - my_r2: 0.8875 - val_loss: 1.6528e-04 - val_my_r2: 0.9445\n",
      "Epoch 306/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6483e-04 - my_r2: 0.9093 - val_loss: 1.5577e-04 - val_my_r2: 0.9477\n",
      "Epoch 307/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4527e-04 - my_r2: 0.8710 - val_loss: 1.6186e-04 - val_my_r2: 0.9461\n",
      "Epoch 308/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6202e-04 - my_r2: 0.9002 - val_loss: 1.6138e-04 - val_my_r2: 0.9458\n",
      "Epoch 309/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8038e-04 - my_r2: 0.8868 - val_loss: 1.5329e-04 - val_my_r2: 0.9481\n",
      "Epoch 310/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2105e-04 - my_r2: 0.8855 - val_loss: 1.4832e-04 - val_my_r2: 0.9488\n",
      "Epoch 311/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9647e-04 - my_r2: 0.9208 - val_loss: 1.4675e-04 - val_my_r2: 0.9495\n",
      "Epoch 312/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2794e-04 - my_r2: 0.7954 - val_loss: 1.4533e-04 - val_my_r2: 0.9497\n",
      "Epoch 313/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9796e-04 - my_r2: 0.8771 - val_loss: 1.4887e-04 - val_my_r2: 0.9479\n",
      "Epoch 314/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4084e-04 - my_r2: 0.8981 - val_loss: 1.5077e-04 - val_my_r2: 0.9485\n",
      "Epoch 315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3831e-04 - my_r2: 0.9273 - val_loss: 1.4773e-04 - val_my_r2: 0.9493\n",
      "Epoch 316/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6266e-04 - my_r2: 0.8579 - val_loss: 1.4876e-04 - val_my_r2: 0.9493\n",
      "Epoch 317/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9552e-04 - my_r2: 0.8360 - val_loss: 1.4904e-04 - val_my_r2: 0.9500\n",
      "Epoch 318/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9163e-04 - my_r2: 0.8898 - val_loss: 1.4438e-04 - val_my_r2: 0.9506\n",
      "Epoch 319/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8663e-04 - my_r2: 0.8851 - val_loss: 1.4459e-04 - val_my_r2: 0.9507\n",
      "Epoch 320/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0318e-04 - my_r2: 0.9110 - val_loss: 1.4109e-04 - val_my_r2: 0.9513\n",
      "Epoch 321/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2579e-04 - my_r2: 0.8037 - val_loss: 1.4065e-04 - val_my_r2: 0.9503\n",
      "Epoch 322/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7449e-04 - my_r2: 0.8391 - val_loss: 1.4232e-04 - val_my_r2: 0.9494\n",
      "Epoch 323/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6988e-04 - my_r2: 0.9024 - val_loss: 1.4149e-04 - val_my_r2: 0.9511\n",
      "Epoch 324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0591e-04 - my_r2: 0.8879 - val_loss: 1.3906e-04 - val_my_r2: 0.9522\n",
      "Epoch 325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9742e-04 - my_r2: 0.8951 - val_loss: 1.3685e-04 - val_my_r2: 0.9526\n",
      "Epoch 326/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5964e-04 - my_r2: 0.8857 - val_loss: 1.3587e-04 - val_my_r2: 0.9531\n",
      "Epoch 327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4294e-04 - my_r2: 0.9107 - val_loss: 1.3460e-04 - val_my_r2: 0.9538\n",
      "Epoch 328/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5436e-04 - my_r2: 0.9189 - val_loss: 1.3315e-04 - val_my_r2: 0.9534\n",
      "Epoch 329/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3237e-04 - my_r2: 0.9258 - val_loss: 1.3060e-04 - val_my_r2: 0.9547\n",
      "Epoch 330/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0312e-04 - my_r2: 0.8913 - val_loss: 1.3042e-04 - val_my_r2: 0.9552\n",
      "Epoch 331/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4628e-04 - my_r2: 0.9124 - val_loss: 1.3292e-04 - val_my_r2: 0.9551\n",
      "Epoch 332/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6561e-04 - my_r2: 0.8793 - val_loss: 1.3454e-04 - val_my_r2: 0.9542\n",
      "Epoch 333/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9835e-04 - my_r2: 0.8148 - val_loss: 1.2918e-04 - val_my_r2: 0.9558\n",
      "Epoch 334/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1980e-04 - my_r2: 0.9024 - val_loss: 1.3118e-04 - val_my_r2: 0.9555\n",
      "Epoch 335/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5146e-04 - my_r2: 0.8984 - val_loss: 1.3633e-04 - val_my_r2: 0.9537\n",
      "Epoch 336/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9676e-04 - my_r2: 0.8803 - val_loss: 1.2770e-04 - val_my_r2: 0.9561\n",
      "Epoch 337/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2478e-04 - my_r2: 0.9086 - val_loss: 1.3289e-04 - val_my_r2: 0.9546\n",
      "Epoch 338/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3231e-04 - my_r2: 0.8856 - val_loss: 1.2799e-04 - val_my_r2: 0.9564\n",
      "Epoch 339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7341e-04 - my_r2: 0.9398 - val_loss: 1.2392e-04 - val_my_r2: 0.9576\n",
      "Epoch 340/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1844e-04 - my_r2: 0.9118 - val_loss: 1.2455e-04 - val_my_r2: 0.9579\n",
      "Epoch 341/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7534e-04 - my_r2: 0.8966 - val_loss: 1.2407e-04 - val_my_r2: 0.9578\n",
      "Epoch 342/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7926e-04 - my_r2: 0.8833 - val_loss: 1.2443e-04 - val_my_r2: 0.9575\n",
      "Epoch 343/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7526e-04 - my_r2: 0.8957 - val_loss: 1.2530e-04 - val_my_r2: 0.9569\n",
      "Epoch 344/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8996e-04 - my_r2: 0.8701 - val_loss: 1.2391e-04 - val_my_r2: 0.9577\n",
      "Epoch 345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9640e-04 - my_r2: 0.8364 - val_loss: 1.2237e-04 - val_my_r2: 0.9586\n",
      "Epoch 346/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5470e-04 - my_r2: 0.9067 - val_loss: 1.2193e-04 - val_my_r2: 0.9590\n",
      "Epoch 347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7076e-04 - my_r2: 0.8258 - val_loss: 1.2169e-04 - val_my_r2: 0.9590\n",
      "Epoch 348/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2356e-04 - my_r2: 0.8990 - val_loss: 1.2056e-04 - val_my_r2: 0.9595\n",
      "Epoch 349/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1573e-04 - my_r2: 0.8542 - val_loss: 1.2084e-04 - val_my_r2: 0.9596\n",
      "Epoch 350/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5441e-04 - my_r2: 0.9125 - val_loss: 1.1959e-04 - val_my_r2: 0.9587\n",
      "Epoch 351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6942e-04 - my_r2: 0.8222 - val_loss: 1.1602e-04 - val_my_r2: 0.9594\n",
      "Epoch 352/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0733e-04 - my_r2: 0.8709 - val_loss: 1.1867e-04 - val_my_r2: 0.9582\n",
      "Epoch 353/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8042e-04 - my_r2: 0.9031 - val_loss: 1.2097e-04 - val_my_r2: 0.9579\n",
      "Epoch 354/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0524e-04 - my_r2: 0.8833 - val_loss: 1.1881e-04 - val_my_r2: 0.9577\n",
      "Epoch 355/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8729e-04 - my_r2: 0.8964 - val_loss: 1.2297e-04 - val_my_r2: 0.9563\n",
      "Epoch 356/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5447e-04 - my_r2: 0.8493 - val_loss: 1.1722e-04 - val_my_r2: 0.9583\n",
      "Epoch 357/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1227e-04 - my_r2: 0.8723 - val_loss: 1.1522e-04 - val_my_r2: 0.9590\n",
      "Epoch 358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2923e-04 - my_r2: 0.9226 - val_loss: 1.1318e-04 - val_my_r2: 0.9598\n",
      "Epoch 359/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9076e-04 - my_r2: 0.8609 - val_loss: 1.2470e-04 - val_my_r2: 0.9562\n",
      "Epoch 360/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3122e-04 - my_r2: 0.8975 - val_loss: 1.1598e-04 - val_my_r2: 0.9598\n",
      "Epoch 361/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.3944e-04 - my_r2: 0.9176 - val_loss: 1.1242e-04 - val_my_r2: 0.9601\n",
      "Epoch 362/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6402e-04 - my_r2: 0.9446 - val_loss: 1.1858e-04 - val_my_r2: 0.9575\n",
      "Epoch 363/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5916e-04 - my_r2: 0.8852 - val_loss: 1.1543e-04 - val_my_r2: 0.9590\n",
      "Epoch 364/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3400e-04 - my_r2: 0.8312 - val_loss: 1.1574e-04 - val_my_r2: 0.9597\n",
      "Epoch 365/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2982e-04 - my_r2: 0.9154 - val_loss: 1.1218e-04 - val_my_r2: 0.9609\n",
      "Epoch 366/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2131e-04 - my_r2: 0.9274 - val_loss: 1.1090e-04 - val_my_r2: 0.9620\n",
      "Epoch 367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4999e-04 - my_r2: 0.9342 - val_loss: 1.0993e-04 - val_my_r2: 0.9618\n",
      "Epoch 368/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3555e-04 - my_r2: 0.8938 - val_loss: 1.0962e-04 - val_my_r2: 0.9618\n",
      "Epoch 369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9191e-04 - my_r2: 0.9074 - val_loss: 1.1767e-04 - val_my_r2: 0.9598\n",
      "Epoch 370/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0905e-04 - my_r2: 0.9235 - val_loss: 1.2743e-04 - val_my_r2: 0.9568\n",
      "Epoch 371/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5236e-04 - my_r2: 0.8885 - val_loss: 1.1684e-04 - val_my_r2: 0.9597\n",
      "Epoch 372/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5430e-04 - my_r2: 0.8725 - val_loss: 1.0546e-04 - val_my_r2: 0.9626\n",
      "Epoch 373/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2045e-04 - my_r2: 0.9203 - val_loss: 1.0485e-04 - val_my_r2: 0.9630\n",
      "Epoch 374/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7575e-04 - my_r2: 0.8943 - val_loss: 1.0568e-04 - val_my_r2: 0.9630\n",
      "Epoch 375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1888e-04 - my_r2: 0.9333 - val_loss: 1.0647e-04 - val_my_r2: 0.9629\n",
      "Epoch 376/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1998e-04 - my_r2: 0.8652 - val_loss: 1.0438e-04 - val_my_r2: 0.9633\n",
      "Epoch 377/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9543e-04 - my_r2: 0.9295 - val_loss: 1.0272e-04 - val_my_r2: 0.9638\n",
      "Epoch 378/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2042e-04 - my_r2: 0.8700 - val_loss: 1.0527e-04 - val_my_r2: 0.9641\n",
      "Epoch 379/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1111e-04 - my_r2: 0.8203 - val_loss: 1.0165e-04 - val_my_r2: 0.9653\n",
      "Epoch 380/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4215e-04 - my_r2: 0.9329 - val_loss: 1.0314e-04 - val_my_r2: 0.9642\n",
      "Epoch 381/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3384e-04 - my_r2: 0.9326 - val_loss: 1.0674e-04 - val_my_r2: 0.9632\n",
      "Epoch 382/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6492e-04 - my_r2: 0.9406 - val_loss: 1.0654e-04 - val_my_r2: 0.9629\n",
      "Epoch 383/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2286e-04 - my_r2: 0.9123 - val_loss: 1.0395e-04 - val_my_r2: 0.9632\n",
      "Epoch 384/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9315e-04 - my_r2: 0.9058 - val_loss: 9.9901e-05 - val_my_r2: 0.9655\n",
      "Epoch 385/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0504e-04 - my_r2: 0.7833 - val_loss: 1.0240e-04 - val_my_r2: 0.9639\n",
      "Epoch 386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6182e-04 - my_r2: 0.8938 - val_loss: 1.0002e-04 - val_my_r2: 0.9648\n",
      "Epoch 387/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0820e-04 - my_r2: 0.9138 - val_loss: 9.8182e-05 - val_my_r2: 0.9666\n",
      "Epoch 388/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5248e-04 - my_r2: 0.9247 - val_loss: 1.0477e-04 - val_my_r2: 0.9650\n",
      "Epoch 389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4044e-04 - my_r2: 0.9119 - val_loss: 9.9253e-05 - val_my_r2: 0.9668\n",
      "Epoch 390/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9330e-04 - my_r2: 0.9322 - val_loss: 9.6812e-05 - val_my_r2: 0.9671\n",
      "Epoch 391/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2985e-04 - my_r2: 0.8509 - val_loss: 1.0463e-04 - val_my_r2: 0.9635\n",
      "Epoch 392/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0624e-04 - my_r2: 0.9156 - val_loss: 1.0744e-04 - val_my_r2: 0.9627\n",
      "Epoch 393/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4538e-04 - my_r2: 0.8741 - val_loss: 9.7018e-05 - val_my_r2: 0.9664\n",
      "Epoch 394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6024e-04 - my_r2: 0.9020 - val_loss: 9.5400e-05 - val_my_r2: 0.9671\n",
      "Epoch 395/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5364e-04 - my_r2: 0.7623 - val_loss: 9.6242e-05 - val_my_r2: 0.9669\n",
      "Epoch 396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1518e-04 - my_r2: 0.9213 - val_loss: 9.7314e-05 - val_my_r2: 0.9668\n",
      "Epoch 397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9738e-04 - my_r2: 0.9353 - val_loss: 9.6869e-05 - val_my_r2: 0.9668\n",
      "Epoch 398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6544e-04 - my_r2: 0.9158 - val_loss: 9.7026e-05 - val_my_r2: 0.9669\n",
      "Epoch 399/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2524e-04 - my_r2: 0.9138 - val_loss: 9.6438e-05 - val_my_r2: 0.9667\n",
      "Epoch 400/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5176e-04 - my_r2: 0.9174 - val_loss: 9.3511e-05 - val_my_r2: 0.9680\n",
      "Epoch 401/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3236e-04 - my_r2: 0.8943 - val_loss: 9.1807e-05 - val_my_r2: 0.9681\n",
      "Epoch 402/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6427e-04 - my_r2: 0.9147 - val_loss: 9.0254e-05 - val_my_r2: 0.9688\n",
      "Epoch 403/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5695e-04 - my_r2: 0.8852 - val_loss: 9.0423e-05 - val_my_r2: 0.9687\n",
      "Epoch 404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0687e-04 - my_r2: 0.9037 - val_loss: 9.0299e-05 - val_my_r2: 0.9680\n",
      "Epoch 405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3963e-04 - my_r2: 0.8841 - val_loss: 8.8202e-05 - val_my_r2: 0.9690\n",
      "Epoch 406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1940e-04 - my_r2: 0.9001 - val_loss: 8.8451e-05 - val_my_r2: 0.9684\n",
      "Epoch 407/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1484e-04 - my_r2: 0.9247 - val_loss: 8.8683e-05 - val_my_r2: 0.9689\n",
      "Epoch 408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0106e-04 - my_r2: 0.9212 - val_loss: 9.0435e-05 - val_my_r2: 0.9686\n",
      "Epoch 409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1846e-04 - my_r2: 0.9132 - val_loss: 8.7348e-05 - val_my_r2: 0.9695\n",
      "Epoch 410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2208e-04 - my_r2: 0.8988 - val_loss: 8.9456e-05 - val_my_r2: 0.9689\n",
      "Epoch 411/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1625e-04 - my_r2: 0.8335 - val_loss: 8.6646e-05 - val_my_r2: 0.9691\n",
      "Epoch 412/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4409e-04 - my_r2: 0.8664 - val_loss: 8.6974e-05 - val_my_r2: 0.9689\n",
      "Epoch 413/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6555e-04 - my_r2: 0.8216 - val_loss: 8.8143e-05 - val_my_r2: 0.9685\n",
      "Epoch 414/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0007e-04 - my_r2: 0.9052 - val_loss: 8.9963e-05 - val_my_r2: 0.9675\n",
      "Epoch 415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0175e-04 - my_r2: 0.8471 - val_loss: 8.4564e-05 - val_my_r2: 0.9707\n",
      "Epoch 416/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4519e-04 - my_r2: 0.9177 - val_loss: 8.1296e-05 - val_my_r2: 0.9714\n",
      "Epoch 417/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.1731e-04 - my_r2: 0.7216 - val_loss: 8.2321e-05 - val_my_r2: 0.9704\n",
      "Epoch 418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2976e-04 - my_r2: 0.9207 - val_loss: 8.5040e-05 - val_my_r2: 0.9700\n",
      "Epoch 419/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3729e-04 - my_r2: 0.8924 - val_loss: 9.4820e-05 - val_my_r2: 0.9662\n",
      "Epoch 420/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6359e-04 - my_r2: 0.9201 - val_loss: 8.9725e-05 - val_my_r2: 0.9671\n",
      "Epoch 421/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4946e-04 - my_r2: 0.9296 - val_loss: 8.5129e-05 - val_my_r2: 0.9685\n",
      "Epoch 422/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7659e-04 - my_r2: 0.9154 - val_loss: 8.3911e-05 - val_my_r2: 0.9693\n",
      "Epoch 423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0200e-04 - my_r2: 0.9267 - val_loss: 8.0193e-05 - val_my_r2: 0.9708\n",
      "Epoch 424/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0155e-04 - my_r2: 0.8787 - val_loss: 8.2898e-05 - val_my_r2: 0.9696\n",
      "Epoch 425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2386e-04 - my_r2: 0.9343 - val_loss: 8.7467e-05 - val_my_r2: 0.9682\n",
      "Epoch 426/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5271e-04 - my_r2: 0.9241 - val_loss: 8.3777e-05 - val_my_r2: 0.9694\n",
      "Epoch 427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9871e-04 - my_r2: 0.9060 - val_loss: 8.1573e-05 - val_my_r2: 0.9706\n",
      "Epoch 428/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0593e-04 - my_r2: 0.9284 - val_loss: 7.8616e-05 - val_my_r2: 0.9720\n",
      "Epoch 429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9082e-04 - my_r2: 0.8819 - val_loss: 7.8548e-05 - val_my_r2: 0.9724\n",
      "Epoch 430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6101e-04 - my_r2: 0.9264 - val_loss: 8.3831e-05 - val_my_r2: 0.9710\n",
      "Epoch 431/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2787e-04 - my_r2: 0.9111 - val_loss: 8.3601e-05 - val_my_r2: 0.9723\n",
      "Epoch 432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6917e-04 - my_r2: 0.9085 - val_loss: 7.8841e-05 - val_my_r2: 0.9728\n",
      "Epoch 433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8943e-04 - my_r2: 0.9107 - val_loss: 7.8155e-05 - val_my_r2: 0.9733\n",
      "Epoch 434/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8953e-04 - my_r2: 0.8922 - val_loss: 7.6430e-05 - val_my_r2: 0.9738\n",
      "Epoch 435/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1541e-04 - my_r2: 0.9023 - val_loss: 7.7162e-05 - val_my_r2: 0.9735\n",
      "Epoch 436/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0406e-04 - my_r2: 0.8715 - val_loss: 7.7890e-05 - val_my_r2: 0.9731\n",
      "Epoch 437/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4605e-04 - my_r2: 0.8754 - val_loss: 7.5672e-05 - val_my_r2: 0.9738\n",
      "Epoch 438/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1069e-04 - my_r2: 0.8694 - val_loss: 7.6259e-05 - val_my_r2: 0.9725\n",
      "Epoch 439/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2153e-04 - my_r2: 0.9116 - val_loss: 7.2400e-05 - val_my_r2: 0.9732\n",
      "Epoch 440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8028e-04 - my_r2: 0.8967 - val_loss: 7.3446e-05 - val_my_r2: 0.9728\n",
      "Epoch 441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0544e-04 - my_r2: 0.7808 - val_loss: 8.1799e-05 - val_my_r2: 0.9700\n",
      "Epoch 442/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6272e-04 - my_r2: 0.9222 - val_loss: 8.1758e-05 - val_my_r2: 0.9704\n",
      "Epoch 443/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2437e-04 - my_r2: 0.9250 - val_loss: 7.3057e-05 - val_my_r2: 0.9735\n",
      "Epoch 444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4001e-04 - my_r2: 0.9032 - val_loss: 7.1262e-05 - val_my_r2: 0.9747\n",
      "Epoch 445/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3384e-04 - my_r2: 0.9377 - val_loss: 7.2006e-05 - val_my_r2: 0.9749\n",
      "Epoch 446/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3787e-04 - my_r2: 0.9326 - val_loss: 7.0335e-05 - val_my_r2: 0.9746\n",
      "Epoch 447/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1586e-04 - my_r2: 0.9108 - val_loss: 7.5627e-05 - val_my_r2: 0.9721\n",
      "Epoch 448/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2527e-04 - my_r2: 0.8745 - val_loss: 7.3275e-05 - val_my_r2: 0.9732\n",
      "Epoch 449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7205e-04 - my_r2: 0.8984 - val_loss: 7.3562e-05 - val_my_r2: 0.9731\n",
      "Epoch 450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0502e-04 - my_r2: 0.9002 - val_loss: 7.2749e-05 - val_my_r2: 0.9738\n",
      "Epoch 451/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2637e-04 - my_r2: 0.9066 - val_loss: 7.0483e-05 - val_my_r2: 0.9745\n",
      "Epoch 452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3036e-04 - my_r2: 0.9079 - val_loss: 7.3935e-05 - val_my_r2: 0.9726\n",
      "Epoch 453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6619e-04 - my_r2: 0.8606 - val_loss: 7.2922e-05 - val_my_r2: 0.9744\n",
      "Epoch 454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8948e-04 - my_r2: 0.9022 - val_loss: 6.9565e-05 - val_my_r2: 0.9751\n",
      "Epoch 455/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9849e-04 - my_r2: 0.8952 - val_loss: 6.9103e-05 - val_my_r2: 0.9745\n",
      "Epoch 456/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4720e-04 - my_r2: 0.9270 - val_loss: 7.0335e-05 - val_my_r2: 0.9732\n",
      "Epoch 457/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4496e-04 - my_r2: 0.8821 - val_loss: 7.2787e-05 - val_my_r2: 0.9721\n",
      "Epoch 458/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.7916e-04 - my_r2: 0.9207 - val_loss: 7.3705e-05 - val_my_r2: 0.9737\n",
      "Epoch 459/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 3.0972e-04 - my_r2: 0.9225 - val_loss: 7.5213e-05 - val_my_r2: 0.9739\n",
      "Epoch 460/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2297e-04 - my_r2: 0.9321 - val_loss: 6.6768e-05 - val_my_r2: 0.9764\n",
      "Epoch 461/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1731e-04 - my_r2: 0.8706 - val_loss: 7.0255e-05 - val_my_r2: 0.9732\n",
      "Epoch 462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5792e-04 - my_r2: 0.8732 - val_loss: 7.0623e-05 - val_my_r2: 0.9729\n",
      "Epoch 463/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2718e-04 - my_r2: 0.9260 - val_loss: 6.8650e-05 - val_my_r2: 0.9755\n",
      "Epoch 464/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9023e-04 - my_r2: 0.9101 - val_loss: 6.8458e-05 - val_my_r2: 0.9754\n",
      "Epoch 465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1132e-04 - my_r2: 0.8964 - val_loss: 6.6217e-05 - val_my_r2: 0.9764\n",
      "Epoch 466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4839e-04 - my_r2: 0.8653 - val_loss: 7.2422e-05 - val_my_r2: 0.9747\n",
      "Epoch 467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5450e-04 - my_r2: 0.9171 - val_loss: 7.1465e-05 - val_my_r2: 0.9752\n",
      "Epoch 468/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3754e-04 - my_r2: 0.8814 - val_loss: 6.5313e-05 - val_my_r2: 0.9774\n",
      "Epoch 469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1771e-04 - my_r2: 0.9226 - val_loss: 6.1977e-05 - val_my_r2: 0.9782\n",
      "Epoch 470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0298e-04 - my_r2: 0.9116 - val_loss: 6.1729e-05 - val_my_r2: 0.9783\n",
      "Epoch 471/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8214e-04 - my_r2: 0.9312 - val_loss: 6.3558e-05 - val_my_r2: 0.9774\n",
      "Epoch 472/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1540e-04 - my_r2: 0.8094 - val_loss: 7.3765e-05 - val_my_r2: 0.9729\n",
      "Epoch 473/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0559e-04 - my_r2: 0.8987 - val_loss: 6.1545e-05 - val_my_r2: 0.9783\n",
      "Epoch 474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3137e-04 - my_r2: 0.9176 - val_loss: 6.5714e-05 - val_my_r2: 0.9777\n",
      "Epoch 475/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7256e-04 - my_r2: 0.9219 - val_loss: 6.5035e-05 - val_my_r2: 0.9772\n",
      "Epoch 476/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8018e-04 - my_r2: 0.9158 - val_loss: 6.6103e-05 - val_my_r2: 0.9764\n",
      "Epoch 477/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5122e-04 - my_r2: 0.8867 - val_loss: 6.0213e-05 - val_my_r2: 0.9783\n",
      "Epoch 478/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6557e-04 - my_r2: 0.9143 - val_loss: 5.9195e-05 - val_my_r2: 0.9787\n",
      "Epoch 479/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4395e-04 - my_r2: 0.8702 - val_loss: 6.8818e-05 - val_my_r2: 0.9764\n",
      "Epoch 480/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8485e-04 - my_r2: 0.9166 - val_loss: 8.2964e-05 - val_my_r2: 0.9721\n",
      "Epoch 481/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6445e-04 - my_r2: 0.9182 - val_loss: 6.2576e-05 - val_my_r2: 0.9778\n",
      "Epoch 482/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6331e-04 - my_r2: 0.8807 - val_loss: 6.2661e-05 - val_my_r2: 0.9780\n",
      "Epoch 483/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6731e-04 - my_r2: 0.9095 - val_loss: 7.1669e-05 - val_my_r2: 0.9755\n",
      "Epoch 484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2892e-04 - my_r2: 0.9070 - val_loss: 6.5659e-05 - val_my_r2: 0.9778\n",
      "Epoch 485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1322e-04 - my_r2: 0.9168 - val_loss: 6.4843e-05 - val_my_r2: 0.9776\n",
      "Epoch 486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3565e-04 - my_r2: 0.9084 - val_loss: 6.6517e-05 - val_my_r2: 0.9759\n",
      "Epoch 487/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7176e-04 - my_r2: 0.9155 - val_loss: 6.1149e-05 - val_my_r2: 0.9779\n",
      "Epoch 488/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9113e-04 - my_r2: 0.9185 - val_loss: 6.0767e-05 - val_my_r2: 0.9784\n",
      "Epoch 489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4201e-04 - my_r2: 0.9225 - val_loss: 5.8446e-05 - val_my_r2: 0.9794\n",
      "Epoch 490/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1794e-04 - my_r2: 0.9172 - val_loss: 5.7887e-05 - val_my_r2: 0.9794\n",
      "Epoch 491/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8453e-04 - my_r2: 0.9258 - val_loss: 5.9964e-05 - val_my_r2: 0.9781\n",
      "Epoch 492/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6052e-04 - my_r2: 0.8907 - val_loss: 6.3533e-05 - val_my_r2: 0.9768\n",
      "Epoch 493/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3376e-04 - my_r2: 0.9219 - val_loss: 6.5992e-05 - val_my_r2: 0.9762\n",
      "Epoch 494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6701e-04 - my_r2: 0.8631 - val_loss: 6.6220e-05 - val_my_r2: 0.9771\n",
      "Epoch 495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9909e-04 - my_r2: 0.8421 - val_loss: 6.8047e-05 - val_my_r2: 0.9757\n",
      "Epoch 496/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5543e-04 - my_r2: 0.9322 - val_loss: 5.7250e-05 - val_my_r2: 0.9796\n",
      "Epoch 497/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8780e-04 - my_r2: 0.9091 - val_loss: 6.1020e-05 - val_my_r2: 0.9780\n",
      "Epoch 498/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5545e-04 - my_r2: 0.9008 - val_loss: 5.7670e-05 - val_my_r2: 0.9792\n",
      "Epoch 499/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2822e-04 - my_r2: 0.9302 - val_loss: 5.5039e-05 - val_my_r2: 0.9804\n",
      "Epoch 500/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6155e-04 - my_r2: 0.8405 - val_loss: 5.6753e-05 - val_my_r2: 0.9806\n",
      "Epoch 501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8955e-04 - my_r2: 0.9358 - val_loss: 5.7702e-05 - val_my_r2: 0.9807\n",
      "Epoch 502/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1155e-04 - my_r2: 0.9042 - val_loss: 6.0732e-05 - val_my_r2: 0.9798\n",
      "Epoch 503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0346e-04 - my_r2: 0.9059 - val_loss: 6.1067e-05 - val_my_r2: 0.9795\n",
      "Epoch 504/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0982e-04 - my_r2: 0.9271 - val_loss: 6.4944e-05 - val_my_r2: 0.9780\n",
      "Epoch 505/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8947e-04 - my_r2: 0.9167 - val_loss: 6.1032e-05 - val_my_r2: 0.9793\n",
      "Epoch 506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9275e-04 - my_r2: 0.9405 - val_loss: 5.6865e-05 - val_my_r2: 0.9805\n",
      "Epoch 507/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0792e-04 - my_r2: 0.9057 - val_loss: 5.8403e-05 - val_my_r2: 0.9796\n",
      "Epoch 508/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7741e-04 - my_r2: 0.8818 - val_loss: 5.9570e-05 - val_my_r2: 0.9793\n",
      "Epoch 509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7435e-04 - my_r2: 0.9312 - val_loss: 6.2168e-05 - val_my_r2: 0.9781\n",
      "Epoch 510/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2754e-04 - my_r2: 0.9119 - val_loss: 6.6408e-05 - val_my_r2: 0.9768\n",
      "Epoch 511/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6625e-04 - my_r2: 0.8615 - val_loss: 5.7162e-05 - val_my_r2: 0.9796\n",
      "Epoch 512/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0748e-04 - my_r2: 0.9327 - val_loss: 5.6720e-05 - val_my_r2: 0.9795\n",
      "Epoch 513/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7959e-04 - my_r2: 0.9423 - val_loss: 5.6061e-05 - val_my_r2: 0.9797\n",
      "Epoch 514/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6983e-04 - my_r2: 0.9198 - val_loss: 5.8612e-05 - val_my_r2: 0.9789\n",
      "Epoch 515/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9528e-04 - my_r2: 0.9314 - val_loss: 5.8055e-05 - val_my_r2: 0.9793\n",
      "Epoch 516/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2037e-04 - my_r2: 0.8954 - val_loss: 5.7491e-05 - val_my_r2: 0.9793\n",
      "Epoch 517/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9227e-04 - my_r2: 0.9464 - val_loss: 6.2017e-05 - val_my_r2: 0.9780\n",
      "Epoch 518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1686e-04 - my_r2: 0.8747 - val_loss: 5.3916e-05 - val_my_r2: 0.9804\n",
      "Epoch 519/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0003e-04 - my_r2: 0.9034 - val_loss: 5.5048e-05 - val_my_r2: 0.9799\n",
      "Epoch 520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8110e-04 - my_r2: 0.9203 - val_loss: 5.7866e-05 - val_my_r2: 0.9791\n",
      "Epoch 521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7282e-04 - my_r2: 0.9477 - val_loss: 6.1304e-05 - val_my_r2: 0.9778\n",
      "Epoch 522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3154e-04 - my_r2: 0.9168 - val_loss: 5.7934e-05 - val_my_r2: 0.9793\n",
      "Epoch 523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7317e-04 - my_r2: 0.9084 - val_loss: 5.6899e-05 - val_my_r2: 0.9801\n",
      "Epoch 524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3109e-04 - my_r2: 0.9113 - val_loss: 6.7305e-05 - val_my_r2: 0.9773\n",
      "Epoch 525/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8935e-04 - my_r2: 0.9194 - val_loss: 5.5281e-05 - val_my_r2: 0.9806\n",
      "Epoch 526/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.1338e-04 - my_r2: 0.7825 - val_loss: 5.5886e-05 - val_my_r2: 0.9793\n",
      "Epoch 527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7023e-04 - my_r2: 0.9210 - val_loss: 6.2242e-05 - val_my_r2: 0.9773\n",
      "Epoch 528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7551e-04 - my_r2: 0.9213 - val_loss: 5.2907e-05 - val_my_r2: 0.9806\n",
      "Epoch 529/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7664e-04 - my_r2: 0.9228 - val_loss: 5.2348e-05 - val_my_r2: 0.9812\n",
      "Epoch 530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9785e-04 - my_r2: 0.7980 - val_loss: 5.0662e-05 - val_my_r2: 0.9819\n",
      "Epoch 531/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7976e-04 - my_r2: 0.9080 - val_loss: 5.2563e-05 - val_my_r2: 0.9811\n",
      "Epoch 532/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3248e-04 - my_r2: 0.9234 - val_loss: 5.3729e-05 - val_my_r2: 0.9811\n",
      "Epoch 533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1787e-04 - my_r2: 0.8199 - val_loss: 5.0336e-05 - val_my_r2: 0.9822\n",
      "Epoch 534/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2856e-04 - my_r2: 0.9126 - val_loss: 5.1033e-05 - val_my_r2: 0.9816\n",
      "Epoch 535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4394e-04 - my_r2: 0.8577 - val_loss: 4.8250e-05 - val_my_r2: 0.9825\n",
      "Epoch 536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1740e-04 - my_r2: 0.8996 - val_loss: 4.7040e-05 - val_my_r2: 0.9831\n",
      "Epoch 537/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0195e-04 - my_r2: 0.8932 - val_loss: 5.0518e-05 - val_my_r2: 0.9820\n",
      "Epoch 538/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4105e-04 - my_r2: 0.8972 - val_loss: 5.5658e-05 - val_my_r2: 0.9803\n",
      "Epoch 539/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1247e-04 - my_r2: 0.9302 - val_loss: 5.1130e-05 - val_my_r2: 0.9817\n",
      "Epoch 540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5153e-04 - my_r2: 0.9151 - val_loss: 4.9572e-05 - val_my_r2: 0.9824\n",
      "Epoch 541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0356e-04 - my_r2: 0.9348 - val_loss: 4.9879e-05 - val_my_r2: 0.9823\n",
      "Epoch 542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2073e-04 - my_r2: 0.9009 - val_loss: 4.5391e-05 - val_my_r2: 0.9836\n",
      "Epoch 543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9511e-04 - my_r2: 0.8448 - val_loss: 4.5444e-05 - val_my_r2: 0.9837\n",
      "Epoch 544/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3553e-04 - my_r2: 0.9309 - val_loss: 4.7576e-05 - val_my_r2: 0.9832\n",
      "Epoch 545/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9285e-04 - my_r2: 0.9144 - val_loss: 5.0962e-05 - val_my_r2: 0.9825\n",
      "Epoch 546/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5846e-04 - my_r2: 0.9414 - val_loss: 5.6028e-05 - val_my_r2: 0.9809\n",
      "Epoch 547/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4566e-04 - my_r2: 0.9060 - val_loss: 5.0046e-05 - val_my_r2: 0.9830\n",
      "Epoch 548/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0007e-04 - my_r2: 0.9137 - val_loss: 4.7485e-05 - val_my_r2: 0.9837\n",
      "Epoch 549/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9738e-04 - my_r2: 0.9377 - val_loss: 4.7699e-05 - val_my_r2: 0.9834\n",
      "Epoch 550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5385e-04 - my_r2: 0.9213 - val_loss: 4.7164e-05 - val_my_r2: 0.9836\n",
      "Epoch 551/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0526e-04 - my_r2: 0.9003 - val_loss: 4.6188e-05 - val_my_r2: 0.9838\n",
      "Epoch 552/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5873e-04 - my_r2: 0.9223 - val_loss: 4.7478e-05 - val_my_r2: 0.9835\n",
      "Epoch 553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6673e-04 - my_r2: 0.9400 - val_loss: 5.0788e-05 - val_my_r2: 0.9824\n",
      "Epoch 554/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8197e-04 - my_r2: 0.9413 - val_loss: 4.7647e-05 - val_my_r2: 0.9826\n",
      "Epoch 555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8012e-04 - my_r2: 0.9291 - val_loss: 4.9188e-05 - val_my_r2: 0.9818\n",
      "Epoch 556/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7572e-04 - my_r2: 0.8975 - val_loss: 4.7966e-05 - val_my_r2: 0.9818\n",
      "Epoch 557/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5423e-04 - my_r2: 0.8701 - val_loss: 4.5703e-05 - val_my_r2: 0.9823\n",
      "Epoch 558/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0841e-04 - my_r2: 0.9016 - val_loss: 4.8355e-05 - val_my_r2: 0.9813\n",
      "Epoch 559/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4485e-04 - my_r2: 0.9179 - val_loss: 5.0869e-05 - val_my_r2: 0.9811\n",
      "Epoch 560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4144e-04 - my_r2: 0.8807 - val_loss: 5.1306e-05 - val_my_r2: 0.9809\n",
      "Epoch 561/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8512e-04 - my_r2: 0.9304 - val_loss: 4.7633e-05 - val_my_r2: 0.9821\n",
      "Epoch 562/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9894e-04 - my_r2: 0.9196 - val_loss: 4.5589e-05 - val_my_r2: 0.9827\n",
      "Epoch 563/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1139e-04 - my_r2: 0.8617 - val_loss: 4.2264e-05 - val_my_r2: 0.9843\n",
      "Epoch 564/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9432e-04 - my_r2: 0.9118 - val_loss: 4.4668e-05 - val_my_r2: 0.9827\n",
      "Epoch 565/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7056e-04 - my_r2: 0.9219 - val_loss: 4.6692e-05 - val_my_r2: 0.9822\n",
      "Epoch 566/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8639e-04 - my_r2: 0.9248 - val_loss: 4.3292e-05 - val_my_r2: 0.9833\n",
      "Epoch 567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1251e-04 - my_r2: 0.9218 - val_loss: 4.5575e-05 - val_my_r2: 0.9825\n",
      "Epoch 568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7807e-04 - my_r2: 0.9256 - val_loss: 4.2234e-05 - val_my_r2: 0.9840\n",
      "Epoch 569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4816e-04 - my_r2: 0.9228 - val_loss: 4.3999e-05 - val_my_r2: 0.9838\n",
      "Epoch 570/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9641e-04 - my_r2: 0.9073 - val_loss: 4.4062e-05 - val_my_r2: 0.9836\n",
      "Epoch 571/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8362e-04 - my_r2: 0.9135 - val_loss: 4.5342e-05 - val_my_r2: 0.9830\n",
      "Epoch 572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2285e-04 - my_r2: 0.9108 - val_loss: 4.3429e-05 - val_my_r2: 0.9834\n",
      "Epoch 573/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0211e-04 - my_r2: 0.8836 - val_loss: 4.2956e-05 - val_my_r2: 0.9837\n",
      "Epoch 574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6835e-04 - my_r2: 0.9053 - val_loss: 4.6867e-05 - val_my_r2: 0.9824\n",
      "Epoch 575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1785e-04 - my_r2: 0.9069 - val_loss: 4.3339e-05 - val_my_r2: 0.9839\n",
      "Epoch 576/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5044e-04 - my_r2: 0.9464 - val_loss: 4.4742e-05 - val_my_r2: 0.9828\n",
      "Epoch 577/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1699e-04 - my_r2: 0.9352 - val_loss: 4.3920e-05 - val_my_r2: 0.9831\n",
      "Epoch 578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3491e-04 - my_r2: 0.9296 - val_loss: 4.5541e-05 - val_my_r2: 0.9825\n",
      "Epoch 579/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9477e-04 - my_r2: 0.9034 - val_loss: 4.5433e-05 - val_my_r2: 0.9821\n",
      "Epoch 580/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1584e-04 - my_r2: 0.9144 - val_loss: 4.5315e-05 - val_my_r2: 0.9820\n",
      "Epoch 581/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9087e-04 - my_r2: 0.8959 - val_loss: 4.8046e-05 - val_my_r2: 0.9811\n",
      "Epoch 582/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4313e-04 - my_r2: 0.9529 - val_loss: 4.6469e-05 - val_my_r2: 0.9832\n",
      "Epoch 583/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0604e-04 - my_r2: 0.8767 - val_loss: 4.1743e-05 - val_my_r2: 0.9850\n",
      "Epoch 584/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3415e-04 - my_r2: 0.9029 - val_loss: 4.2590e-05 - val_my_r2: 0.9838\n",
      "Epoch 585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7695e-04 - my_r2: 0.9317 - val_loss: 4.1019e-05 - val_my_r2: 0.9849\n",
      "Epoch 586/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0096e-04 - my_r2: 0.9458 - val_loss: 4.5841e-05 - val_my_r2: 0.9834\n",
      "Epoch 587/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0854e-04 - my_r2: 0.9325 - val_loss: 3.9552e-05 - val_my_r2: 0.9855\n",
      "Epoch 588/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9074e-04 - my_r2: 0.9181 - val_loss: 3.8611e-05 - val_my_r2: 0.9856\n",
      "Epoch 589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5835e-04 - my_r2: 0.9303 - val_loss: 4.0143e-05 - val_my_r2: 0.9849\n",
      "Epoch 590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7528e-04 - my_r2: 0.9334 - val_loss: 4.1220e-05 - val_my_r2: 0.9841\n",
      "Epoch 591/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6555e-04 - my_r2: 0.9149 - val_loss: 4.2394e-05 - val_my_r2: 0.9832\n",
      "Epoch 592/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8394e-04 - my_r2: 0.9335 - val_loss: 4.1042e-05 - val_my_r2: 0.9841\n",
      "Epoch 593/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8001e-04 - my_r2: 0.9168 - val_loss: 4.0547e-05 - val_my_r2: 0.9842\n",
      "Epoch 594/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1270e-04 - my_r2: 0.9029 - val_loss: 3.8619e-05 - val_my_r2: 0.9852\n",
      "Epoch 595/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6940e-04 - my_r2: 0.9028 - val_loss: 3.6429e-05 - val_my_r2: 0.9869\n",
      "Epoch 596/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2271e-04 - my_r2: 0.9572 - val_loss: 3.9035e-05 - val_my_r2: 0.9864\n",
      "Epoch 597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6764e-04 - my_r2: 0.8877 - val_loss: 3.7445e-05 - val_my_r2: 0.9861\n",
      "Epoch 598/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9658e-04 - my_r2: 0.8208 - val_loss: 3.8787e-05 - val_my_r2: 0.9854\n",
      "Epoch 599/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1276e-04 - my_r2: 0.8922 - val_loss: 3.8461e-05 - val_my_r2: 0.9859\n",
      "Epoch 600/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9122e-04 - my_r2: 0.9236 - val_loss: 4.4658e-05 - val_my_r2: 0.9846\n",
      "Epoch 601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1242e-04 - my_r2: 0.9191 - val_loss: 4.2426e-05 - val_my_r2: 0.9850\n",
      "Epoch 602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8923e-04 - my_r2: 0.9224 - val_loss: 4.5662e-05 - val_my_r2: 0.9847\n",
      "Epoch 603/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5667e-04 - my_r2: 0.8682 - val_loss: 4.8471e-05 - val_my_r2: 0.9840\n",
      "Epoch 604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6753e-04 - my_r2: 0.8957 - val_loss: 4.1191e-05 - val_my_r2: 0.9863\n",
      "Epoch 605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4777e-04 - my_r2: 0.9334 - val_loss: 3.8035e-05 - val_my_r2: 0.9873\n",
      "Epoch 606/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1318e-04 - my_r2: 0.9095 - val_loss: 4.3594e-05 - val_my_r2: 0.9858\n",
      "Epoch 607/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2521e-04 - my_r2: 0.9077 - val_loss: 4.0724e-05 - val_my_r2: 0.9861\n",
      "Epoch 608/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8476e-04 - my_r2: 0.9385 - val_loss: 4.0971e-05 - val_my_r2: 0.9862\n",
      "Epoch 609/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1233e-04 - my_r2: 0.9009 - val_loss: 4.0673e-05 - val_my_r2: 0.9860\n",
      "Epoch 610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5447e-04 - my_r2: 0.9011 - val_loss: 3.7790e-05 - val_my_r2: 0.9871\n",
      "Epoch 611/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6031e-04 - my_r2: 0.9087 - val_loss: 3.7055e-05 - val_my_r2: 0.9875\n",
      "Epoch 612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7672e-04 - my_r2: 0.9207 - val_loss: 4.0466e-05 - val_my_r2: 0.9863\n",
      "Epoch 613/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1823e-04 - my_r2: 0.8663 - val_loss: 3.8509e-05 - val_my_r2: 0.9868\n",
      "Epoch 614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3693e-04 - my_r2: 0.9389 - val_loss: 3.7249e-05 - val_my_r2: 0.9873\n",
      "Epoch 615/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5961e-04 - my_r2: 0.8801 - val_loss: 3.6000e-05 - val_my_r2: 0.9877\n",
      "Epoch 616/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6247e-04 - my_r2: 0.8762 - val_loss: 4.1075e-05 - val_my_r2: 0.9862\n",
      "Epoch 617/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8121e-04 - my_r2: 0.9334 - val_loss: 3.6816e-05 - val_my_r2: 0.9867\n",
      "Epoch 618/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7244e-04 - my_r2: 0.8675 - val_loss: 4.1642e-05 - val_my_r2: 0.9833\n",
      "Epoch 619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0436e-04 - my_r2: 0.8958 - val_loss: 4.1135e-05 - val_my_r2: 0.9843\n",
      "Epoch 620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8386e-04 - my_r2: 0.9341 - val_loss: 4.3306e-05 - val_my_r2: 0.9847\n",
      "Epoch 621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0201e-04 - my_r2: 0.9080 - val_loss: 4.8445e-05 - val_my_r2: 0.9836\n",
      "Epoch 622/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7169e-04 - my_r2: 0.9263 - val_loss: 3.5538e-05 - val_my_r2: 0.9875\n",
      "Epoch 623/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0501e-04 - my_r2: 0.9112 - val_loss: 3.5695e-05 - val_my_r2: 0.9870\n",
      "Epoch 624/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6492e-04 - my_r2: 0.9305 - val_loss: 3.6121e-05 - val_my_r2: 0.9867\n",
      "Epoch 625/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6860e-04 - my_r2: 0.8878 - val_loss: 3.7878e-05 - val_my_r2: 0.9866\n",
      "Epoch 626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8618e-04 - my_r2: 0.8986 - val_loss: 3.9998e-05 - val_my_r2: 0.9860\n",
      "Epoch 627/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8297e-04 - my_r2: 0.9317 - val_loss: 4.5313e-05 - val_my_r2: 0.9840\n",
      "Epoch 628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0258e-04 - my_r2: 0.9186 - val_loss: 4.0695e-05 - val_my_r2: 0.9855\n",
      "Epoch 629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4416e-04 - my_r2: 0.9183 - val_loss: 4.0544e-05 - val_my_r2: 0.9857\n",
      "Epoch 630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8726e-04 - my_r2: 0.9103 - val_loss: 3.2250e-05 - val_my_r2: 0.9886\n",
      "Epoch 631/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5801e-04 - my_r2: 0.9171 - val_loss: 3.3380e-05 - val_my_r2: 0.9882\n",
      "Epoch 632/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6364e-04 - my_r2: 0.9232 - val_loss: 3.5837e-05 - val_my_r2: 0.9877\n",
      "Epoch 633/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0782e-04 - my_r2: 0.9448 - val_loss: 3.4099e-05 - val_my_r2: 0.9881\n",
      "Epoch 634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4143e-04 - my_r2: 0.8199 - val_loss: 3.3644e-05 - val_my_r2: 0.9880\n",
      "Epoch 635/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4832e-04 - my_r2: 0.8468 - val_loss: 3.8559e-05 - val_my_r2: 0.9860\n",
      "Epoch 636/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0108e-04 - my_r2: 0.8776 - val_loss: 3.5720e-05 - val_my_r2: 0.9873\n",
      "Epoch 637/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9522e-04 - my_r2: 0.9219 - val_loss: 3.6132e-05 - val_my_r2: 0.9871\n",
      "Epoch 638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0208e-04 - my_r2: 0.8878 - val_loss: 3.5501e-05 - val_my_r2: 0.9870\n",
      "Epoch 639/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3514e-04 - my_r2: 0.9310 - val_loss: 3.3482e-05 - val_my_r2: 0.9879\n",
      "Epoch 640/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6276e-04 - my_r2: 0.8896 - val_loss: 3.2841e-05 - val_my_r2: 0.9882\n",
      "Epoch 641/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5584e-04 - my_r2: 0.9368 - val_loss: 3.4764e-05 - val_my_r2: 0.9874\n",
      "Epoch 642/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2963e-04 - my_r2: 0.9201 - val_loss: 3.4538e-05 - val_my_r2: 0.9875\n",
      "Epoch 643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8121e-04 - my_r2: 0.9132 - val_loss: 3.4828e-05 - val_my_r2: 0.9873\n",
      "Epoch 644/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5703e-04 - my_r2: 0.9352 - val_loss: 3.2154e-05 - val_my_r2: 0.9884\n",
      "Epoch 645/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2686e-04 - my_r2: 0.9439 - val_loss: 3.1808e-05 - val_my_r2: 0.9884\n",
      "Epoch 646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7166e-04 - my_r2: 0.9353 - val_loss: 3.9139e-05 - val_my_r2: 0.9859\n",
      "Epoch 647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7196e-04 - my_r2: 0.9425 - val_loss: 4.0462e-05 - val_my_r2: 0.9857\n",
      "Epoch 648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8845e-04 - my_r2: 0.8709 - val_loss: 3.4273e-05 - val_my_r2: 0.9875\n",
      "Epoch 649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2393e-04 - my_r2: 0.9154 - val_loss: 3.2509e-05 - val_my_r2: 0.9882\n",
      "Epoch 650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4202e-04 - my_r2: 0.9386 - val_loss: 3.6207e-05 - val_my_r2: 0.9866\n",
      "Epoch 651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6344e-04 - my_r2: 0.9178 - val_loss: 3.8252e-05 - val_my_r2: 0.9862\n",
      "Epoch 652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9208e-04 - my_r2: 0.9187 - val_loss: 3.8370e-05 - val_my_r2: 0.9868\n",
      "Epoch 653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2821e-04 - my_r2: 0.9377 - val_loss: 3.8363e-05 - val_my_r2: 0.9867\n",
      "Epoch 654/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9350e-04 - my_r2: 0.9110 - val_loss: 3.2915e-05 - val_my_r2: 0.9888\n",
      "Epoch 655/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9001e-04 - my_r2: 0.9199 - val_loss: 3.9334e-05 - val_my_r2: 0.9866\n",
      "Epoch 656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0158e-04 - my_r2: 0.9079 - val_loss: 3.2316e-05 - val_my_r2: 0.9883\n",
      "Epoch 657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3869e-04 - my_r2: 0.9251 - val_loss: 3.2626e-05 - val_my_r2: 0.9878\n",
      "Epoch 658/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6510e-04 - my_r2: 0.9213 - val_loss: 3.5009e-05 - val_my_r2: 0.9869\n",
      "Epoch 659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8272e-04 - my_r2: 0.9222 - val_loss: 3.4165e-05 - val_my_r2: 0.9870\n",
      "Epoch 660/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1971e-04 - my_r2: 0.8995 - val_loss: 3.2916e-05 - val_my_r2: 0.9881\n",
      "Epoch 661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7805e-04 - my_r2: 0.9387 - val_loss: 3.6806e-05 - val_my_r2: 0.9875\n",
      "Epoch 662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3157e-04 - my_r2: 0.9211 - val_loss: 3.7638e-05 - val_my_r2: 0.9877\n",
      "Epoch 663/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6055e-04 - my_r2: 0.9354 - val_loss: 3.2115e-05 - val_my_r2: 0.9886\n",
      "Epoch 664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1659e-04 - my_r2: 0.9350 - val_loss: 4.0343e-05 - val_my_r2: 0.9851\n",
      "Epoch 665/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4932e-04 - my_r2: 0.9374 - val_loss: 3.4527e-05 - val_my_r2: 0.9883\n",
      "Epoch 666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6263e-04 - my_r2: 0.9209 - val_loss: 3.0702e-05 - val_my_r2: 0.9893\n",
      "Epoch 667/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2081e-04 - my_r2: 0.8966 - val_loss: 2.9760e-05 - val_my_r2: 0.9898\n",
      "Epoch 668/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3351e-04 - my_r2: 0.8952 - val_loss: 3.0442e-05 - val_my_r2: 0.9890\n",
      "Epoch 669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9335e-04 - my_r2: 0.9362 - val_loss: 3.1472e-05 - val_my_r2: 0.9884\n",
      "Epoch 670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7710e-04 - my_r2: 0.9379 - val_loss: 3.1576e-05 - val_my_r2: 0.9885\n",
      "Epoch 671/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4133e-04 - my_r2: 0.9198 - val_loss: 2.9692e-05 - val_my_r2: 0.9894\n",
      "Epoch 672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9132e-04 - my_r2: 0.9343 - val_loss: 2.9096e-05 - val_my_r2: 0.9898\n",
      "Epoch 673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3687e-04 - my_r2: 0.9181 - val_loss: 2.9537e-05 - val_my_r2: 0.9896\n",
      "Epoch 674/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2648e-04 - my_r2: 0.9012 - val_loss: 2.7298e-05 - val_my_r2: 0.9900\n",
      "Epoch 675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6608e-04 - my_r2: 0.9123 - val_loss: 2.7754e-05 - val_my_r2: 0.9899\n",
      "Epoch 676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1291e-04 - my_r2: 0.9203 - val_loss: 3.0553e-05 - val_my_r2: 0.9886\n",
      "Epoch 677/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6920e-04 - my_r2: 0.9134 - val_loss: 3.0495e-05 - val_my_r2: 0.9890\n",
      "Epoch 678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7126e-04 - my_r2: 0.9156 - val_loss: 2.9560e-05 - val_my_r2: 0.9889\n",
      "Epoch 679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8987e-04 - my_r2: 0.9234 - val_loss: 2.8217e-05 - val_my_r2: 0.9898\n",
      "Epoch 680/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3553e-04 - my_r2: 0.9224 - val_loss: 2.7206e-05 - val_my_r2: 0.9898\n",
      "Epoch 681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3391e-04 - my_r2: 0.9162 - val_loss: 2.6698e-05 - val_my_r2: 0.9897\n",
      "Epoch 682/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6775e-04 - my_r2: 0.8997 - val_loss: 2.6851e-05 - val_my_r2: 0.9895\n",
      "Epoch 683/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4813e-04 - my_r2: 0.9257 - val_loss: 2.9567e-05 - val_my_r2: 0.9887\n",
      "Epoch 684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7308e-04 - my_r2: 0.9014 - val_loss: 3.1432e-05 - val_my_r2: 0.9882\n",
      "Epoch 685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3856e-04 - my_r2: 0.8996 - val_loss: 3.6115e-05 - val_my_r2: 0.9868\n",
      "Epoch 686/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5851e-04 - my_r2: 0.9069 - val_loss: 3.8512e-05 - val_my_r2: 0.9848\n",
      "Epoch 687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0396e-04 - my_r2: 0.9094 - val_loss: 4.2308e-05 - val_my_r2: 0.9842\n",
      "Epoch 688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2683e-04 - my_r2: 0.9286 - val_loss: 4.4361e-05 - val_my_r2: 0.9832\n",
      "Epoch 689/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9855e-04 - my_r2: 0.9214 - val_loss: 3.1409e-05 - val_my_r2: 0.9876\n",
      "Epoch 690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7982e-04 - my_r2: 0.9359 - val_loss: 3.3477e-05 - val_my_r2: 0.9870\n",
      "Epoch 691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6144e-04 - my_r2: 0.9317 - val_loss: 2.9744e-05 - val_my_r2: 0.9885\n",
      "Epoch 692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9090e-04 - my_r2: 0.9125 - val_loss: 2.6650e-05 - val_my_r2: 0.9893\n",
      "Epoch 693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0969e-04 - my_r2: 0.9153 - val_loss: 2.8705e-05 - val_my_r2: 0.9889\n",
      "Epoch 694/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2261e-04 - my_r2: 0.9230 - val_loss: 3.4622e-05 - val_my_r2: 0.9864\n",
      "Epoch 695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9009e-04 - my_r2: 0.9338 - val_loss: 3.1995e-05 - val_my_r2: 0.9871\n",
      "Epoch 696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0669e-04 - my_r2: 0.8970 - val_loss: 3.8025e-05 - val_my_r2: 0.9852\n",
      "Epoch 697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2926e-04 - my_r2: 0.9154 - val_loss: 3.1796e-05 - val_my_r2: 0.9878\n",
      "Epoch 698/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8811e-04 - my_r2: 0.9315 - val_loss: 2.7895e-05 - val_my_r2: 0.9891\n",
      "Epoch 699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5768e-04 - my_r2: 0.8894 - val_loss: 3.4732e-05 - val_my_r2: 0.9867\n",
      "Epoch 700/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9124e-04 - my_r2: 0.9314 - val_loss: 4.6686e-05 - val_my_r2: 0.9825\n",
      "Epoch 701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9549e-04 - my_r2: 0.9364 - val_loss: 4.2809e-05 - val_my_r2: 0.9846\n",
      "Epoch 702/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2059e-04 - my_r2: 0.9252 - val_loss: 3.6394e-05 - val_my_r2: 0.9870\n",
      "Epoch 703/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8116e-04 - my_r2: 0.9269 - val_loss: 2.9401e-05 - val_my_r2: 0.9891\n",
      "Epoch 704/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6951e-04 - my_r2: 0.8973 - val_loss: 2.8139e-05 - val_my_r2: 0.9890\n",
      "Epoch 705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4441e-04 - my_r2: 0.9466 - val_loss: 2.7727e-05 - val_my_r2: 0.9890\n",
      "Epoch 706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1517e-04 - my_r2: 0.9192 - val_loss: 2.7716e-05 - val_my_r2: 0.9888\n",
      "Epoch 707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1373e-04 - my_r2: 0.9490 - val_loss: 2.6615e-05 - val_my_r2: 0.9895\n",
      "Epoch 708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5746e-04 - my_r2: 0.9129 - val_loss: 2.9146e-05 - val_my_r2: 0.9888\n",
      "Epoch 709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6167e-04 - my_r2: 0.9195 - val_loss: 3.1647e-05 - val_my_r2: 0.9877\n",
      "Epoch 710/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2409e-04 - my_r2: 0.8661 - val_loss: 2.6489e-05 - val_my_r2: 0.9894\n",
      "Epoch 711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9689e-04 - my_r2: 0.9397 - val_loss: 2.8488e-05 - val_my_r2: 0.9890\n",
      "Epoch 712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7942e-04 - my_r2: 0.9376 - val_loss: 3.5303e-05 - val_my_r2: 0.9862\n",
      "Epoch 713/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7092e-04 - my_r2: 0.9340 - val_loss: 3.0119e-05 - val_my_r2: 0.9885\n",
      "Epoch 714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6371e-04 - my_r2: 0.9431 - val_loss: 3.0555e-05 - val_my_r2: 0.9888\n",
      "Epoch 715/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6804e-04 - my_r2: 0.9036 - val_loss: 2.8204e-05 - val_my_r2: 0.9895\n",
      "Epoch 716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6994e-04 - my_r2: 0.9275 - val_loss: 3.3636e-05 - val_my_r2: 0.9876\n",
      "Epoch 717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0461e-04 - my_r2: 0.9352 - val_loss: 2.8619e-05 - val_my_r2: 0.9895\n",
      "Epoch 718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7129e-04 - my_r2: 0.9357 - val_loss: 3.0645e-05 - val_my_r2: 0.9887\n",
      "Epoch 719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6122e-04 - my_r2: 0.9177 - val_loss: 2.6512e-05 - val_my_r2: 0.9900\n",
      "Epoch 720/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8011e-04 - my_r2: 0.9278 - val_loss: 2.5962e-05 - val_my_r2: 0.9900\n",
      "Epoch 721/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7755e-04 - my_r2: 0.8741 - val_loss: 2.9267e-05 - val_my_r2: 0.9886\n",
      "Epoch 722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7205e-04 - my_r2: 0.9425 - val_loss: 2.6322e-05 - val_my_r2: 0.9893\n",
      "Epoch 723/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0578e-04 - my_r2: 0.9064 - val_loss: 2.5132e-05 - val_my_r2: 0.9895\n",
      "Epoch 724/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0808e-04 - my_r2: 0.9309 - val_loss: 2.5702e-05 - val_my_r2: 0.9896\n",
      "Epoch 725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8554e-04 - my_r2: 0.9134 - val_loss: 2.6607e-05 - val_my_r2: 0.9899\n",
      "Epoch 726/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0071e-04 - my_r2: 0.8966 - val_loss: 2.9395e-05 - val_my_r2: 0.9882\n",
      "Epoch 727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3186e-04 - my_r2: 0.9084 - val_loss: 3.3621e-05 - val_my_r2: 0.9867\n",
      "Epoch 728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5062e-04 - my_r2: 0.9440 - val_loss: 3.2494e-05 - val_my_r2: 0.9871\n",
      "Epoch 729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5330e-04 - my_r2: 0.9357 - val_loss: 2.7230e-05 - val_my_r2: 0.9897\n",
      "Epoch 730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1869e-04 - my_r2: 0.9133 - val_loss: 2.7889e-05 - val_my_r2: 0.9898\n",
      "Epoch 731/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6428e-04 - my_r2: 0.9296 - val_loss: 2.6100e-05 - val_my_r2: 0.9902\n",
      "Epoch 732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3834e-04 - my_r2: 0.9492 - val_loss: 2.4000e-05 - val_my_r2: 0.9910\n",
      "Epoch 733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3456e-04 - my_r2: 0.9039 - val_loss: 2.3860e-05 - val_my_r2: 0.9908\n",
      "Epoch 734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0457e-04 - my_r2: 0.8976 - val_loss: 2.3884e-05 - val_my_r2: 0.9910\n",
      "Epoch 735/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3420e-04 - my_r2: 0.9309 - val_loss: 2.3921e-05 - val_my_r2: 0.9912\n",
      "Epoch 736/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3561e-04 - my_r2: 0.9478 - val_loss: 2.3162e-05 - val_my_r2: 0.9912\n",
      "Epoch 737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5978e-04 - my_r2: 0.8986 - val_loss: 2.2478e-05 - val_my_r2: 0.9913\n",
      "Epoch 738/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5270e-04 - my_r2: 0.9136 - val_loss: 2.3480e-05 - val_my_r2: 0.9909\n",
      "Epoch 739/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9905e-04 - my_r2: 0.9114 - val_loss: 2.4310e-05 - val_my_r2: 0.9907\n",
      "Epoch 740/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6438e-04 - my_r2: 0.9384 - val_loss: 2.5559e-05 - val_my_r2: 0.9906\n",
      "Epoch 741/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3585e-04 - my_r2: 0.9294 - val_loss: 2.7041e-05 - val_my_r2: 0.9902\n",
      "Epoch 742/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7520e-04 - my_r2: 0.8760 - val_loss: 2.5382e-05 - val_my_r2: 0.9908\n",
      "Epoch 743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7963e-04 - my_r2: 0.9256 - val_loss: 2.4196e-05 - val_my_r2: 0.9912\n",
      "Epoch 744/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0967e-04 - my_r2: 0.9486 - val_loss: 2.5779e-05 - val_my_r2: 0.9904\n",
      "Epoch 745/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5262e-04 - my_r2: 0.9185 - val_loss: 2.3793e-05 - val_my_r2: 0.9913\n",
      "Epoch 746/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5080e-04 - my_r2: 0.9519 - val_loss: 2.3195e-05 - val_my_r2: 0.9912\n",
      "Epoch 747/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5806e-04 - my_r2: 0.9302 - val_loss: 2.2282e-05 - val_my_r2: 0.9915\n",
      "Epoch 748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9039e-04 - my_r2: 0.9292 - val_loss: 2.2811e-05 - val_my_r2: 0.9914\n",
      "Epoch 749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6911e-04 - my_r2: 0.9313 - val_loss: 2.5079e-05 - val_my_r2: 0.9908\n",
      "Epoch 750/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8258e-04 - my_r2: 0.9212 - val_loss: 2.6279e-05 - val_my_r2: 0.9903\n",
      "Epoch 751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9065e-04 - my_r2: 0.8936 - val_loss: 2.7332e-05 - val_my_r2: 0.9900\n",
      "Epoch 752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4210e-04 - my_r2: 0.8400 - val_loss: 2.4944e-05 - val_my_r2: 0.9907\n",
      "Epoch 753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9906e-04 - my_r2: 0.9252 - val_loss: 2.3470e-05 - val_my_r2: 0.9917\n",
      "Epoch 754/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1844e-04 - my_r2: 0.9108 - val_loss: 2.4061e-05 - val_my_r2: 0.9915\n",
      "Epoch 755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7394e-04 - my_r2: 0.9271 - val_loss: 2.5987e-05 - val_my_r2: 0.9907\n",
      "Epoch 756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7233e-04 - my_r2: 0.9041 - val_loss: 2.5061e-05 - val_my_r2: 0.9910\n",
      "Epoch 757/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0040e-04 - my_r2: 0.9229 - val_loss: 2.3361e-05 - val_my_r2: 0.9919\n",
      "Epoch 758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0640e-04 - my_r2: 0.8624 - val_loss: 2.4494e-05 - val_my_r2: 0.9913\n",
      "Epoch 759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5942e-04 - my_r2: 0.9142 - val_loss: 2.4995e-05 - val_my_r2: 0.9908\n",
      "Epoch 760/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2952e-04 - my_r2: 0.9281 - val_loss: 2.8457e-05 - val_my_r2: 0.9894\n",
      "Epoch 761/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2165e-04 - my_r2: 0.9137 - val_loss: 2.9976e-05 - val_my_r2: 0.9890\n",
      "Epoch 762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2489e-04 - my_r2: 0.7512 - val_loss: 2.5469e-05 - val_my_r2: 0.9906\n",
      "Epoch 763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7683e-04 - my_r2: 0.9270 - val_loss: 2.8877e-05 - val_my_r2: 0.9890\n",
      "Epoch 764/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0626e-04 - my_r2: 0.9173 - val_loss: 3.1683e-05 - val_my_r2: 0.9874\n",
      "Epoch 765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2063e-04 - my_r2: 0.9497 - val_loss: 2.8545e-05 - val_my_r2: 0.9889\n",
      "Epoch 766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0757e-04 - my_r2: 0.8815 - val_loss: 3.4612e-05 - val_my_r2: 0.9862\n",
      "Epoch 767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7410e-04 - my_r2: 0.9343 - val_loss: 2.7590e-05 - val_my_r2: 0.9890\n",
      "Epoch 768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8217e-04 - my_r2: 0.9294 - val_loss: 2.5870e-05 - val_my_r2: 0.9901\n",
      "Epoch 769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2582e-04 - my_r2: 0.9459 - val_loss: 2.8062e-05 - val_my_r2: 0.9893\n",
      "Epoch 770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4258e-04 - my_r2: 0.9071 - val_loss: 2.4029e-05 - val_my_r2: 0.9911\n",
      "Epoch 771/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1813e-04 - my_r2: 0.8776 - val_loss: 2.4905e-05 - val_my_r2: 0.9910\n",
      "Epoch 772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7332e-04 - my_r2: 0.9412 - val_loss: 2.6079e-05 - val_my_r2: 0.9904\n",
      "Epoch 773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7906e-04 - my_r2: 0.9000 - val_loss: 2.3751e-05 - val_my_r2: 0.9911\n",
      "Epoch 774/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6562e-04 - my_r2: 0.9350 - val_loss: 2.4776e-05 - val_my_r2: 0.9905\n",
      "Epoch 775/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0743e-04 - my_r2: 0.9129 - val_loss: 2.5659e-05 - val_my_r2: 0.9904\n",
      "Epoch 776/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5043e-04 - my_r2: 0.9333 - val_loss: 2.8878e-05 - val_my_r2: 0.9892\n",
      "Epoch 777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7766e-04 - my_r2: 0.9231 - val_loss: 3.0342e-05 - val_my_r2: 0.9895\n",
      "Epoch 778/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3542e-04 - my_r2: 0.9079 - val_loss: 2.6698e-05 - val_my_r2: 0.9903\n",
      "Epoch 779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1597e-04 - my_r2: 0.9256 - val_loss: 2.3138e-05 - val_my_r2: 0.9916\n",
      "Epoch 780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5427e-04 - my_r2: 0.8898 - val_loss: 2.1121e-05 - val_my_r2: 0.9922\n",
      "Epoch 781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7224e-04 - my_r2: 0.9330 - val_loss: 2.2295e-05 - val_my_r2: 0.9920\n",
      "Epoch 782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3271e-04 - my_r2: 0.8299 - val_loss: 2.1504e-05 - val_my_r2: 0.9923\n",
      "Epoch 783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9777e-04 - my_r2: 0.8544 - val_loss: 2.0499e-05 - val_my_r2: 0.9928\n",
      "Epoch 784/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9517e-04 - my_r2: 0.9276 - val_loss: 2.1700e-05 - val_my_r2: 0.9924\n",
      "Epoch 785/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6818e-04 - my_r2: 0.8775 - val_loss: 2.1491e-05 - val_my_r2: 0.9928\n",
      "Epoch 786/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6588e-04 - my_r2: 0.9466 - val_loss: 2.2725e-05 - val_my_r2: 0.9920\n",
      "Epoch 787/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1390e-04 - my_r2: 0.8957 - val_loss: 2.0918e-05 - val_my_r2: 0.9925\n",
      "Epoch 788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3466e-04 - my_r2: 0.9202 - val_loss: 2.2779e-05 - val_my_r2: 0.9917\n",
      "Epoch 789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5290e-04 - my_r2: 0.9242 - val_loss: 1.9798e-05 - val_my_r2: 0.9925\n",
      "Epoch 790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0394e-04 - my_r2: 0.9348 - val_loss: 2.1313e-05 - val_my_r2: 0.9919\n",
      "Epoch 791/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0717e-04 - my_r2: 0.9492 - val_loss: 2.1287e-05 - val_my_r2: 0.9920\n",
      "Epoch 792/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6479e-04 - my_r2: 0.9483 - val_loss: 2.0581e-05 - val_my_r2: 0.9924\n",
      "Epoch 793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2576e-04 - my_r2: 0.9483 - val_loss: 2.6882e-05 - val_my_r2: 0.9903\n",
      "Epoch 794/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4221e-04 - my_r2: 0.9377 - val_loss: 2.3425e-05 - val_my_r2: 0.9918\n",
      "Epoch 795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9504e-04 - my_r2: 0.9395 - val_loss: 2.2661e-05 - val_my_r2: 0.9923\n",
      "Epoch 796/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4394e-04 - my_r2: 0.9388 - val_loss: 2.1704e-05 - val_my_r2: 0.9920\n",
      "Epoch 797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9728e-04 - my_r2: 0.9333 - val_loss: 2.3396e-05 - val_my_r2: 0.9910\n",
      "Epoch 798/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7995e-04 - my_r2: 0.9162 - val_loss: 2.0377e-05 - val_my_r2: 0.9920\n",
      "Epoch 799/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7394e-04 - my_r2: 0.9195 - val_loss: 2.0361e-05 - val_my_r2: 0.9925\n",
      "Epoch 800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7644e-04 - my_r2: 0.9384 - val_loss: 2.2513e-05 - val_my_r2: 0.9921\n",
      "Epoch 801/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5084e-04 - my_r2: 0.8989 - val_loss: 2.2510e-05 - val_my_r2: 0.9923\n",
      "Epoch 802/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3070e-04 - my_r2: 0.8624 - val_loss: 1.9020e-05 - val_my_r2: 0.9935\n",
      "Epoch 803/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6632e-04 - my_r2: 0.9342 - val_loss: 2.2815e-05 - val_my_r2: 0.9927\n",
      "Epoch 804/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5847e-04 - my_r2: 0.8864 - val_loss: 1.8457e-05 - val_my_r2: 0.9936\n",
      "Epoch 805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2806e-04 - my_r2: 0.9512 - val_loss: 2.0873e-05 - val_my_r2: 0.9925\n",
      "Epoch 806/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9768e-04 - my_r2: 0.9354 - val_loss: 2.0811e-05 - val_my_r2: 0.9929\n",
      "Epoch 807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7859e-04 - my_r2: 0.9226 - val_loss: 1.8742e-05 - val_my_r2: 0.9935\n",
      "Epoch 808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5831e-04 - my_r2: 0.9195 - val_loss: 1.7588e-05 - val_my_r2: 0.9941\n",
      "Epoch 809/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1327e-04 - my_r2: 0.9105 - val_loss: 1.8424e-05 - val_my_r2: 0.9938\n",
      "Epoch 810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3624e-04 - my_r2: 0.8525 - val_loss: 1.6578e-05 - val_my_r2: 0.9939\n",
      "Epoch 811/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4867e-04 - my_r2: 0.8941 - val_loss: 1.7685e-05 - val_my_r2: 0.9936\n",
      "Epoch 812/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9171e-04 - my_r2: 0.8645 - val_loss: 2.3633e-05 - val_my_r2: 0.9915\n",
      "Epoch 813/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7712e-04 - my_r2: 0.9194 - val_loss: 2.1353e-05 - val_my_r2: 0.9921\n",
      "Epoch 814/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0821e-04 - my_r2: 0.8496 - val_loss: 1.9630e-05 - val_my_r2: 0.9925\n",
      "Epoch 815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9461e-04 - my_r2: 0.9110 - val_loss: 2.3056e-05 - val_my_r2: 0.9913\n",
      "Epoch 816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3730e-04 - my_r2: 0.9402 - val_loss: 2.4946e-05 - val_my_r2: 0.9909\n",
      "Epoch 817/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7095e-04 - my_r2: 0.9410 - val_loss: 3.4472e-05 - val_my_r2: 0.9882\n",
      "Epoch 818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5282e-04 - my_r2: 0.9238 - val_loss: 3.5119e-05 - val_my_r2: 0.9877\n",
      "Epoch 819/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0385e-04 - my_r2: 0.9083 - val_loss: 2.5369e-05 - val_my_r2: 0.9912\n",
      "Epoch 820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6263e-04 - my_r2: 0.9115 - val_loss: 2.5137e-05 - val_my_r2: 0.9913\n",
      "Epoch 821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7570e-04 - my_r2: 0.9196 - val_loss: 2.1744e-05 - val_my_r2: 0.9922\n",
      "Epoch 822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0834e-04 - my_r2: 0.8737 - val_loss: 2.1453e-05 - val_my_r2: 0.9923\n",
      "Epoch 823/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0978e-04 - my_r2: 0.9486 - val_loss: 2.4952e-05 - val_my_r2: 0.9910\n",
      "Epoch 824/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8892e-04 - my_r2: 0.9189 - val_loss: 2.6238e-05 - val_my_r2: 0.9912\n",
      "Epoch 825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1522e-04 - my_r2: 0.9586 - val_loss: 2.9944e-05 - val_my_r2: 0.9903\n",
      "Epoch 826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5823e-04 - my_r2: 0.9285 - val_loss: 2.2910e-05 - val_my_r2: 0.9924\n",
      "Epoch 827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2052e-04 - my_r2: 0.9229 - val_loss: 2.0832e-05 - val_my_r2: 0.9926\n",
      "Epoch 828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4227e-04 - my_r2: 0.9369 - val_loss: 2.1974e-05 - val_my_r2: 0.9922\n",
      "Epoch 829/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5605e-04 - my_r2: 0.9221 - val_loss: 1.8604e-05 - val_my_r2: 0.9938\n",
      "Epoch 830/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6528e-04 - my_r2: 0.8530 - val_loss: 2.2748e-05 - val_my_r2: 0.9931\n",
      "Epoch 831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3667e-04 - my_r2: 0.9544 - val_loss: 2.8955e-05 - val_my_r2: 0.9912\n",
      "Epoch 832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5193e-04 - my_r2: 0.9439 - val_loss: 2.0399e-05 - val_my_r2: 0.9935\n",
      "Epoch 833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4495e-04 - my_r2: 0.9340 - val_loss: 2.3633e-05 - val_my_r2: 0.9911\n",
      "Epoch 834/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9424e-04 - my_r2: 0.9320 - val_loss: 1.9848e-05 - val_my_r2: 0.9927\n",
      "Epoch 835/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4833e-04 - my_r2: 0.9258 - val_loss: 2.0363e-05 - val_my_r2: 0.9926\n",
      "Epoch 836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5564e-04 - my_r2: 0.9312 - val_loss: 1.9874e-05 - val_my_r2: 0.9927\n",
      "Epoch 837/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8216e-04 - my_r2: 0.9288 - val_loss: 1.7810e-05 - val_my_r2: 0.9937\n",
      "Epoch 838/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4586e-04 - my_r2: 0.9296 - val_loss: 1.9720e-05 - val_my_r2: 0.9933\n",
      "Epoch 839/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5447e-04 - my_r2: 0.9268 - val_loss: 1.9444e-05 - val_my_r2: 0.9932\n",
      "Epoch 840/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7290e-04 - my_r2: 0.9300 - val_loss: 2.8079e-05 - val_my_r2: 0.9899\n",
      "Epoch 841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0814e-04 - my_r2: 0.9328 - val_loss: 2.1218e-05 - val_my_r2: 0.9924\n",
      "Epoch 842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9108e-04 - my_r2: 0.9058 - val_loss: 1.8574e-05 - val_my_r2: 0.9931\n",
      "Epoch 843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6617e-04 - my_r2: 0.9185 - val_loss: 1.9217e-05 - val_my_r2: 0.9929\n",
      "Epoch 844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2913e-04 - my_r2: 0.9481 - val_loss: 1.9000e-05 - val_my_r2: 0.9935\n",
      "Epoch 845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3285e-04 - my_r2: 0.9465 - val_loss: 1.9725e-05 - val_my_r2: 0.9932\n",
      "Epoch 846/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1850e-04 - my_r2: 0.9183 - val_loss: 1.9133e-05 - val_my_r2: 0.9933\n",
      "Epoch 847/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1838e-04 - my_r2: 0.9262 - val_loss: 1.9873e-05 - val_my_r2: 0.9928\n",
      "Epoch 848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4938e-04 - my_r2: 0.9296 - val_loss: 2.4185e-05 - val_my_r2: 0.9914\n",
      "Epoch 849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2402e-04 - my_r2: 0.9530 - val_loss: 2.1222e-05 - val_my_r2: 0.9925\n",
      "Epoch 850/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5037e-04 - my_r2: 0.9239 - val_loss: 2.0586e-05 - val_my_r2: 0.9930\n",
      "Epoch 851/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4845e-04 - my_r2: 0.9308 - val_loss: 1.6985e-05 - val_my_r2: 0.9939\n",
      "Epoch 852/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7065e-04 - my_r2: 0.9053 - val_loss: 1.6656e-05 - val_my_r2: 0.9942\n",
      "Epoch 853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7229e-04 - my_r2: 0.9361 - val_loss: 1.8993e-05 - val_my_r2: 0.9933\n",
      "Epoch 854/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4998e-04 - my_r2: 0.9425 - val_loss: 2.6461e-05 - val_my_r2: 0.9911\n",
      "Epoch 855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0942e-04 - my_r2: 0.8925 - val_loss: 2.1321e-05 - val_my_r2: 0.9928\n",
      "Epoch 856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9065e-04 - my_r2: 0.9279 - val_loss: 2.1022e-05 - val_my_r2: 0.9928\n",
      "Epoch 857/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7765e-04 - my_r2: 0.9296 - val_loss: 2.2684e-05 - val_my_r2: 0.9921\n",
      "Epoch 858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6100e-04 - my_r2: 0.9273 - val_loss: 2.2083e-05 - val_my_r2: 0.9922\n",
      "Epoch 859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7285e-04 - my_r2: 0.9357 - val_loss: 2.1694e-05 - val_my_r2: 0.9925\n",
      "Epoch 860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8323e-04 - my_r2: 0.9189 - val_loss: 2.1549e-05 - val_my_r2: 0.9929\n",
      "Epoch 861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7447e-04 - my_r2: 0.9364 - val_loss: 1.9114e-05 - val_my_r2: 0.9939\n",
      "Epoch 862/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4146e-04 - my_r2: 0.9320 - val_loss: 1.8388e-05 - val_my_r2: 0.9941\n",
      "Epoch 863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2213e-04 - my_r2: 0.9375 - val_loss: 1.6791e-05 - val_my_r2: 0.9945\n",
      "Epoch 864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5644e-04 - my_r2: 0.9188 - val_loss: 1.8262e-05 - val_my_r2: 0.9939\n",
      "Epoch 865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0326e-04 - my_r2: 0.9260 - val_loss: 1.9689e-05 - val_my_r2: 0.9934\n",
      "Epoch 866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1102e-04 - my_r2: 0.9009 - val_loss: 1.6474e-05 - val_my_r2: 0.9945\n",
      "Epoch 867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6989e-04 - my_r2: 0.8233 - val_loss: 1.9689e-05 - val_my_r2: 0.9933\n",
      "Epoch 868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1238e-04 - my_r2: 0.8062 - val_loss: 2.0041e-05 - val_my_r2: 0.9934\n",
      "Epoch 869/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2152e-04 - my_r2: 0.9448 - val_loss: 2.0907e-05 - val_my_r2: 0.9933\n",
      "Epoch 870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7548e-04 - my_r2: 0.9126 - val_loss: 1.5118e-05 - val_my_r2: 0.9947\n",
      "Epoch 871/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5113e-04 - my_r2: 0.8860 - val_loss: 1.8265e-05 - val_my_r2: 0.9926\n",
      "Epoch 872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0978e-04 - my_r2: 0.9166 - val_loss: 1.9160e-05 - val_my_r2: 0.9927\n",
      "Epoch 873/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6418e-04 - my_r2: 0.9268 - val_loss: 2.2531e-05 - val_my_r2: 0.9912\n",
      "Epoch 874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2586e-04 - my_r2: 0.9495 - val_loss: 1.9667e-05 - val_my_r2: 0.9921\n",
      "Epoch 875/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1802e-04 - my_r2: 0.7826 - val_loss: 1.9211e-05 - val_my_r2: 0.9930\n",
      "Epoch 876/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5513e-04 - my_r2: 0.9414 - val_loss: 2.0496e-05 - val_my_r2: 0.9926\n",
      "Epoch 877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0653e-04 - my_r2: 0.9490 - val_loss: 1.7298e-05 - val_my_r2: 0.9939\n",
      "Epoch 878/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.4107e-04 - my_r2: 0.8650 - val_loss: 1.6746e-05 - val_my_r2: 0.9941\n",
      "Epoch 879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6675e-04 - my_r2: 0.9181 - val_loss: 2.0919e-05 - val_my_r2: 0.9928\n",
      "Epoch 880/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.9342e-04 - my_r2: 0.9190 - val_loss: 1.7982e-05 - val_my_r2: 0.9935\n",
      "Epoch 881/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0734e-04 - my_r2: 0.9062 - val_loss: 2.0173e-05 - val_my_r2: 0.9928\n",
      "Epoch 882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3477e-04 - my_r2: 0.9292 - val_loss: 2.0874e-05 - val_my_r2: 0.9925\n",
      "Epoch 883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2014e-04 - my_r2: 0.9512 - val_loss: 1.9810e-05 - val_my_r2: 0.9928\n",
      "Epoch 884/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0267e-04 - my_r2: 0.9534 - val_loss: 1.8007e-05 - val_my_r2: 0.9932\n",
      "Epoch 885/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3351e-04 - my_r2: 0.9472 - val_loss: 1.7530e-05 - val_my_r2: 0.9936\n",
      "Epoch 886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1884e-04 - my_r2: 0.9396 - val_loss: 1.5453e-05 - val_my_r2: 0.9945\n",
      "Epoch 887/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8411e-04 - my_r2: 0.9419 - val_loss: 1.5774e-05 - val_my_r2: 0.9941\n",
      "Epoch 888/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3536e-04 - my_r2: 0.9220 - val_loss: 1.4869e-05 - val_my_r2: 0.9946\n",
      "Epoch 889/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7245e-04 - my_r2: 0.9270 - val_loss: 1.8210e-05 - val_my_r2: 0.9935\n",
      "Epoch 890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5085e-04 - my_r2: 0.9407 - val_loss: 1.4881e-05 - val_my_r2: 0.9946\n",
      "Epoch 891/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8168e-04 - my_r2: 0.9138 - val_loss: 1.9057e-05 - val_my_r2: 0.9929\n",
      "Epoch 892/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5274e-04 - my_r2: 0.9237 - val_loss: 1.9486e-05 - val_my_r2: 0.9928\n",
      "Epoch 893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6055e-04 - my_r2: 0.9346 - val_loss: 1.5583e-05 - val_my_r2: 0.9944\n",
      "Epoch 894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7489e-04 - my_r2: 0.9252 - val_loss: 1.6273e-05 - val_my_r2: 0.9943\n",
      "Epoch 895/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5452e-04 - my_r2: 0.9440 - val_loss: 1.8316e-05 - val_my_r2: 0.9934\n",
      "Epoch 896/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6216e-04 - my_r2: 0.8958 - val_loss: 1.5824e-05 - val_my_r2: 0.9945\n",
      "Epoch 897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9546e-04 - my_r2: 0.9251 - val_loss: 1.6386e-05 - val_my_r2: 0.9942\n",
      "Epoch 898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0381e-04 - my_r2: 0.9531 - val_loss: 1.5021e-05 - val_my_r2: 0.9949\n",
      "Epoch 899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8915e-04 - my_r2: 0.9366 - val_loss: 1.5390e-05 - val_my_r2: 0.9948\n",
      "Epoch 900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1134e-04 - my_r2: 0.9342 - val_loss: 1.9871e-05 - val_my_r2: 0.9935\n",
      "Epoch 901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5229e-04 - my_r2: 0.9306 - val_loss: 1.4831e-05 - val_my_r2: 0.9952\n",
      "Epoch 902/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4137e-04 - my_r2: 0.9384 - val_loss: 1.7453e-05 - val_my_r2: 0.9943\n",
      "Epoch 903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4626e-04 - my_r2: 0.9126 - val_loss: 2.1169e-05 - val_my_r2: 0.9925\n",
      "Epoch 904/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1476e-04 - my_r2: 0.9010 - val_loss: 1.8960e-05 - val_my_r2: 0.9931\n",
      "Epoch 905/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1741e-04 - my_r2: 0.9060 - val_loss: 1.8787e-05 - val_my_r2: 0.9937\n",
      "Epoch 906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3161e-04 - my_r2: 0.9268 - val_loss: 1.9240e-05 - val_my_r2: 0.9935\n",
      "Epoch 907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5436e-04 - my_r2: 0.8378 - val_loss: 1.8132e-05 - val_my_r2: 0.9940\n",
      "Epoch 908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9151e-04 - my_r2: 0.9259 - val_loss: 1.5682e-05 - val_my_r2: 0.9948\n",
      "Epoch 909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8704e-04 - my_r2: 0.8977 - val_loss: 1.4343e-05 - val_my_r2: 0.9953\n",
      "Epoch 910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8785e-04 - my_r2: 0.9003 - val_loss: 1.7991e-05 - val_my_r2: 0.9938\n",
      "Epoch 911/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1698e-04 - my_r2: 0.9245 - val_loss: 1.7360e-05 - val_my_r2: 0.9943\n",
      "Epoch 912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7307e-04 - my_r2: 0.9241 - val_loss: 1.6481e-05 - val_my_r2: 0.9942\n",
      "Epoch 913/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8465e-04 - my_r2: 0.9374 - val_loss: 1.5295e-05 - val_my_r2: 0.9946\n",
      "Epoch 914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7601e-04 - my_r2: 0.9185 - val_loss: 1.8503e-05 - val_my_r2: 0.9946\n",
      "Epoch 915/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6581e-04 - my_r2: 0.9295 - val_loss: 1.6667e-05 - val_my_r2: 0.9951\n",
      "Epoch 916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9308e-04 - my_r2: 0.9058 - val_loss: 1.7020e-05 - val_my_r2: 0.9944\n",
      "Epoch 917/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8687e-04 - my_r2: 0.9394 - val_loss: 1.7413e-05 - val_my_r2: 0.9943\n",
      "Epoch 918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3665e-04 - my_r2: 0.9408 - val_loss: 1.4951e-05 - val_my_r2: 0.9951\n",
      "Epoch 919/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1435e-04 - my_r2: 0.9238 - val_loss: 1.5739e-05 - val_my_r2: 0.9947\n",
      "Epoch 920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8102e-04 - my_r2: 0.9261 - val_loss: 1.5241e-05 - val_my_r2: 0.9949\n",
      "Epoch 921/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8104e-04 - my_r2: 0.9252 - val_loss: 1.4340e-05 - val_my_r2: 0.9950\n",
      "Epoch 922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9149e-04 - my_r2: 0.9292 - val_loss: 1.5393e-05 - val_my_r2: 0.9943\n",
      "Epoch 923/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2814e-04 - my_r2: 0.9369 - val_loss: 2.0272e-05 - val_my_r2: 0.9925\n",
      "Epoch 924/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6884e-04 - my_r2: 0.9153 - val_loss: 1.8504e-05 - val_my_r2: 0.9938\n",
      "Epoch 925/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4015e-04 - my_r2: 0.9434 - val_loss: 1.8758e-05 - val_my_r2: 0.9939\n",
      "Epoch 926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9440e-04 - my_r2: 0.9337 - val_loss: 1.5992e-05 - val_my_r2: 0.9946\n",
      "Epoch 927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5502e-04 - my_r2: 0.9208 - val_loss: 1.7448e-05 - val_my_r2: 0.9941\n",
      "Epoch 928/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4744e-04 - my_r2: 0.9172 - val_loss: 2.2698e-05 - val_my_r2: 0.9924\n",
      "Epoch 929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4529e-04 - my_r2: 0.9209 - val_loss: 1.6357e-05 - val_my_r2: 0.9946\n",
      "Epoch 930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9930e-04 - my_r2: 0.9224 - val_loss: 1.7904e-05 - val_my_r2: 0.9940\n",
      "Epoch 931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0351e-04 - my_r2: 0.9220 - val_loss: 1.6780e-05 - val_my_r2: 0.9943\n",
      "Epoch 932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2399e-04 - my_r2: 0.9425 - val_loss: 1.7427e-05 - val_my_r2: 0.9939\n",
      "Epoch 933/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8701e-04 - my_r2: 0.8599 - val_loss: 1.9580e-05 - val_my_r2: 0.9933\n",
      "Epoch 934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3659e-04 - my_r2: 0.9004 - val_loss: 1.5612e-05 - val_my_r2: 0.9947\n",
      "Epoch 935/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4817e-04 - my_r2: 0.9394 - val_loss: 1.8451e-05 - val_my_r2: 0.9940\n",
      "Epoch 936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8609e-04 - my_r2: 0.9223 - val_loss: 2.9893e-05 - val_my_r2: 0.9897\n",
      "Epoch 937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8871e-04 - my_r2: 0.9161 - val_loss: 2.5387e-05 - val_my_r2: 0.9912\n",
      "Epoch 938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1197e-04 - my_r2: 0.9021 - val_loss: 2.5771e-05 - val_my_r2: 0.9917\n",
      "Epoch 939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9905e-04 - my_r2: 0.9310 - val_loss: 1.8947e-05 - val_my_r2: 0.9942\n",
      "Epoch 940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6656e-04 - my_r2: 0.8969 - val_loss: 1.9649e-05 - val_my_r2: 0.9940\n",
      "Epoch 941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2897e-04 - my_r2: 0.9396 - val_loss: 1.8636e-05 - val_my_r2: 0.9940\n",
      "Epoch 942/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3642e-04 - my_r2: 0.9488 - val_loss: 1.9376e-05 - val_my_r2: 0.9939\n",
      "Epoch 943/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3348e-04 - my_r2: 0.9513 - val_loss: 1.8265e-05 - val_my_r2: 0.9941\n",
      "Epoch 944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7991e-04 - my_r2: 0.9389 - val_loss: 1.3762e-05 - val_my_r2: 0.9954\n",
      "Epoch 945/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8516e-04 - my_r2: 0.9365 - val_loss: 1.5339e-05 - val_my_r2: 0.9954\n",
      "Epoch 946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9543e-04 - my_r2: 0.9303 - val_loss: 1.5897e-05 - val_my_r2: 0.9952\n",
      "Epoch 947/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2779e-04 - my_r2: 0.9540 - val_loss: 1.5858e-05 - val_my_r2: 0.9950\n",
      "Epoch 948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8939e-04 - my_r2: 0.9463 - val_loss: 1.5401e-05 - val_my_r2: 0.9952\n",
      "Epoch 949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6668e-04 - my_r2: 0.9354 - val_loss: 2.2479e-05 - val_my_r2: 0.9929\n",
      "Epoch 950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7842e-04 - my_r2: 0.9523 - val_loss: 1.7758e-05 - val_my_r2: 0.9943\n",
      "Epoch 951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6528e-04 - my_r2: 0.9221 - val_loss: 1.4417e-05 - val_my_r2: 0.9954\n",
      "Epoch 952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5996e-04 - my_r2: 0.9291 - val_loss: 1.4399e-05 - val_my_r2: 0.9952\n",
      "Epoch 953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0841e-04 - my_r2: 0.9437 - val_loss: 1.5200e-05 - val_my_r2: 0.9949\n",
      "Epoch 954/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0676e-04 - my_r2: 0.9390 - val_loss: 1.7440e-05 - val_my_r2: 0.9943\n",
      "Epoch 955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7924e-04 - my_r2: 0.9209 - val_loss: 1.4554e-05 - val_my_r2: 0.9953\n",
      "Epoch 956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1211e-04 - my_r2: 0.9213 - val_loss: 1.4684e-05 - val_my_r2: 0.9953\n",
      "Epoch 957/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5801e-04 - my_r2: 0.9253 - val_loss: 1.5661e-05 - val_my_r2: 0.9952\n",
      "Epoch 958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8764e-04 - my_r2: 0.9204 - val_loss: 1.8797e-05 - val_my_r2: 0.9944\n",
      "Epoch 959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3169e-04 - my_r2: 0.8867 - val_loss: 1.7804e-05 - val_my_r2: 0.9946\n",
      "Epoch 960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5832e-04 - my_r2: 0.8864 - val_loss: 1.8169e-05 - val_my_r2: 0.9944\n",
      "Epoch 961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6127e-04 - my_r2: 0.9296 - val_loss: 1.8980e-05 - val_my_r2: 0.9938\n",
      "Epoch 962/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.5598e-04 - my_r2: 0.9303 - val_loss: 2.1535e-05 - val_my_r2: 0.9932\n",
      "Epoch 963/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5404e-04 - my_r2: 0.9352 - val_loss: 1.4719e-05 - val_my_r2: 0.9952\n",
      "Epoch 964/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3903e-04 - my_r2: 0.8924 - val_loss: 1.8296e-05 - val_my_r2: 0.9939\n",
      "Epoch 965/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8225e-04 - my_r2: 0.8972 - val_loss: 1.3451e-05 - val_my_r2: 0.9958\n",
      "Epoch 966/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3248e-04 - my_r2: 0.9238 - val_loss: 1.3919e-05 - val_my_r2: 0.9956\n",
      "Epoch 967/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6110e-04 - my_r2: 0.9000 - val_loss: 1.5000e-05 - val_my_r2: 0.9949\n",
      "Epoch 968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2369e-04 - my_r2: 0.9535 - val_loss: 1.7455e-05 - val_my_r2: 0.9946\n",
      "Epoch 969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3935e-04 - my_r2: 0.9129 - val_loss: 1.6505e-05 - val_my_r2: 0.9946\n",
      "Epoch 970/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7781e-04 - my_r2: 0.9417 - val_loss: 1.3596e-05 - val_my_r2: 0.9951\n",
      "Epoch 971/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6749e-04 - my_r2: 0.9035 - val_loss: 1.2728e-05 - val_my_r2: 0.9955\n",
      "Epoch 972/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1574e-04 - my_r2: 0.9543 - val_loss: 1.3942e-05 - val_my_r2: 0.9950\n",
      "Epoch 973/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1629e-04 - my_r2: 0.9096 - val_loss: 1.5250e-05 - val_my_r2: 0.9949\n",
      "Epoch 974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2343e-04 - my_r2: 0.9462 - val_loss: 1.6909e-05 - val_my_r2: 0.9946\n",
      "Epoch 975/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8464e-04 - my_r2: 0.9107 - val_loss: 1.6173e-05 - val_my_r2: 0.9948\n",
      "Epoch 976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4090e-04 - my_r2: 0.9465 - val_loss: 1.6404e-05 - val_my_r2: 0.9946\n",
      "Epoch 977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8328e-04 - my_r2: 0.9260 - val_loss: 1.6655e-05 - val_my_r2: 0.9943\n",
      "Epoch 978/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4907e-04 - my_r2: 0.9178 - val_loss: 1.7326e-05 - val_my_r2: 0.9939\n",
      "Epoch 979/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0856e-04 - my_r2: 0.9365 - val_loss: 1.9526e-05 - val_my_r2: 0.9929\n",
      "Epoch 980/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7470e-04 - my_r2: 0.9362 - val_loss: 1.8069e-05 - val_my_r2: 0.9936\n",
      "Epoch 981/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4900e-04 - my_r2: 0.9057 - val_loss: 1.5096e-05 - val_my_r2: 0.9946\n",
      "Epoch 982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0678e-04 - my_r2: 0.9420 - val_loss: 1.5100e-05 - val_my_r2: 0.9946\n",
      "Epoch 983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1021e-04 - my_r2: 0.8915 - val_loss: 1.6290e-05 - val_my_r2: 0.9945\n",
      "Epoch 984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4850e-04 - my_r2: 0.8931 - val_loss: 1.9701e-05 - val_my_r2: 0.9935\n",
      "Epoch 985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3131e-04 - my_r2: 0.9107 - val_loss: 2.3387e-05 - val_my_r2: 0.9928\n",
      "Epoch 986/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9554e-04 - my_r2: 0.9318 - val_loss: 2.4075e-05 - val_my_r2: 0.9926\n",
      "Epoch 987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8680e-04 - my_r2: 0.8889 - val_loss: 2.3795e-05 - val_my_r2: 0.9930\n",
      "Epoch 988/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0025e-04 - my_r2: 0.9366 - val_loss: 1.8106e-05 - val_my_r2: 0.9944\n",
      "Epoch 989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3294e-04 - my_r2: 0.9419 - val_loss: 1.9680e-05 - val_my_r2: 0.9934\n",
      "Epoch 990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6445e-04 - my_r2: 0.9279 - val_loss: 1.6982e-05 - val_my_r2: 0.9941\n",
      "Epoch 991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3343e-04 - my_r2: 0.9323 - val_loss: 1.8435e-05 - val_my_r2: 0.9939\n",
      "Epoch 992/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7482e-04 - my_r2: 0.9313 - val_loss: 2.2885e-05 - val_my_r2: 0.9919\n",
      "Epoch 993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5990e-04 - my_r2: 0.9290 - val_loss: 1.4772e-05 - val_my_r2: 0.9945\n",
      "Epoch 994/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0180e-04 - my_r2: 0.9481 - val_loss: 1.4892e-05 - val_my_r2: 0.9946\n",
      "Epoch 995/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4417e-04 - my_r2: 0.9312 - val_loss: 1.2019e-05 - val_my_r2: 0.9959\n",
      "Epoch 996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6680e-04 - my_r2: 0.8604 - val_loss: 1.3716e-05 - val_my_r2: 0.9952\n",
      "Epoch 997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9119e-04 - my_r2: 0.9466 - val_loss: 1.2661e-05 - val_my_r2: 0.9956\n",
      "Epoch 998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2301e-04 - my_r2: 0.8384 - val_loss: 1.6868e-05 - val_my_r2: 0.9940\n",
      "Epoch 999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3376e-04 - my_r2: 0.9427 - val_loss: 1.5961e-05 - val_my_r2: 0.9942\n",
      "Epoch 1000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4278e-04 - my_r2: 0.9423 - val_loss: 1.4533e-05 - val_my_r2: 0.9948\n",
      "Epoch 1001/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9037e-04 - my_r2: 0.7190 - val_loss: 1.4635e-05 - val_my_r2: 0.9948\n",
      "Epoch 1002/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2699e-04 - my_r2: 0.9485 - val_loss: 1.3284e-05 - val_my_r2: 0.9953\n",
      "Epoch 1003/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2141e-04 - my_r2: 0.9007 - val_loss: 1.3190e-05 - val_my_r2: 0.9956\n",
      "Epoch 1004/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8230e-04 - my_r2: 0.9238 - val_loss: 1.5033e-05 - val_my_r2: 0.9947\n",
      "Epoch 1005/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0726e-04 - my_r2: 0.9180 - val_loss: 1.3542e-05 - val_my_r2: 0.9954\n",
      "Epoch 1006/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.7517e-04 - my_r2: 0.9428 - val_loss: 1.4721e-05 - val_my_r2: 0.9951\n",
      "Epoch 1007/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9323e-04 - my_r2: 0.9209 - val_loss: 1.5614e-05 - val_my_r2: 0.9947\n",
      "Epoch 1008/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1672e-04 - my_r2: 0.9408 - val_loss: 1.4854e-05 - val_my_r2: 0.9950\n",
      "Epoch 1009/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5618e-04 - my_r2: 0.9310 - val_loss: 1.5438e-05 - val_my_r2: 0.9944\n",
      "Epoch 1010/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2506e-04 - my_r2: 0.9274 - val_loss: 1.5123e-05 - val_my_r2: 0.9946\n",
      "Epoch 1011/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0825e-04 - my_r2: 0.9138 - val_loss: 1.6600e-05 - val_my_r2: 0.9943\n",
      "Epoch 1012/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3156e-04 - my_r2: 0.9181 - val_loss: 1.6591e-05 - val_my_r2: 0.9943\n",
      "Epoch 1013/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4413e-04 - my_r2: 0.9344 - val_loss: 1.5340e-05 - val_my_r2: 0.9946\n",
      "Epoch 1014/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0631e-04 - my_r2: 0.9148 - val_loss: 1.9760e-05 - val_my_r2: 0.9931\n",
      "Epoch 1015/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5381e-04 - my_r2: 0.9460 - val_loss: 1.7152e-05 - val_my_r2: 0.9945\n",
      "Epoch 1016/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1513e-04 - my_r2: 0.9391 - val_loss: 1.5156e-05 - val_my_r2: 0.9950\n",
      "Epoch 1017/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2774e-04 - my_r2: 0.9444 - val_loss: 1.5000e-05 - val_my_r2: 0.9950\n",
      "Epoch 1018/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8041e-04 - my_r2: 0.9242 - val_loss: 1.5459e-05 - val_my_r2: 0.9947\n",
      "Epoch 1019/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7898e-04 - my_r2: 0.9201 - val_loss: 1.4885e-05 - val_my_r2: 0.9948\n",
      "Epoch 1020/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0324e-04 - my_r2: 0.9475 - val_loss: 1.5590e-05 - val_my_r2: 0.9945\n",
      "Epoch 1021/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1992e-04 - my_r2: 0.9582 - val_loss: 1.5410e-05 - val_my_r2: 0.9947\n",
      "Epoch 1022/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9569e-04 - my_r2: 0.9080 - val_loss: 1.9124e-05 - val_my_r2: 0.9932\n",
      "Epoch 1023/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4479e-04 - my_r2: 0.9416 - val_loss: 1.8084e-05 - val_my_r2: 0.9934\n",
      "Epoch 1024/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6525e-04 - my_r2: 0.9145 - val_loss: 1.6019e-05 - val_my_r2: 0.9943\n",
      "Epoch 1025/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2671e-04 - my_r2: 0.9338 - val_loss: 1.5870e-05 - val_my_r2: 0.9943\n",
      "Epoch 1026/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2254e-04 - my_r2: 0.9516 - val_loss: 1.5530e-05 - val_my_r2: 0.9944\n",
      "Epoch 1027/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9461e-04 - my_r2: 0.8810 - val_loss: 2.0418e-05 - val_my_r2: 0.9931\n",
      "Epoch 1028/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5611e-04 - my_r2: 0.9341 - val_loss: 2.1581e-05 - val_my_r2: 0.9928\n",
      "Epoch 1029/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4035e-04 - my_r2: 0.9371 - val_loss: 2.5667e-05 - val_my_r2: 0.9910\n",
      "Epoch 1030/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4233e-04 - my_r2: 0.9178 - val_loss: 1.8986e-05 - val_my_r2: 0.9935\n",
      "Epoch 1031/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4551e-04 - my_r2: 0.9343 - val_loss: 1.8130e-05 - val_my_r2: 0.9941\n",
      "Epoch 1032/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6994e-04 - my_r2: 0.9373 - val_loss: 1.8046e-05 - val_my_r2: 0.9945\n",
      "Epoch 1033/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2550e-04 - my_r2: 0.9518 - val_loss: 1.5678e-05 - val_my_r2: 0.9946\n",
      "Epoch 1034/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9045e-04 - my_r2: 0.9434 - val_loss: 1.7454e-05 - val_my_r2: 0.9942\n",
      "Epoch 1035/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9023e-04 - my_r2: 0.9302 - val_loss: 1.8647e-05 - val_my_r2: 0.9942\n",
      "Epoch 1036/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7895e-04 - my_r2: 0.8217 - val_loss: 1.8018e-05 - val_my_r2: 0.9942\n",
      "Epoch 1037/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3179e-04 - my_r2: 0.8976 - val_loss: 1.7270e-05 - val_my_r2: 0.9943\n",
      "Epoch 1038/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8850e-04 - my_r2: 0.8981 - val_loss: 1.8957e-05 - val_my_r2: 0.9935\n",
      "Epoch 1039/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1724e-04 - my_r2: 0.9096 - val_loss: 1.6422e-05 - val_my_r2: 0.9941\n",
      "Epoch 1040/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8515e-04 - my_r2: 0.9118 - val_loss: 1.5586e-05 - val_my_r2: 0.9946\n",
      "Epoch 1041/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3310e-04 - my_r2: 0.8513 - val_loss: 1.6207e-05 - val_my_r2: 0.9946\n",
      "Epoch 1042/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4479e-04 - my_r2: 0.9400 - val_loss: 1.5249e-05 - val_my_r2: 0.9948\n",
      "Epoch 1043/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4307e-04 - my_r2: 0.9106 - val_loss: 1.9113e-05 - val_my_r2: 0.9935\n",
      "Epoch 1044/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4244e-04 - my_r2: 0.9329 - val_loss: 1.9846e-05 - val_my_r2: 0.9928\n",
      "Epoch 1045/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.2770e-04 - my_r2: 0.9024 - val_loss: 1.5130e-05 - val_my_r2: 0.9952\n",
      "Epoch 1046/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0689e-04 - my_r2: 0.9210 - val_loss: 1.6956e-05 - val_my_r2: 0.9946\n",
      "Epoch 1047/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6579e-04 - my_r2: 0.9279 - val_loss: 1.5934e-05 - val_my_r2: 0.9949\n",
      "Epoch 1048/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6221e-04 - my_r2: 0.9499 - val_loss: 1.4958e-05 - val_my_r2: 0.9949\n",
      "Epoch 1049/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0439e-04 - my_r2: 0.9008 - val_loss: 1.5049e-05 - val_my_r2: 0.9950\n",
      "Epoch 1050/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2935e-04 - my_r2: 0.8981 - val_loss: 1.5956e-05 - val_my_r2: 0.9947\n",
      "Epoch 1051/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8294e-04 - my_r2: 0.9274 - val_loss: 1.3895e-05 - val_my_r2: 0.9951\n",
      "Epoch 1052/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2145e-04 - my_r2: 0.9205 - val_loss: 1.7801e-05 - val_my_r2: 0.9939\n",
      "Epoch 1053/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3607e-04 - my_r2: 0.9382 - val_loss: 1.3933e-05 - val_my_r2: 0.9949\n",
      "Epoch 1054/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3991e-04 - my_r2: 0.9073 - val_loss: 1.7484e-05 - val_my_r2: 0.9936\n",
      "Epoch 1055/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8611e-04 - my_r2: 0.9340 - val_loss: 1.3774e-05 - val_my_r2: 0.9950\n",
      "Epoch 1056/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6040e-04 - my_r2: 0.9411 - val_loss: 1.3430e-05 - val_my_r2: 0.9953\n",
      "Epoch 1057/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7869e-04 - my_r2: 0.9496 - val_loss: 1.6592e-05 - val_my_r2: 0.9943\n",
      "Epoch 1058/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3470e-04 - my_r2: 0.9117 - val_loss: 1.3991e-05 - val_my_r2: 0.9949\n",
      "Epoch 1059/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3114e-04 - my_r2: 0.9394 - val_loss: 1.4432e-05 - val_my_r2: 0.9948\n",
      "Epoch 1060/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3899e-04 - my_r2: 0.9123 - val_loss: 1.4386e-05 - val_my_r2: 0.9952\n",
      "Epoch 1061/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0520e-04 - my_r2: 0.9088 - val_loss: 2.8880e-05 - val_my_r2: 0.9904\n",
      "Epoch 1062/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7344e-04 - my_r2: 0.9466 - val_loss: 2.7307e-05 - val_my_r2: 0.9912\n",
      "Epoch 1063/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5121e-04 - my_r2: 0.9296 - val_loss: 1.9030e-05 - val_my_r2: 0.9941\n",
      "Epoch 1064/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2686e-04 - my_r2: 0.8893 - val_loss: 1.5211e-05 - val_my_r2: 0.9953\n",
      "Epoch 1065/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3969e-04 - my_r2: 0.9495 - val_loss: 1.4109e-05 - val_my_r2: 0.9956\n",
      "Epoch 1066/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3835e-04 - my_r2: 0.9397 - val_loss: 1.5892e-05 - val_my_r2: 0.9951\n",
      "Epoch 1067/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8409e-04 - my_r2: 0.9260 - val_loss: 1.4822e-05 - val_my_r2: 0.9953\n",
      "Epoch 1068/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1888e-04 - my_r2: 0.9349 - val_loss: 1.5990e-05 - val_my_r2: 0.9943\n",
      "Epoch 1069/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3492e-04 - my_r2: 0.9166 - val_loss: 1.9734e-05 - val_my_r2: 0.9931\n",
      "Epoch 1070/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2175e-04 - my_r2: 0.9009 - val_loss: 1.9672e-05 - val_my_r2: 0.9939\n",
      "Epoch 1071/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9111e-04 - my_r2: 0.9360 - val_loss: 2.1862e-05 - val_my_r2: 0.9931\n",
      "Epoch 1072/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8427e-04 - my_r2: 0.8407 - val_loss: 2.0691e-05 - val_my_r2: 0.9934\n",
      "Epoch 1073/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2541e-04 - my_r2: 0.9212 - val_loss: 2.0841e-05 - val_my_r2: 0.9935\n",
      "Epoch 1074/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8570e-04 - my_r2: 0.8752 - val_loss: 1.8638e-05 - val_my_r2: 0.9941\n",
      "Epoch 1075/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3661e-04 - my_r2: 0.9074 - val_loss: 1.5557e-05 - val_my_r2: 0.9948\n",
      "Epoch 1076/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1518e-04 - my_r2: 0.9017 - val_loss: 1.4617e-05 - val_my_r2: 0.9949\n",
      "Epoch 1077/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2238e-04 - my_r2: 0.9475 - val_loss: 1.3522e-05 - val_my_r2: 0.9951\n",
      "Epoch 1078/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0200e-04 - my_r2: 0.8998 - val_loss: 1.3430e-05 - val_my_r2: 0.9950\n",
      "Epoch 1079/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9126e-04 - my_r2: 0.9298 - val_loss: 1.4879e-05 - val_my_r2: 0.9946\n",
      "Epoch 1080/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5314e-04 - my_r2: 0.9315 - val_loss: 1.5145e-05 - val_my_r2: 0.9945\n",
      "Epoch 1081/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2239e-04 - my_r2: 0.9412 - val_loss: 1.5521e-05 - val_my_r2: 0.9945\n",
      "Epoch 1082/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4443e-04 - my_r2: 0.9488 - val_loss: 1.6170e-05 - val_my_r2: 0.9946\n",
      "Epoch 1083/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8977e-04 - my_r2: 0.9026 - val_loss: 2.2168e-05 - val_my_r2: 0.9924\n",
      "Epoch 1084/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2842e-04 - my_r2: 0.9053 - val_loss: 2.3818e-05 - val_my_r2: 0.9913\n",
      "Epoch 1085/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8282e-04 - my_r2: 0.8959 - val_loss: 3.0055e-05 - val_my_r2: 0.9889\n",
      "Epoch 1086/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9531e-04 - my_r2: 0.9256 - val_loss: 2.3283e-05 - val_my_r2: 0.9915\n",
      "Epoch 1087/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3979e-04 - my_r2: 0.9163 - val_loss: 1.7309e-05 - val_my_r2: 0.9940\n",
      "Epoch 1088/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4849e-04 - my_r2: 0.9168 - val_loss: 1.3026e-05 - val_my_r2: 0.9960\n",
      "Epoch 1089/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5743e-04 - my_r2: 0.9423 - val_loss: 1.2938e-05 - val_my_r2: 0.9959\n",
      "Epoch 1090/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4776e-04 - my_r2: 0.9225 - val_loss: 1.5025e-05 - val_my_r2: 0.9953\n",
      "Epoch 1091/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4263e-04 - my_r2: 0.8610 - val_loss: 1.5552e-05 - val_my_r2: 0.9951\n",
      "Epoch 1092/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7826e-04 - my_r2: 0.9367 - val_loss: 2.0240e-05 - val_my_r2: 0.9937\n",
      "Epoch 1093/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3919e-04 - my_r2: 0.9456 - val_loss: 1.7817e-05 - val_my_r2: 0.9937\n",
      "Epoch 1094/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5523e-04 - my_r2: 0.9391 - val_loss: 1.5774e-05 - val_my_r2: 0.9944\n",
      "Epoch 1095/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6234e-04 - my_r2: 0.9275 - val_loss: 1.7745e-05 - val_my_r2: 0.9940\n",
      "Epoch 1096/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1838e-04 - my_r2: 0.9227 - val_loss: 1.8284e-05 - val_my_r2: 0.9934\n",
      "Epoch 1097/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7978e-04 - my_r2: 0.9131 - val_loss: 1.8853e-05 - val_my_r2: 0.9935\n",
      "Epoch 1098/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5101e-04 - my_r2: 0.9468 - val_loss: 1.3647e-05 - val_my_r2: 0.9956\n",
      "Epoch 1099/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5164e-04 - my_r2: 0.9278 - val_loss: 1.1082e-05 - val_my_r2: 0.9964\n",
      "Epoch 1100/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7489e-04 - my_r2: 0.9229 - val_loss: 1.6137e-05 - val_my_r2: 0.9947\n",
      "Epoch 1101/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4705e-04 - my_r2: 0.9141 - val_loss: 1.5581e-05 - val_my_r2: 0.9942\n",
      "Epoch 1102/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2389e-04 - my_r2: 0.9251 - val_loss: 1.6291e-05 - val_my_r2: 0.9940\n",
      "Epoch 1103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1513e-04 - my_r2: 0.9463 - val_loss: 1.4195e-05 - val_my_r2: 0.9950\n",
      "Epoch 1104/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4552e-04 - my_r2: 0.9314 - val_loss: 1.4505e-05 - val_my_r2: 0.9949\n",
      "Epoch 1105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6896e-04 - my_r2: 0.9471 - val_loss: 1.3778e-05 - val_my_r2: 0.9952\n",
      "Epoch 1106/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2607e-04 - my_r2: 0.9429 - val_loss: 1.3923e-05 - val_my_r2: 0.9952\n",
      "Epoch 1107/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1518e-04 - my_r2: 0.9483 - val_loss: 1.5731e-05 - val_my_r2: 0.9944\n",
      "Epoch 1108/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5320e-04 - my_r2: 0.9424 - val_loss: 1.4076e-05 - val_my_r2: 0.9951\n",
      "Epoch 1109/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7252e-04 - my_r2: 0.9081 - val_loss: 1.9141e-05 - val_my_r2: 0.9934\n",
      "Epoch 1110/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3501e-04 - my_r2: 0.9323 - val_loss: 1.9713e-05 - val_my_r2: 0.9935\n",
      "Epoch 1111/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7655e-04 - my_r2: 0.9014 - val_loss: 1.2651e-05 - val_my_r2: 0.9954\n",
      "Epoch 1112/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0385e-04 - my_r2: 0.9295 - val_loss: 1.2140e-05 - val_my_r2: 0.9952\n",
      "Epoch 1113/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6654e-04 - my_r2: 0.8779 - val_loss: 1.4623e-05 - val_my_r2: 0.9941\n",
      "Epoch 1114/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1595e-04 - my_r2: 0.9185 - val_loss: 1.4623e-05 - val_my_r2: 0.9935\n",
      "Epoch 1115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0598e-04 - my_r2: 0.9208 - val_loss: 1.6990e-05 - val_my_r2: 0.9928\n",
      "Epoch 1116/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4641e-04 - my_r2: 0.9358 - val_loss: 2.0154e-05 - val_my_r2: 0.9916\n",
      "Epoch 1117/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5310e-04 - my_r2: 0.9397 - val_loss: 1.8454e-05 - val_my_r2: 0.9922\n",
      "Epoch 1118/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4457e-04 - my_r2: 0.9371 - val_loss: 1.5590e-05 - val_my_r2: 0.9933\n",
      "Epoch 1119/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9918e-04 - my_r2: 0.9025 - val_loss: 1.4091e-05 - val_my_r2: 0.9944\n",
      "Epoch 1120/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0559e-04 - my_r2: 0.9338 - val_loss: 1.1851e-05 - val_my_r2: 0.9951\n",
      "Epoch 1121/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3693e-04 - my_r2: -0.2421 - val_loss: 1.3358e-05 - val_my_r2: 0.9950\n",
      "Epoch 1122/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8751e-04 - my_r2: 0.9219 - val_loss: 2.4500e-05 - val_my_r2: 0.9913\n",
      "Epoch 1123/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2936e-04 - my_r2: 0.9388 - val_loss: 1.7145e-05 - val_my_r2: 0.9938\n",
      "Epoch 1124/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7397e-04 - my_r2: 0.9400 - val_loss: 1.3655e-05 - val_my_r2: 0.9951\n",
      "Epoch 1125/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8916e-04 - my_r2: 0.9484 - val_loss: 1.3794e-05 - val_my_r2: 0.9949\n",
      "Epoch 1126/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2187e-04 - my_r2: 0.9435 - val_loss: 1.6107e-05 - val_my_r2: 0.9940\n",
      "Epoch 1127/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4980e-04 - my_r2: 0.9279 - val_loss: 1.4955e-05 - val_my_r2: 0.9944\n",
      "Epoch 1128/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4934e-04 - my_r2: 0.9398 - val_loss: 1.4749e-05 - val_my_r2: 0.9949\n",
      "Epoch 1129/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2442e-04 - my_r2: 0.9284 - val_loss: 1.8900e-05 - val_my_r2: 0.9934\n",
      "Epoch 1130/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4614e-04 - my_r2: 0.9285 - val_loss: 1.6547e-05 - val_my_r2: 0.9939\n",
      "Epoch 1131/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8616e-04 - my_r2: 0.9305 - val_loss: 1.2821e-05 - val_my_r2: 0.9952\n",
      "Epoch 1132/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4439e-04 - my_r2: 0.8295 - val_loss: 1.3019e-05 - val_my_r2: 0.9955\n",
      "Epoch 1133/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2179e-04 - my_r2: 0.8777 - val_loss: 1.4301e-05 - val_my_r2: 0.9952\n",
      "Epoch 1134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8063e-04 - my_r2: 0.8993 - val_loss: 1.3420e-05 - val_my_r2: 0.9959\n",
      "Epoch 1135/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7177e-04 - my_r2: 0.9171 - val_loss: 2.3113e-05 - val_my_r2: 0.9931\n",
      "Epoch 1136/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9683e-04 - my_r2: 0.9438 - val_loss: 2.0912e-05 - val_my_r2: 0.9942\n",
      "Epoch 1137/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4520e-04 - my_r2: 0.9368 - val_loss: 1.7635e-05 - val_my_r2: 0.9949\n",
      "Epoch 1138/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6161e-04 - my_r2: 0.9400 - val_loss: 1.4130e-05 - val_my_r2: 0.9960\n",
      "Epoch 1139/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8273e-04 - my_r2: 0.9080 - val_loss: 1.6507e-05 - val_my_r2: 0.9948\n",
      "Epoch 1140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2958e-04 - my_r2: 0.9128 - val_loss: 1.4510e-05 - val_my_r2: 0.9953\n",
      "Epoch 1141/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5405e-04 - my_r2: 0.9319 - val_loss: 1.7886e-05 - val_my_r2: 0.9947\n",
      "Epoch 1142/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5485e-04 - my_r2: 0.9309 - val_loss: 2.1003e-05 - val_my_r2: 0.9932\n",
      "Epoch 1143/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8098e-04 - my_r2: 0.9140 - val_loss: 1.6572e-05 - val_my_r2: 0.9945\n",
      "Epoch 1144/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6021e-04 - my_r2: 0.9420 - val_loss: 1.2614e-05 - val_my_r2: 0.9957\n",
      "Epoch 1145/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7477e-04 - my_r2: 0.9326 - val_loss: 1.3951e-05 - val_my_r2: 0.9956\n",
      "Epoch 1146/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9779e-04 - my_r2: 0.6609 - val_loss: 1.6371e-05 - val_my_r2: 0.9950\n",
      "Epoch 1147/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9897e-04 - my_r2: 0.9060 - val_loss: 1.3403e-05 - val_my_r2: 0.9958\n",
      "Epoch 1148/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9857e-04 - my_r2: 0.9176 - val_loss: 1.4444e-05 - val_my_r2: 0.9952\n",
      "Epoch 1149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2300e-04 - my_r2: 0.9138 - val_loss: 1.8424e-05 - val_my_r2: 0.9937\n",
      "Epoch 1150/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3456e-04 - my_r2: 0.9346 - val_loss: 1.7212e-05 - val_my_r2: 0.9942\n",
      "Epoch 1151/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8178e-04 - my_r2: 0.9466 - val_loss: 1.9202e-05 - val_my_r2: 0.9936\n",
      "Epoch 1152/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3576e-04 - my_r2: 0.9465 - val_loss: 1.4996e-05 - val_my_r2: 0.9950\n",
      "Epoch 1153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2103e-04 - my_r2: 0.9538 - val_loss: 1.4748e-05 - val_my_r2: 0.9948\n",
      "Epoch 1154/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2067e-04 - my_r2: 0.9472 - val_loss: 1.4665e-05 - val_my_r2: 0.9947\n",
      "Epoch 1155/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9465e-04 - my_r2: 0.8932 - val_loss: 1.3199e-05 - val_my_r2: 0.9958\n",
      "Epoch 1156/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4130e-04 - my_r2: 0.9323 - val_loss: 1.1780e-05 - val_my_r2: 0.9963\n",
      "Epoch 1157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9924e-04 - my_r2: 0.9126 - val_loss: 1.1129e-05 - val_my_r2: 0.9964\n",
      "Epoch 1158/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1349e-04 - my_r2: 0.9231 - val_loss: 1.2144e-05 - val_my_r2: 0.9962\n",
      "Epoch 1159/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6463e-04 - my_r2: 0.9201 - val_loss: 1.0600e-05 - val_my_r2: 0.9965\n",
      "Epoch 1160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8196e-04 - my_r2: 0.9392 - val_loss: 1.1258e-05 - val_my_r2: 0.9953\n",
      "Epoch 1161/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0890e-04 - my_r2: 0.9442 - val_loss: 1.1889e-05 - val_my_r2: 0.9954\n",
      "Epoch 1162/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7962e-04 - my_r2: 0.9648 - val_loss: 1.2001e-05 - val_my_r2: 0.9957\n",
      "Epoch 1163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3797e-04 - my_r2: 0.9427 - val_loss: 1.0975e-05 - val_my_r2: 0.9962\n",
      "Epoch 1164/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1957e-04 - my_r2: 0.9111 - val_loss: 1.4419e-05 - val_my_r2: 0.9948\n",
      "Epoch 1165/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.6704e-04 - my_r2: 0.9468 - val_loss: 1.3210e-05 - val_my_r2: 0.9951\n",
      "Epoch 1166/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2726e-04 - my_r2: 0.9333 - val_loss: 1.0639e-05 - val_my_r2: 0.9963\n",
      "Epoch 1167/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4452e-04 - my_r2: 0.9243 - val_loss: 1.2152e-05 - val_my_r2: 0.9949\n",
      "Epoch 1168/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1040e-04 - my_r2: 0.9169 - val_loss: 1.3364e-05 - val_my_r2: 0.9944\n",
      "Epoch 1169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7947e-04 - my_r2: 0.9450 - val_loss: 1.4568e-05 - val_my_r2: 0.9945\n",
      "Epoch 1170/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6281e-04 - my_r2: 0.9212 - val_loss: 1.2220e-05 - val_my_r2: 0.9954\n",
      "Epoch 1171/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8486e-04 - my_r2: 0.9260 - val_loss: 1.2241e-05 - val_my_r2: 0.9955\n",
      "Epoch 1172/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7580e-04 - my_r2: 0.9425 - val_loss: 1.7178e-05 - val_my_r2: 0.9939\n",
      "Epoch 1173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8958e-04 - my_r2: 0.9137 - val_loss: 1.3918e-05 - val_my_r2: 0.9950\n",
      "Epoch 1174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6708e-04 - my_r2: 0.8733 - val_loss: 1.2491e-05 - val_my_r2: 0.9955\n",
      "Epoch 1175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7782e-04 - my_r2: 0.9082 - val_loss: 1.3321e-05 - val_my_r2: 0.9947\n",
      "Epoch 1176/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6957e-04 - my_r2: 0.9312 - val_loss: 9.9210e-06 - val_my_r2: 0.9963\n",
      "Epoch 1177/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4816e-04 - my_r2: 0.9209 - val_loss: 9.2691e-06 - val_my_r2: 0.9967\n",
      "Epoch 1178/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0885e-04 - my_r2: 0.9257 - val_loss: 1.4880e-05 - val_my_r2: 0.9950\n",
      "Epoch 1179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8049e-04 - my_r2: 0.9370 - val_loss: 1.1331e-05 - val_my_r2: 0.9962\n",
      "Epoch 1180/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1930e-04 - my_r2: 0.9418 - val_loss: 1.2734e-05 - val_my_r2: 0.9955\n",
      "Epoch 1181/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3276e-04 - my_r2: 0.9342 - val_loss: 1.9041e-05 - val_my_r2: 0.9937\n",
      "Epoch 1182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1516e-04 - my_r2: 0.9002 - val_loss: 1.4115e-05 - val_my_r2: 0.9953\n",
      "Epoch 1183/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6251e-04 - my_r2: 0.8604 - val_loss: 1.1071e-05 - val_my_r2: 0.9961\n",
      "Epoch 1184/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6620e-04 - my_r2: 0.8785 - val_loss: 1.0449e-05 - val_my_r2: 0.9966\n",
      "Epoch 1185/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1626e-04 - my_r2: 0.9306 - val_loss: 1.2449e-05 - val_my_r2: 0.9962\n",
      "Epoch 1186/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7923e-04 - my_r2: 0.9443 - val_loss: 1.2938e-05 - val_my_r2: 0.9962\n",
      "Epoch 1187/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4819e-04 - my_r2: 0.9397 - val_loss: 1.1404e-05 - val_my_r2: 0.9965\n",
      "Epoch 1188/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8979e-04 - my_r2: 0.9028 - val_loss: 1.4143e-05 - val_my_r2: 0.9953\n",
      "Epoch 1189/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4763e-04 - my_r2: 0.8650 - val_loss: 1.3679e-05 - val_my_r2: 0.9953\n",
      "Epoch 1190/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4549e-04 - my_r2: 0.9435 - val_loss: 1.6264e-05 - val_my_r2: 0.9941\n",
      "Epoch 1191/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3874e-04 - my_r2: 0.9098 - val_loss: 1.4134e-05 - val_my_r2: 0.9950\n",
      "Epoch 1192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5615e-04 - my_r2: 0.9253 - val_loss: 1.5391e-05 - val_my_r2: 0.9943\n",
      "Epoch 1193/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6897e-04 - my_r2: 0.9137 - val_loss: 1.4689e-05 - val_my_r2: 0.9941\n",
      "Epoch 1194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6183e-04 - my_r2: 0.8427 - val_loss: 1.5853e-05 - val_my_r2: 0.9939\n",
      "Epoch 1195/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6010e-04 - my_r2: 0.9411 - val_loss: 1.6271e-05 - val_my_r2: 0.9939\n",
      "Epoch 1196/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0559e-04 - my_r2: 0.9341 - val_loss: 1.2394e-05 - val_my_r2: 0.9954\n",
      "Epoch 1197/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8847e-04 - my_r2: 0.9523 - val_loss: 1.0237e-05 - val_my_r2: 0.9962\n",
      "Epoch 1198/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0449e-04 - my_r2: 0.9464 - val_loss: 1.7552e-05 - val_my_r2: 0.9938\n",
      "Epoch 1199/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7092e-04 - my_r2: 0.9439 - val_loss: 1.2786e-05 - val_my_r2: 0.9956\n",
      "Epoch 1200/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7740e-04 - my_r2: 0.8080 - val_loss: 1.4251e-05 - val_my_r2: 0.9951\n",
      "Epoch 1201/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4845e-04 - my_r2: 0.9128 - val_loss: 1.5499e-05 - val_my_r2: 0.9949\n",
      "Epoch 1202/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6126e-04 - my_r2: 0.9347 - val_loss: 1.1470e-05 - val_my_r2: 0.9959\n",
      "Epoch 1203/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9055e-04 - my_r2: 0.9370 - val_loss: 1.0435e-05 - val_my_r2: 0.9961\n",
      "Epoch 1204/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0799e-04 - my_r2: 0.9192 - val_loss: 9.0795e-06 - val_my_r2: 0.9967\n",
      "Epoch 1205/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8078e-04 - my_r2: 0.9165 - val_loss: 1.4949e-05 - val_my_r2: 0.9948\n",
      "Epoch 1206/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8212e-04 - my_r2: 0.9132 - val_loss: 2.1939e-05 - val_my_r2: 0.9926\n",
      "Epoch 1207/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2082e-04 - my_r2: 0.9246 - val_loss: 8.9438e-06 - val_my_r2: 0.9969\n",
      "Epoch 1208/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0544e-04 - my_r2: 0.9503 - val_loss: 1.2656e-05 - val_my_r2: 0.9952\n",
      "Epoch 1209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1932e-04 - my_r2: 0.9444 - val_loss: 1.4371e-05 - val_my_r2: 0.9949\n",
      "Epoch 1210/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2466e-04 - my_r2: 0.9318 - val_loss: 1.2522e-05 - val_my_r2: 0.9954\n",
      "Epoch 1211/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4918e-04 - my_r2: 0.9099 - val_loss: 1.2292e-05 - val_my_r2: 0.9953\n",
      "Epoch 1212/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5882e-04 - my_r2: 0.9261 - val_loss: 1.4883e-05 - val_my_r2: 0.9947\n",
      "Epoch 1213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6891e-04 - my_r2: 0.9510 - val_loss: 1.2703e-05 - val_my_r2: 0.9954\n",
      "Epoch 1214/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9646e-04 - my_r2: 0.9275 - val_loss: 1.3298e-05 - val_my_r2: 0.9954\n",
      "Epoch 1215/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3620e-04 - my_r2: 0.9400 - val_loss: 1.2725e-05 - val_my_r2: 0.9958\n",
      "Epoch 1216/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8906e-04 - my_r2: 0.9251 - val_loss: 1.5101e-05 - val_my_r2: 0.9954\n",
      "Epoch 1217/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7747e-04 - my_r2: 0.9248 - val_loss: 1.6046e-05 - val_my_r2: 0.9951\n",
      "Epoch 1218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6545e-04 - my_r2: 0.9365 - val_loss: 2.3647e-05 - val_my_r2: 0.9929\n",
      "Epoch 1219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6101e-04 - my_r2: 0.8238 - val_loss: 1.5351e-05 - val_my_r2: 0.9953\n",
      "Epoch 1220/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8864e-04 - my_r2: 0.9370 - val_loss: 1.1564e-05 - val_my_r2: 0.9964\n",
      "Epoch 1221/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4463e-04 - my_r2: 0.9317 - val_loss: 1.1450e-05 - val_my_r2: 0.9962\n",
      "Epoch 1222/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1841e-04 - my_r2: 0.9158 - val_loss: 1.1809e-05 - val_my_r2: 0.9961\n",
      "Epoch 1223/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6941e-04 - my_r2: 0.8544 - val_loss: 1.0014e-05 - val_my_r2: 0.9965\n",
      "Epoch 1224/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2337e-04 - my_r2: 0.9485 - val_loss: 1.5770e-05 - val_my_r2: 0.9947\n",
      "Epoch 1225/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0403e-04 - my_r2: 0.9167 - val_loss: 1.3266e-05 - val_my_r2: 0.9955\n",
      "Epoch 1226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6003e-04 - my_r2: 0.9348 - val_loss: 1.0827e-05 - val_my_r2: 0.9965\n",
      "Epoch 1227/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8781e-04 - my_r2: 0.8966 - val_loss: 1.0776e-05 - val_my_r2: 0.9964\n",
      "Epoch 1228/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1132e-04 - my_r2: 0.9312 - val_loss: 1.1813e-05 - val_my_r2: 0.9958\n",
      "Epoch 1229/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8261e-04 - my_r2: 0.9128 - val_loss: 1.2853e-05 - val_my_r2: 0.9957\n",
      "Epoch 1230/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7976e-04 - my_r2: 0.9318 - val_loss: 1.2782e-05 - val_my_r2: 0.9958\n",
      "Epoch 1231/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5786e-04 - my_r2: 0.9261 - val_loss: 1.6028e-05 - val_my_r2: 0.9939\n",
      "Epoch 1232/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2143e-04 - my_r2: 0.9419 - val_loss: 1.3177e-05 - val_my_r2: 0.9945\n",
      "Epoch 1233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2049e-04 - my_r2: 0.9288 - val_loss: 1.5869e-05 - val_my_r2: 0.9936\n",
      "Epoch 1234/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3742e-04 - my_r2: 0.9419 - val_loss: 1.6132e-05 - val_my_r2: 0.9935\n",
      "Epoch 1235/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5024e-04 - my_r2: 0.8874 - val_loss: 1.6086e-05 - val_my_r2: 0.9929\n",
      "Epoch 1236/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0502e-04 - my_r2: 0.9293 - val_loss: 1.3650e-05 - val_my_r2: 0.9939\n",
      "Epoch 1237/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6821e-04 - my_r2: 0.9541 - val_loss: 1.1379e-05 - val_my_r2: 0.9951\n",
      "Epoch 1238/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1806e-04 - my_r2: 0.9513 - val_loss: 1.1462e-05 - val_my_r2: 0.9955\n",
      "Epoch 1239/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8015e-04 - my_r2: 0.9277 - val_loss: 1.2271e-05 - val_my_r2: 0.9953\n",
      "Epoch 1240/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2798e-04 - my_r2: 0.9209 - val_loss: 1.4260e-05 - val_my_r2: 0.9948\n",
      "Epoch 1241/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2872e-04 - my_r2: 0.9523 - val_loss: 1.4596e-05 - val_my_r2: 0.9950\n",
      "Epoch 1242/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6839e-04 - my_r2: 0.9162 - val_loss: 1.4794e-05 - val_my_r2: 0.9951\n",
      "Epoch 1243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0252e-04 - my_r2: 0.8600 - val_loss: 1.4234e-05 - val_my_r2: 0.9952\n",
      "Epoch 1244/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0217e-04 - my_r2: 0.9500 - val_loss: 1.5631e-05 - val_my_r2: 0.9947\n",
      "Epoch 1245/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5646e-04 - my_r2: 0.9362 - val_loss: 1.2957e-05 - val_my_r2: 0.9955\n",
      "Epoch 1246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6004e-04 - my_r2: 0.9417 - val_loss: 1.4055e-05 - val_my_r2: 0.9944\n",
      "Epoch 1247/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5193e-04 - my_r2: 0.9311 - val_loss: 1.5481e-05 - val_my_r2: 0.9944\n",
      "Epoch 1248/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2627e-04 - my_r2: 0.9304 - val_loss: 1.5151e-05 - val_my_r2: 0.9948\n",
      "Epoch 1249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7118e-04 - my_r2: 0.9154 - val_loss: 1.8104e-05 - val_my_r2: 0.9940\n",
      "Epoch 1250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1867e-04 - my_r2: 0.9332 - val_loss: 1.7088e-05 - val_my_r2: 0.9942\n",
      "Epoch 1251/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9421e-04 - my_r2: 0.9288 - val_loss: 1.6322e-05 - val_my_r2: 0.9944\n",
      "Epoch 1252/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8810e-04 - my_r2: 0.9283 - val_loss: 1.3374e-05 - val_my_r2: 0.9950\n",
      "Epoch 1253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2013e-04 - my_r2: 0.9479 - val_loss: 1.4867e-05 - val_my_r2: 0.9944\n",
      "Epoch 1254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4260e-04 - my_r2: 0.9362 - val_loss: 1.4097e-05 - val_my_r2: 0.9946\n",
      "Epoch 1255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2547e-04 - my_r2: 0.8964 - val_loss: 1.4488e-05 - val_my_r2: 0.9948\n",
      "Epoch 1256/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2350e-04 - my_r2: 0.9094 - val_loss: 1.2882e-05 - val_my_r2: 0.9959\n",
      "Epoch 1257/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9911e-04 - my_r2: 0.9293 - val_loss: 1.2749e-05 - val_my_r2: 0.9961\n",
      "Epoch 1258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9233e-04 - my_r2: 0.9575 - val_loss: 1.1609e-05 - val_my_r2: 0.9963\n",
      "Epoch 1259/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6664e-04 - my_r2: 0.9267 - val_loss: 1.1912e-05 - val_my_r2: 0.9962\n",
      "Epoch 1260/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6606e-04 - my_r2: 0.9023 - val_loss: 1.4351e-05 - val_my_r2: 0.9953\n",
      "Epoch 1261/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9439e-04 - my_r2: 0.9351 - val_loss: 1.2311e-05 - val_my_r2: 0.9961\n",
      "Epoch 1262/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6394e-04 - my_r2: 0.9239 - val_loss: 1.3238e-05 - val_my_r2: 0.9953\n",
      "Epoch 1263/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9851e-04 - my_r2: 0.9254 - val_loss: 1.3857e-05 - val_my_r2: 0.9947\n",
      "Epoch 1264/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8209e-04 - my_r2: 0.9541 - val_loss: 1.5230e-05 - val_my_r2: 0.9944\n",
      "Epoch 1265/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8853e-04 - my_r2: 0.9259 - val_loss: 1.5041e-05 - val_my_r2: 0.9945\n",
      "Epoch 1266/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1872e-04 - my_r2: 0.9169 - val_loss: 1.7514e-05 - val_my_r2: 0.9934\n",
      "Epoch 1267/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0591e-04 - my_r2: 0.8633 - val_loss: 1.0838e-05 - val_my_r2: 0.9960\n",
      "Epoch 1268/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8347e-04 - my_r2: 0.9173 - val_loss: 1.1146e-05 - val_my_r2: 0.9961\n",
      "Epoch 1269/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8824e-04 - my_r2: 0.9405 - val_loss: 1.5182e-05 - val_my_r2: 0.9949\n",
      "Epoch 1270/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3408e-04 - my_r2: 0.9395 - val_loss: 1.7026e-05 - val_my_r2: 0.9943\n",
      "Epoch 1271/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9928e-04 - my_r2: 0.9317 - val_loss: 1.5789e-05 - val_my_r2: 0.9946\n",
      "Epoch 1272/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7097e-04 - my_r2: 0.9330 - val_loss: 1.0654e-05 - val_my_r2: 0.9966\n",
      "Epoch 1273/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3720e-04 - my_r2: 0.9026 - val_loss: 1.0860e-05 - val_my_r2: 0.9965\n",
      "Epoch 1274/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0432e-04 - my_r2: 0.8968 - val_loss: 1.1843e-05 - val_my_r2: 0.9961\n",
      "Epoch 1275/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0352e-04 - my_r2: 0.9101 - val_loss: 1.2687e-05 - val_my_r2: 0.9955\n",
      "Epoch 1276/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2445e-04 - my_r2: 0.9489 - val_loss: 1.2623e-05 - val_my_r2: 0.9954\n",
      "Epoch 1277/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0353e-04 - my_r2: 0.9265 - val_loss: 1.1053e-05 - val_my_r2: 0.9960\n",
      "Epoch 1278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8834e-04 - my_r2: 0.9069 - val_loss: 1.0855e-05 - val_my_r2: 0.9957\n",
      "Epoch 1279/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4324e-04 - my_r2: 0.9349 - val_loss: 1.1289e-05 - val_my_r2: 0.9956\n",
      "Epoch 1280/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0346e-04 - my_r2: 0.9541 - val_loss: 1.1128e-05 - val_my_r2: 0.9958\n",
      "Epoch 1281/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3486e-04 - my_r2: 0.9322 - val_loss: 1.1777e-05 - val_my_r2: 0.9957\n",
      "Epoch 1282/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3324e-04 - my_r2: 0.8977 - val_loss: 1.3357e-05 - val_my_r2: 0.9948\n",
      "Epoch 1283/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9877e-04 - my_r2: 0.9199 - val_loss: 1.7215e-05 - val_my_r2: 0.9930\n",
      "Epoch 1284/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1597e-04 - my_r2: 0.9089 - val_loss: 1.8598e-05 - val_my_r2: 0.9925\n",
      "Epoch 1285/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7308e-04 - my_r2: 0.9156 - val_loss: 1.4142e-05 - val_my_r2: 0.9943\n",
      "Epoch 1286/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4681e-04 - my_r2: 0.8847 - val_loss: 1.0970e-05 - val_my_r2: 0.9958\n",
      "Epoch 1287/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9466e-04 - my_r2: 0.9298 - val_loss: 1.1138e-05 - val_my_r2: 0.9956\n",
      "Epoch 1288/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7017e-04 - my_r2: 0.9428 - val_loss: 1.1174e-05 - val_my_r2: 0.9959\n",
      "Epoch 1289/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3117e-04 - my_r2: 0.9382 - val_loss: 1.0688e-05 - val_my_r2: 0.9962\n",
      "Epoch 1290/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0208e-04 - my_r2: 0.9192 - val_loss: 1.0342e-05 - val_my_r2: 0.9964\n",
      "Epoch 1291/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8840e-04 - my_r2: 0.9473 - val_loss: 1.1850e-05 - val_my_r2: 0.9958\n",
      "Epoch 1292/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6379e-04 - my_r2: 0.9362 - val_loss: 1.1264e-05 - val_my_r2: 0.9961\n",
      "Epoch 1293/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8834e-04 - my_r2: 0.8386 - val_loss: 1.0742e-05 - val_my_r2: 0.9960\n",
      "Epoch 1294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4211e-04 - my_r2: 0.9355 - val_loss: 1.3162e-05 - val_my_r2: 0.9953\n",
      "Epoch 1295/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6435e-04 - my_r2: 0.9301 - val_loss: 1.2781e-05 - val_my_r2: 0.9958\n",
      "Epoch 1296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1541e-04 - my_r2: 0.9481 - val_loss: 1.6984e-05 - val_my_r2: 0.9947\n",
      "Epoch 1297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7066e-04 - my_r2: 0.8932 - val_loss: 1.1810e-05 - val_my_r2: 0.9963\n",
      "Epoch 1298/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5612e-04 - my_r2: 0.9349 - val_loss: 1.7133e-05 - val_my_r2: 0.9946\n",
      "Epoch 1299/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3927e-04 - my_r2: 0.9334 - val_loss: 1.3059e-05 - val_my_r2: 0.9954\n",
      "Epoch 1300/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7750e-04 - my_r2: 0.9156 - val_loss: 1.3489e-05 - val_my_r2: 0.9950\n",
      "Epoch 1301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7759e-04 - my_r2: 0.8906 - val_loss: 1.3723e-05 - val_my_r2: 0.9949\n",
      "Epoch 1302/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9313e-04 - my_r2: 0.9426 - val_loss: 1.5386e-05 - val_my_r2: 0.9944\n",
      "Epoch 1303/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0143e-04 - my_r2: 0.9476 - val_loss: 1.4013e-05 - val_my_r2: 0.9951\n",
      "Epoch 1304/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9718e-04 - my_r2: 0.9616 - val_loss: 1.6046e-05 - val_my_r2: 0.9944\n",
      "Epoch 1305/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4587e-04 - my_r2: 0.9286 - val_loss: 1.5531e-05 - val_my_r2: 0.9942\n",
      "Epoch 1306/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0767e-04 - my_r2: 0.9231 - val_loss: 1.2728e-05 - val_my_r2: 0.9953\n",
      "Epoch 1307/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3665e-04 - my_r2: 0.8724 - val_loss: 1.3640e-05 - val_my_r2: 0.9947\n",
      "Epoch 1308/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1185e-04 - my_r2: 0.9469 - val_loss: 1.5891e-05 - val_my_r2: 0.9939\n",
      "Epoch 1309/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2687e-04 - my_r2: 0.9499 - val_loss: 1.6078e-05 - val_my_r2: 0.9937\n",
      "Epoch 1310/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8027e-04 - my_r2: 0.9308 - val_loss: 1.5603e-05 - val_my_r2: 0.9942\n",
      "Epoch 1311/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4276e-04 - my_r2: 0.9428 - val_loss: 1.2168e-05 - val_my_r2: 0.9957\n",
      "Epoch 1312/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4952e-04 - my_r2: 0.9361 - val_loss: 1.4009e-05 - val_my_r2: 0.9951\n",
      "Epoch 1313/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5431e-04 - my_r2: 0.9404 - val_loss: 1.7940e-05 - val_my_r2: 0.9938\n",
      "Epoch 1314/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7941e-04 - my_r2: 0.9369 - val_loss: 2.3231e-05 - val_my_r2: 0.9922\n",
      "Epoch 1315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4443e-04 - my_r2: 0.9410 - val_loss: 1.8761e-05 - val_my_r2: 0.9934\n",
      "Epoch 1316/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3813e-04 - my_r2: 0.9109 - val_loss: 1.5867e-05 - val_my_r2: 0.9943\n",
      "Epoch 1317/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9713e-04 - my_r2: 0.9524 - val_loss: 1.3635e-05 - val_my_r2: 0.9947\n",
      "Epoch 1318/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8243e-04 - my_r2: 0.9053 - val_loss: 1.5049e-05 - val_my_r2: 0.9945\n",
      "Epoch 1319/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2761e-04 - my_r2: 0.8792 - val_loss: 1.9337e-05 - val_my_r2: 0.9932\n",
      "Epoch 1320/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1858e-04 - my_r2: 0.9283 - val_loss: 1.5654e-05 - val_my_r2: 0.9946\n",
      "Epoch 1321/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0547e-04 - my_r2: 0.9425 - val_loss: 1.2306e-05 - val_my_r2: 0.9959\n",
      "Epoch 1322/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3045e-04 - my_r2: 0.8797 - val_loss: 1.0092e-05 - val_my_r2: 0.9963\n",
      "Epoch 1323/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8598e-04 - my_r2: 0.9223 - val_loss: 1.1639e-05 - val_my_r2: 0.9960\n",
      "Epoch 1324/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.7122e-04 - my_r2: 0.8228 - val_loss: 1.0558e-05 - val_my_r2: 0.9962\n",
      "Epoch 1325/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8683e-04 - my_r2: 0.9096 - val_loss: 1.1180e-05 - val_my_r2: 0.9964\n",
      "Epoch 1326/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4386e-04 - my_r2: 0.9253 - val_loss: 1.1697e-05 - val_my_r2: 0.9962\n",
      "Epoch 1327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3029e-04 - my_r2: 0.9224 - val_loss: 1.5711e-05 - val_my_r2: 0.9949\n",
      "Epoch 1328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9240e-04 - my_r2: 0.9319 - val_loss: 1.3705e-05 - val_my_r2: 0.9954\n",
      "Epoch 1329/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4937e-04 - my_r2: 0.9457 - val_loss: 1.3382e-05 - val_my_r2: 0.9956\n",
      "Epoch 1330/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8009e-04 - my_r2: 0.9250 - val_loss: 1.4311e-05 - val_my_r2: 0.9953\n",
      "Epoch 1331/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2215e-04 - my_r2: 0.9090 - val_loss: 1.2282e-05 - val_my_r2: 0.9958\n",
      "Epoch 1332/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1111e-04 - my_r2: 0.9079 - val_loss: 1.1613e-05 - val_my_r2: 0.9955\n",
      "Epoch 1333/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6438e-04 - my_r2: 0.9349 - val_loss: 1.4516e-05 - val_my_r2: 0.9941\n",
      "Epoch 1334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9765e-04 - my_r2: 0.8803 - val_loss: 1.3602e-05 - val_my_r2: 0.9944\n",
      "Epoch 1335/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6616e-04 - my_r2: 0.9267 - val_loss: 1.2452e-05 - val_my_r2: 0.9954\n",
      "Epoch 1336/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8004e-04 - my_r2: 0.9123 - val_loss: 1.1719e-05 - val_my_r2: 0.9955\n",
      "Epoch 1337/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3794e-04 - my_r2: 0.9449 - val_loss: 1.0046e-05 - val_my_r2: 0.9962\n",
      "Epoch 1338/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4829e-04 - my_r2: 0.9148 - val_loss: 1.1614e-05 - val_my_r2: 0.9956\n",
      "Epoch 1339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1288e-04 - my_r2: 0.9300 - val_loss: 1.4701e-05 - val_my_r2: 0.9943\n",
      "Epoch 1340/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3567e-04 - my_r2: 0.9430 - val_loss: 1.6374e-05 - val_my_r2: 0.9936\n",
      "Epoch 1341/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2620e-04 - my_r2: 0.8925 - val_loss: 1.5136e-05 - val_my_r2: 0.9945\n",
      "Epoch 1342/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1163e-04 - my_r2: 0.9140 - val_loss: 2.0014e-05 - val_my_r2: 0.9933\n",
      "Epoch 1343/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9321e-04 - my_r2: 0.9321 - val_loss: 1.7982e-05 - val_my_r2: 0.9940\n",
      "Epoch 1344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6658e-04 - my_r2: 0.9513 - val_loss: 1.8284e-05 - val_my_r2: 0.9938\n",
      "Epoch 1345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5760e-04 - my_r2: 0.9507 - val_loss: 1.2559e-05 - val_my_r2: 0.9957\n",
      "Epoch 1346/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4749e-04 - my_r2: 0.9307 - val_loss: 1.1233e-05 - val_my_r2: 0.9960\n",
      "Epoch 1347/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4420e-04 - my_r2: 0.9427 - val_loss: 1.3481e-05 - val_my_r2: 0.9952\n",
      "Epoch 1348/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3151e-04 - my_r2: 0.9062 - val_loss: 1.5397e-05 - val_my_r2: 0.9946\n",
      "Epoch 1349/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6812e-04 - my_r2: 0.8817 - val_loss: 1.1381e-05 - val_my_r2: 0.9961\n",
      "Epoch 1350/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4505e-04 - my_r2: 0.9411 - val_loss: 1.8758e-05 - val_my_r2: 0.9936\n",
      "Epoch 1351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0546e-04 - my_r2: 0.8955 - val_loss: 1.3826e-05 - val_my_r2: 0.9953\n",
      "Epoch 1352/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5833e-04 - my_r2: 0.9356 - val_loss: 1.4316e-05 - val_my_r2: 0.9955\n",
      "Epoch 1353/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8909e-04 - my_r2: 0.9027 - val_loss: 1.4131e-05 - val_my_r2: 0.9955\n",
      "Epoch 1354/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1379e-04 - my_r2: 0.9561 - val_loss: 1.3678e-05 - val_my_r2: 0.9957\n",
      "Epoch 1355/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8405e-04 - my_r2: 0.9378 - val_loss: 1.4787e-05 - val_my_r2: 0.9954\n",
      "Epoch 1356/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9586e-04 - my_r2: 0.9193 - val_loss: 1.2143e-05 - val_my_r2: 0.9961\n",
      "Epoch 1357/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2379e-04 - my_r2: 0.9220 - val_loss: 1.2003e-05 - val_my_r2: 0.9960\n",
      "Epoch 1358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8739e-04 - my_r2: 0.9361 - val_loss: 1.0740e-05 - val_my_r2: 0.9965\n",
      "Epoch 1359/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2677e-04 - my_r2: 0.9563 - val_loss: 1.2960e-05 - val_my_r2: 0.9956\n",
      "Epoch 1360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6092e-04 - my_r2: 0.9285 - val_loss: 1.9024e-05 - val_my_r2: 0.9931\n",
      "Epoch 1361/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8661e-04 - my_r2: 0.9257 - val_loss: 1.2148e-05 - val_my_r2: 0.9957\n",
      "Epoch 1362/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8607e-04 - my_r2: 0.9218 - val_loss: 1.1456e-05 - val_my_r2: 0.9959\n",
      "Epoch 1363/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4321e-04 - my_r2: 0.9002 - val_loss: 1.3933e-05 - val_my_r2: 0.9954\n",
      "Epoch 1364/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7965e-04 - my_r2: 0.9645 - val_loss: 1.3056e-05 - val_my_r2: 0.9958\n",
      "Epoch 1365/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2663e-04 - my_r2: 0.9457 - val_loss: 1.1011e-05 - val_my_r2: 0.9963\n",
      "Epoch 1366/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6170e-04 - my_r2: 0.9389 - val_loss: 9.4207e-06 - val_my_r2: 0.9967\n",
      "Epoch 1367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0367e-04 - my_r2: 0.9250 - val_loss: 1.0190e-05 - val_my_r2: 0.9966\n",
      "Epoch 1368/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7215e-04 - my_r2: 0.9029 - val_loss: 1.0172e-05 - val_my_r2: 0.9964\n",
      "Epoch 1369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2532e-04 - my_r2: 0.9478 - val_loss: 1.1014e-05 - val_my_r2: 0.9964\n",
      "Epoch 1370/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0852e-04 - my_r2: 0.9059 - val_loss: 8.3083e-06 - val_my_r2: 0.9971\n",
      "Epoch 1371/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3847e-04 - my_r2: 0.9190 - val_loss: 1.7018e-05 - val_my_r2: 0.9941\n",
      "Epoch 1372/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9748e-04 - my_r2: 0.9489 - val_loss: 1.3802e-05 - val_my_r2: 0.9954\n",
      "Epoch 1373/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9841e-04 - my_r2: 0.9525 - val_loss: 1.6301e-05 - val_my_r2: 0.9946\n",
      "Epoch 1374/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3814e-04 - my_r2: 0.9226 - val_loss: 1.3849e-05 - val_my_r2: 0.9952\n",
      "Epoch 1375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0201e-04 - my_r2: 0.9607 - val_loss: 1.1075e-05 - val_my_r2: 0.9962\n",
      "Epoch 1376/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1453e-04 - my_r2: 0.9500 - val_loss: 8.6817e-06 - val_my_r2: 0.9970\n",
      "Epoch 1377/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1704e-04 - my_r2: 0.9321 - val_loss: 9.7344e-06 - val_my_r2: 0.9966\n",
      "Epoch 1378/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5947e-04 - my_r2: 0.9039 - val_loss: 9.0423e-06 - val_my_r2: 0.9970\n",
      "Epoch 1379/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6392e-04 - my_r2: 0.9569 - val_loss: 8.7379e-06 - val_my_r2: 0.9971\n",
      "Epoch 1380/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8719e-04 - my_r2: 0.9208 - val_loss: 9.0060e-06 - val_my_r2: 0.9969\n",
      "Epoch 1381/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4165e-04 - my_r2: 0.9371 - val_loss: 1.0074e-05 - val_my_r2: 0.9965\n",
      "Epoch 1382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4708e-04 - my_r2: 0.9507 - val_loss: 1.0531e-05 - val_my_r2: 0.9965\n",
      "Epoch 1383/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3783e-04 - my_r2: 0.9420 - val_loss: 1.2636e-05 - val_my_r2: 0.9956\n",
      "Epoch 1384/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0089e-04 - my_r2: 0.9322 - val_loss: 1.1313e-05 - val_my_r2: 0.9962\n",
      "Epoch 1385/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5531e-04 - my_r2: 0.9246 - val_loss: 1.3956e-05 - val_my_r2: 0.9955\n",
      "Epoch 1386/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1743e-04 - my_r2: 0.8946 - val_loss: 1.1801e-05 - val_my_r2: 0.9961\n",
      "Epoch 1387/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6323e-04 - my_r2: 0.9350 - val_loss: 1.3135e-05 - val_my_r2: 0.9955\n",
      "Epoch 1388/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4343e-04 - my_r2: 0.9408 - val_loss: 1.1497e-05 - val_my_r2: 0.9961\n",
      "Epoch 1389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3398e-04 - my_r2: 0.8547 - val_loss: 1.1779e-05 - val_my_r2: 0.9956\n",
      "Epoch 1390/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2572e-04 - my_r2: 0.9015 - val_loss: 1.2536e-05 - val_my_r2: 0.9953\n",
      "Epoch 1391/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3273e-04 - my_r2: 0.9446 - val_loss: 1.0567e-05 - val_my_r2: 0.9962\n",
      "Epoch 1392/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4473e-04 - my_r2: 0.9505 - val_loss: 1.1266e-05 - val_my_r2: 0.9963\n",
      "Epoch 1393/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5583e-04 - my_r2: 0.9343 - val_loss: 1.2140e-05 - val_my_r2: 0.9965\n",
      "Epoch 1394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6076e-04 - my_r2: 0.9441 - val_loss: 1.5700e-05 - val_my_r2: 0.9952\n",
      "Epoch 1395/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6368e-04 - my_r2: 0.7925 - val_loss: 1.3910e-05 - val_my_r2: 0.9961\n",
      "Epoch 1396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4329e-04 - my_r2: 0.8897 - val_loss: 2.0264e-05 - val_my_r2: 0.9936\n",
      "Epoch 1397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6827e-04 - my_r2: 0.9221 - val_loss: 1.0788e-05 - val_my_r2: 0.9964\n",
      "Epoch 1398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9521e-04 - my_r2: 0.9245 - val_loss: 1.0196e-05 - val_my_r2: 0.9968\n",
      "Epoch 1399/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8341e-04 - my_r2: 0.9398 - val_loss: 1.2283e-05 - val_my_r2: 0.9966\n",
      "Epoch 1400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5048e-04 - my_r2: 0.9135 - val_loss: 2.3800e-05 - val_my_r2: 0.9929\n",
      "Epoch 1401/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7161e-04 - my_r2: 0.9352 - val_loss: 9.9974e-06 - val_my_r2: 0.9968\n",
      "Epoch 1402/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6866e-04 - my_r2: 0.9407 - val_loss: 9.7326e-06 - val_my_r2: 0.9968\n",
      "Epoch 1403/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3694e-04 - my_r2: 0.9315 - val_loss: 1.3167e-05 - val_my_r2: 0.9958\n",
      "Epoch 1404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8512e-04 - my_r2: 0.9260 - val_loss: 1.0218e-05 - val_my_r2: 0.9965\n",
      "Epoch 1405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2594e-04 - my_r2: 0.9347 - val_loss: 1.2988e-05 - val_my_r2: 0.9952\n",
      "Epoch 1406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8471e-04 - my_r2: 0.9330 - val_loss: 1.8875e-05 - val_my_r2: 0.9927\n",
      "Epoch 1407/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2355e-04 - my_r2: 0.9468 - val_loss: 1.1381e-05 - val_my_r2: 0.9968\n",
      "Epoch 1408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0781e-04 - my_r2: 0.9197 - val_loss: 1.2916e-05 - val_my_r2: 0.9963\n",
      "Epoch 1409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1190e-04 - my_r2: 0.8757 - val_loss: 1.1765e-05 - val_my_r2: 0.9965\n",
      "Epoch 1410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3973e-04 - my_r2: 0.9435 - val_loss: 1.0803e-05 - val_my_r2: 0.9967\n",
      "Epoch 1411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2230e-04 - my_r2: 0.9423 - val_loss: 9.7724e-06 - val_my_r2: 0.9971\n",
      "Epoch 1412/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2104e-04 - my_r2: 0.9151 - val_loss: 1.0259e-05 - val_my_r2: 0.9970\n",
      "Epoch 1413/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6397e-04 - my_r2: 0.9250 - val_loss: 1.2864e-05 - val_my_r2: 0.9960\n",
      "Epoch 1414/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7162e-04 - my_r2: 0.9060 - val_loss: 1.6868e-05 - val_my_r2: 0.9940\n",
      "Epoch 1415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0282e-04 - my_r2: 0.9507 - val_loss: 1.2642e-05 - val_my_r2: 0.9956\n",
      "Epoch 1416/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4745e-04 - my_r2: 0.4806 - val_loss: 1.1929e-05 - val_my_r2: 0.9957\n",
      "Epoch 1417/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2545e-04 - my_r2: 0.9371 - val_loss: 1.1512e-05 - val_my_r2: 0.9960\n",
      "Epoch 1418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0818e-04 - my_r2: 0.9203 - val_loss: 1.5870e-05 - val_my_r2: 0.9943\n",
      "Epoch 1419/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4476e-04 - my_r2: 0.8861 - val_loss: 1.0900e-05 - val_my_r2: 0.9958\n",
      "Epoch 1420/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3442e-04 - my_r2: 0.9493 - val_loss: 1.3011e-05 - val_my_r2: 0.9952\n",
      "Epoch 1421/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1816e-04 - my_r2: 0.8883 - val_loss: 9.4701e-06 - val_my_r2: 0.9962\n",
      "Epoch 1422/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3961e-04 - my_r2: 0.9140 - val_loss: 8.9295e-06 - val_my_r2: 0.9963\n",
      "Epoch 1423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2222e-04 - my_r2: 0.9302 - val_loss: 1.0828e-05 - val_my_r2: 0.9957\n",
      "Epoch 1424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7451e-04 - my_r2: 0.9297 - val_loss: 1.0734e-05 - val_my_r2: 0.9961\n",
      "Epoch 1425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6207e-04 - my_r2: 0.9182 - val_loss: 1.3575e-05 - val_my_r2: 0.9948\n",
      "Epoch 1426/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4056e-04 - my_r2: 0.9267 - val_loss: 1.3457e-05 - val_my_r2: 0.9947\n",
      "Epoch 1427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1382e-04 - my_r2: 0.9355 - val_loss: 1.0703e-05 - val_my_r2: 0.9960\n",
      "Epoch 1428/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2995e-04 - my_r2: 0.9331 - val_loss: 1.0739e-05 - val_my_r2: 0.9963\n",
      "Epoch 1429/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1413e-04 - my_r2: 0.9355 - val_loss: 9.8798e-06 - val_my_r2: 0.9967\n",
      "Epoch 1430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0823e-04 - my_r2: 0.9527 - val_loss: 1.0261e-05 - val_my_r2: 0.9967\n",
      "Epoch 1431/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8674e-04 - my_r2: 0.9152 - val_loss: 1.2805e-05 - val_my_r2: 0.9960\n",
      "Epoch 1432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4095e-04 - my_r2: 0.9515 - val_loss: 9.8578e-06 - val_my_r2: 0.9968\n",
      "Epoch 1433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5835e-04 - my_r2: 0.9272 - val_loss: 1.2040e-05 - val_my_r2: 0.9961\n",
      "Epoch 1434/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3085e-04 - my_r2: 0.9341 - val_loss: 1.0458e-05 - val_my_r2: 0.9965\n",
      "Epoch 1435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5652e-04 - my_r2: 0.9213 - val_loss: 1.0225e-05 - val_my_r2: 0.9964\n",
      "Epoch 1436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4459e-04 - my_r2: 0.9401 - val_loss: 1.0228e-05 - val_my_r2: 0.9965\n",
      "Epoch 1437/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8928e-04 - my_r2: 0.9109 - val_loss: 1.1783e-05 - val_my_r2: 0.9962\n",
      "Epoch 1438/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9325e-04 - my_r2: 0.8217 - val_loss: 1.2898e-05 - val_my_r2: 0.9961\n",
      "Epoch 1439/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4298e-04 - my_r2: 0.9423 - val_loss: 1.1303e-05 - val_my_r2: 0.9966\n",
      "Epoch 1440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3216e-04 - my_r2: 0.9311 - val_loss: 1.2533e-05 - val_my_r2: 0.9961\n",
      "Epoch 1441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6700e-04 - my_r2: 0.9182 - val_loss: 1.2995e-05 - val_my_r2: 0.9958\n",
      "Epoch 1442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7547e-04 - my_r2: 0.9317 - val_loss: 1.0323e-05 - val_my_r2: 0.9968\n",
      "Epoch 1443/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1626e-04 - my_r2: 0.9426 - val_loss: 1.2196e-05 - val_my_r2: 0.9962\n",
      "Epoch 1444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5058e-04 - my_r2: 0.9267 - val_loss: 1.3875e-05 - val_my_r2: 0.9950\n",
      "Epoch 1445/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8155e-04 - my_r2: 0.9392 - val_loss: 1.2952e-05 - val_my_r2: 0.9954\n",
      "Epoch 1446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4578e-04 - my_r2: 0.9367 - val_loss: 9.8945e-06 - val_my_r2: 0.9968\n",
      "Epoch 1447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7839e-04 - my_r2: 0.9331 - val_loss: 1.1053e-05 - val_my_r2: 0.9963\n",
      "Epoch 1448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4675e-04 - my_r2: 0.9258 - val_loss: 1.0951e-05 - val_my_r2: 0.9962\n",
      "Epoch 1449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7759e-04 - my_r2: 0.9317 - val_loss: 1.0044e-05 - val_my_r2: 0.9964\n",
      "Epoch 1450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4594e-04 - my_r2: 0.9374 - val_loss: 1.0450e-05 - val_my_r2: 0.9964\n",
      "Epoch 1451/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1197e-04 - my_r2: 0.9542 - val_loss: 9.1013e-06 - val_my_r2: 0.9968\n",
      "Epoch 1452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1319e-04 - my_r2: 0.9581 - val_loss: 8.1432e-06 - val_my_r2: 0.9971\n",
      "Epoch 1453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8196e-04 - my_r2: 0.9249 - val_loss: 9.5314e-06 - val_my_r2: 0.9969\n",
      "Epoch 1454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4204e-04 - my_r2: 0.8916 - val_loss: 7.2077e-06 - val_my_r2: 0.9976\n",
      "Epoch 1455/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2834e-04 - my_r2: 0.9403 - val_loss: 8.7906e-06 - val_my_r2: 0.9970\n",
      "Epoch 1456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3338e-04 - my_r2: 0.9318 - val_loss: 1.3920e-05 - val_my_r2: 0.9954\n",
      "Epoch 1457/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6050e-04 - my_r2: 0.9476 - val_loss: 1.0444e-05 - val_my_r2: 0.9961\n",
      "Epoch 1458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5529e-04 - my_r2: 0.9123 - val_loss: 9.5776e-06 - val_my_r2: 0.9963\n",
      "Epoch 1459/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9997e-04 - my_r2: 0.9009 - val_loss: 7.9315e-06 - val_my_r2: 0.9970\n",
      "Epoch 1460/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4223e-04 - my_r2: 0.9142 - val_loss: 1.1400e-05 - val_my_r2: 0.9958\n",
      "Epoch 1461/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5707e-04 - my_r2: 0.8712 - val_loss: 1.0285e-05 - val_my_r2: 0.9956\n",
      "Epoch 1462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1932e-04 - my_r2: 0.9517 - val_loss: 1.3526e-05 - val_my_r2: 0.9944\n",
      "Epoch 1463/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9574e-04 - my_r2: 0.8024 - val_loss: 1.1104e-05 - val_my_r2: 0.9961\n",
      "Epoch 1464/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0023e-04 - my_r2: 0.9079 - val_loss: 1.1294e-05 - val_my_r2: 0.9964\n",
      "Epoch 1465/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3006e-04 - my_r2: 0.9406 - val_loss: 9.8054e-06 - val_my_r2: 0.9968\n",
      "Epoch 1466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9233e-04 - my_r2: 0.9484 - val_loss: 9.5351e-06 - val_my_r2: 0.9965\n",
      "Epoch 1467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3027e-04 - my_r2: 0.9116 - val_loss: 8.4549e-06 - val_my_r2: 0.9966\n",
      "Epoch 1468/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4774e-04 - my_r2: 0.9428 - val_loss: 1.4841e-05 - val_my_r2: 0.9955\n",
      "Epoch 1469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4930e-04 - my_r2: 0.9424 - val_loss: 1.2996e-05 - val_my_r2: 0.9960\n",
      "Epoch 1470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2259e-04 - my_r2: 0.9155 - val_loss: 1.0506e-05 - val_my_r2: 0.9961\n",
      "Epoch 1471/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0727e-04 - my_r2: 0.9281 - val_loss: 1.0201e-05 - val_my_r2: 0.9962\n",
      "Epoch 1472/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3705e-04 - my_r2: 0.9220 - val_loss: 1.0928e-05 - val_my_r2: 0.9959\n",
      "Epoch 1473/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2386e-04 - my_r2: 0.9415 - val_loss: 9.2699e-06 - val_my_r2: 0.9969\n",
      "Epoch 1474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4141e-04 - my_r2: 0.9085 - val_loss: 1.1473e-05 - val_my_r2: 0.9959\n",
      "Epoch 1475/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0849e-04 - my_r2: 0.9498 - val_loss: 9.1357e-06 - val_my_r2: 0.9967\n",
      "Epoch 1476/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6608e-04 - my_r2: 0.8254 - val_loss: 1.1201e-05 - val_my_r2: 0.9960\n",
      "Epoch 1477/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6306e-04 - my_r2: 0.9328 - val_loss: 7.7166e-06 - val_my_r2: 0.9976\n",
      "Epoch 1478/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0718e-04 - my_r2: 0.8888 - val_loss: 1.3494e-05 - val_my_r2: 0.9959\n",
      "Epoch 1479/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3540e-04 - my_r2: 0.9433 - val_loss: 1.3692e-05 - val_my_r2: 0.9957\n",
      "Epoch 1480/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3218e-04 - my_r2: 0.9470 - val_loss: 1.1995e-05 - val_my_r2: 0.9964\n",
      "Epoch 1481/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2242e-04 - my_r2: 0.9427 - val_loss: 1.3226e-05 - val_my_r2: 0.9960\n",
      "Epoch 1482/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5138e-04 - my_r2: 0.9388 - val_loss: 1.8704e-05 - val_my_r2: 0.9941\n",
      "Epoch 1483/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4949e-04 - my_r2: 0.9298 - val_loss: 1.1193e-05 - val_my_r2: 0.9962\n",
      "Epoch 1484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2741e-04 - my_r2: 0.9246 - val_loss: 1.0781e-05 - val_my_r2: 0.9964\n",
      "Epoch 1485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2424e-04 - my_r2: 0.9392 - val_loss: 7.9803e-06 - val_my_r2: 0.9974\n",
      "Epoch 1486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4183e-04 - my_r2: 0.9034 - val_loss: 8.6601e-06 - val_my_r2: 0.9975\n",
      "Epoch 1487/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4668e-04 - my_r2: 0.9460 - val_loss: 1.0017e-05 - val_my_r2: 0.9970\n",
      "Epoch 1488/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5581e-04 - my_r2: 0.9057 - val_loss: 1.4421e-05 - val_my_r2: 0.9953\n",
      "Epoch 1489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6430e-04 - my_r2: 0.8996 - val_loss: 1.3089e-05 - val_my_r2: 0.9951\n",
      "Epoch 1490/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2594e-04 - my_r2: 0.9461 - val_loss: 1.6961e-05 - val_my_r2: 0.9938\n",
      "Epoch 1491/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8367e-04 - my_r2: 0.8979 - val_loss: 1.0629e-05 - val_my_r2: 0.9967\n",
      "Epoch 1492/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3457e-04 - my_r2: 0.9116 - val_loss: 1.0248e-05 - val_my_r2: 0.9966\n",
      "Epoch 1493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4489e-04 - my_r2: 0.9355 - val_loss: 1.0433e-05 - val_my_r2: 0.9963\n",
      "Epoch 1494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0609e-04 - my_r2: 0.9490 - val_loss: 9.0918e-06 - val_my_r2: 0.9969\n",
      "Epoch 1495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4744e-04 - my_r2: 0.9333 - val_loss: 9.2122e-06 - val_my_r2: 0.9968\n",
      "Epoch 1496/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8204e-04 - my_r2: 0.9359 - val_loss: 1.9474e-05 - val_my_r2: 0.9941\n",
      "Epoch 1497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1778e-04 - my_r2: 0.9383 - val_loss: 1.6485e-05 - val_my_r2: 0.9950\n",
      "Epoch 1498/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0256e-04 - my_r2: 0.9569 - val_loss: 9.6674e-06 - val_my_r2: 0.9968\n",
      "Epoch 1499/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6055e-04 - my_r2: 0.9371 - val_loss: 1.2960e-05 - val_my_r2: 0.9956\n",
      "Epoch 1500/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0427e-04 - my_r2: 0.9221 - val_loss: 1.5025e-05 - val_my_r2: 0.9950\n",
      "Epoch 1501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1010e-04 - my_r2: 0.9154 - val_loss: 9.7690e-06 - val_my_r2: 0.9961\n",
      "Epoch 1502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0205e-04 - my_r2: 0.9289 - val_loss: 9.0983e-06 - val_my_r2: 0.9966\n",
      "Epoch 1503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6939e-04 - my_r2: 0.9365 - val_loss: 8.4890e-06 - val_my_r2: 0.9967\n",
      "Epoch 1504/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8662e-04 - my_r2: 0.9127 - val_loss: 8.1887e-06 - val_my_r2: 0.9970\n",
      "Epoch 1505/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2744e-04 - my_r2: 0.9499 - val_loss: 8.7004e-06 - val_my_r2: 0.9966\n",
      "Epoch 1506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4219e-04 - my_r2: 0.9299 - val_loss: 8.9780e-06 - val_my_r2: 0.9967\n",
      "Epoch 1507/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4236e-04 - my_r2: 0.9466 - val_loss: 8.7295e-06 - val_my_r2: 0.9969\n",
      "Epoch 1508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9991e-04 - my_r2: 0.9111 - val_loss: 1.5529e-05 - val_my_r2: 0.9944\n",
      "Epoch 1509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7116e-04 - my_r2: 0.9343 - val_loss: 1.3259e-05 - val_my_r2: 0.9957\n",
      "Epoch 1510/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3782e-04 - my_r2: 0.9452 - val_loss: 1.2594e-05 - val_my_r2: 0.9960\n",
      "Epoch 1511/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3438e-04 - my_r2: 0.9410 - val_loss: 1.1623e-05 - val_my_r2: 0.9959\n",
      "Epoch 1512/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4545e-04 - my_r2: 0.9509 - val_loss: 8.7093e-06 - val_my_r2: 0.9967\n",
      "Epoch 1513/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0034e-04 - my_r2: 0.9410 - val_loss: 1.1503e-05 - val_my_r2: 0.9960\n",
      "Epoch 1514/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9527e-04 - my_r2: 0.9559 - val_loss: 1.0719e-05 - val_my_r2: 0.9963\n",
      "Epoch 1515/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3799e-04 - my_r2: 0.9005 - val_loss: 1.1814e-05 - val_my_r2: 0.9960\n",
      "Epoch 1516/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4505e-04 - my_r2: 0.9011 - val_loss: 1.3509e-05 - val_my_r2: 0.9956\n",
      "Epoch 1517/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8648e-04 - my_r2: 0.9404 - val_loss: 1.0648e-05 - val_my_r2: 0.9962\n",
      "Epoch 1518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7499e-04 - my_r2: 0.9380 - val_loss: 1.0727e-05 - val_my_r2: 0.9960\n",
      "Epoch 1519/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9183e-04 - my_r2: 0.9327 - val_loss: 1.0054e-05 - val_my_r2: 0.9962\n",
      "Epoch 1520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7338e-04 - my_r2: 0.9307 - val_loss: 8.0697e-06 - val_my_r2: 0.9970\n",
      "Epoch 1521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0594e-04 - my_r2: 0.9354 - val_loss: 7.3501e-06 - val_my_r2: 0.9973\n",
      "Epoch 1522/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7577e-04 - my_r2: 0.9217 - val_loss: 9.2771e-06 - val_my_r2: 0.9967\n",
      "Epoch 1523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4416e-04 - my_r2: 0.9333 - val_loss: 8.6036e-06 - val_my_r2: 0.9970\n",
      "Epoch 1524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4907e-04 - my_r2: 0.9503 - val_loss: 1.1141e-05 - val_my_r2: 0.9960\n",
      "Epoch 1525/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2290e-04 - my_r2: 0.9127 - val_loss: 8.7266e-06 - val_my_r2: 0.9968\n",
      "Epoch 1526/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6930e-04 - my_r2: 0.8977 - val_loss: 1.0134e-05 - val_my_r2: 0.9963\n",
      "Epoch 1527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6324e-04 - my_r2: 0.9297 - val_loss: 9.6625e-06 - val_my_r2: 0.9967\n",
      "Epoch 1528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1576e-04 - my_r2: 0.9201 - val_loss: 8.1035e-06 - val_my_r2: 0.9970\n",
      "Epoch 1529/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6663e-04 - my_r2: 0.8191 - val_loss: 1.1652e-05 - val_my_r2: 0.9956\n",
      "Epoch 1530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8563e-04 - my_r2: 0.9101 - val_loss: 1.1323e-05 - val_my_r2: 0.9959\n",
      "Epoch 1531/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6298e-04 - my_r2: 0.9456 - val_loss: 1.3757e-05 - val_my_r2: 0.9953\n",
      "Epoch 1532/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1428e-04 - my_r2: 0.9223 - val_loss: 1.0699e-05 - val_my_r2: 0.9962\n",
      "Epoch 1533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6993e-04 - my_r2: 0.9419 - val_loss: 9.0280e-06 - val_my_r2: 0.9964\n",
      "Epoch 1534/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0008e-04 - my_r2: 0.9528 - val_loss: 1.0911e-05 - val_my_r2: 0.9958\n",
      "Epoch 1535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5414e-04 - my_r2: 0.9384 - val_loss: 9.4630e-06 - val_my_r2: 0.9966\n",
      "Epoch 1536/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5042e-04 - my_r2: 0.9201 - val_loss: 9.7058e-06 - val_my_r2: 0.9964\n",
      "Epoch 1537/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4479e-04 - my_r2: 0.9515 - val_loss: 1.3226e-05 - val_my_r2: 0.9948\n",
      "Epoch 1538/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2591e-04 - my_r2: 0.9557 - val_loss: 8.8133e-06 - val_my_r2: 0.9969\n",
      "Epoch 1539/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8374e-04 - my_r2: 0.9247 - val_loss: 1.3082e-05 - val_my_r2: 0.9951\n",
      "Epoch 1540/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3771e-04 - my_r2: 0.9221 - val_loss: 1.3300e-05 - val_my_r2: 0.9952\n",
      "Epoch 1541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2640e-04 - my_r2: 0.9348 - val_loss: 1.2563e-05 - val_my_r2: 0.9954\n",
      "Epoch 1542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7422e-04 - my_r2: 0.9223 - val_loss: 1.2880e-05 - val_my_r2: 0.9954\n",
      "Epoch 1543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5265e-04 - my_r2: 0.9265 - val_loss: 1.5511e-05 - val_my_r2: 0.9945\n",
      "Epoch 1544/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4632e-04 - my_r2: 0.9243 - val_loss: 1.1546e-05 - val_my_r2: 0.9962\n",
      "Epoch 1545/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3576e-04 - my_r2: 0.8488 - val_loss: 8.2059e-06 - val_my_r2: 0.9972\n",
      "Epoch 1546/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7043e-04 - my_r2: 0.8513 - val_loss: 7.7341e-06 - val_my_r2: 0.9975\n",
      "Epoch 1547/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6933e-04 - my_r2: 0.9554 - val_loss: 1.0605e-05 - val_my_r2: 0.9968\n",
      "Epoch 1548/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5311e-04 - my_r2: 0.9153 - val_loss: 1.4375e-05 - val_my_r2: 0.9961\n",
      "Epoch 1549/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9044e-04 - my_r2: 0.9328 - val_loss: 1.3671e-05 - val_my_r2: 0.9954\n",
      "Epoch 1550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8452e-04 - my_r2: 0.9475 - val_loss: 1.0199e-05 - val_my_r2: 0.9961\n",
      "Epoch 1551/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6467e-04 - my_r2: 0.9226 - val_loss: 1.6015e-05 - val_my_r2: 0.9942\n",
      "Epoch 1552/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6486e-04 - my_r2: 0.9271 - val_loss: 2.2168e-05 - val_my_r2: 0.9921\n",
      "Epoch 1553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2472e-04 - my_r2: 0.9208 - val_loss: 1.2658e-05 - val_my_r2: 0.9951\n",
      "Epoch 1554/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6303e-04 - my_r2: 0.9025 - val_loss: 1.2448e-05 - val_my_r2: 0.9947\n",
      "Epoch 1555/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9810e-04 - my_r2: 0.9136 - val_loss: 1.1405e-05 - val_my_r2: 0.9952\n",
      "Epoch 1556/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3791e-04 - my_r2: 0.9122 - val_loss: 9.6651e-06 - val_my_r2: 0.9959\n",
      "Epoch 1557/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8011e-04 - my_r2: 0.8953 - val_loss: 1.1702e-05 - val_my_r2: 0.9951\n",
      "Epoch 1558/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8961e-04 - my_r2: 0.9186 - val_loss: 1.2101e-05 - val_my_r2: 0.9947\n",
      "Epoch 1559/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9573e-04 - my_r2: 0.9247 - val_loss: 1.6759e-05 - val_my_r2: 0.9929\n",
      "Epoch 1560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8289e-04 - my_r2: 0.8823 - val_loss: 1.1006e-05 - val_my_r2: 0.9956\n",
      "Epoch 1561/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8752e-04 - my_r2: 0.9284 - val_loss: 1.1349e-05 - val_my_r2: 0.9955\n",
      "Epoch 1562/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0882e-04 - my_r2: 0.9421 - val_loss: 1.2347e-05 - val_my_r2: 0.9950\n",
      "Epoch 1563/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4243e-04 - my_r2: 0.9148 - val_loss: 1.3700e-05 - val_my_r2: 0.9945\n",
      "Epoch 1564/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7977e-04 - my_r2: 0.9188 - val_loss: 9.6313e-06 - val_my_r2: 0.9964\n",
      "Epoch 1565/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5054e-04 - my_r2: 0.9342 - val_loss: 1.2719e-05 - val_my_r2: 0.9948\n",
      "Epoch 1566/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3307e-04 - my_r2: 0.8667 - val_loss: 1.3720e-05 - val_my_r2: 0.9949\n",
      "Epoch 1567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3851e-04 - my_r2: 0.9255 - val_loss: 9.8141e-06 - val_my_r2: 0.9965\n",
      "Epoch 1568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0050e-04 - my_r2: 0.9268 - val_loss: 1.2085e-05 - val_my_r2: 0.9962\n",
      "Epoch 1569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3429e-04 - my_r2: 0.9471 - val_loss: 1.0810e-05 - val_my_r2: 0.9965\n",
      "Epoch 1570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3941e-04 - my_r2: 0.9424 - val_loss: 1.3192e-05 - val_my_r2: 0.9953\n",
      "Epoch 1571/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2695e-04 - my_r2: 0.9178 - val_loss: 1.0768e-05 - val_my_r2: 0.9961\n",
      "Epoch 1572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5059e-04 - my_r2: 0.9381 - val_loss: 9.6434e-06 - val_my_r2: 0.9965\n",
      "Epoch 1573/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5521e-04 - my_r2: 0.9508 - val_loss: 9.7734e-06 - val_my_r2: 0.9966\n",
      "Epoch 1574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5369e-04 - my_r2: 0.9367 - val_loss: 9.2479e-06 - val_my_r2: 0.9963\n",
      "Epoch 1575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1090e-04 - my_r2: 0.9526 - val_loss: 7.9069e-06 - val_my_r2: 0.9968\n",
      "Epoch 1576/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1846e-04 - my_r2: 0.9194 - val_loss: 9.1072e-06 - val_my_r2: 0.9963\n",
      "Epoch 1577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2398e-04 - my_r2: 0.9480 - val_loss: 1.4670e-05 - val_my_r2: 0.9943\n",
      "Epoch 1578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3508e-04 - my_r2: 0.9414 - val_loss: 9.3750e-06 - val_my_r2: 0.9968\n",
      "Epoch 1579/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1670e-04 - my_r2: 0.9489 - val_loss: 9.0059e-06 - val_my_r2: 0.9971\n",
      "Epoch 1580/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4734e-04 - my_r2: 0.9148 - val_loss: 7.8157e-06 - val_my_r2: 0.9973\n",
      "Epoch 1581/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3563e-04 - my_r2: 0.9206 - val_loss: 9.1294e-06 - val_my_r2: 0.9964\n",
      "Epoch 1582/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4400e-04 - my_r2: 0.9313 - val_loss: 9.1674e-06 - val_my_r2: 0.9966\n",
      "Epoch 1583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3705e-04 - my_r2: 0.9393 - val_loss: 8.2279e-06 - val_my_r2: 0.9973\n",
      "Epoch 1584/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6084e-04 - my_r2: 0.9088 - val_loss: 7.7459e-06 - val_my_r2: 0.9973\n",
      "Epoch 1585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4825e-04 - my_r2: 0.8958 - val_loss: 8.1989e-06 - val_my_r2: 0.9968\n",
      "Epoch 1586/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5098e-04 - my_r2: 0.9341 - val_loss: 8.7525e-06 - val_my_r2: 0.9965\n",
      "Epoch 1587/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2209e-04 - my_r2: 0.9441 - val_loss: 9.7409e-06 - val_my_r2: 0.9964\n",
      "Epoch 1588/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9587e-04 - my_r2: 0.9194 - val_loss: 9.5813e-06 - val_my_r2: 0.9963\n",
      "Epoch 1589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1912e-04 - my_r2: 0.9488 - val_loss: 1.0310e-05 - val_my_r2: 0.9959\n",
      "Epoch 1590/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8197e-04 - my_r2: 0.9273 - val_loss: 9.0380e-06 - val_my_r2: 0.9969\n",
      "Epoch 1591/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4434e-04 - my_r2: 0.9425 - val_loss: 7.6399e-06 - val_my_r2: 0.9975\n",
      "Epoch 1592/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2419e-04 - my_r2: 0.9499 - val_loss: 7.2606e-06 - val_my_r2: 0.9975\n",
      "Epoch 1593/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1276e-04 - my_r2: 0.9306 - val_loss: 1.1805e-05 - val_my_r2: 0.9956\n",
      "Epoch 1594/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2231e-04 - my_r2: 0.8751 - val_loss: 9.7674e-06 - val_my_r2: 0.9965\n",
      "Epoch 1595/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1794e-04 - my_r2: 0.9570 - val_loss: 9.6054e-06 - val_my_r2: 0.9966\n",
      "Epoch 1596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0182e-04 - my_r2: 0.8918 - val_loss: 8.8261e-06 - val_my_r2: 0.9970\n",
      "Epoch 1597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6608e-04 - my_r2: 0.8799 - val_loss: 8.1432e-06 - val_my_r2: 0.9972\n",
      "Epoch 1598/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4621e-04 - my_r2: 0.9167 - val_loss: 7.0505e-06 - val_my_r2: 0.9974\n",
      "Epoch 1599/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3836e-04 - my_r2: 0.9395 - val_loss: 7.2375e-06 - val_my_r2: 0.9974\n",
      "Epoch 1600/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2316e-04 - my_r2: 0.9373 - val_loss: 1.0794e-05 - val_my_r2: 0.9962\n",
      "Epoch 1601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1353e-04 - my_r2: 0.9378 - val_loss: 9.5825e-06 - val_my_r2: 0.9967\n",
      "Epoch 1602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2912e-04 - my_r2: 0.9503 - val_loss: 1.0121e-05 - val_my_r2: 0.9967\n",
      "Epoch 1603/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0051e-04 - my_r2: 0.9275 - val_loss: 1.0852e-05 - val_my_r2: 0.9966\n",
      "Epoch 1604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1789e-04 - my_r2: 0.9507 - val_loss: 1.1102e-05 - val_my_r2: 0.9964\n",
      "Epoch 1605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4820e-04 - my_r2: 0.9199 - val_loss: 9.9622e-06 - val_my_r2: 0.9967\n",
      "Epoch 1606/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1074e-04 - my_r2: 0.9159 - val_loss: 9.4297e-06 - val_my_r2: 0.9966\n",
      "Epoch 1607/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1744e-04 - my_r2: 0.9349 - val_loss: 9.4055e-06 - val_my_r2: 0.9966\n",
      "Epoch 1608/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6059e-04 - my_r2: 0.9314 - val_loss: 8.8762e-06 - val_my_r2: 0.9969\n",
      "Epoch 1609/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3410e-04 - my_r2: 0.9486 - val_loss: 1.1973e-05 - val_my_r2: 0.9960\n",
      "Epoch 1610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2281e-04 - my_r2: 0.9485 - val_loss: 9.5609e-06 - val_my_r2: 0.9967\n",
      "Epoch 1611/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2222e-04 - my_r2: 0.9100 - val_loss: 9.3212e-06 - val_my_r2: 0.9968\n",
      "Epoch 1612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9076e-04 - my_r2: 0.9212 - val_loss: 9.7600e-06 - val_my_r2: 0.9965\n",
      "Epoch 1613/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1574e-04 - my_r2: 0.9477 - val_loss: 8.2922e-06 - val_my_r2: 0.9971\n",
      "Epoch 1614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9844e-04 - my_r2: 0.9488 - val_loss: 1.0425e-05 - val_my_r2: 0.9962\n",
      "Epoch 1615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8535e-04 - my_r2: 0.9268 - val_loss: 1.0065e-05 - val_my_r2: 0.9964\n",
      "Epoch 1616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5008e-04 - my_r2: 0.9175 - val_loss: 1.0940e-05 - val_my_r2: 0.9958\n",
      "Epoch 1617/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2763e-04 - my_r2: 0.9435 - val_loss: 1.0841e-05 - val_my_r2: 0.9961\n",
      "Epoch 1618/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2788e-04 - my_r2: 0.9483 - val_loss: 1.0067e-05 - val_my_r2: 0.9964\n",
      "Epoch 1619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9665e-04 - my_r2: 0.9403 - val_loss: 1.0464e-05 - val_my_r2: 0.9965\n",
      "Epoch 1620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8227e-04 - my_r2: 0.9255 - val_loss: 9.3350e-06 - val_my_r2: 0.9969\n",
      "Epoch 1621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4434e-04 - my_r2: 0.9452 - val_loss: 1.0100e-05 - val_my_r2: 0.9967\n",
      "Epoch 1622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1799e-04 - my_r2: 0.9031 - val_loss: 9.9139e-06 - val_my_r2: 0.9965\n",
      "Epoch 1623/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7436e-04 - my_r2: 0.9273 - val_loss: 1.1987e-05 - val_my_r2: 0.9961\n",
      "Epoch 1624/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4077e-04 - my_r2: 0.9301 - val_loss: 1.0791e-05 - val_my_r2: 0.9961\n",
      "Epoch 1625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2275e-04 - my_r2: 0.9362 - val_loss: 1.2164e-05 - val_my_r2: 0.9958\n",
      "Epoch 1626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1722e-04 - my_r2: 0.9435 - val_loss: 1.1790e-05 - val_my_r2: 0.9958\n",
      "Epoch 1627/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9449e-04 - my_r2: 0.9486 - val_loss: 1.1899e-05 - val_my_r2: 0.9958\n",
      "Epoch 1628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6467e-04 - my_r2: 0.9410 - val_loss: 1.0553e-05 - val_my_r2: 0.9964\n",
      "Epoch 1629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0102e-04 - my_r2: 0.9586 - val_loss: 1.1849e-05 - val_my_r2: 0.9961\n",
      "Epoch 1630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4802e-04 - my_r2: 0.9083 - val_loss: 1.4247e-05 - val_my_r2: 0.9956\n",
      "Epoch 1631/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7514e-04 - my_r2: 0.8756 - val_loss: 1.3163e-05 - val_my_r2: 0.9958\n",
      "Epoch 1632/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5084e-04 - my_r2: 0.9253 - val_loss: 9.2259e-06 - val_my_r2: 0.9969\n",
      "Epoch 1633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2612e-04 - my_r2: 0.9292 - val_loss: 8.4854e-06 - val_my_r2: 0.9972\n",
      "Epoch 1634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5643e-04 - my_r2: 0.9426 - val_loss: 1.0104e-05 - val_my_r2: 0.9965\n",
      "Epoch 1635/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8891e-04 - my_r2: 0.9293 - val_loss: 9.0940e-06 - val_my_r2: 0.9970\n",
      "Epoch 1636/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0877e-04 - my_r2: 0.9160 - val_loss: 1.1026e-05 - val_my_r2: 0.9965\n",
      "Epoch 1637/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2207e-04 - my_r2: 0.8735 - val_loss: 9.4132e-06 - val_my_r2: 0.9967\n",
      "Epoch 1638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3060e-04 - my_r2: 0.9423 - val_loss: 8.6612e-06 - val_my_r2: 0.9969\n",
      "Epoch 1639/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3666e-04 - my_r2: 0.9391 - val_loss: 9.0348e-06 - val_my_r2: 0.9967\n",
      "Epoch 1640/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1562e-04 - my_r2: 0.9467 - val_loss: 7.6168e-06 - val_my_r2: 0.9976\n",
      "Epoch 1641/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4165e-04 - my_r2: 0.9459 - val_loss: 1.1923e-05 - val_my_r2: 0.9954\n",
      "Epoch 1642/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5750e-04 - my_r2: 0.9302 - val_loss: 8.9402e-06 - val_my_r2: 0.9966\n",
      "Epoch 1643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8643e-04 - my_r2: 0.9092 - val_loss: 8.8429e-06 - val_my_r2: 0.9965\n",
      "Epoch 1644/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6158e-04 - my_r2: 0.9404 - val_loss: 7.9186e-06 - val_my_r2: 0.9969\n",
      "Epoch 1645/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3017e-04 - my_r2: 0.9195 - val_loss: 1.1249e-05 - val_my_r2: 0.9963\n",
      "Epoch 1646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3405e-04 - my_r2: 0.8791 - val_loss: 8.4736e-06 - val_my_r2: 0.9969\n",
      "Epoch 1647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3145e-04 - my_r2: 0.9415 - val_loss: 7.4879e-06 - val_my_r2: 0.9971\n",
      "Epoch 1648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3094e-04 - my_r2: 0.9142 - val_loss: 1.2177e-05 - val_my_r2: 0.9951\n",
      "Epoch 1649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4497e-04 - my_r2: 0.9358 - val_loss: 7.8646e-06 - val_my_r2: 0.9971\n",
      "Epoch 1650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8650e-04 - my_r2: 0.9183 - val_loss: 1.1162e-05 - val_my_r2: 0.9965\n",
      "Epoch 1651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1200e-04 - my_r2: 0.8937 - val_loss: 9.3900e-06 - val_my_r2: 0.9968\n",
      "Epoch 1652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7341e-04 - my_r2: 0.9531 - val_loss: 1.1016e-05 - val_my_r2: 0.9958\n",
      "Epoch 1653/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3645e-04 - my_r2: 0.9478 - val_loss: 9.4082e-06 - val_my_r2: 0.9967\n",
      "Epoch 1654/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 2.3120e-04 - my_r2: 0.9494 - val_loss: 1.2932e-05 - val_my_r2: 0.9953\n",
      "Epoch 1655/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 2.5942e-04 - my_r2: 0.9057 - val_loss: 1.1010e-05 - val_my_r2: 0.9967\n",
      "Epoch 1656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2560e-04 - my_r2: 0.9322 - val_loss: 8.4727e-06 - val_my_r2: 0.9975\n",
      "Epoch 1657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2923e-04 - my_r2: 0.9389 - val_loss: 8.0536e-06 - val_my_r2: 0.9974\n",
      "Epoch 1658/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6257e-04 - my_r2: 0.9391 - val_loss: 7.7556e-06 - val_my_r2: 0.9974\n",
      "Epoch 1659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9046e-04 - my_r2: 0.8841 - val_loss: 1.2470e-05 - val_my_r2: 0.9957\n",
      "Epoch 1660/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5310e-04 - my_r2: 0.9276 - val_loss: 1.0229e-05 - val_my_r2: 0.9963\n",
      "Epoch 1661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9661e-04 - my_r2: 0.9234 - val_loss: 8.1083e-06 - val_my_r2: 0.9973\n",
      "Epoch 1662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5204e-04 - my_r2: 0.9316 - val_loss: 7.4104e-06 - val_my_r2: 0.9974\n",
      "Epoch 1663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3514e-04 - my_r2: 0.9239 - val_loss: 7.0439e-06 - val_my_r2: 0.9976\n",
      "Epoch 1664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0871e-04 - my_r2: 0.9488 - val_loss: 8.1669e-06 - val_my_r2: 0.9975\n",
      "Epoch 1665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6643e-04 - my_r2: 0.9404 - val_loss: 1.1238e-05 - val_my_r2: 0.9966\n",
      "Epoch 1666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7647e-04 - my_r2: 0.9422 - val_loss: 8.0612e-06 - val_my_r2: 0.9973\n",
      "Epoch 1667/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9386e-04 - my_r2: 0.8126 - val_loss: 1.8781e-05 - val_my_r2: 0.9939\n",
      "Epoch 1668/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5441e-04 - my_r2: 0.9238 - val_loss: 1.5957e-05 - val_my_r2: 0.9946\n",
      "Epoch 1669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4142e-04 - my_r2: 0.9383 - val_loss: 1.1991e-05 - val_my_r2: 0.9958\n",
      "Epoch 1670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5674e-04 - my_r2: 0.9328 - val_loss: 1.0210e-05 - val_my_r2: 0.9963\n",
      "Epoch 1671/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9981e-04 - my_r2: 0.9535 - val_loss: 8.6528e-06 - val_my_r2: 0.9967\n",
      "Epoch 1672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7785e-04 - my_r2: 0.9234 - val_loss: 9.1184e-06 - val_my_r2: 0.9969\n",
      "Epoch 1673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9925e-04 - my_r2: 0.9188 - val_loss: 1.0295e-05 - val_my_r2: 0.9964\n",
      "Epoch 1674/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8901e-04 - my_r2: 0.9138 - val_loss: 1.2206e-05 - val_my_r2: 0.9959\n",
      "Epoch 1675/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3689e-04 - my_r2: 0.9325 - val_loss: 1.2734e-05 - val_my_r2: 0.9958\n",
      "Epoch 1676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9989e-04 - my_r2: 0.9231 - val_loss: 1.2445e-05 - val_my_r2: 0.9959\n",
      "Epoch 1677/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7682e-04 - my_r2: 0.9408 - val_loss: 9.8278e-06 - val_my_r2: 0.9963\n",
      "Epoch 1678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5130e-04 - my_r2: 0.9399 - val_loss: 9.1537e-06 - val_my_r2: 0.9966\n",
      "Epoch 1679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2054e-04 - my_r2: 0.9408 - val_loss: 8.7781e-06 - val_my_r2: 0.9967\n",
      "Epoch 1680/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2605e-04 - my_r2: 0.8703 - val_loss: 1.2511e-05 - val_my_r2: 0.9957\n",
      "Epoch 1681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2918e-04 - my_r2: 0.9425 - val_loss: 8.8775e-06 - val_my_r2: 0.9969\n",
      "Epoch 1682/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1840e-04 - my_r2: 0.9514 - val_loss: 1.0279e-05 - val_my_r2: 0.9965\n",
      "Epoch 1683/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4592e-04 - my_r2: 0.9409 - val_loss: 1.2917e-05 - val_my_r2: 0.9957\n",
      "Epoch 1684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7448e-04 - my_r2: 0.9226 - val_loss: 1.2173e-05 - val_my_r2: 0.9956\n",
      "Epoch 1685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4393e-04 - my_r2: 0.9269 - val_loss: 8.8793e-06 - val_my_r2: 0.9969\n",
      "Epoch 1686/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5733e-04 - my_r2: 0.9393 - val_loss: 1.2588e-05 - val_my_r2: 0.9960\n",
      "Epoch 1687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6054e-04 - my_r2: 0.9032 - val_loss: 1.3375e-05 - val_my_r2: 0.9961\n",
      "Epoch 1688/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6792e-04 - my_r2: 0.9179 - val_loss: 9.4382e-06 - val_my_r2: 0.9969\n",
      "Epoch 1689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4273e-04 - my_r2: 0.9383 - val_loss: 8.1682e-06 - val_my_r2: 0.9973\n",
      "Epoch 1690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7961e-04 - my_r2: 0.9237 - val_loss: 1.0452e-05 - val_my_r2: 0.9964\n",
      "Epoch 1691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1606e-04 - my_r2: 0.9424 - val_loss: 1.5025e-05 - val_my_r2: 0.9953\n",
      "Epoch 1692/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.4993e-04 - my_r2: 0.9279 - val_loss: 1.3232e-05 - val_my_r2: 0.9958\n",
      "Epoch 1693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5763e-04 - my_r2: 0.8706 - val_loss: 9.8702e-06 - val_my_r2: 0.9967\n",
      "Epoch 1694/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1572e-04 - my_r2: 0.9381 - val_loss: 1.1287e-05 - val_my_r2: 0.9961\n",
      "Epoch 1695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5401e-04 - my_r2: 0.9343 - val_loss: 1.0545e-05 - val_my_r2: 0.9965\n",
      "Epoch 1696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4952e-04 - my_r2: 0.9401 - val_loss: 1.1591e-05 - val_my_r2: 0.9963\n",
      "Epoch 1697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6613e-04 - my_r2: 0.9184 - val_loss: 1.3492e-05 - val_my_r2: 0.9957\n",
      "Epoch 1698/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0936e-04 - my_r2: 0.9460 - val_loss: 1.0980e-05 - val_my_r2: 0.9966\n",
      "Epoch 1699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7266e-04 - my_r2: 0.8915 - val_loss: 1.0662e-05 - val_my_r2: 0.9963\n",
      "Epoch 1700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7242e-04 - my_r2: 0.9243 - val_loss: 1.4980e-05 - val_my_r2: 0.9951\n",
      "Epoch 1701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0627e-04 - my_r2: 0.9469 - val_loss: 1.4354e-05 - val_my_r2: 0.9954\n",
      "Epoch 1702/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8725e-04 - my_r2: 0.9516 - val_loss: 9.4246e-06 - val_my_r2: 0.9965\n",
      "Epoch 1703/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5323e-04 - my_r2: 0.9521 - val_loss: 1.0469e-05 - val_my_r2: 0.9965\n",
      "Epoch 1704/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7699e-04 - my_r2: 0.9013 - val_loss: 1.2315e-05 - val_my_r2: 0.9958\n",
      "Epoch 1705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3477e-04 - my_r2: 0.9193 - val_loss: 1.1184e-05 - val_my_r2: 0.9956\n",
      "Epoch 1706/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3231e-04 - my_r2: 0.9516 - val_loss: 1.0053e-05 - val_my_r2: 0.9957\n",
      "Epoch 1707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6772e-04 - my_r2: 0.9373 - val_loss: 8.7119e-06 - val_my_r2: 0.9967\n",
      "Epoch 1708/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1400e-04 - my_r2: 0.8903 - val_loss: 9.4722e-06 - val_my_r2: 0.9963\n",
      "Epoch 1709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4937e-04 - my_r2: 0.9162 - val_loss: 1.0657e-05 - val_my_r2: 0.9958\n",
      "Epoch 1710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8595e-04 - my_r2: 0.9552 - val_loss: 8.9764e-06 - val_my_r2: 0.9964\n",
      "Epoch 1711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8875e-04 - my_r2: 0.9201 - val_loss: 7.2904e-06 - val_my_r2: 0.9973\n",
      "Epoch 1712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0084e-04 - my_r2: 0.9302 - val_loss: 9.0529e-06 - val_my_r2: 0.9963\n",
      "Epoch 1713/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8484e-04 - my_r2: 0.8975 - val_loss: 8.8390e-06 - val_my_r2: 0.9965\n",
      "Epoch 1714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3902e-04 - my_r2: 0.9459 - val_loss: 8.3953e-06 - val_my_r2: 0.9968\n",
      "Epoch 1715/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0210e-04 - my_r2: 0.9533 - val_loss: 8.4288e-06 - val_my_r2: 0.9969\n",
      "Epoch 1716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3556e-04 - my_r2: 0.9413 - val_loss: 1.0210e-05 - val_my_r2: 0.9967\n",
      "Epoch 1717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2230e-04 - my_r2: 0.9503 - val_loss: 1.2470e-05 - val_my_r2: 0.9963\n",
      "Epoch 1718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5328e-04 - my_r2: 0.9281 - val_loss: 1.2685e-05 - val_my_r2: 0.9959\n",
      "Epoch 1719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6069e-04 - my_r2: 0.9438 - val_loss: 8.7638e-06 - val_my_r2: 0.9968\n",
      "Epoch 1720/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7345e-04 - my_r2: 0.8820 - val_loss: 1.3796e-05 - val_my_r2: 0.9947\n",
      "Epoch 1721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3405e-04 - my_r2: 0.9552 - val_loss: 1.1705e-05 - val_my_r2: 0.9954\n",
      "Epoch 1722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2226e-04 - my_r2: 0.9169 - val_loss: 7.5493e-06 - val_my_r2: 0.9971\n",
      "Epoch 1723/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2356e-04 - my_r2: 0.9518 - val_loss: 7.1127e-06 - val_my_r2: 0.9975\n",
      "Epoch 1724/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.4929e-04 - my_r2: 0.9596 - val_loss: 7.3508e-06 - val_my_r2: 0.9974\n",
      "Epoch 1725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4822e-04 - my_r2: 0.8619 - val_loss: 6.8854e-06 - val_my_r2: 0.9976\n",
      "Epoch 1726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7444e-04 - my_r2: 0.9183 - val_loss: 8.6399e-06 - val_my_r2: 0.9968\n",
      "Epoch 1727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8680e-04 - my_r2: 0.9356 - val_loss: 7.7794e-06 - val_my_r2: 0.9970\n",
      "Epoch 1728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2740e-04 - my_r2: 0.9535 - val_loss: 7.9981e-06 - val_my_r2: 0.9970\n",
      "Epoch 1729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7253e-04 - my_r2: 0.9422 - val_loss: 6.5304e-06 - val_my_r2: 0.9977\n",
      "Epoch 1730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7256e-04 - my_r2: 0.9497 - val_loss: 6.8689e-06 - val_my_r2: 0.9975\n",
      "Epoch 1731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5563e-04 - my_r2: 0.9456 - val_loss: 7.4926e-06 - val_my_r2: 0.9972\n",
      "Epoch 1732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3827e-04 - my_r2: 0.9411 - val_loss: 6.3328e-06 - val_my_r2: 0.9978\n",
      "Epoch 1733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7173e-04 - my_r2: 0.9350 - val_loss: 6.9233e-06 - val_my_r2: 0.9976\n",
      "Epoch 1734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2200e-04 - my_r2: 0.9438 - val_loss: 7.3151e-06 - val_my_r2: 0.9972\n",
      "Epoch 1735/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8739e-04 - my_r2: 0.8872 - val_loss: 9.6798e-06 - val_my_r2: 0.9967\n",
      "Epoch 1736/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5041e-04 - my_r2: 0.9437 - val_loss: 5.9964e-06 - val_my_r2: 0.9979\n",
      "Epoch 1737/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5131e-04 - my_r2: 0.9355 - val_loss: 9.2894e-06 - val_my_r2: 0.9969\n",
      "Epoch 1738/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5849e-04 - my_r2: 0.9131 - val_loss: 1.1385e-05 - val_my_r2: 0.9962\n",
      "Epoch 1739/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7178e-04 - my_r2: 0.9188 - val_loss: 9.2498e-06 - val_my_r2: 0.9969\n",
      "Epoch 1740/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3554e-04 - my_r2: 0.9445 - val_loss: 6.8136e-06 - val_my_r2: 0.9977\n",
      "Epoch 1741/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4644e-04 - my_r2: 0.9366 - val_loss: 8.0241e-06 - val_my_r2: 0.9969\n",
      "Epoch 1742/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6254e-04 - my_r2: 0.9123 - val_loss: 8.8644e-06 - val_my_r2: 0.9967\n",
      "Epoch 1743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1473e-04 - my_r2: 0.9515 - val_loss: 7.2694e-06 - val_my_r2: 0.9973\n",
      "Epoch 1744/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2358e-04 - my_r2: 0.9315 - val_loss: 7.6604e-06 - val_my_r2: 0.9973\n",
      "Epoch 1745/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1775e-04 - my_r2: 0.9314 - val_loss: 9.8780e-06 - val_my_r2: 0.9965\n",
      "Epoch 1746/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2504e-04 - my_r2: 0.9065 - val_loss: 1.4249e-05 - val_my_r2: 0.9951\n",
      "Epoch 1747/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8001e-04 - my_r2: 0.9218 - val_loss: 1.0109e-05 - val_my_r2: 0.9966\n",
      "Epoch 1748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6334e-04 - my_r2: 0.9246 - val_loss: 1.2889e-05 - val_my_r2: 0.9956\n",
      "Epoch 1749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3846e-04 - my_r2: 0.8841 - val_loss: 1.1139e-05 - val_my_r2: 0.9959\n",
      "Epoch 1750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6172e-04 - my_r2: 0.9393 - val_loss: 1.4418e-05 - val_my_r2: 0.9950\n",
      "Epoch 1751/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8497e-04 - my_r2: 0.9090 - val_loss: 1.2556e-05 - val_my_r2: 0.9958\n",
      "Epoch 1752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2543e-04 - my_r2: 0.8880 - val_loss: 1.0686e-05 - val_my_r2: 0.9964\n",
      "Epoch 1753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9597e-04 - my_r2: 0.9063 - val_loss: 1.1693e-05 - val_my_r2: 0.9962\n",
      "Epoch 1754/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8759e-04 - my_r2: 0.9039 - val_loss: 9.7447e-06 - val_my_r2: 0.9966\n",
      "Epoch 1755/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4474e-04 - my_r2: 0.9037 - val_loss: 7.7888e-06 - val_my_r2: 0.9974\n",
      "Epoch 1756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0689e-04 - my_r2: 0.9520 - val_loss: 9.4304e-06 - val_my_r2: 0.9968\n",
      "Epoch 1757/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8773e-04 - my_r2: 0.9432 - val_loss: 1.5047e-05 - val_my_r2: 0.9945\n",
      "Epoch 1758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5833e-04 - my_r2: 0.9147 - val_loss: 8.8945e-06 - val_my_r2: 0.9968\n",
      "Epoch 1759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1969e-04 - my_r2: 0.9157 - val_loss: 7.2479e-06 - val_my_r2: 0.9973\n",
      "Epoch 1760/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4404e-04 - my_r2: 0.9221 - val_loss: 9.1079e-06 - val_my_r2: 0.9972\n",
      "Epoch 1761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3727e-04 - my_r2: 0.9485 - val_loss: 1.2909e-05 - val_my_r2: 0.9957\n",
      "Epoch 1762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4626e-04 - my_r2: 0.9206 - val_loss: 1.0265e-05 - val_my_r2: 0.9966\n",
      "Epoch 1763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2440e-04 - my_r2: 0.9407 - val_loss: 1.2951e-05 - val_my_r2: 0.9958\n",
      "Epoch 1764/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7272e-04 - my_r2: 0.9196 - val_loss: 9.8948e-06 - val_my_r2: 0.9963\n",
      "Epoch 1765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4889e-04 - my_r2: 0.9436 - val_loss: 9.0486e-06 - val_my_r2: 0.9967\n",
      "Epoch 1766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1673e-04 - my_r2: 0.9423 - val_loss: 1.0792e-05 - val_my_r2: 0.9965\n",
      "Epoch 1767/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7266e-04 - my_r2: 0.9316 - val_loss: 1.6739e-05 - val_my_r2: 0.9949\n",
      "Epoch 1768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9948e-04 - my_r2: 0.8457 - val_loss: 1.2306e-05 - val_my_r2: 0.9964\n",
      "Epoch 1769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6627e-04 - my_r2: 0.8839 - val_loss: 1.0830e-05 - val_my_r2: 0.9968\n",
      "Epoch 1770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2964e-04 - my_r2: 0.9228 - val_loss: 6.8796e-06 - val_my_r2: 0.9977\n",
      "Epoch 1771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6805e-04 - my_r2: 0.9150 - val_loss: 7.5356e-06 - val_my_r2: 0.9972\n",
      "Epoch 1772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5777e-04 - my_r2: 0.8842 - val_loss: 9.5674e-06 - val_my_r2: 0.9964\n",
      "Epoch 1773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3610e-04 - my_r2: 0.9200 - val_loss: 9.1587e-06 - val_my_r2: 0.9962\n",
      "Epoch 1774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8874e-04 - my_r2: 0.9487 - val_loss: 9.6942e-06 - val_my_r2: 0.9963\n",
      "Epoch 1775/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1958e-04 - my_r2: 0.9317 - val_loss: 6.9301e-06 - val_my_r2: 0.9971\n",
      "Epoch 1776/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6718e-04 - my_r2: 0.9275 - val_loss: 8.3974e-06 - val_my_r2: 0.9968\n",
      "Epoch 1777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5028e-04 - my_r2: 0.9321 - val_loss: 7.1317e-06 - val_my_r2: 0.9974\n",
      "Epoch 1778/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7020e-04 - my_r2: 0.7710 - val_loss: 7.7565e-06 - val_my_r2: 0.9971\n",
      "Epoch 1779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2037e-04 - my_r2: 0.9476 - val_loss: 6.6765e-06 - val_my_r2: 0.9975\n",
      "Epoch 1780/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4418e-04 - my_r2: 0.9277 - val_loss: 1.0748e-05 - val_my_r2: 0.9960\n",
      "Epoch 1781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6223e-04 - my_r2: 0.9396 - val_loss: 7.1343e-06 - val_my_r2: 0.9974\n",
      "Epoch 1782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2289e-04 - my_r2: 0.9430 - val_loss: 9.3503e-06 - val_my_r2: 0.9965\n",
      "Epoch 1783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1488e-04 - my_r2: 0.9516 - val_loss: 9.8725e-06 - val_my_r2: 0.9964\n",
      "Epoch 1784/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6626e-04 - my_r2: 0.9479 - val_loss: 1.1718e-05 - val_my_r2: 0.9955\n",
      "Epoch 1785/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0006e-04 - my_r2: 0.9004 - val_loss: 1.0285e-05 - val_my_r2: 0.9962\n",
      "Epoch 1786/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4477e-04 - my_r2: 0.9352 - val_loss: 8.6140e-06 - val_my_r2: 0.9970\n",
      "Epoch 1787/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3183e-04 - my_r2: 0.9556 - val_loss: 8.8681e-06 - val_my_r2: 0.9968\n",
      "Epoch 1788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1407e-04 - my_r2: 0.9504 - val_loss: 1.1311e-05 - val_my_r2: 0.9958\n",
      "Epoch 1789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8885e-04 - my_r2: 0.9281 - val_loss: 1.0357e-05 - val_my_r2: 0.9961\n",
      "Epoch 1790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3452e-04 - my_r2: 0.9489 - val_loss: 1.7472e-05 - val_my_r2: 0.9935\n",
      "Epoch 1791/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6523e-04 - my_r2: 0.9265 - val_loss: 7.9815e-06 - val_my_r2: 0.9971\n",
      "Epoch 1792/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1137e-04 - my_r2: 0.9320 - val_loss: 9.8751e-06 - val_my_r2: 0.9966\n",
      "Epoch 1793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2058e-04 - my_r2: 0.9538 - val_loss: 1.1444e-05 - val_my_r2: 0.9965\n",
      "Epoch 1794/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8822e-04 - my_r2: 0.9613 - val_loss: 1.3950e-05 - val_my_r2: 0.9957\n",
      "Epoch 1795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9739e-04 - my_r2: 0.9238 - val_loss: 1.3811e-05 - val_my_r2: 0.9956\n",
      "Epoch 1796/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8467e-04 - my_r2: 0.9225 - val_loss: 9.4692e-06 - val_my_r2: 0.9968\n",
      "Epoch 1797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7223e-04 - my_r2: 0.9131 - val_loss: 1.6047e-05 - val_my_r2: 0.9945\n",
      "Epoch 1798/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9324e-04 - my_r2: 0.9407 - val_loss: 8.7885e-06 - val_my_r2: 0.9971\n",
      "Epoch 1799/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2341e-04 - my_r2: 0.9234 - val_loss: 7.6265e-06 - val_my_r2: 0.9977\n",
      "Epoch 1800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3318e-04 - my_r2: 0.8893 - val_loss: 8.2873e-06 - val_my_r2: 0.9973\n",
      "Epoch 1801/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4440e-04 - my_r2: 0.8974 - val_loss: 9.5883e-06 - val_my_r2: 0.9968\n",
      "Epoch 1802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2402e-04 - my_r2: 0.9472 - val_loss: 8.6502e-06 - val_my_r2: 0.9973\n",
      "Epoch 1803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4904e-04 - my_r2: 0.8997 - val_loss: 9.4503e-06 - val_my_r2: 0.9971\n",
      "Epoch 1804/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6965e-04 - my_r2: 0.9560 - val_loss: 1.0005e-05 - val_my_r2: 0.9971\n",
      "Epoch 1805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1685e-04 - my_r2: 0.9414 - val_loss: 1.0851e-05 - val_my_r2: 0.9969\n",
      "Epoch 1806/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8787e-04 - my_r2: 0.8917 - val_loss: 9.0325e-06 - val_my_r2: 0.9973\n",
      "Epoch 1807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2292e-04 - my_r2: 0.9286 - val_loss: 9.8262e-06 - val_my_r2: 0.9969\n",
      "Epoch 1808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5556e-04 - my_r2: 0.9259 - val_loss: 1.0107e-05 - val_my_r2: 0.9968\n",
      "Epoch 1809/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7859e-04 - my_r2: 0.9458 - val_loss: 1.5905e-05 - val_my_r2: 0.9945\n",
      "Epoch 1810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0350e-04 - my_r2: 0.9367 - val_loss: 1.5014e-05 - val_my_r2: 0.9948\n",
      "Epoch 1811/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1362e-04 - my_r2: 0.9368 - val_loss: 9.9144e-06 - val_my_r2: 0.9970\n",
      "Epoch 1812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6142e-04 - my_r2: 0.9164 - val_loss: 9.6725e-06 - val_my_r2: 0.9970\n",
      "Epoch 1813/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4462e-04 - my_r2: 0.9165 - val_loss: 1.2429e-05 - val_my_r2: 0.9958\n",
      "Epoch 1814/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4641e-04 - my_r2: 0.9402 - val_loss: 8.3065e-06 - val_my_r2: 0.9974\n",
      "Epoch 1815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6927e-04 - my_r2: 0.8801 - val_loss: 1.0110e-05 - val_my_r2: 0.9969\n",
      "Epoch 1816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9364e-04 - my_r2: 0.8355 - val_loss: 1.4353e-05 - val_my_r2: 0.9947\n",
      "Epoch 1817/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7855e-04 - my_r2: 0.9313 - val_loss: 1.3042e-05 - val_my_r2: 0.9951\n",
      "Epoch 1818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4998e-04 - my_r2: 0.9167 - val_loss: 1.5413e-05 - val_my_r2: 0.9949\n",
      "Epoch 1819/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3465e-04 - my_r2: 0.9348 - val_loss: 1.1262e-05 - val_my_r2: 0.9960\n",
      "Epoch 1820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3482e-04 - my_r2: 0.9331 - val_loss: 7.7876e-06 - val_my_r2: 0.9970\n",
      "Epoch 1821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3586e-04 - my_r2: 0.9480 - val_loss: 7.1685e-06 - val_my_r2: 0.9972\n",
      "Epoch 1822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1443e-04 - my_r2: 0.9386 - val_loss: 7.0083e-06 - val_my_r2: 0.9973\n",
      "Epoch 1823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5647e-04 - my_r2: 0.9262 - val_loss: 9.7824e-06 - val_my_r2: 0.9965\n",
      "Epoch 1824/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0997e-04 - my_r2: 0.9070 - val_loss: 9.6217e-06 - val_my_r2: 0.9969\n",
      "Epoch 1825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4461e-04 - my_r2: 0.9362 - val_loss: 1.0068e-05 - val_my_r2: 0.9965\n",
      "Epoch 1826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3406e-04 - my_r2: 0.9467 - val_loss: 9.0775e-06 - val_my_r2: 0.9968\n",
      "Epoch 1827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2117e-04 - my_r2: 0.9471 - val_loss: 1.1390e-05 - val_my_r2: 0.9957\n",
      "Epoch 1828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5568e-04 - my_r2: 0.8387 - val_loss: 8.8889e-06 - val_my_r2: 0.9968\n",
      "Epoch 1829/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3442e-04 - my_r2: 0.9403 - val_loss: 8.9695e-06 - val_my_r2: 0.9965\n",
      "Epoch 1830/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8702e-04 - my_r2: 0.9345 - val_loss: 8.9423e-06 - val_my_r2: 0.9967\n",
      "Epoch 1831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5006e-04 - my_r2: 0.9347 - val_loss: 7.9488e-06 - val_my_r2: 0.9970\n",
      "Epoch 1832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2423e-04 - my_r2: 0.9405 - val_loss: 1.0448e-05 - val_my_r2: 0.9963\n",
      "Epoch 1833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5868e-04 - my_r2: 0.9468 - val_loss: 8.5275e-06 - val_my_r2: 0.9970\n",
      "Epoch 1834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4267e-04 - my_r2: 0.9107 - val_loss: 8.8106e-06 - val_my_r2: 0.9970\n",
      "Epoch 1835/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2530e-04 - my_r2: 0.9397 - val_loss: 8.8261e-06 - val_my_r2: 0.9966\n",
      "Epoch 1836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5445e-04 - my_r2: 0.9431 - val_loss: 1.1490e-05 - val_my_r2: 0.9953\n",
      "Epoch 1837/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3045e-04 - my_r2: 0.9569 - val_loss: 1.4853e-05 - val_my_r2: 0.9948\n",
      "Epoch 1838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2107e-04 - my_r2: 0.9448 - val_loss: 1.6145e-05 - val_my_r2: 0.9949\n",
      "Epoch 1839/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3489e-04 - my_r2: 0.9424 - val_loss: 1.2622e-05 - val_my_r2: 0.9959\n",
      "Epoch 1840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6758e-04 - my_r2: 0.9092 - val_loss: 8.0747e-06 - val_my_r2: 0.9972\n",
      "Epoch 1841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3140e-04 - my_r2: 0.7610 - val_loss: 7.3678e-06 - val_my_r2: 0.9974\n",
      "Epoch 1842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5895e-04 - my_r2: 0.9277 - val_loss: 8.5119e-06 - val_my_r2: 0.9971\n",
      "Epoch 1843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2668e-04 - my_r2: 0.9425 - val_loss: 9.5686e-06 - val_my_r2: 0.9967\n",
      "Epoch 1844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8613e-04 - my_r2: 0.9396 - val_loss: 8.1234e-06 - val_my_r2: 0.9973\n",
      "Epoch 1845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6298e-04 - my_r2: 0.8945 - val_loss: 6.7109e-06 - val_my_r2: 0.9977\n",
      "Epoch 1846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8701e-04 - my_r2: 0.9203 - val_loss: 1.0819e-05 - val_my_r2: 0.9963\n",
      "Epoch 1847/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3518e-04 - my_r2: 0.9386 - val_loss: 6.2123e-06 - val_my_r2: 0.9978\n",
      "Epoch 1848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0200e-04 - my_r2: 0.8816 - val_loss: 5.1057e-06 - val_my_r2: 0.9984\n",
      "Epoch 1849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9920e-04 - my_r2: 0.9584 - val_loss: 6.3456e-06 - val_my_r2: 0.9977\n",
      "Epoch 1850/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5652e-04 - my_r2: 0.9199 - val_loss: 8.1809e-06 - val_my_r2: 0.9968\n",
      "Epoch 1851/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3278e-04 - my_r2: 0.9294 - val_loss: 7.7859e-06 - val_my_r2: 0.9974\n",
      "Epoch 1852/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3522e-04 - my_r2: 0.9462 - val_loss: 9.4034e-06 - val_my_r2: 0.9968\n",
      "Epoch 1853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2425e-04 - my_r2: 0.9339 - val_loss: 8.4796e-06 - val_my_r2: 0.9973\n",
      "Epoch 1854/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6001e-04 - my_r2: 0.9405 - val_loss: 7.1823e-06 - val_my_r2: 0.9978\n",
      "Epoch 1855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0612e-04 - my_r2: 0.9117 - val_loss: 8.9695e-06 - val_my_r2: 0.9971\n",
      "Epoch 1856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1617e-04 - my_r2: 0.9539 - val_loss: 9.2976e-06 - val_my_r2: 0.9971\n",
      "Epoch 1857/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1258e-04 - my_r2: 0.9207 - val_loss: 8.3823e-06 - val_my_r2: 0.9972\n",
      "Epoch 1858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2742e-04 - my_r2: 0.9573 - val_loss: 1.1888e-05 - val_my_r2: 0.9959\n",
      "Epoch 1859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2442e-04 - my_r2: 0.9269 - val_loss: 1.3205e-05 - val_my_r2: 0.9950\n",
      "Epoch 1860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0590e-04 - my_r2: 0.9223 - val_loss: 8.6292e-06 - val_my_r2: 0.9969\n",
      "Epoch 1861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3273e-04 - my_r2: 0.9345 - val_loss: 8.6606e-06 - val_my_r2: 0.9971\n",
      "Epoch 1862/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9931e-04 - my_r2: 0.9385 - val_loss: 1.4141e-05 - val_my_r2: 0.9955\n",
      "Epoch 1863/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4189e-04 - my_r2: 0.9355 - val_loss: 1.3994e-05 - val_my_r2: 0.9957\n",
      "Epoch 1864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1750e-04 - my_r2: 0.9252 - val_loss: 1.4865e-05 - val_my_r2: 0.9956\n",
      "Epoch 1865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6943e-04 - my_r2: 0.9249 - val_loss: 2.1778e-05 - val_my_r2: 0.9937\n",
      "Epoch 1866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2954e-04 - my_r2: 0.9468 - val_loss: 1.5368e-05 - val_my_r2: 0.9954\n",
      "Epoch 1867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5818e-04 - my_r2: 0.9326 - val_loss: 1.1892e-05 - val_my_r2: 0.9962\n",
      "Epoch 1868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1168e-04 - my_r2: 0.9473 - val_loss: 1.9321e-05 - val_my_r2: 0.9942\n",
      "Epoch 1869/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7671e-04 - my_r2: 0.9427 - val_loss: 1.2537e-05 - val_my_r2: 0.9958\n",
      "Epoch 1870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4325e-04 - my_r2: 0.9383 - val_loss: 1.2867e-05 - val_my_r2: 0.9951\n",
      "Epoch 1871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1671e-04 - my_r2: 0.9507 - val_loss: 9.9915e-06 - val_my_r2: 0.9962\n",
      "Epoch 1872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3860e-04 - my_r2: 0.9424 - val_loss: 1.0974e-05 - val_my_r2: 0.9960\n",
      "Epoch 1873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7557e-04 - my_r2: 0.9310 - val_loss: 8.1967e-06 - val_my_r2: 0.9971\n",
      "Epoch 1874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5615e-04 - my_r2: 0.9371 - val_loss: 1.0209e-05 - val_my_r2: 0.9959\n",
      "Epoch 1875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2992e-04 - my_r2: 0.8519 - val_loss: 1.0292e-05 - val_my_r2: 0.9958\n",
      "Epoch 1876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4940e-04 - my_r2: 0.9351 - val_loss: 1.0663e-05 - val_my_r2: 0.9959\n",
      "Epoch 1877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6053e-04 - my_r2: 0.9085 - val_loss: 9.0150e-06 - val_my_r2: 0.9969\n",
      "Epoch 1878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6801e-04 - my_r2: 0.9418 - val_loss: 8.4364e-06 - val_my_r2: 0.9971\n",
      "Epoch 1879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9137e-04 - my_r2: 0.9322 - val_loss: 8.3286e-06 - val_my_r2: 0.9969\n",
      "Epoch 1880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3181e-04 - my_r2: 0.9472 - val_loss: 6.9476e-06 - val_my_r2: 0.9974\n",
      "Epoch 1881/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5083e-04 - my_r2: 0.9356 - val_loss: 7.3071e-06 - val_my_r2: 0.9976\n",
      "Epoch 1882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3774e-04 - my_r2: 0.8886 - val_loss: 6.4872e-06 - val_my_r2: 0.9978\n",
      "Epoch 1883/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2643e-04 - my_r2: 0.9242 - val_loss: 6.0033e-06 - val_my_r2: 0.9979\n",
      "Epoch 1884/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3474e-04 - my_r2: 0.9054 - val_loss: 9.7603e-06 - val_my_r2: 0.9967\n",
      "Epoch 1885/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3318e-04 - my_r2: 0.9424 - val_loss: 1.0657e-05 - val_my_r2: 0.9964\n",
      "Epoch 1886/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6685e-04 - my_r2: 0.9298 - val_loss: 1.1906e-05 - val_my_r2: 0.9958\n",
      "Epoch 1887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7530e-04 - my_r2: 0.9354 - val_loss: 9.2187e-06 - val_my_r2: 0.9969\n",
      "Epoch 1888/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5609e-04 - my_r2: 0.9397 - val_loss: 7.4036e-06 - val_my_r2: 0.9973\n",
      "Epoch 1889/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0337e-04 - my_r2: 0.9258 - val_loss: 1.0048e-05 - val_my_r2: 0.9961\n",
      "Epoch 1890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4967e-04 - my_r2: 0.9330 - val_loss: 8.7211e-06 - val_my_r2: 0.9966\n",
      "Epoch 1891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3302e-04 - my_r2: 0.9195 - val_loss: 7.0058e-06 - val_my_r2: 0.9977\n",
      "Epoch 1892/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8271e-04 - my_r2: 0.9347 - val_loss: 9.8719e-06 - val_my_r2: 0.9969\n",
      "Epoch 1893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7841e-04 - my_r2: 0.9279 - val_loss: 9.4606e-06 - val_my_r2: 0.9971\n",
      "Epoch 1894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2438e-04 - my_r2: 0.9496 - val_loss: 1.1163e-05 - val_my_r2: 0.9963\n",
      "Epoch 1895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3329e-04 - my_r2: 0.9541 - val_loss: 1.0123e-05 - val_my_r2: 0.9971\n",
      "Epoch 1896/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4771e-04 - my_r2: 0.9326 - val_loss: 1.0053e-05 - val_my_r2: 0.9967\n",
      "Epoch 1897/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3797e-04 - my_r2: 0.9488 - val_loss: 1.4599e-05 - val_my_r2: 0.9947\n",
      "Epoch 1898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2407e-04 - my_r2: 0.9051 - val_loss: 1.3367e-05 - val_my_r2: 0.9949\n",
      "Epoch 1899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4979e-04 - my_r2: 0.9406 - val_loss: 1.6062e-05 - val_my_r2: 0.9939\n",
      "Epoch 1900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0774e-04 - my_r2: 0.9457 - val_loss: 1.0200e-05 - val_my_r2: 0.9962\n",
      "Epoch 1901/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2077e-04 - my_r2: 0.9297 - val_loss: 6.8173e-06 - val_my_r2: 0.9978\n",
      "Epoch 1902/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0533e-04 - my_r2: 0.9488 - val_loss: 6.7731e-06 - val_my_r2: 0.9978\n",
      "Epoch 1903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3314e-04 - my_r2: 0.9354 - val_loss: 1.2510e-05 - val_my_r2: 0.9960\n",
      "Epoch 1904/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0851e-04 - my_r2: 0.9372 - val_loss: 6.2185e-06 - val_my_r2: 0.9981\n",
      "Epoch 1905/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4943e-04 - my_r2: 0.9096 - val_loss: 7.5141e-06 - val_my_r2: 0.9974\n",
      "Epoch 1906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5404e-04 - my_r2: 0.9277 - val_loss: 1.0280e-05 - val_my_r2: 0.9963\n",
      "Epoch 1907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7634e-04 - my_r2: 0.9045 - val_loss: 8.8314e-06 - val_my_r2: 0.9966\n",
      "Epoch 1908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6993e-04 - my_r2: 0.9403 - val_loss: 1.1944e-05 - val_my_r2: 0.9953\n",
      "Epoch 1909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0130e-04 - my_r2: 0.9176 - val_loss: 1.6367e-05 - val_my_r2: 0.9938\n",
      "Epoch 1910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5517e-04 - my_r2: 0.9392 - val_loss: 1.3740e-05 - val_my_r2: 0.9951\n",
      "Epoch 1911/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1767e-04 - my_r2: 0.9125 - val_loss: 1.2356e-05 - val_my_r2: 0.9957\n",
      "Epoch 1912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5035e-04 - my_r2: 0.9380 - val_loss: 1.0206e-05 - val_my_r2: 0.9966\n",
      "Epoch 1913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0744e-04 - my_r2: 0.9599 - val_loss: 1.0036e-05 - val_my_r2: 0.9969\n",
      "Epoch 1914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3484e-04 - my_r2: 0.9497 - val_loss: 9.3735e-06 - val_my_r2: 0.9971\n",
      "Epoch 1915/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3499e-04 - my_r2: 0.9333 - val_loss: 8.5583e-06 - val_my_r2: 0.9974\n",
      "Epoch 1916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2862e-04 - my_r2: 0.9461 - val_loss: 6.7772e-06 - val_my_r2: 0.9978\n",
      "Epoch 1917/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4671e-04 - my_r2: 0.9230 - val_loss: 8.0818e-06 - val_my_r2: 0.9975\n",
      "Epoch 1918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0278e-04 - my_r2: 0.8780 - val_loss: 8.4832e-06 - val_my_r2: 0.9975\n",
      "Epoch 1919/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4010e-04 - my_r2: 0.9284 - val_loss: 1.1251e-05 - val_my_r2: 0.9965\n",
      "Epoch 1920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1241e-04 - my_r2: 0.9442 - val_loss: 1.1905e-05 - val_my_r2: 0.9961\n",
      "Epoch 1921/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9847e-04 - my_r2: 0.9345 - val_loss: 1.2698e-05 - val_my_r2: 0.9962\n",
      "Epoch 1922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8266e-04 - my_r2: 0.9366 - val_loss: 1.6369e-05 - val_my_r2: 0.9951\n",
      "Epoch 1923/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4244e-04 - my_r2: 0.9280 - val_loss: 7.5782e-06 - val_my_r2: 0.9973\n",
      "Epoch 1924/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6227e-04 - my_r2: 0.9135 - val_loss: 8.0249e-06 - val_my_r2: 0.9973\n",
      "Epoch 1925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0156e-04 - my_r2: 0.9408 - val_loss: 1.2812e-05 - val_my_r2: 0.9961\n",
      "Epoch 1926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2831e-04 - my_r2: 0.8929 - val_loss: 1.2224e-05 - val_my_r2: 0.9966\n",
      "Epoch 1927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0321e-04 - my_r2: 0.9226 - val_loss: 7.1782e-06 - val_my_r2: 0.9979\n",
      "Epoch 1928/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9242e-04 - my_r2: 0.9520 - val_loss: 8.6677e-06 - val_my_r2: 0.9974\n",
      "Epoch 1929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7597e-04 - my_r2: 0.8876 - val_loss: 8.3694e-06 - val_my_r2: 0.9976\n",
      "Epoch 1930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6185e-04 - my_r2: 0.9367 - val_loss: 1.3242e-05 - val_my_r2: 0.9964\n",
      "Epoch 1931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5134e-04 - my_r2: 0.9184 - val_loss: 9.1119e-06 - val_my_r2: 0.9973\n",
      "Epoch 1932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4172e-04 - my_r2: 0.9353 - val_loss: 1.1159e-05 - val_my_r2: 0.9967\n",
      "Epoch 1933/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4764e-04 - my_r2: 0.9432 - val_loss: 1.0981e-05 - val_my_r2: 0.9959\n",
      "Epoch 1934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4042e-04 - my_r2: 0.8878 - val_loss: 9.2971e-06 - val_my_r2: 0.9964\n",
      "Epoch 1935/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5045e-04 - my_r2: 0.9297 - val_loss: 1.4445e-05 - val_my_r2: 0.9951\n",
      "Epoch 1936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0230e-04 - my_r2: 0.9078 - val_loss: 1.1738e-05 - val_my_r2: 0.9962\n",
      "Epoch 1937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0982e-04 - my_r2: 0.9516 - val_loss: 1.0059e-05 - val_my_r2: 0.9967\n",
      "Epoch 1938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3585e-04 - my_r2: 0.9235 - val_loss: 9.1565e-06 - val_my_r2: 0.9969\n",
      "Epoch 1939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4376e-04 - my_r2: 0.9312 - val_loss: 8.5169e-06 - val_my_r2: 0.9971\n",
      "Epoch 1940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6719e-04 - my_r2: 0.9289 - val_loss: 8.4618e-06 - val_my_r2: 0.9968\n",
      "Epoch 1941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4358e-04 - my_r2: 0.9322 - val_loss: 9.7754e-06 - val_my_r2: 0.9970\n",
      "Epoch 1942/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5813e-04 - my_r2: 0.9283 - val_loss: 1.0701e-05 - val_my_r2: 0.9967\n",
      "Epoch 1943/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2807e-04 - my_r2: 0.9494 - val_loss: 1.0519e-05 - val_my_r2: 0.9961\n",
      "Epoch 1944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9232e-04 - my_r2: 0.9313 - val_loss: 1.3002e-05 - val_my_r2: 0.9947\n",
      "Epoch 1945/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4643e-04 - my_r2: 0.8889 - val_loss: 1.0998e-05 - val_my_r2: 0.9954\n",
      "Epoch 1946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5793e-04 - my_r2: 0.9190 - val_loss: 1.4635e-05 - val_my_r2: 0.9951\n",
      "Epoch 1947/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3909e-04 - my_r2: 0.9435 - val_loss: 1.0214e-05 - val_my_r2: 0.9962\n",
      "Epoch 1948/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6141e-04 - my_r2: 0.8906 - val_loss: 7.2075e-06 - val_my_r2: 0.9977\n",
      "Epoch 1949/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9968e-04 - my_r2: 0.9135 - val_loss: 8.3012e-06 - val_my_r2: 0.9975\n",
      "Epoch 1950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9145e-04 - my_r2: 0.9388 - val_loss: 6.9357e-06 - val_my_r2: 0.9979\n",
      "Epoch 1951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2269e-04 - my_r2: 0.9506 - val_loss: 8.6143e-06 - val_my_r2: 0.9973\n",
      "Epoch 1952/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4881e-04 - my_r2: 0.9502 - val_loss: 9.0335e-06 - val_my_r2: 0.9966\n",
      "Epoch 1953/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4441e-04 - my_r2: 0.9395 - val_loss: 1.0540e-05 - val_my_r2: 0.9960\n",
      "Epoch 1954/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7710e-04 - my_r2: 0.8854 - val_loss: 6.7085e-06 - val_my_r2: 0.9978\n",
      "Epoch 1955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2202e-04 - my_r2: 0.9617 - val_loss: 7.5249e-06 - val_my_r2: 0.9975\n",
      "Epoch 1956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3368e-04 - my_r2: 0.9424 - val_loss: 7.4305e-06 - val_my_r2: 0.9975\n",
      "Epoch 1957/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2106e-04 - my_r2: 0.9489 - val_loss: 7.6992e-06 - val_my_r2: 0.9973\n",
      "Epoch 1958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3416e-04 - my_r2: 0.9456 - val_loss: 1.0453e-05 - val_my_r2: 0.9964\n",
      "Epoch 1959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8017e-04 - my_r2: 0.9374 - val_loss: 9.4613e-06 - val_my_r2: 0.9965\n",
      "Epoch 1960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4464e-04 - my_r2: 0.9338 - val_loss: 7.7172e-06 - val_my_r2: 0.9972\n",
      "Epoch 1961/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0116e-04 - my_r2: 0.9320 - val_loss: 8.1567e-06 - val_my_r2: 0.9972\n",
      "Epoch 1962/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3372e-04 - my_r2: 0.9334 - val_loss: 1.1590e-05 - val_my_r2: 0.9958\n",
      "Epoch 1963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1680e-04 - my_r2: 0.9363 - val_loss: 1.3334e-05 - val_my_r2: 0.9950\n",
      "Epoch 1964/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3477e-04 - my_r2: 0.9434 - val_loss: 1.2809e-05 - val_my_r2: 0.9954\n",
      "Epoch 1965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3044e-04 - my_r2: 0.9498 - val_loss: 7.8798e-06 - val_my_r2: 0.9975\n",
      "Epoch 1966/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8945e-04 - my_r2: 0.8722 - val_loss: 8.8859e-06 - val_my_r2: 0.9974\n",
      "Epoch 1967/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7515e-04 - my_r2: 0.9629 - val_loss: 7.2981e-06 - val_my_r2: 0.9976\n",
      "Epoch 1968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2518e-04 - my_r2: 0.9240 - val_loss: 8.2525e-06 - val_my_r2: 0.9970\n",
      "Epoch 1969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0894e-04 - my_r2: 0.9604 - val_loss: 8.5272e-06 - val_my_r2: 0.9965\n",
      "Epoch 1970/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7962e-04 - my_r2: 0.9501 - val_loss: 7.9273e-06 - val_my_r2: 0.9972\n",
      "Epoch 1971/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8816e-04 - my_r2: 0.9492 - val_loss: 7.5312e-06 - val_my_r2: 0.9972\n",
      "Epoch 1972/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9983e-04 - my_r2: 0.9356 - val_loss: 7.4583e-06 - val_my_r2: 0.9973\n",
      "Epoch 1973/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2312e-04 - my_r2: 0.9088 - val_loss: 7.7393e-06 - val_my_r2: 0.9975\n",
      "Epoch 1974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5601e-04 - my_r2: 0.9436 - val_loss: 8.0236e-06 - val_my_r2: 0.9974\n",
      "Epoch 1975/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0369e-04 - my_r2: 0.9443 - val_loss: 9.5464e-06 - val_my_r2: 0.9970\n",
      "Epoch 1976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6787e-04 - my_r2: 0.9249 - val_loss: 1.2552e-05 - val_my_r2: 0.9960\n",
      "Epoch 1977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3469e-04 - my_r2: 0.9372 - val_loss: 1.3509e-05 - val_my_r2: 0.9950\n",
      "Epoch 1978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9204e-04 - my_r2: 0.8670 - val_loss: 1.5684e-05 - val_my_r2: 0.9941\n",
      "Epoch 1979/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.3254e-04 - my_r2: 0.9396 - val_loss: 1.2147e-05 - val_my_r2: 0.9952\n",
      "Epoch 1980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3680e-04 - my_r2: 0.9453 - val_loss: 1.4698e-05 - val_my_r2: 0.9944\n",
      "Epoch 1981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2200e-04 - my_r2: 0.9275 - val_loss: 1.0398e-05 - val_my_r2: 0.9958\n",
      "Epoch 1982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2614e-04 - my_r2: 0.9176 - val_loss: 9.9112e-06 - val_my_r2: 0.9963\n",
      "Epoch 1983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3217e-04 - my_r2: 0.9330 - val_loss: 1.1861e-05 - val_my_r2: 0.9956\n",
      "Epoch 1984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6037e-04 - my_r2: 0.9279 - val_loss: 1.0602e-05 - val_my_r2: 0.9963\n",
      "Epoch 1985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4868e-04 - my_r2: 0.9521 - val_loss: 1.0123e-05 - val_my_r2: 0.9968\n",
      "Epoch 1986/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4276e-04 - my_r2: 0.9212 - val_loss: 9.3702e-06 - val_my_r2: 0.9971\n",
      "Epoch 1987/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4971e-04 - my_r2: 0.9252 - val_loss: 1.0917e-05 - val_my_r2: 0.9964\n",
      "Epoch 1988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5608e-04 - my_r2: 0.9351 - val_loss: 9.8081e-06 - val_my_r2: 0.9970\n",
      "Epoch 1989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3511e-04 - my_r2: 0.9433 - val_loss: 1.1243e-05 - val_my_r2: 0.9964\n",
      "Epoch 1990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9093e-04 - my_r2: 0.9036 - val_loss: 1.2662e-05 - val_my_r2: 0.9961\n",
      "Epoch 1991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7341e-04 - my_r2: 0.9314 - val_loss: 1.1790e-05 - val_my_r2: 0.9961\n",
      "Epoch 1992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6131e-04 - my_r2: 0.9240 - val_loss: 1.1454e-05 - val_my_r2: 0.9964\n",
      "Epoch 1993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4597e-04 - my_r2: 0.9293 - val_loss: 1.0263e-05 - val_my_r2: 0.9967\n",
      "Epoch 1994/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2809e-04 - my_r2: 0.9432 - val_loss: 1.2381e-05 - val_my_r2: 0.9960\n",
      "Epoch 1995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3715e-04 - my_r2: 0.9402 - val_loss: 8.8112e-06 - val_my_r2: 0.9972\n",
      "Epoch 1996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7576e-04 - my_r2: 0.9354 - val_loss: 9.5557e-06 - val_my_r2: 0.9971\n",
      "Epoch 1997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3937e-04 - my_r2: 0.9448 - val_loss: 1.2340e-05 - val_my_r2: 0.9959\n",
      "Epoch 1998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8091e-04 - my_r2: 0.9193 - val_loss: 1.1445e-05 - val_my_r2: 0.9964\n",
      "Epoch 1999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5609e-04 - my_r2: 0.9358 - val_loss: 7.9078e-06 - val_my_r2: 0.9976\n",
      "Epoch 2000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8699e-04 - my_r2: 0.9117 - val_loss: 8.2084e-06 - val_my_r2: 0.9975\n",
      "---------- 1\n",
      "---------- 1\n",
      "train = 1.00 test = 1.00 loss-train = -1.000000 loss-test = -1.000000 iter=0\n",
      "Stats for iML1515_ec6_UB_AMN_QP CPU-time 1402.8003\n",
      "R2 = 0.9988 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Q2 = 0.9988 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Iter 2 Collated Q2 0.9988456378017168\n",
      "number of reactions:  1186 1186\n",
      "number of metabolites:  2084\n",
      "filtered measurements size:  1\n",
      "RC reservoir file: ./Reservoir/iML1515_ec6_UB_AMN_QP\n",
      "RC model type: RC\n",
      "RC scaler: 0.0\n",
      "RC model input dim: 38\n",
      "RC model output dim: 1\n",
      "RC model medium bound: UB\n",
      "training set size (110, 38) (110, 1)\n",
      "reservoir S, Pin, Pout matrices (2084, 1186) (38, 1186) (1, 1186)\n",
      "RC training epochs: 2000\n",
      "RC training regression: True\n",
      "RC training learn rate: 0.0001\n",
      "RC training dropout: 0.25\n",
      "RC training batch size: 5\n",
      "RC training validation iter: 0\n",
      "RC training xfold: 0\n",
      "RC training early stopping: False\n",
      "--------prior network --------\n",
      "training file: None\n",
      "model type: ANN_Dense\n",
      "model scaler: 0.0\n",
      "model input dim: 10\n",
      "model output dim: 10\n",
      "model medium bound: \n",
      "timestep: 0\n",
      "no training set provided\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "--------reservoir network-----\n",
      "training file: ./Dataset_model/iML1515_ec6_UB\n",
      "model type: AMN_QP\n",
      "model scaler: 7.95\n",
      "model input dim: 38\n",
      "model output dim: 2376\n",
      "model medium bound: UB\n",
      "timestep: 4\n",
      "training set size (11000, 38) (11000, 1)\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "gradient learn rate: 0.001\n",
      "gradient decay rate: 0.9\n",
      "training epochs: 20\n",
      "training regression: True\n",
      "training learn rate: 0.001\n",
      "training dropout: 0.25\n",
      "training batch size: 100\n",
      "training validation iter: 0\n",
      "training xfold: 5\n",
      "training early stopping: False\n",
      "AMN scaler 0.0\n",
      "RC input shape (110, 38) (110, 1)\n",
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "Physical devices cannot be modified after being initialized\n",
      "----------------------------------- RC\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 10 relu True\n",
      "Prior inputs and outputs (None, 10) (None, 10)\n",
      "Res inputs added to Prior_outputs 28\n",
      "Res inputs (final) (None, 38)\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 1186 relu False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:08:44.100368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMN output shapes for PoutV, SV, PinV, Vpos, V, outputs (None, 1) (None, 1) (None, 1) (None, 1) (None, 1186) (None, 2376)\n",
      "=======================\n",
      "PoutV: (None, 1)\n",
      "SV: (None, 1)\n",
      "PinV: (None, 1)\n",
      "Vpos: (None, 1)\n",
      "V: (None, 1186)\n",
      "V0: KerasTensor(type_spec=TensorSpec(shape=(None, 1186), dtype=tf.float32, name=None), name='tf.__operators__.add_43/AddV2:0', description=\"created by layer 'tf.__operators__.add_43'\")\n",
      "Vin: KerasTensor(type_spec=TensorSpec(shape=(None, 38), dtype=tf.float32, name=None), name='tf.math.truediv_96/truediv:0', description=\"created by layer 'tf.math.truediv_96'\")\n",
      "Vout: tf.Tensor([], shape=(0, 0), dtype=float32)\n",
      "Res_outputs-------------------- (None, 2376)\n",
      "SV, PinV, Vpos, V-------------- (None, 1) (None, 1) (None, 1) (None, 1186)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_22 (Lambda)             (None, 10)           0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 500)          5500        ['lambda_22[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 500)          0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_21 (Lambda)             (None, 28)           0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 10)           5010        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 38)           0           ['lambda_21[0][0]',              \n",
      "                                                                  'dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_42 (TFOp  (None, 38)          0           ['input_4[0][0]',                \n",
      " Lambda)                                                          'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_84 (TFOpLambd  (None, 38)          0           ['concatenate_9[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_42[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_96 (TFOpLambda  (None, 38)          0           ['tf.math.multiply_84[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 500)          19500       ['tf.math.truediv_96[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 500)          0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 1186)         594186      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_60 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_96[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_33 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_60[0][0]',    \n",
      " a)                                                               'dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.relu_33 (TFOpLambda)     (None, 1186)         0           ['tf.math.subtract_33[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_43 (TFOp  (None, 1186)        0           ['tf.nn.relu_33[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_33[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract_34 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_43[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_85 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_43[0][0]'\n",
      " a)                                                              , 'dense_15[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply_86 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_34[0][0]',    \n",
      " a)                                                               'tf.linalg.matmul_60[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_87 (TFOpLambd  (None, 1186)        0           ['dense_15[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_42 (TFOpL  (None, 1186)        0           ['tf.math.multiply_85[0][0]',    \n",
      " ambda)                                                           'tf.math.multiply_86[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_43 (TFOpL  (None, 1186)        0           ['tf.math.multiply_87[0][0]',    \n",
      " ambda)                                                           'tf.__operators__.add_42[0][0]']\n",
      "                                                                                                  \n",
      " tf.linalg.matmul_63 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_43[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_35 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_63[0][0]',    \n",
      " a)                                                               'tf.math.truediv_96[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_34 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_35[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_27 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_43[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_61 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_43[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_44 (TFOp  (None, 38)          0           ['tf.nn.relu_34[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_34[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_35 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_27[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_62 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_61[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_89 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_34[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_44[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_45 (TFOp  (None, 1186)        0           ['tf.nn.relu_35[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_35[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_98 (TFOpLambda  (None, 1186)        0           ['tf.linalg.matmul_62[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_64 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_89[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_28 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_45[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_99 (TFOpLambda  (None, 1186)        0           ['tf.math.truediv_98[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_101 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_64[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_90 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_35[0][0]',          \n",
      " a)                                                               'tf.math.negative_28[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_44 (TFOpL  (None, 1186)        0           ['tf.math.truediv_99[0][0]',     \n",
      " ambda)                                                           'tf.math.truediv_101[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_103 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_90[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_45 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_44[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_103[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_88 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_43[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_91 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_45[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_92 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_88[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_93 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_91[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_36 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_92[0][0]',    \n",
      " a)                                                               'tf.math.multiply_93[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_46 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_43[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_36[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_67 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_46[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_37 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_67[0][0]',    \n",
      " a)                                                               'tf.math.truediv_96[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_36 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_37[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_29 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_46[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_65 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_46[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_47 (TFOp  (None, 38)          0           ['tf.nn.relu_36[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_36[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_37 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_29[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_66 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_65[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_94 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_36[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_47[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_48 (TFOp  (None, 1186)        0           ['tf.nn.relu_37[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_37[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_105 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_66[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_68 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_94[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_30 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_48[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_106 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_105[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_108 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_68[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_95 (TFOpLambd  (None, 1186)        0           ['tf.nn.relu_37[0][0]',          \n",
      " a)                                                               'tf.math.negative_30[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_47 (TFOpL  (None, 1186)        0           ['tf.math.truediv_106[0][0]',    \n",
      " ambda)                                                           'tf.math.truediv_108[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_110 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_95[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_48 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_47[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_110[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_96 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_48[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_97 (TFOpLambd  (None, 1186)        0           ['tf.math.subtract_36[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_98 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_96[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_38 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_97[0][0]',    \n",
      " a)                                                               'tf.math.multiply_98[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_49 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_46[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_38[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_71 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_49[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_39 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_71[0][0]',    \n",
      " a)                                                               'tf.math.truediv_96[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_38 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_39[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_31 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_49[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_69 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_49[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_50 (TFOp  (None, 38)          0           ['tf.nn.relu_38[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_38[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_39 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_31[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_70 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_69[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_99 (TFOpLambd  (None, 38)          0           ['tf.nn.relu_38[0][0]',          \n",
      " a)                                                               'tf.math.divide_no_nan_50[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_51 (TFOp  (None, 1186)        0           ['tf.nn.relu_39[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_39[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_112 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_70[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_72 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_99[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_32 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_51[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_113 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_112[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_115 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_72[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_100 (TFOpLamb  (None, 1186)        0           ['tf.nn.relu_39[0][0]',          \n",
      " da)                                                              'tf.math.negative_32[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_50 (TFOpL  (None, 1186)        0           ['tf.math.truediv_113[0][0]',    \n",
      " ambda)                                                           'tf.math.truediv_115[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_117 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_100[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_51 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_50[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_117[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_101 (TFOpLamb  (None, 1186)        0           ['tf.__operators__.add_51[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_102 (TFOpLamb  (None, 1186)        0           ['tf.math.subtract_38[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_103 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_101[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_40 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_102[0][0]',   \n",
      " a)                                                               'tf.math.multiply_103[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_52 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_49[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_40[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_75 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_52[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_41 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_75[0][0]',    \n",
      " a)                                                               'tf.math.truediv_96[0][0]']     \n",
      "                                                                                                  \n",
      " tf.nn.relu_40 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_41[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_33 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_52[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_73 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_52[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_58 (TFOp  (None, 38)          0           ['tf.nn.relu_40[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_40[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_41 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_33[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_74 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_73[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_104 (TFOpLamb  (None, 38)          0           ['tf.nn.relu_40[0][0]',          \n",
      " da)                                                              'tf.math.divide_no_nan_58[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_61 (TFOp  (None, 1186)        0           ['tf.nn.relu_41[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_41[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_119 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_74[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_76 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_104[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.negative_34 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_61[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_120 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_119[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_122 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_76[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_105 (TFOpLamb  (None, 1186)        0           ['tf.nn.relu_41[0][0]',          \n",
      " da)                                                              'tf.math.negative_34[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_53 (TFOpL  (None, 1186)        0           ['tf.math.truediv_120[0][0]',    \n",
      " ambda)                                                           'tf.math.truediv_122[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_124 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_105[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_54 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_53[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_124[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_106 (TFOpLamb  (None, 1186)        0           ['tf.__operators__.add_54[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_107 (TFOpLamb  (None, 1186)        0           ['tf.math.subtract_40[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_132 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_106[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_42 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_107[0][0]',   \n",
      " a)                                                               'tf.math.multiply_132[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_55 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_52[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_42[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_79 (TFOpLambd  (None, 38)          0           ['tf.__operators__.add_55[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract_43 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_79[0][0]',    \n",
      " a)                                                               'tf.math.truediv_96[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.negative_35 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_55[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_78 (TFOpLambd  (None, 2084)        0           ['tf.__operators__.add_55[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.nn.relu_42 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_43[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_43 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_35[0][0]']    \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_57 (TFOpLamb  (None, 1)           0           ['tf.linalg.matmul_78[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_58 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_42[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_59 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_43[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_77 (TFOpLambd  (None, 1)           0           ['tf.__operators__.add_55[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_127 (TFOpLambd  (None, 1)           0           ['tf.compat.v1.norm_57[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_129 (TFOpLambd  (None, 1)           0           ['tf.compat.v1.norm_58[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_131 (TFOpLambd  (None, 1)           0           ['tf.compat.v1.norm_59[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 2376)         0           ['tf.linalg.matmul_77[0][0]',    \n",
      "                                                                  'tf.math.truediv_127[0][0]',    \n",
      "                                                                  'tf.math.truediv_129[0][0]',    \n",
      "                                                                  'tf.math.truediv_131[0][0]',    \n",
      "                                                                  'tf.__operators__.add_55[0][0]',\n",
      "                                                                  'tf.__operators__.add_43[0][0]']\n",
      "                                                                                                  \n",
      " lambda_23 (Lambda)             (None, 1)            0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_24 (Lambda)             (None, 1)            0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_25 (Lambda)             (None, 1)            0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_26 (Lambda)             (None, 1)            0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_27 (Lambda)             (None, 1186)         0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 1228)         0           ['lambda_23[0][0]',              \n",
      "                                                                  'lambda_24[0][0]',              \n",
      "                                                                  'lambda_25[0][0]',              \n",
      "                                                                  'lambda_26[0][0]',              \n",
      "                                                                  'lambda_27[0][0]',              \n",
      "                                                                  'tf.math.truediv_96[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 624,196\n",
      "Trainable params: 10,510\n",
      "Non-trainable params: 613,686\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "nbr parameters: 624196\n",
      "Epoch 1/2000\n",
      "22/22 [==============================] - 2s 46ms/step - loss: 0.0360 - my_r2: -11.9480 - val_loss: 0.0349 - val_my_r2: -9.7637\n",
      "Epoch 2/2000\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.0322 - my_r2: -6.1583 - val_loss: 0.0308 - val_my_r2: -8.5021\n",
      "Epoch 3/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.0296 - my_r2: -5.8383 - val_loss: 0.0261 - val_my_r2: -7.0787\n",
      "Epoch 4/2000\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.0242 - my_r2: -9.6621 - val_loss: 0.0216 - val_my_r2: -5.6886\n",
      "Epoch 5/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0193 - my_r2: -5.1239 - val_loss: 0.0178 - val_my_r2: -4.5168\n",
      "Epoch 6/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0169 - my_r2: -3.2126 - val_loss: 0.0147 - val_my_r2: -3.5728\n",
      "Epoch 7/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0137 - my_r2: -3.0445 - val_loss: 0.0121 - val_my_r2: -2.7834\n",
      "Epoch 8/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0111 - my_r2: -2.1713 - val_loss: 0.0100 - val_my_r2: -2.1442\n",
      "Epoch 9/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0098 - my_r2: -2.0002 - val_loss: 0.0086 - val_my_r2: -1.7002\n",
      "Epoch 10/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0083 - my_r2: -1.1864 - val_loss: 0.0073 - val_my_r2: -1.3109\n",
      "Epoch 11/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0072 - my_r2: -0.9189 - val_loss: 0.0064 - val_my_r2: -1.0083\n",
      "Epoch 12/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0064 - my_r2: -0.6334 - val_loss: 0.0056 - val_my_r2: -0.7749\n",
      "Epoch 13/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0063 - my_r2: -0.5197 - val_loss: 0.0050 - val_my_r2: -0.5841\n",
      "Epoch 14/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0053 - my_r2: -0.4152 - val_loss: 0.0045 - val_my_r2: -0.4283\n",
      "Epoch 15/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0045 - my_r2: -0.3645 - val_loss: 0.0042 - val_my_r2: -0.3172\n",
      "Epoch 16/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0043 - my_r2: -0.3492 - val_loss: 0.0039 - val_my_r2: -0.2218\n",
      "Epoch 17/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0039 - my_r2: 0.1132 - val_loss: 0.0036 - val_my_r2: -0.1402\n",
      "Epoch 18/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0037 - my_r2: 0.0137 - val_loss: 0.0034 - val_my_r2: -0.0746\n",
      "Epoch 19/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0036 - my_r2: -0.1106 - val_loss: 0.0032 - val_my_r2: -0.0214\n",
      "Epoch 20/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0034 - my_r2: -0.1086 - val_loss: 0.0030 - val_my_r2: 0.0330\n",
      "Epoch 21/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0030 - my_r2: 0.0923 - val_loss: 0.0029 - val_my_r2: 0.0764\n",
      "Epoch 22/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0033 - my_r2: -1.0930 - val_loss: 0.0028 - val_my_r2: 0.1199\n",
      "Epoch 23/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 0.0029 - my_r2: -0.0468 - val_loss: 0.0026 - val_my_r2: 0.1554\n",
      "Epoch 24/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0028 - my_r2: 0.3449 - val_loss: 0.0025 - val_my_r2: 0.1872\n",
      "Epoch 25/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0027 - my_r2: 0.5106 - val_loss: 0.0024 - val_my_r2: 0.2158\n",
      "Epoch 26/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0026 - my_r2: 0.3653 - val_loss: 0.0023 - val_my_r2: 0.2438\n",
      "Epoch 27/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0029 - my_r2: -0.5515 - val_loss: 0.0023 - val_my_r2: 0.2689\n",
      "Epoch 28/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0023 - my_r2: 0.1699 - val_loss: 0.0022 - val_my_r2: 0.2946\n",
      "Epoch 29/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0025 - my_r2: 0.3218 - val_loss: 0.0021 - val_my_r2: 0.3169\n",
      "Epoch 30/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0023 - my_r2: -0.0865 - val_loss: 0.0020 - val_my_r2: 0.3386\n",
      "Epoch 31/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0024 - my_r2: 0.1976 - val_loss: 0.0020 - val_my_r2: 0.3612\n",
      "Epoch 32/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0024 - my_r2: 0.3444 - val_loss: 0.0019 - val_my_r2: 0.3800\n",
      "Epoch 33/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0021 - my_r2: 0.4491 - val_loss: 0.0018 - val_my_r2: 0.3941\n",
      "Epoch 34/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0021 - my_r2: 0.3577 - val_loss: 0.0018 - val_my_r2: 0.4111\n",
      "Epoch 35/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0021 - my_r2: 0.3684 - val_loss: 0.0017 - val_my_r2: 0.4276\n",
      "Epoch 36/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0022 - my_r2: 0.4388 - val_loss: 0.0017 - val_my_r2: 0.4424\n",
      "Epoch 37/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0019 - my_r2: -0.0080 - val_loss: 0.0016 - val_my_r2: 0.4578\n",
      "Epoch 38/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: 0.5364 - val_loss: 0.0016 - val_my_r2: 0.4730\n",
      "Epoch 39/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: 0.3515 - val_loss: 0.0015 - val_my_r2: 0.4861\n",
      "Epoch 40/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0015 - my_r2: 0.6250 - val_loss: 0.0015 - val_my_r2: 0.4968\n",
      "Epoch 41/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0016 - my_r2: 0.5954 - val_loss: 0.0015 - val_my_r2: 0.5082\n",
      "Epoch 42/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0016 - my_r2: 0.6525 - val_loss: 0.0014 - val_my_r2: 0.5176\n",
      "Epoch 43/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0018 - my_r2: 0.4707 - val_loss: 0.0014 - val_my_r2: 0.5299\n",
      "Epoch 44/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0018 - my_r2: 0.5636 - val_loss: 0.0014 - val_my_r2: 0.5396\n",
      "Epoch 45/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.3511 - val_loss: 0.0014 - val_my_r2: 0.5504\n",
      "Epoch 46/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.7393 - val_loss: 0.0013 - val_my_r2: 0.5587\n",
      "Epoch 47/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6871 - val_loss: 0.0013 - val_my_r2: 0.5673\n",
      "Epoch 48/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0016 - my_r2: 0.5736 - val_loss: 0.0013 - val_my_r2: 0.5738\n",
      "Epoch 49/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.0991 - val_loss: 0.0012 - val_my_r2: 0.5813\n",
      "Epoch 50/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.6221 - val_loss: 0.0012 - val_my_r2: 0.5886\n",
      "Epoch 51/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0016 - my_r2: 0.6433 - val_loss: 0.0012 - val_my_r2: 0.5964\n",
      "Epoch 52/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.6355 - val_loss: 0.0012 - val_my_r2: 0.6040\n",
      "Epoch 53/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.6965 - val_loss: 0.0012 - val_my_r2: 0.6098\n",
      "Epoch 54/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.6317 - val_loss: 0.0011 - val_my_r2: 0.6161\n",
      "Epoch 55/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.3357 - val_loss: 0.0011 - val_my_r2: 0.6235\n",
      "Epoch 56/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0015 - my_r2: 0.6220 - val_loss: 0.0011 - val_my_r2: 0.6291\n",
      "Epoch 57/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.1322 - val_loss: 0.0011 - val_my_r2: 0.6368\n",
      "Epoch 58/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6461 - val_loss: 0.0011 - val_my_r2: 0.6447\n",
      "Epoch 59/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.6255 - val_loss: 0.0010 - val_my_r2: 0.6517\n",
      "Epoch 60/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.6163 - val_loss: 0.0010 - val_my_r2: 0.6565\n",
      "Epoch 61/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0015 - my_r2: 0.5536 - val_loss: 0.0010 - val_my_r2: 0.6622\n",
      "Epoch 62/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 0.0013 - my_r2: 0.6664 - val_loss: 9.8453e-04 - val_my_r2: 0.6686\n",
      "Epoch 63/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.5209 - val_loss: 9.7079e-04 - val_my_r2: 0.6731\n",
      "Epoch 64/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6694 - val_loss: 9.5500e-04 - val_my_r2: 0.6777\n",
      "Epoch 65/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.6641 - val_loss: 9.3963e-04 - val_my_r2: 0.6831\n",
      "Epoch 66/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.7282 - val_loss: 9.2847e-04 - val_my_r2: 0.6875\n",
      "Epoch 67/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.4662 - val_loss: 9.1367e-04 - val_my_r2: 0.6905\n",
      "Epoch 68/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.6912 - val_loss: 8.9854e-04 - val_my_r2: 0.6937\n",
      "Epoch 69/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.4671 - val_loss: 8.8557e-04 - val_my_r2: 0.6971\n",
      "Epoch 70/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.6049 - val_loss: 8.7409e-04 - val_my_r2: 0.6999\n",
      "Epoch 71/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7307 - val_loss: 8.6289e-04 - val_my_r2: 0.7034\n",
      "Epoch 72/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.4984 - val_loss: 8.5297e-04 - val_my_r2: 0.7059\n",
      "Epoch 73/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0012 - my_r2: 0.5878 - val_loss: 8.4049e-04 - val_my_r2: 0.7111\n",
      "Epoch 74/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.6451 - val_loss: 8.2800e-04 - val_my_r2: 0.7146\n",
      "Epoch 75/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.3001e-04 - my_r2: 0.7353 - val_loss: 8.1787e-04 - val_my_r2: 0.7157\n",
      "Epoch 76/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7228 - val_loss: 8.0738e-04 - val_my_r2: 0.7190\n",
      "Epoch 77/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.6909 - val_loss: 7.9625e-04 - val_my_r2: 0.7251\n",
      "Epoch 78/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.4511e-04 - my_r2: 0.6853 - val_loss: 7.8578e-04 - val_my_r2: 0.7288\n",
      "Epoch 79/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.7969e-04 - my_r2: 0.7562 - val_loss: 7.7783e-04 - val_my_r2: 0.7335\n",
      "Epoch 80/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.1972e-04 - my_r2: 0.7839 - val_loss: 7.6837e-04 - val_my_r2: 0.7364\n",
      "Epoch 81/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.5749e-04 - my_r2: 0.7133 - val_loss: 7.6015e-04 - val_my_r2: 0.7375\n",
      "Epoch 82/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7153 - val_loss: 7.5003e-04 - val_my_r2: 0.7438\n",
      "Epoch 83/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.4787e-04 - my_r2: 0.7764 - val_loss: 7.4044e-04 - val_my_r2: 0.7463\n",
      "Epoch 84/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7177 - val_loss: 7.3185e-04 - val_my_r2: 0.7493\n",
      "Epoch 85/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7178 - val_loss: 7.2237e-04 - val_my_r2: 0.7517\n",
      "Epoch 86/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.2547e-04 - my_r2: 0.7790 - val_loss: 7.1577e-04 - val_my_r2: 0.7566\n",
      "Epoch 87/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.9996e-04 - my_r2: 0.7572 - val_loss: 7.0804e-04 - val_my_r2: 0.7584\n",
      "Epoch 88/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7442 - val_loss: 6.9742e-04 - val_my_r2: 0.7604\n",
      "Epoch 89/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8222e-04 - my_r2: 0.6993 - val_loss: 6.9161e-04 - val_my_r2: 0.7634\n",
      "Epoch 90/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7278 - val_loss: 6.8348e-04 - val_my_r2: 0.7654\n",
      "Epoch 91/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0010 - my_r2: 0.7671 - val_loss: 6.7547e-04 - val_my_r2: 0.7694\n",
      "Epoch 92/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.8853e-04 - my_r2: 0.7998 - val_loss: 6.6852e-04 - val_my_r2: 0.7709\n",
      "Epoch 93/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.9553e-04 - my_r2: 0.5575 - val_loss: 6.5886e-04 - val_my_r2: 0.7749\n",
      "Epoch 94/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.4665e-04 - my_r2: 0.8041 - val_loss: 6.5250e-04 - val_my_r2: 0.7773\n",
      "Epoch 95/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.4798e-04 - my_r2: 0.6831 - val_loss: 6.4584e-04 - val_my_r2: 0.7801\n",
      "Epoch 96/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.4798e-04 - my_r2: 0.7979 - val_loss: 6.3798e-04 - val_my_r2: 0.7807\n",
      "Epoch 97/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.1894e-04 - my_r2: 0.6746 - val_loss: 6.3286e-04 - val_my_r2: 0.7837\n",
      "Epoch 98/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.7974e-04 - my_r2: 0.7013 - val_loss: 6.2609e-04 - val_my_r2: 0.7855\n",
      "Epoch 99/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.6092e-04 - my_r2: 0.7619 - val_loss: 6.1673e-04 - val_my_r2: 0.7850\n",
      "Epoch 100/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.4995 - val_loss: 6.1034e-04 - val_my_r2: 0.7862\n",
      "Epoch 101/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.2666e-04 - my_r2: 0.7672 - val_loss: 6.0442e-04 - val_my_r2: 0.7864\n",
      "Epoch 102/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.0476e-04 - my_r2: 0.8136 - val_loss: 5.9861e-04 - val_my_r2: 0.7863\n",
      "Epoch 103/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.6890e-04 - my_r2: 0.6765 - val_loss: 5.9079e-04 - val_my_r2: 0.7903\n",
      "Epoch 104/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.8392e-04 - my_r2: 0.8238 - val_loss: 5.8389e-04 - val_my_r2: 0.7948\n",
      "Epoch 105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.2424e-04 - my_r2: 0.7505 - val_loss: 5.7778e-04 - val_my_r2: 0.7982\n",
      "Epoch 106/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.0114e-04 - my_r2: 0.7302 - val_loss: 5.7218e-04 - val_my_r2: 0.8010\n",
      "Epoch 107/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.7034e-04 - my_r2: 0.7891 - val_loss: 5.6791e-04 - val_my_r2: 0.7999\n",
      "Epoch 108/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.8505e-04 - my_r2: 0.8369 - val_loss: 5.6282e-04 - val_my_r2: 0.8037\n",
      "Epoch 109/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3849e-04 - my_r2: 0.7564 - val_loss: 5.5754e-04 - val_my_r2: 0.8059\n",
      "Epoch 110/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.7929e-04 - my_r2: 0.7963 - val_loss: 5.5184e-04 - val_my_r2: 0.8075\n",
      "Epoch 111/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.0601e-04 - my_r2: 0.7779 - val_loss: 5.4592e-04 - val_my_r2: 0.8104\n",
      "Epoch 112/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8230e-04 - my_r2: 0.8186 - val_loss: 5.4065e-04 - val_my_r2: 0.8121\n",
      "Epoch 113/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.0898e-04 - my_r2: 0.7561 - val_loss: 5.3785e-04 - val_my_r2: 0.8117\n",
      "Epoch 114/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.2813e-04 - my_r2: 0.8229 - val_loss: 5.3413e-04 - val_my_r2: 0.8154\n",
      "Epoch 115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0759e-04 - my_r2: 0.7890 - val_loss: 5.2912e-04 - val_my_r2: 0.8166\n",
      "Epoch 116/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0954e-04 - my_r2: 0.7046 - val_loss: 5.2330e-04 - val_my_r2: 0.8186\n",
      "Epoch 117/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.0162e-04 - my_r2: 0.7508 - val_loss: 5.1803e-04 - val_my_r2: 0.8189\n",
      "Epoch 118/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3247e-04 - my_r2: 0.8463 - val_loss: 5.1416e-04 - val_my_r2: 0.8207\n",
      "Epoch 119/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.9346e-04 - my_r2: 0.8255 - val_loss: 5.0976e-04 - val_my_r2: 0.8214\n",
      "Epoch 120/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6658e-04 - my_r2: 0.8204 - val_loss: 5.0480e-04 - val_my_r2: 0.8228\n",
      "Epoch 121/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1834e-04 - my_r2: 0.7850 - val_loss: 5.0192e-04 - val_my_r2: 0.8244\n",
      "Epoch 122/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.4172e-04 - my_r2: -0.9790 - val_loss: 4.9935e-04 - val_my_r2: 0.8224\n",
      "Epoch 123/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.8624e-04 - my_r2: 0.8412 - val_loss: 4.9375e-04 - val_my_r2: 0.8267\n",
      "Epoch 124/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7405e-04 - my_r2: 0.7947 - val_loss: 4.8897e-04 - val_my_r2: 0.8281\n",
      "Epoch 125/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6316e-04 - my_r2: 0.8534 - val_loss: 4.8274e-04 - val_my_r2: 0.8306\n",
      "Epoch 126/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.9333e-04 - my_r2: 0.8599 - val_loss: 4.7839e-04 - val_my_r2: 0.8320\n",
      "Epoch 127/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.1933e-04 - my_r2: 0.7751 - val_loss: 4.7310e-04 - val_my_r2: 0.8349\n",
      "Epoch 128/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.1356e-04 - my_r2: 0.8401 - val_loss: 4.7072e-04 - val_my_r2: 0.8362\n",
      "Epoch 129/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.8062e-04 - my_r2: 0.8028 - val_loss: 4.6815e-04 - val_my_r2: 0.8358\n",
      "Epoch 130/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.7141e-04 - my_r2: 0.6591 - val_loss: 4.6210e-04 - val_my_r2: 0.8376\n",
      "Epoch 131/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7486e-04 - my_r2: 0.8385 - val_loss: 4.5662e-04 - val_my_r2: 0.8390\n",
      "Epoch 132/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.3390e-04 - my_r2: 0.7546 - val_loss: 4.5308e-04 - val_my_r2: 0.8410\n",
      "Epoch 133/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.7156e-04 - my_r2: 0.8098 - val_loss: 4.4926e-04 - val_my_r2: 0.8412\n",
      "Epoch 134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.5888e-04 - my_r2: 0.8522 - val_loss: 4.4492e-04 - val_my_r2: 0.8417\n",
      "Epoch 135/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.1057e-04 - my_r2: 0.8118 - val_loss: 4.4235e-04 - val_my_r2: 0.8428\n",
      "Epoch 136/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0097e-04 - my_r2: 0.8605 - val_loss: 4.4057e-04 - val_my_r2: 0.8442\n",
      "Epoch 137/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8828e-04 - my_r2: 0.8303 - val_loss: 4.3798e-04 - val_my_r2: 0.8450\n",
      "Epoch 138/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7426e-04 - my_r2: 0.8531 - val_loss: 4.3328e-04 - val_my_r2: 0.8474\n",
      "Epoch 139/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.0668e-04 - my_r2: 0.8230 - val_loss: 4.2901e-04 - val_my_r2: 0.8493\n",
      "Epoch 140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8198e-04 - my_r2: 0.8552 - val_loss: 4.2515e-04 - val_my_r2: 0.8515\n",
      "Epoch 141/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.7927e-04 - my_r2: 0.7417 - val_loss: 4.2191e-04 - val_my_r2: 0.8544\n",
      "Epoch 142/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 6.5468e-04 - my_r2: 0.8564 - val_loss: 4.1865e-04 - val_my_r2: 0.8554\n",
      "Epoch 143/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.6641e-04 - my_r2: 0.8236 - val_loss: 4.1578e-04 - val_my_r2: 0.8568\n",
      "Epoch 144/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.8893e-04 - my_r2: 0.8215 - val_loss: 4.1031e-04 - val_my_r2: 0.8582\n",
      "Epoch 145/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.7334e-04 - my_r2: 0.8090 - val_loss: 4.0882e-04 - val_my_r2: 0.8589\n",
      "Epoch 146/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.2230e-04 - my_r2: 0.8507 - val_loss: 4.0491e-04 - val_my_r2: 0.8608\n",
      "Epoch 147/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0504e-04 - my_r2: 0.6427 - val_loss: 4.0190e-04 - val_my_r2: 0.8621\n",
      "Epoch 148/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.9379e-04 - my_r2: 0.6150 - val_loss: 3.9797e-04 - val_my_r2: 0.8629\n",
      "Epoch 149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.5328e-04 - my_r2: 0.6758 - val_loss: 3.9551e-04 - val_my_r2: 0.8623\n",
      "Epoch 150/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9334e-04 - my_r2: 0.8307 - val_loss: 3.9216e-04 - val_my_r2: 0.8627\n",
      "Epoch 151/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.1448e-04 - my_r2: 0.7691 - val_loss: 3.8955e-04 - val_my_r2: 0.8643\n",
      "Epoch 152/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7119e-04 - my_r2: 0.8485 - val_loss: 3.8734e-04 - val_my_r2: 0.8644\n",
      "Epoch 153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.2894e-04 - my_r2: 0.7824 - val_loss: 3.8689e-04 - val_my_r2: 0.8632\n",
      "Epoch 154/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.0271e-04 - my_r2: 0.7456 - val_loss: 3.8659e-04 - val_my_r2: 0.8647\n",
      "Epoch 155/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3468e-04 - my_r2: 0.8257 - val_loss: 3.8393e-04 - val_my_r2: 0.8672\n",
      "Epoch 156/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.0050e-04 - my_r2: 0.8198 - val_loss: 3.7797e-04 - val_my_r2: 0.8685\n",
      "Epoch 157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8591e-04 - my_r2: 0.8325 - val_loss: 3.7411e-04 - val_my_r2: 0.8686\n",
      "Epoch 158/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.5187e-04 - my_r2: 0.8397 - val_loss: 3.7092e-04 - val_my_r2: 0.8700\n",
      "Epoch 159/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2095e-04 - my_r2: 0.8653 - val_loss: 3.7150e-04 - val_my_r2: 0.8704\n",
      "Epoch 160/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.3274e-04 - my_r2: 0.8124 - val_loss: 3.7034e-04 - val_my_r2: 0.8708\n",
      "Epoch 161/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.0868e-04 - my_r2: 0.8474 - val_loss: 3.6254e-04 - val_my_r2: 0.8728\n",
      "Epoch 162/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.7111e-04 - my_r2: 0.7116 - val_loss: 3.5939e-04 - val_my_r2: 0.8741\n",
      "Epoch 163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.1692e-04 - my_r2: 0.8747 - val_loss: 3.5705e-04 - val_my_r2: 0.8731\n",
      "Epoch 164/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3716e-04 - my_r2: 0.8448 - val_loss: 3.5319e-04 - val_my_r2: 0.8756\n",
      "Epoch 165/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8906e-04 - my_r2: 0.8484 - val_loss: 3.5148e-04 - val_my_r2: 0.8770\n",
      "Epoch 166/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1290e-04 - my_r2: 0.8640 - val_loss: 3.4801e-04 - val_my_r2: 0.8766\n",
      "Epoch 167/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.5856e-04 - my_r2: 0.8554 - val_loss: 3.4570e-04 - val_my_r2: 0.8772\n",
      "Epoch 168/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.4434e-04 - my_r2: 0.8698 - val_loss: 3.4436e-04 - val_my_r2: 0.8781\n",
      "Epoch 169/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 6.3584e-04 - my_r2: 0.8288 - val_loss: 3.4313e-04 - val_my_r2: 0.8788\n",
      "Epoch 170/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6362e-04 - my_r2: 0.8134 - val_loss: 3.3855e-04 - val_my_r2: 0.8802\n",
      "Epoch 171/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.7318e-04 - my_r2: 0.8159 - val_loss: 3.3579e-04 - val_my_r2: 0.8807\n",
      "Epoch 172/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0649e-04 - my_r2: 0.8359 - val_loss: 3.3312e-04 - val_my_r2: 0.8817\n",
      "Epoch 173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5914e-04 - my_r2: 0.8400 - val_loss: 3.3110e-04 - val_my_r2: 0.8819\n",
      "Epoch 174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5668e-04 - my_r2: 0.8617 - val_loss: 3.3002e-04 - val_my_r2: 0.8825\n",
      "Epoch 175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6106e-04 - my_r2: 0.8434 - val_loss: 3.2942e-04 - val_my_r2: 0.8837\n",
      "Epoch 176/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7315e-04 - my_r2: 0.8239 - val_loss: 3.3050e-04 - val_my_r2: 0.8841\n",
      "Epoch 177/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.4739e-04 - my_r2: 0.7370 - val_loss: 3.2926e-04 - val_my_r2: 0.8846\n",
      "Epoch 178/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8892e-04 - my_r2: 0.8229 - val_loss: 3.2295e-04 - val_my_r2: 0.8864\n",
      "Epoch 179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6765e-04 - my_r2: 0.8239 - val_loss: 3.2045e-04 - val_my_r2: 0.8877\n",
      "Epoch 180/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2082e-04 - my_r2: 0.7827 - val_loss: 3.1703e-04 - val_my_r2: 0.8896\n",
      "Epoch 181/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6834e-04 - my_r2: 0.8797 - val_loss: 3.1670e-04 - val_my_r2: 0.8897\n",
      "Epoch 182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1264e-04 - my_r2: 0.6392 - val_loss: 3.1739e-04 - val_my_r2: 0.8897\n",
      "Epoch 183/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8441e-04 - my_r2: 0.7959 - val_loss: 3.1198e-04 - val_my_r2: 0.8905\n",
      "Epoch 184/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6553e-04 - my_r2: 0.8586 - val_loss: 3.1180e-04 - val_my_r2: 0.8902\n",
      "Epoch 185/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7017e-04 - my_r2: 0.8839 - val_loss: 3.1112e-04 - val_my_r2: 0.8921\n",
      "Epoch 186/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0384e-04 - my_r2: 0.8962 - val_loss: 3.0958e-04 - val_my_r2: 0.8929\n",
      "Epoch 187/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9132e-04 - my_r2: 0.8349 - val_loss: 3.0607e-04 - val_my_r2: 0.8944\n",
      "Epoch 188/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2350e-04 - my_r2: 0.8129 - val_loss: 3.0288e-04 - val_my_r2: 0.8950\n",
      "Epoch 189/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7757e-04 - my_r2: 0.8911 - val_loss: 3.0209e-04 - val_my_r2: 0.8957\n",
      "Epoch 190/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8893e-04 - my_r2: 0.8389 - val_loss: 3.0060e-04 - val_my_r2: 0.8942\n",
      "Epoch 191/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8806e-04 - my_r2: 0.7828 - val_loss: 2.9709e-04 - val_my_r2: 0.8977\n",
      "Epoch 192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8254e-04 - my_r2: 0.8651 - val_loss: 2.9565e-04 - val_my_r2: 0.8964\n",
      "Epoch 193/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6094e-04 - my_r2: 0.7717 - val_loss: 2.9244e-04 - val_my_r2: 0.8980\n",
      "Epoch 194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7593e-04 - my_r2: 0.8334 - val_loss: 2.9013e-04 - val_my_r2: 0.8991\n",
      "Epoch 195/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1062e-04 - my_r2: 0.8887 - val_loss: 2.8976e-04 - val_my_r2: 0.8988\n",
      "Epoch 196/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0925e-04 - my_r2: 0.8821 - val_loss: 2.8873e-04 - val_my_r2: 0.8980\n",
      "Epoch 197/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.2299e-04 - my_r2: 0.7605 - val_loss: 2.8936e-04 - val_my_r2: 0.8977\n",
      "Epoch 198/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8063e-04 - my_r2: -0.0342 - val_loss: 2.8767e-04 - val_my_r2: 0.8991\n",
      "Epoch 199/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5164e-04 - my_r2: 0.8649 - val_loss: 2.8479e-04 - val_my_r2: 0.8987\n",
      "Epoch 200/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9587e-04 - my_r2: 0.9100 - val_loss: 2.8101e-04 - val_my_r2: 0.9002\n",
      "Epoch 201/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5947e-04 - my_r2: 0.9110 - val_loss: 2.7850e-04 - val_my_r2: 0.9011\n",
      "Epoch 202/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2641e-04 - my_r2: 0.8287 - val_loss: 2.7829e-04 - val_my_r2: 0.9014\n",
      "Epoch 203/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9453e-04 - my_r2: 0.8057 - val_loss: 2.7696e-04 - val_my_r2: 0.9037\n",
      "Epoch 204/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6234e-04 - my_r2: 0.8548 - val_loss: 2.7629e-04 - val_my_r2: 0.9060\n",
      "Epoch 205/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0332e-04 - my_r2: 0.8709 - val_loss: 2.7411e-04 - val_my_r2: 0.9073\n",
      "Epoch 206/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0004e-04 - my_r2: 0.8554 - val_loss: 2.7373e-04 - val_my_r2: 0.9063\n",
      "Epoch 207/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8672e-04 - my_r2: 0.8769 - val_loss: 2.6912e-04 - val_my_r2: 0.9072\n",
      "Epoch 208/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1062e-04 - my_r2: 0.8238 - val_loss: 2.6712e-04 - val_my_r2: 0.9075\n",
      "Epoch 209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4841e-04 - my_r2: 0.8749 - val_loss: 2.6409e-04 - val_my_r2: 0.9082\n",
      "Epoch 210/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5682e-04 - my_r2: 0.8658 - val_loss: 2.6198e-04 - val_my_r2: 0.9083\n",
      "Epoch 211/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4148e-04 - my_r2: 0.8917 - val_loss: 2.5979e-04 - val_my_r2: 0.9090\n",
      "Epoch 212/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5772e-04 - my_r2: 0.9041 - val_loss: 2.6005e-04 - val_my_r2: 0.9093\n",
      "Epoch 213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1731e-04 - my_r2: 0.8606 - val_loss: 2.5628e-04 - val_my_r2: 0.9106\n",
      "Epoch 214/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 5.3740e-04 - my_r2: 0.8655 - val_loss: 2.5411e-04 - val_my_r2: 0.9115\n",
      "Epoch 215/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2873e-04 - my_r2: 0.8556 - val_loss: 2.5238e-04 - val_my_r2: 0.9127\n",
      "Epoch 216/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2721e-04 - my_r2: 0.8040 - val_loss: 2.5215e-04 - val_my_r2: 0.9128\n",
      "Epoch 217/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7053e-04 - my_r2: 0.8682 - val_loss: 2.5083e-04 - val_my_r2: 0.9113\n",
      "Epoch 218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3860e-04 - my_r2: 0.8275 - val_loss: 2.5099e-04 - val_my_r2: 0.9111\n",
      "Epoch 219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4096e-04 - my_r2: 0.8671 - val_loss: 2.4980e-04 - val_my_r2: 0.9112\n",
      "Epoch 220/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3203e-04 - my_r2: 0.7730 - val_loss: 2.4863e-04 - val_my_r2: 0.9128\n",
      "Epoch 221/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4394e-04 - my_r2: 0.7590 - val_loss: 2.4526e-04 - val_my_r2: 0.9123\n",
      "Epoch 222/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0310e-04 - my_r2: 0.8108 - val_loss: 2.4588e-04 - val_my_r2: 0.9131\n",
      "Epoch 223/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2415e-04 - my_r2: 0.8918 - val_loss: 2.4272e-04 - val_my_r2: 0.9144\n",
      "Epoch 224/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3425e-04 - my_r2: 0.8687 - val_loss: 2.3981e-04 - val_my_r2: 0.9161\n",
      "Epoch 225/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8901e-04 - my_r2: 0.8590 - val_loss: 2.3889e-04 - val_my_r2: 0.9177\n",
      "Epoch 226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7838e-04 - my_r2: 0.8823 - val_loss: 2.3783e-04 - val_my_r2: 0.9185\n",
      "Epoch 227/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3755e-04 - my_r2: 0.8795 - val_loss: 2.3916e-04 - val_my_r2: 0.9179\n",
      "Epoch 228/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5405e-04 - my_r2: 0.8850 - val_loss: 2.3643e-04 - val_my_r2: 0.9193\n",
      "Epoch 229/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2879e-04 - my_r2: 0.8831 - val_loss: 2.3353e-04 - val_my_r2: 0.9199\n",
      "Epoch 230/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3927e-04 - my_r2: 0.8701 - val_loss: 2.3519e-04 - val_my_r2: 0.9192\n",
      "Epoch 231/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0741e-04 - my_r2: 0.8440 - val_loss: 2.3353e-04 - val_my_r2: 0.9182\n",
      "Epoch 232/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1763e-04 - my_r2: 0.9071 - val_loss: 2.3404e-04 - val_my_r2: 0.9174\n",
      "Epoch 233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8863e-04 - my_r2: 0.8839 - val_loss: 2.3542e-04 - val_my_r2: 0.9177\n",
      "Epoch 234/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8488e-04 - my_r2: 0.8727 - val_loss: 2.3400e-04 - val_my_r2: 0.9202\n",
      "Epoch 235/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5222e-04 - my_r2: 0.8656 - val_loss: 2.3380e-04 - val_my_r2: 0.9205\n",
      "Epoch 236/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0659e-04 - my_r2: 0.8742 - val_loss: 2.2440e-04 - val_my_r2: 0.9227\n",
      "Epoch 237/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8010e-04 - my_r2: 0.8472 - val_loss: 2.2225e-04 - val_my_r2: 0.9229\n",
      "Epoch 238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2935e-04 - my_r2: 0.8935 - val_loss: 2.2012e-04 - val_my_r2: 0.9244\n",
      "Epoch 239/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6499e-04 - my_r2: 0.8743 - val_loss: 2.2091e-04 - val_my_r2: 0.9244\n",
      "Epoch 240/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8519e-04 - my_r2: 0.8178 - val_loss: 2.2233e-04 - val_my_r2: 0.9253\n",
      "Epoch 241/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.4753e-04 - my_r2: 0.8180 - val_loss: 2.1764e-04 - val_my_r2: 0.9259\n",
      "Epoch 242/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8961e-04 - my_r2: 0.8156 - val_loss: 2.1715e-04 - val_my_r2: 0.9262\n",
      "Epoch 243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1695e-04 - my_r2: 0.8900 - val_loss: 2.1814e-04 - val_my_r2: 0.9261\n",
      "Epoch 244/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3309e-04 - my_r2: 0.8489 - val_loss: 2.1690e-04 - val_my_r2: 0.9259\n",
      "Epoch 245/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7902e-04 - my_r2: 0.8560 - val_loss: 2.1135e-04 - val_my_r2: 0.9275\n",
      "Epoch 246/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0276e-04 - my_r2: 0.8719 - val_loss: 2.1435e-04 - val_my_r2: 0.9272\n",
      "Epoch 247/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7906e-04 - my_r2: 0.8541 - val_loss: 2.1280e-04 - val_my_r2: 0.9274\n",
      "Epoch 248/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6579e-04 - my_r2: 0.8670 - val_loss: 2.0940e-04 - val_my_r2: 0.9284\n",
      "Epoch 249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9386e-04 - my_r2: 0.9122 - val_loss: 2.0681e-04 - val_my_r2: 0.9288\n",
      "Epoch 250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4835e-04 - my_r2: 0.9169 - val_loss: 2.0526e-04 - val_my_r2: 0.9297\n",
      "Epoch 251/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1370e-04 - my_r2: 0.8468 - val_loss: 2.0759e-04 - val_my_r2: 0.9297\n",
      "Epoch 252/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7496e-04 - my_r2: 0.7882 - val_loss: 2.0841e-04 - val_my_r2: 0.9295\n",
      "Epoch 253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1742e-04 - my_r2: 0.8879 - val_loss: 2.0510e-04 - val_my_r2: 0.9300\n",
      "Epoch 254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1166e-04 - my_r2: 0.8571 - val_loss: 2.0040e-04 - val_my_r2: 0.9310\n",
      "Epoch 255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2873e-04 - my_r2: 0.8158 - val_loss: 1.9838e-04 - val_my_r2: 0.9313\n",
      "Epoch 256/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5675e-04 - my_r2: 0.8761 - val_loss: 1.9939e-04 - val_my_r2: 0.9314\n",
      "Epoch 257/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0168e-04 - my_r2: 0.8850 - val_loss: 1.9713e-04 - val_my_r2: 0.9318\n",
      "Epoch 258/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0562e-04 - my_r2: 0.8997 - val_loss: 1.9708e-04 - val_my_r2: 0.9309\n",
      "Epoch 259/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7066e-04 - my_r2: 0.8671 - val_loss: 1.9523e-04 - val_my_r2: 0.9315\n",
      "Epoch 260/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9096e-04 - my_r2: 0.8790 - val_loss: 1.9352e-04 - val_my_r2: 0.9322\n",
      "Epoch 261/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8367e-04 - my_r2: 0.8822 - val_loss: 1.9442e-04 - val_my_r2: 0.9329\n",
      "Epoch 262/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4993e-04 - my_r2: 0.9007 - val_loss: 1.9341e-04 - val_my_r2: 0.9343\n",
      "Epoch 263/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4343e-04 - my_r2: 0.8955 - val_loss: 1.9469e-04 - val_my_r2: 0.9348\n",
      "Epoch 264/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8929e-04 - my_r2: 0.9182 - val_loss: 1.9388e-04 - val_my_r2: 0.9350\n",
      "Epoch 265/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0018e-04 - my_r2: 0.8257 - val_loss: 1.9213e-04 - val_my_r2: 0.9349\n",
      "Epoch 266/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5020e-04 - my_r2: 0.8989 - val_loss: 1.8889e-04 - val_my_r2: 0.9343\n",
      "Epoch 267/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4645e-04 - my_r2: 0.8052 - val_loss: 1.8878e-04 - val_my_r2: 0.9340\n",
      "Epoch 268/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4374e-04 - my_r2: 0.9048 - val_loss: 1.8517e-04 - val_my_r2: 0.9353\n",
      "Epoch 269/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4397e-04 - my_r2: 0.8711 - val_loss: 1.8311e-04 - val_my_r2: 0.9365\n",
      "Epoch 270/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3694e-04 - my_r2: 0.8881 - val_loss: 1.8531e-04 - val_my_r2: 0.9368\n",
      "Epoch 271/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6268e-04 - my_r2: 0.8670 - val_loss: 1.9807e-04 - val_my_r2: 0.9333\n",
      "Epoch 272/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9717e-04 - my_r2: 0.9096 - val_loss: 1.8671e-04 - val_my_r2: 0.9365\n",
      "Epoch 273/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 4.9504e-04 - my_r2: 0.8970 - val_loss: 1.9391e-04 - val_my_r2: 0.9345\n",
      "Epoch 274/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9593e-04 - my_r2: 0.8585 - val_loss: 1.8526e-04 - val_my_r2: 0.9361\n",
      "Epoch 275/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2819e-04 - my_r2: 0.8532 - val_loss: 1.9089e-04 - val_my_r2: 0.9357\n",
      "Epoch 276/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4571e-04 - my_r2: 0.8344 - val_loss: 1.7973e-04 - val_my_r2: 0.9387\n",
      "Epoch 277/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4873e-04 - my_r2: 0.9098 - val_loss: 1.7754e-04 - val_my_r2: 0.9394\n",
      "Epoch 278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3107e-04 - my_r2: 0.8872 - val_loss: 1.7750e-04 - val_my_r2: 0.9403\n",
      "Epoch 279/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4925e-04 - my_r2: 0.8213 - val_loss: 1.7671e-04 - val_my_r2: 0.9407\n",
      "Epoch 280/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1500e-04 - my_r2: 0.8269 - val_loss: 1.7502e-04 - val_my_r2: 0.9405\n",
      "Epoch 281/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1498e-04 - my_r2: 0.8550 - val_loss: 1.7216e-04 - val_my_r2: 0.9402\n",
      "Epoch 282/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7640e-04 - my_r2: 0.8853 - val_loss: 1.7179e-04 - val_my_r2: 0.9399\n",
      "Epoch 283/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1788e-04 - my_r2: 0.9109 - val_loss: 1.7269e-04 - val_my_r2: 0.9413\n",
      "Epoch 284/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8687e-04 - my_r2: 0.8707 - val_loss: 1.7187e-04 - val_my_r2: 0.9423\n",
      "Epoch 285/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9398e-04 - my_r2: 0.8863 - val_loss: 1.6958e-04 - val_my_r2: 0.9434\n",
      "Epoch 286/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4581e-04 - my_r2: 0.8830 - val_loss: 1.6949e-04 - val_my_r2: 0.9419\n",
      "Epoch 287/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7215e-04 - my_r2: 0.8672 - val_loss: 1.6699e-04 - val_my_r2: 0.9441\n",
      "Epoch 288/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4478e-04 - my_r2: 0.8204 - val_loss: 1.7094e-04 - val_my_r2: 0.9446\n",
      "Epoch 289/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8934e-04 - my_r2: 0.6672 - val_loss: 1.7486e-04 - val_my_r2: 0.9444\n",
      "Epoch 290/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1775e-04 - my_r2: 0.9062 - val_loss: 1.6694e-04 - val_my_r2: 0.9460\n",
      "Epoch 291/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9050e-04 - my_r2: 0.9022 - val_loss: 1.6697e-04 - val_my_r2: 0.9451\n",
      "Epoch 292/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2268e-04 - my_r2: 0.8540 - val_loss: 1.6974e-04 - val_my_r2: 0.9443\n",
      "Epoch 293/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4583e-04 - my_r2: 0.8834 - val_loss: 1.6848e-04 - val_my_r2: 0.9446\n",
      "Epoch 294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7155e-04 - my_r2: 0.8440 - val_loss: 1.6202e-04 - val_my_r2: 0.9459\n",
      "Epoch 295/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4521e-04 - my_r2: 0.8705 - val_loss: 1.6118e-04 - val_my_r2: 0.9461\n",
      "Epoch 296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6814e-04 - my_r2: 0.9086 - val_loss: 1.6152e-04 - val_my_r2: 0.9463\n",
      "Epoch 297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9263e-04 - my_r2: 0.9127 - val_loss: 1.6626e-04 - val_my_r2: 0.9452\n",
      "Epoch 298/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1399e-04 - my_r2: 0.6025 - val_loss: 1.6130e-04 - val_my_r2: 0.9456\n",
      "Epoch 299/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7820e-04 - my_r2: 0.5971 - val_loss: 1.5680e-04 - val_my_r2: 0.9471\n",
      "Epoch 300/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.7962e-04 - my_r2: 0.9016 - val_loss: 1.5882e-04 - val_my_r2: 0.9467\n",
      "Epoch 301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0018e-04 - my_r2: 0.8659 - val_loss: 1.5659e-04 - val_my_r2: 0.9474\n",
      "Epoch 302/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9300e-04 - my_r2: 0.8964 - val_loss: 1.5504e-04 - val_my_r2: 0.9477\n",
      "Epoch 303/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1951e-04 - my_r2: 0.8729 - val_loss: 1.5664e-04 - val_my_r2: 0.9460\n",
      "Epoch 304/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0575e-04 - my_r2: 0.9256 - val_loss: 1.5443e-04 - val_my_r2: 0.9467\n",
      "Epoch 305/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1769e-04 - my_r2: 0.9255 - val_loss: 1.5333e-04 - val_my_r2: 0.9475\n",
      "Epoch 306/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.2536e-04 - my_r2: 0.8446 - val_loss: 1.4952e-04 - val_my_r2: 0.9483\n",
      "Epoch 307/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.6882e-04 - my_r2: 0.8463 - val_loss: 1.4956e-04 - val_my_r2: 0.9492\n",
      "Epoch 308/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0227e-04 - my_r2: 0.8772 - val_loss: 1.5059e-04 - val_my_r2: 0.9493\n",
      "Epoch 309/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0857e-04 - my_r2: 0.9149 - val_loss: 1.4699e-04 - val_my_r2: 0.9494\n",
      "Epoch 310/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3539e-04 - my_r2: 0.9144 - val_loss: 1.4621e-04 - val_my_r2: 0.9498\n",
      "Epoch 311/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1159e-04 - my_r2: 0.9112 - val_loss: 1.4575e-04 - val_my_r2: 0.9501\n",
      "Epoch 312/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4098e-04 - my_r2: 0.9017 - val_loss: 1.4398e-04 - val_my_r2: 0.9504\n",
      "Epoch 313/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0609e-04 - my_r2: 0.8852 - val_loss: 1.4331e-04 - val_my_r2: 0.9508\n",
      "Epoch 314/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8638e-04 - my_r2: 0.8752 - val_loss: 1.4582e-04 - val_my_r2: 0.9504\n",
      "Epoch 315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9543e-04 - my_r2: 0.9119 - val_loss: 1.4948e-04 - val_my_r2: 0.9497\n",
      "Epoch 316/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0573e-04 - my_r2: 0.9092 - val_loss: 1.4641e-04 - val_my_r2: 0.9502\n",
      "Epoch 317/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6662e-04 - my_r2: 0.9156 - val_loss: 1.4013e-04 - val_my_r2: 0.9516\n",
      "Epoch 318/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7514e-04 - my_r2: 0.8946 - val_loss: 1.4104e-04 - val_my_r2: 0.9513\n",
      "Epoch 319/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5747e-04 - my_r2: 0.9018 - val_loss: 1.4004e-04 - val_my_r2: 0.9513\n",
      "Epoch 320/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8919e-04 - my_r2: 0.8907 - val_loss: 1.4575e-04 - val_my_r2: 0.9486\n",
      "Epoch 321/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3168e-04 - my_r2: 0.8733 - val_loss: 1.4458e-04 - val_my_r2: 0.9491\n",
      "Epoch 322/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9590e-04 - my_r2: 0.8199 - val_loss: 1.4022e-04 - val_my_r2: 0.9510\n",
      "Epoch 323/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7435e-04 - my_r2: 0.9124 - val_loss: 1.3961e-04 - val_my_r2: 0.9523\n",
      "Epoch 324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3228e-04 - my_r2: 0.9089 - val_loss: 1.3952e-04 - val_my_r2: 0.9523\n",
      "Epoch 325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6358e-04 - my_r2: 0.8384 - val_loss: 1.3548e-04 - val_my_r2: 0.9530\n",
      "Epoch 326/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7266e-04 - my_r2: 0.8850 - val_loss: 1.3465e-04 - val_my_r2: 0.9538\n",
      "Epoch 327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9715e-04 - my_r2: 0.8963 - val_loss: 1.3462e-04 - val_my_r2: 0.9539\n",
      "Epoch 328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2419e-04 - my_r2: 0.8654 - val_loss: 1.3344e-04 - val_my_r2: 0.9538\n",
      "Epoch 329/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3752e-04 - my_r2: 0.8673 - val_loss: 1.3513e-04 - val_my_r2: 0.9536\n",
      "Epoch 330/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2785e-04 - my_r2: 0.8811 - val_loss: 1.3616e-04 - val_my_r2: 0.9528\n",
      "Epoch 331/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7172e-04 - my_r2: 0.9056 - val_loss: 1.3911e-04 - val_my_r2: 0.9520\n",
      "Epoch 332/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1029e-04 - my_r2: 0.8906 - val_loss: 1.3381e-04 - val_my_r2: 0.9543\n",
      "Epoch 333/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4035e-04 - my_r2: 0.8355 - val_loss: 1.4050e-04 - val_my_r2: 0.9517\n",
      "Epoch 334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9458e-04 - my_r2: 0.8638 - val_loss: 1.3425e-04 - val_my_r2: 0.9530\n",
      "Epoch 335/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0300e-04 - my_r2: 0.9190 - val_loss: 1.3214e-04 - val_my_r2: 0.9531\n",
      "Epoch 336/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6431e-04 - my_r2: 0.7877 - val_loss: 1.3376e-04 - val_my_r2: 0.9531\n",
      "Epoch 337/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2116e-04 - my_r2: 0.8534 - val_loss: 1.3158e-04 - val_my_r2: 0.9539\n",
      "Epoch 338/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2988e-04 - my_r2: 0.8947 - val_loss: 1.3008e-04 - val_my_r2: 0.9546\n",
      "Epoch 339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9639e-04 - my_r2: 0.8946 - val_loss: 1.2887e-04 - val_my_r2: 0.9549\n",
      "Epoch 340/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0540e-04 - my_r2: 0.8353 - val_loss: 1.2724e-04 - val_my_r2: 0.9552\n",
      "Epoch 341/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1870e-04 - my_r2: 0.9211 - val_loss: 1.2657e-04 - val_my_r2: 0.9551\n",
      "Epoch 342/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3002e-04 - my_r2: 0.8960 - val_loss: 1.2645e-04 - val_my_r2: 0.9546\n",
      "Epoch 343/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4472e-04 - my_r2: 0.9229 - val_loss: 1.2733e-04 - val_my_r2: 0.9544\n",
      "Epoch 344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2958e-04 - my_r2: 0.9251 - val_loss: 1.2460e-04 - val_my_r2: 0.9555\n",
      "Epoch 345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7882e-04 - my_r2: 0.8728 - val_loss: 1.2176e-04 - val_my_r2: 0.9569\n",
      "Epoch 346/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6927e-04 - my_r2: 0.8521 - val_loss: 1.2273e-04 - val_my_r2: 0.9565\n",
      "Epoch 347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2886e-04 - my_r2: 0.8329 - val_loss: 1.2230e-04 - val_my_r2: 0.9557\n",
      "Epoch 348/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6627e-04 - my_r2: 0.8796 - val_loss: 1.2614e-04 - val_my_r2: 0.9545\n",
      "Epoch 349/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3623e-04 - my_r2: 0.8818 - val_loss: 1.2598e-04 - val_my_r2: 0.9544\n",
      "Epoch 350/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3088e-04 - my_r2: 0.9002 - val_loss: 1.2989e-04 - val_my_r2: 0.9534\n",
      "Epoch 351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1476e-04 - my_r2: 0.9048 - val_loss: 1.2830e-04 - val_my_r2: 0.9549\n",
      "Epoch 352/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6013e-04 - my_r2: 0.9328 - val_loss: 1.2150e-04 - val_my_r2: 0.9575\n",
      "Epoch 353/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7616e-04 - my_r2: 0.8636 - val_loss: 1.1954e-04 - val_my_r2: 0.9568\n",
      "Epoch 354/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9538e-04 - my_r2: 0.9022 - val_loss: 1.2113e-04 - val_my_r2: 0.9563\n",
      "Epoch 355/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3472e-04 - my_r2: 0.8851 - val_loss: 1.2320e-04 - val_my_r2: 0.9561\n",
      "Epoch 356/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6535e-04 - my_r2: 0.8834 - val_loss: 1.2203e-04 - val_my_r2: 0.9581\n",
      "Epoch 357/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9318e-04 - my_r2: 0.8605 - val_loss: 1.1921e-04 - val_my_r2: 0.9598\n",
      "Epoch 358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7935e-04 - my_r2: 0.9077 - val_loss: 1.1780e-04 - val_my_r2: 0.9601\n",
      "Epoch 359/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0494e-04 - my_r2: 0.9303 - val_loss: 1.1674e-04 - val_my_r2: 0.9600\n",
      "Epoch 360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4759e-04 - my_r2: 0.8697 - val_loss: 1.1652e-04 - val_my_r2: 0.9591\n",
      "Epoch 361/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5073e-04 - my_r2: 0.9011 - val_loss: 1.1509e-04 - val_my_r2: 0.9599\n",
      "Epoch 362/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0091e-04 - my_r2: 0.8900 - val_loss: 1.1801e-04 - val_my_r2: 0.9602\n",
      "Epoch 363/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 4.0644e-04 - my_r2: 0.7983 - val_loss: 1.1480e-04 - val_my_r2: 0.9600\n",
      "Epoch 364/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5346e-04 - my_r2: 0.8661 - val_loss: 1.1665e-04 - val_my_r2: 0.9594\n",
      "Epoch 365/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.1587e-04 - my_r2: 0.9186 - val_loss: 1.1667e-04 - val_my_r2: 0.9601\n",
      "Epoch 366/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9743e-04 - my_r2: 0.8779 - val_loss: 1.1215e-04 - val_my_r2: 0.9610\n",
      "Epoch 367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7442e-04 - my_r2: 0.9072 - val_loss: 1.1041e-04 - val_my_r2: 0.9626\n",
      "Epoch 368/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6837e-04 - my_r2: 0.9212 - val_loss: 1.1197e-04 - val_my_r2: 0.9622\n",
      "Epoch 369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1671e-04 - my_r2: 0.9113 - val_loss: 1.1356e-04 - val_my_r2: 0.9604\n",
      "Epoch 370/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1526e-04 - my_r2: 0.9254 - val_loss: 1.1307e-04 - val_my_r2: 0.9605\n",
      "Epoch 371/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4152e-04 - my_r2: 0.8590 - val_loss: 1.1200e-04 - val_my_r2: 0.9608\n",
      "Epoch 372/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3275e-04 - my_r2: 0.8850 - val_loss: 1.1106e-04 - val_my_r2: 0.9613\n",
      "Epoch 373/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2897e-04 - my_r2: 0.8987 - val_loss: 1.1002e-04 - val_my_r2: 0.9614\n",
      "Epoch 374/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5778e-04 - my_r2: 0.8911 - val_loss: 1.0960e-04 - val_my_r2: 0.9618\n",
      "Epoch 375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4287e-04 - my_r2: 0.9071 - val_loss: 1.1361e-04 - val_my_r2: 0.9612\n",
      "Epoch 376/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4503e-04 - my_r2: 0.9256 - val_loss: 1.2865e-04 - val_my_r2: 0.9572\n",
      "Epoch 377/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3334e-04 - my_r2: 0.9239 - val_loss: 1.1076e-04 - val_my_r2: 0.9621\n",
      "Epoch 378/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5169e-04 - my_r2: 0.8909 - val_loss: 1.0770e-04 - val_my_r2: 0.9621\n",
      "Epoch 379/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1213e-04 - my_r2: 0.9258 - val_loss: 1.1182e-04 - val_my_r2: 0.9624\n",
      "Epoch 380/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4381e-04 - my_r2: 0.8859 - val_loss: 1.0610e-04 - val_my_r2: 0.9644\n",
      "Epoch 381/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5124e-04 - my_r2: 0.8979 - val_loss: 1.0538e-04 - val_my_r2: 0.9646\n",
      "Epoch 382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3099e-04 - my_r2: 0.9313 - val_loss: 1.0827e-04 - val_my_r2: 0.9643\n",
      "Epoch 383/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7397e-04 - my_r2: 0.9187 - val_loss: 1.0542e-04 - val_my_r2: 0.9650\n",
      "Epoch 384/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0763e-04 - my_r2: 0.9273 - val_loss: 1.0318e-04 - val_my_r2: 0.9652\n",
      "Epoch 385/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6223e-04 - my_r2: 0.8998 - val_loss: 1.0350e-04 - val_my_r2: 0.9645\n",
      "Epoch 386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4160e-04 - my_r2: 0.9287 - val_loss: 1.0486e-04 - val_my_r2: 0.9646\n",
      "Epoch 387/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6208e-04 - my_r2: 0.9158 - val_loss: 1.0667e-04 - val_my_r2: 0.9641\n",
      "Epoch 388/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5119e-04 - my_r2: 0.8988 - val_loss: 1.0490e-04 - val_my_r2: 0.9633\n",
      "Epoch 389/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4577e-04 - my_r2: 0.9216 - val_loss: 1.0796e-04 - val_my_r2: 0.9618\n",
      "Epoch 390/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9543e-04 - my_r2: 0.9156 - val_loss: 1.0434e-04 - val_my_r2: 0.9632\n",
      "Epoch 391/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7156e-04 - my_r2: 0.8775 - val_loss: 9.8919e-05 - val_my_r2: 0.9659\n",
      "Epoch 392/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2980e-04 - my_r2: 0.9046 - val_loss: 9.8468e-05 - val_my_r2: 0.9662\n",
      "Epoch 393/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2722e-04 - my_r2: 0.8967 - val_loss: 1.0036e-04 - val_my_r2: 0.9658\n",
      "Epoch 394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2892e-04 - my_r2: 0.8920 - val_loss: 9.8951e-05 - val_my_r2: 0.9664\n",
      "Epoch 395/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9907e-04 - my_r2: 0.8936 - val_loss: 9.9856e-05 - val_my_r2: 0.9661\n",
      "Epoch 396/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7328e-04 - my_r2: 0.8854 - val_loss: 9.9058e-05 - val_my_r2: 0.9664\n",
      "Epoch 397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5964e-04 - my_r2: 0.9194 - val_loss: 9.6358e-05 - val_my_r2: 0.9672\n",
      "Epoch 398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5660e-04 - my_r2: 0.8989 - val_loss: 9.6572e-05 - val_my_r2: 0.9669\n",
      "Epoch 399/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0594e-04 - my_r2: 0.9323 - val_loss: 9.8307e-05 - val_my_r2: 0.9670\n",
      "Epoch 400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5868e-04 - my_r2: 0.8882 - val_loss: 9.8094e-05 - val_my_r2: 0.9665\n",
      "Epoch 401/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6283e-04 - my_r2: 0.9046 - val_loss: 9.5448e-05 - val_my_r2: 0.9669\n",
      "Epoch 402/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0130e-04 - my_r2: 0.9066 - val_loss: 9.3284e-05 - val_my_r2: 0.9673\n",
      "Epoch 403/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3145e-04 - my_r2: 0.7134 - val_loss: 9.7545e-05 - val_my_r2: 0.9668\n",
      "Epoch 404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7655e-04 - my_r2: 0.9025 - val_loss: 1.0073e-04 - val_my_r2: 0.9649\n",
      "Epoch 405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9100e-04 - my_r2: 0.9097 - val_loss: 9.8527e-05 - val_my_r2: 0.9655\n",
      "Epoch 406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5664e-04 - my_r2: 0.9154 - val_loss: 9.6677e-05 - val_my_r2: 0.9661\n",
      "Epoch 407/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4908e-04 - my_r2: 0.8770 - val_loss: 9.2906e-05 - val_my_r2: 0.9668\n",
      "Epoch 408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4755e-04 - my_r2: 0.9298 - val_loss: 9.2921e-05 - val_my_r2: 0.9673\n",
      "Epoch 409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8603e-04 - my_r2: 0.8594 - val_loss: 9.1674e-05 - val_my_r2: 0.9678\n",
      "Epoch 410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1276e-04 - my_r2: 0.9261 - val_loss: 9.1293e-05 - val_my_r2: 0.9685\n",
      "Epoch 411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1559e-04 - my_r2: 0.9227 - val_loss: 9.2820e-05 - val_my_r2: 0.9681\n",
      "Epoch 412/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3912e-04 - my_r2: 0.7847 - val_loss: 1.0047e-04 - val_my_r2: 0.9657\n",
      "Epoch 413/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3341e-04 - my_r2: 0.8910 - val_loss: 9.1034e-05 - val_my_r2: 0.9685\n",
      "Epoch 414/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2084e-04 - my_r2: 0.9178 - val_loss: 8.8681e-05 - val_my_r2: 0.9696\n",
      "Epoch 415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6341e-04 - my_r2: 0.9117 - val_loss: 9.1869e-05 - val_my_r2: 0.9694\n",
      "Epoch 416/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2101e-04 - my_r2: 0.9340 - val_loss: 9.0037e-05 - val_my_r2: 0.9701\n",
      "Epoch 417/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7050e-04 - my_r2: 0.8992 - val_loss: 9.1212e-05 - val_my_r2: 0.9696\n",
      "Epoch 418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4352e-04 - my_r2: 0.9206 - val_loss: 9.2052e-05 - val_my_r2: 0.9682\n",
      "Epoch 419/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5413e-04 - my_r2: 0.9199 - val_loss: 8.9043e-05 - val_my_r2: 0.9696\n",
      "Epoch 420/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2354e-04 - my_r2: 0.8873 - val_loss: 8.7613e-05 - val_my_r2: 0.9695\n",
      "Epoch 421/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3419e-04 - my_r2: 0.9059 - val_loss: 8.6907e-05 - val_my_r2: 0.9701\n",
      "Epoch 422/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0777e-04 - my_r2: 0.8569 - val_loss: 8.6206e-05 - val_my_r2: 0.9706\n",
      "Epoch 423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7500e-04 - my_r2: 0.9247 - val_loss: 8.7785e-05 - val_my_r2: 0.9703\n",
      "Epoch 424/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2629e-04 - my_r2: 0.8443 - val_loss: 8.7797e-05 - val_my_r2: 0.9708\n",
      "Epoch 425/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1607e-04 - my_r2: 0.9174 - val_loss: 8.3172e-05 - val_my_r2: 0.9715\n",
      "Epoch 426/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5484e-04 - my_r2: 0.9142 - val_loss: 8.3294e-05 - val_my_r2: 0.9711\n",
      "Epoch 427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5654e-04 - my_r2: 0.8012 - val_loss: 8.3193e-05 - val_my_r2: 0.9713\n",
      "Epoch 428/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2378e-04 - my_r2: 0.9202 - val_loss: 8.4960e-05 - val_my_r2: 0.9708\n",
      "Epoch 429/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.8042e-04 - my_r2: 0.8934 - val_loss: 8.2120e-05 - val_my_r2: 0.9715\n",
      "Epoch 430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6156e-04 - my_r2: 0.9325 - val_loss: 8.3027e-05 - val_my_r2: 0.9710\n",
      "Epoch 431/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3337e-04 - my_r2: 0.9260 - val_loss: 8.2478e-05 - val_my_r2: 0.9720\n",
      "Epoch 432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2157e-04 - my_r2: 0.9106 - val_loss: 8.7186e-05 - val_my_r2: 0.9712\n",
      "Epoch 433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2659e-04 - my_r2: 0.8952 - val_loss: 8.6700e-05 - val_my_r2: 0.9710\n",
      "Epoch 434/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0881e-04 - my_r2: 0.9245 - val_loss: 8.6267e-05 - val_my_r2: 0.9713\n",
      "Epoch 435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5212e-04 - my_r2: 0.9048 - val_loss: 8.1442e-05 - val_my_r2: 0.9724\n",
      "Epoch 436/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0773e-04 - my_r2: 0.8788 - val_loss: 7.7967e-05 - val_my_r2: 0.9731\n",
      "Epoch 437/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5371e-04 - my_r2: 0.9139 - val_loss: 8.1389e-05 - val_my_r2: 0.9720\n",
      "Epoch 438/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7115e-04 - my_r2: 0.9001 - val_loss: 8.0490e-05 - val_my_r2: 0.9716\n",
      "Epoch 439/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6219e-04 - my_r2: 0.9317 - val_loss: 7.7979e-05 - val_my_r2: 0.9724\n",
      "Epoch 440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7238e-04 - my_r2: 0.9307 - val_loss: 7.7625e-05 - val_my_r2: 0.9727\n",
      "Epoch 441/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6900e-04 - my_r2: 0.9060 - val_loss: 7.6626e-05 - val_my_r2: 0.9729\n",
      "Epoch 442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1208e-04 - my_r2: 0.9324 - val_loss: 7.9345e-05 - val_my_r2: 0.9716\n",
      "Epoch 443/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4576e-04 - my_r2: 0.9107 - val_loss: 7.9361e-05 - val_my_r2: 0.9717\n",
      "Epoch 444/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0830e-04 - my_r2: 0.8937 - val_loss: 8.1175e-05 - val_my_r2: 0.9717\n",
      "Epoch 445/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6078e-04 - my_r2: 0.8661 - val_loss: 8.7601e-05 - val_my_r2: 0.9704\n",
      "Epoch 446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7317e-04 - my_r2: 0.9002 - val_loss: 8.1748e-05 - val_my_r2: 0.9721\n",
      "Epoch 447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2141e-04 - my_r2: 0.9153 - val_loss: 8.7500e-05 - val_my_r2: 0.9704\n",
      "Epoch 448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7193e-04 - my_r2: 0.9257 - val_loss: 7.9378e-05 - val_my_r2: 0.9725\n",
      "Epoch 449/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2750e-04 - my_r2: 0.8584 - val_loss: 7.7789e-05 - val_my_r2: 0.9730\n",
      "Epoch 450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9743e-04 - my_r2: 0.9252 - val_loss: 7.5212e-05 - val_my_r2: 0.9737\n",
      "Epoch 451/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2468e-04 - my_r2: 0.8657 - val_loss: 7.7503e-05 - val_my_r2: 0.9733\n",
      "Epoch 452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7692e-04 - my_r2: 0.9187 - val_loss: 8.9550e-05 - val_my_r2: 0.9698\n",
      "Epoch 453/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2589e-04 - my_r2: 0.8873 - val_loss: 7.6326e-05 - val_my_r2: 0.9737\n",
      "Epoch 454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2868e-04 - my_r2: 0.9207 - val_loss: 7.3934e-05 - val_my_r2: 0.9741\n",
      "Epoch 455/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9675e-04 - my_r2: 0.9070 - val_loss: 7.5778e-05 - val_my_r2: 0.9738\n",
      "Epoch 456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5797e-04 - my_r2: 0.9235 - val_loss: 8.9049e-05 - val_my_r2: 0.9693\n",
      "Epoch 457/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7461e-04 - my_r2: 0.8825 - val_loss: 8.4014e-05 - val_my_r2: 0.9700\n",
      "Epoch 458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1374e-04 - my_r2: 0.8878 - val_loss: 7.7321e-05 - val_my_r2: 0.9715\n",
      "Epoch 459/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4512e-04 - my_r2: 0.8724 - val_loss: 7.4518e-05 - val_my_r2: 0.9725\n",
      "Epoch 460/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0521e-04 - my_r2: 0.8850 - val_loss: 7.0386e-05 - val_my_r2: 0.9740\n",
      "Epoch 461/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1276e-04 - my_r2: 0.9155 - val_loss: 7.0912e-05 - val_my_r2: 0.9741\n",
      "Epoch 462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2473e-04 - my_r2: 0.9177 - val_loss: 7.2275e-05 - val_my_r2: 0.9734\n",
      "Epoch 463/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7046e-04 - my_r2: 0.8732 - val_loss: 7.1932e-05 - val_my_r2: 0.9732\n",
      "Epoch 464/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2354e-04 - my_r2: 0.9170 - val_loss: 7.0225e-05 - val_my_r2: 0.9741\n",
      "Epoch 465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5662e-04 - my_r2: 0.9219 - val_loss: 7.4348e-05 - val_my_r2: 0.9720\n",
      "Epoch 466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1367e-04 - my_r2: 0.9162 - val_loss: 7.4433e-05 - val_my_r2: 0.9720\n",
      "Epoch 467/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1526e-04 - my_r2: 0.9275 - val_loss: 7.0906e-05 - val_my_r2: 0.9741\n",
      "Epoch 468/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1602e-04 - my_r2: 0.9015 - val_loss: 7.1052e-05 - val_my_r2: 0.9743\n",
      "Epoch 469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1051e-04 - my_r2: 0.9314 - val_loss: 6.8721e-05 - val_my_r2: 0.9747\n",
      "Epoch 470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4048e-04 - my_r2: 0.9340 - val_loss: 7.0253e-05 - val_my_r2: 0.9744\n",
      "Epoch 471/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0013e-04 - my_r2: 0.9392 - val_loss: 7.3462e-05 - val_my_r2: 0.9734\n",
      "Epoch 472/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5622e-04 - my_r2: 0.9163 - val_loss: 7.4788e-05 - val_my_r2: 0.9727\n",
      "Epoch 473/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1721e-04 - my_r2: 0.9095 - val_loss: 7.0175e-05 - val_my_r2: 0.9739\n",
      "Epoch 474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1285e-04 - my_r2: 0.9051 - val_loss: 6.7933e-05 - val_my_r2: 0.9746\n",
      "Epoch 475/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5597e-04 - my_r2: 0.9116 - val_loss: 6.9534e-05 - val_my_r2: 0.9745\n",
      "Epoch 476/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2508e-04 - my_r2: 0.9165 - val_loss: 6.7573e-05 - val_my_r2: 0.9754\n",
      "Epoch 477/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0513e-04 - my_r2: 0.8874 - val_loss: 6.5797e-05 - val_my_r2: 0.9767\n",
      "Epoch 478/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8566e-04 - my_r2: 0.8955 - val_loss: 6.4099e-05 - val_my_r2: 0.9762\n",
      "Epoch 479/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9845e-04 - my_r2: 0.8500 - val_loss: 6.9233e-05 - val_my_r2: 0.9754\n",
      "Epoch 480/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9560e-04 - my_r2: 0.9223 - val_loss: 6.7297e-05 - val_my_r2: 0.9753\n",
      "Epoch 481/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3463e-04 - my_r2: 0.9304 - val_loss: 6.4453e-05 - val_my_r2: 0.9766\n",
      "Epoch 482/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7336e-04 - my_r2: 0.9202 - val_loss: 7.0325e-05 - val_my_r2: 0.9756\n",
      "Epoch 483/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8419e-04 - my_r2: 0.8926 - val_loss: 7.1922e-05 - val_my_r2: 0.9747\n",
      "Epoch 484/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4954e-04 - my_r2: 0.9182 - val_loss: 6.6885e-05 - val_my_r2: 0.9758\n",
      "Epoch 485/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0108e-04 - my_r2: 0.9048 - val_loss: 6.6375e-05 - val_my_r2: 0.9767\n",
      "Epoch 486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1544e-04 - my_r2: 0.8903 - val_loss: 6.5740e-05 - val_my_r2: 0.9771\n",
      "Epoch 487/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2174e-04 - my_r2: 0.9125 - val_loss: 6.5867e-05 - val_my_r2: 0.9768\n",
      "Epoch 488/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2928e-04 - my_r2: 0.9376 - val_loss: 6.3624e-05 - val_my_r2: 0.9777\n",
      "Epoch 489/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1349e-04 - my_r2: 0.9145 - val_loss: 6.9491e-05 - val_my_r2: 0.9766\n",
      "Epoch 490/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4473e-04 - my_r2: 0.7966 - val_loss: 7.6579e-05 - val_my_r2: 0.9740\n",
      "Epoch 491/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3325e-04 - my_r2: 0.9311 - val_loss: 6.9685e-05 - val_my_r2: 0.9761\n",
      "Epoch 492/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7691e-04 - my_r2: 0.9025 - val_loss: 6.2594e-05 - val_my_r2: 0.9783\n",
      "Epoch 493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8149e-04 - my_r2: 0.9394 - val_loss: 6.0371e-05 - val_my_r2: 0.9782\n",
      "Epoch 494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9236e-04 - my_r2: 0.9354 - val_loss: 6.6925e-05 - val_my_r2: 0.9767\n",
      "Epoch 495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5309e-04 - my_r2: 0.9323 - val_loss: 6.7569e-05 - val_my_r2: 0.9768\n",
      "Epoch 496/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7097e-04 - my_r2: 0.8365 - val_loss: 6.0951e-05 - val_my_r2: 0.9788\n",
      "Epoch 497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2349e-04 - my_r2: 0.9258 - val_loss: 6.2191e-05 - val_my_r2: 0.9789\n",
      "Epoch 498/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2263e-04 - my_r2: 0.8650 - val_loss: 6.1316e-05 - val_my_r2: 0.9787\n",
      "Epoch 499/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2717e-04 - my_r2: 0.9245 - val_loss: 6.0272e-05 - val_my_r2: 0.9789\n",
      "Epoch 500/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7145e-04 - my_r2: 0.1312 - val_loss: 6.7571e-05 - val_my_r2: 0.9750\n",
      "Epoch 501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5981e-04 - my_r2: 0.9306 - val_loss: 6.2774e-05 - val_my_r2: 0.9771\n",
      "Epoch 502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3509e-04 - my_r2: 0.8796 - val_loss: 5.9869e-05 - val_my_r2: 0.9779\n",
      "Epoch 503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3559e-04 - my_r2: 0.9070 - val_loss: 6.0738e-05 - val_my_r2: 0.9772\n",
      "Epoch 504/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1184e-04 - my_r2: 0.9160 - val_loss: 6.1508e-05 - val_my_r2: 0.9776\n",
      "Epoch 505/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9613e-04 - my_r2: 0.9175 - val_loss: 6.3899e-05 - val_my_r2: 0.9761\n",
      "Epoch 506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8224e-04 - my_r2: 0.8891 - val_loss: 5.9930e-05 - val_my_r2: 0.9778\n",
      "Epoch 507/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6938e-04 - my_r2: 0.8774 - val_loss: 6.0499e-05 - val_my_r2: 0.9773\n",
      "Epoch 508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4864e-04 - my_r2: 0.8929 - val_loss: 5.9800e-05 - val_my_r2: 0.9774\n",
      "Epoch 509/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1682e-04 - my_r2: 0.8287 - val_loss: 6.0360e-05 - val_my_r2: 0.9777\n",
      "Epoch 510/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2296e-04 - my_r2: 0.9187 - val_loss: 6.2642e-05 - val_my_r2: 0.9760\n",
      "Epoch 511/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6626e-04 - my_r2: 0.8834 - val_loss: 6.0749e-05 - val_my_r2: 0.9761\n",
      "Epoch 512/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3029e-04 - my_r2: 0.9087 - val_loss: 5.7913e-05 - val_my_r2: 0.9782\n",
      "Epoch 513/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5189e-04 - my_r2: 0.9145 - val_loss: 5.6592e-05 - val_my_r2: 0.9793\n",
      "Epoch 514/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.7135e-04 - my_r2: 0.9197 - val_loss: 5.9758e-05 - val_my_r2: 0.9792\n",
      "Epoch 515/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3098e-04 - my_r2: 0.9118 - val_loss: 5.8819e-05 - val_my_r2: 0.9788\n",
      "Epoch 516/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3916e-04 - my_r2: 0.9517 - val_loss: 6.1158e-05 - val_my_r2: 0.9790\n",
      "Epoch 517/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6341e-04 - my_r2: 0.9200 - val_loss: 5.8871e-05 - val_my_r2: 0.9794\n",
      "Epoch 518/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0543e-04 - my_r2: 0.8874 - val_loss: 5.6311e-05 - val_my_r2: 0.9795\n",
      "Epoch 519/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1891e-04 - my_r2: 0.9263 - val_loss: 5.5554e-05 - val_my_r2: 0.9802\n",
      "Epoch 520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3183e-04 - my_r2: 0.8706 - val_loss: 5.9979e-05 - val_my_r2: 0.9794\n",
      "Epoch 521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3486e-04 - my_r2: 0.8732 - val_loss: 6.2005e-05 - val_my_r2: 0.9792\n",
      "Epoch 522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3942e-04 - my_r2: 0.8930 - val_loss: 6.4178e-05 - val_my_r2: 0.9784\n",
      "Epoch 523/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0849e-04 - my_r2: 0.9135 - val_loss: 5.4590e-05 - val_my_r2: 0.9801\n",
      "Epoch 524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3105e-04 - my_r2: 0.9417 - val_loss: 5.6856e-05 - val_my_r2: 0.9792\n",
      "Epoch 525/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9999e-04 - my_r2: 0.9285 - val_loss: 5.5991e-05 - val_my_r2: 0.9796\n",
      "Epoch 526/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2403e-04 - my_r2: 0.9410 - val_loss: 5.5768e-05 - val_my_r2: 0.9803\n",
      "Epoch 527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2670e-04 - my_r2: 0.9162 - val_loss: 5.7733e-05 - val_my_r2: 0.9804\n",
      "Epoch 528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1991e-04 - my_r2: 0.9099 - val_loss: 5.6165e-05 - val_my_r2: 0.9799\n",
      "Epoch 529/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1244e-04 - my_r2: 0.9142 - val_loss: 5.4147e-05 - val_my_r2: 0.9806\n",
      "Epoch 530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9117e-04 - my_r2: 0.8547 - val_loss: 5.4098e-05 - val_my_r2: 0.9805\n",
      "Epoch 531/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2270e-04 - my_r2: 0.9312 - val_loss: 5.2837e-05 - val_my_r2: 0.9811\n",
      "Epoch 532/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4947e-04 - my_r2: 0.9084 - val_loss: 5.2123e-05 - val_my_r2: 0.9819\n",
      "Epoch 533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8309e-04 - my_r2: 0.9162 - val_loss: 5.1911e-05 - val_my_r2: 0.9817\n",
      "Epoch 534/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2284e-04 - my_r2: 0.9203 - val_loss: 5.2422e-05 - val_my_r2: 0.9817\n",
      "Epoch 535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8612e-04 - my_r2: 0.9142 - val_loss: 5.2532e-05 - val_my_r2: 0.9811\n",
      "Epoch 536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7317e-04 - my_r2: 0.9391 - val_loss: 5.1231e-05 - val_my_r2: 0.9818\n",
      "Epoch 537/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0796e-04 - my_r2: 0.9141 - val_loss: 5.1252e-05 - val_my_r2: 0.9814\n",
      "Epoch 538/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1339e-04 - my_r2: 0.9093 - val_loss: 5.0783e-05 - val_my_r2: 0.9822\n",
      "Epoch 539/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1294e-04 - my_r2: 0.9324 - val_loss: 5.2184e-05 - val_my_r2: 0.9825\n",
      "Epoch 540/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1153e-04 - my_r2: 0.9379 - val_loss: 5.3346e-05 - val_my_r2: 0.9820\n",
      "Epoch 541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1541e-04 - my_r2: 0.9096 - val_loss: 5.1453e-05 - val_my_r2: 0.9825\n",
      "Epoch 542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2164e-04 - my_r2: 0.9042 - val_loss: 5.0079e-05 - val_my_r2: 0.9828\n",
      "Epoch 543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8847e-04 - my_r2: 0.7956 - val_loss: 5.2638e-05 - val_my_r2: 0.9824\n",
      "Epoch 544/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8800e-04 - my_r2: 0.8328 - val_loss: 5.2074e-05 - val_my_r2: 0.9826\n",
      "Epoch 545/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3581e-04 - my_r2: 0.8388 - val_loss: 5.4017e-05 - val_my_r2: 0.9826\n",
      "Epoch 546/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4424e-04 - my_r2: 0.9039 - val_loss: 5.2754e-05 - val_my_r2: 0.9828\n",
      "Epoch 547/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5454e-04 - my_r2: 0.9329 - val_loss: 5.0285e-05 - val_my_r2: 0.9834\n",
      "Epoch 548/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5637e-04 - my_r2: 0.9042 - val_loss: 5.3938e-05 - val_my_r2: 0.9817\n",
      "Epoch 549/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2138e-04 - my_r2: 0.9245 - val_loss: 5.5152e-05 - val_my_r2: 0.9812\n",
      "Epoch 550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7982e-04 - my_r2: 0.9248 - val_loss: 5.7556e-05 - val_my_r2: 0.9800\n",
      "Epoch 551/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1126e-04 - my_r2: 0.8908 - val_loss: 5.4392e-05 - val_my_r2: 0.9808\n",
      "Epoch 552/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5935e-04 - my_r2: 0.9150 - val_loss: 5.2546e-05 - val_my_r2: 0.9811\n",
      "Epoch 553/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1299e-04 - my_r2: 0.9352 - val_loss: 5.2158e-05 - val_my_r2: 0.9808\n",
      "Epoch 554/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3609e-04 - my_r2: 0.9341 - val_loss: 4.8545e-05 - val_my_r2: 0.9828\n",
      "Epoch 555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2463e-04 - my_r2: 0.9086 - val_loss: 4.9456e-05 - val_my_r2: 0.9826\n",
      "Epoch 556/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2803e-04 - my_r2: 0.9324 - val_loss: 5.0890e-05 - val_my_r2: 0.9821\n",
      "Epoch 557/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6578e-04 - my_r2: 0.9038 - val_loss: 5.4124e-05 - val_my_r2: 0.9818\n",
      "Epoch 558/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4928e-04 - my_r2: 0.9066 - val_loss: 5.0098e-05 - val_my_r2: 0.9825\n",
      "Epoch 559/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2847e-04 - my_r2: 0.9059 - val_loss: 4.8180e-05 - val_my_r2: 0.9828\n",
      "Epoch 560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8818e-04 - my_r2: 0.9049 - val_loss: 5.1000e-05 - val_my_r2: 0.9822\n",
      "Epoch 561/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1267e-04 - my_r2: 0.9148 - val_loss: 4.7981e-05 - val_my_r2: 0.9833\n",
      "Epoch 562/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8320e-04 - my_r2: 0.8406 - val_loss: 4.7890e-05 - val_my_r2: 0.9835\n",
      "Epoch 563/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8371e-04 - my_r2: 0.9212 - val_loss: 5.0783e-05 - val_my_r2: 0.9831\n",
      "Epoch 564/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9660e-04 - my_r2: 0.9196 - val_loss: 4.7848e-05 - val_my_r2: 0.9837\n",
      "Epoch 565/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9389e-04 - my_r2: 0.8831 - val_loss: 4.7166e-05 - val_my_r2: 0.9839\n",
      "Epoch 566/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7428e-04 - my_r2: 0.9219 - val_loss: 4.8856e-05 - val_my_r2: 0.9828\n",
      "Epoch 567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3578e-04 - my_r2: 0.8809 - val_loss: 4.9703e-05 - val_my_r2: 0.9824\n",
      "Epoch 568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5741e-04 - my_r2: 0.8991 - val_loss: 5.1507e-05 - val_my_r2: 0.9815\n",
      "Epoch 569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1730e-04 - my_r2: 0.9021 - val_loss: 5.4282e-05 - val_my_r2: 0.9811\n",
      "Epoch 570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9694e-04 - my_r2: 0.9268 - val_loss: 4.7034e-05 - val_my_r2: 0.9833\n",
      "Epoch 571/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1716e-04 - my_r2: 0.9365 - val_loss: 4.7484e-05 - val_my_r2: 0.9832\n",
      "Epoch 572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8271e-04 - my_r2: 0.9210 - val_loss: 4.9919e-05 - val_my_r2: 0.9834\n",
      "Epoch 573/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0515e-04 - my_r2: 0.9370 - val_loss: 5.2294e-05 - val_my_r2: 0.9819\n",
      "Epoch 574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7055e-04 - my_r2: 0.9456 - val_loss: 5.0025e-05 - val_my_r2: 0.9833\n",
      "Epoch 575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6684e-04 - my_r2: 0.9279 - val_loss: 4.9820e-05 - val_my_r2: 0.9840\n",
      "Epoch 576/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.9083e-04 - my_r2: 0.9040 - val_loss: 5.0388e-05 - val_my_r2: 0.9833\n",
      "Epoch 577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0328e-04 - my_r2: 0.9320 - val_loss: 5.0310e-05 - val_my_r2: 0.9823\n",
      "Epoch 578/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3011e-04 - my_r2: 0.8572 - val_loss: 5.1057e-05 - val_my_r2: 0.9811\n",
      "Epoch 579/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4620e-04 - my_r2: 0.9026 - val_loss: 4.5988e-05 - val_my_r2: 0.9834\n",
      "Epoch 580/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9830e-04 - my_r2: 0.9224 - val_loss: 4.2750e-05 - val_my_r2: 0.9847\n",
      "Epoch 581/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6390e-04 - my_r2: 0.9202 - val_loss: 4.1683e-05 - val_my_r2: 0.9847\n",
      "Epoch 582/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9618e-04 - my_r2: 0.9093 - val_loss: 4.3100e-05 - val_my_r2: 0.9845\n",
      "Epoch 583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1064e-04 - my_r2: 0.8065 - val_loss: 5.8385e-05 - val_my_r2: 0.9787\n",
      "Epoch 584/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2159e-04 - my_r2: 0.9041 - val_loss: 4.2353e-05 - val_my_r2: 0.9845\n",
      "Epoch 585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6773e-04 - my_r2: 0.9196 - val_loss: 4.7328e-05 - val_my_r2: 0.9832\n",
      "Epoch 586/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3852e-04 - my_r2: 0.8587 - val_loss: 4.4326e-05 - val_my_r2: 0.9844\n",
      "Epoch 587/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4302e-04 - my_r2: 0.9085 - val_loss: 4.4736e-05 - val_my_r2: 0.9845\n",
      "Epoch 588/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3182e-04 - my_r2: 0.9239 - val_loss: 4.8618e-05 - val_my_r2: 0.9834\n",
      "Epoch 589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2620e-04 - my_r2: 0.9266 - val_loss: 4.4034e-05 - val_my_r2: 0.9846\n",
      "Epoch 590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1896e-04 - my_r2: 0.8341 - val_loss: 4.5658e-05 - val_my_r2: 0.9839\n",
      "Epoch 591/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3837e-04 - my_r2: 0.8730 - val_loss: 4.8299e-05 - val_my_r2: 0.9824\n",
      "Epoch 592/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0267e-04 - my_r2: 0.9250 - val_loss: 4.8923e-05 - val_my_r2: 0.9818\n",
      "Epoch 593/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8038e-04 - my_r2: 0.9518 - val_loss: 4.3377e-05 - val_my_r2: 0.9838\n",
      "Epoch 594/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8879e-04 - my_r2: 0.8923 - val_loss: 4.4979e-05 - val_my_r2: 0.9832\n",
      "Epoch 595/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4579e-04 - my_r2: 0.9169 - val_loss: 4.9478e-05 - val_my_r2: 0.9822\n",
      "Epoch 596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2497e-04 - my_r2: 0.9044 - val_loss: 4.1869e-05 - val_my_r2: 0.9850\n",
      "Epoch 597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3700e-04 - my_r2: 0.9085 - val_loss: 4.0832e-05 - val_my_r2: 0.9855\n",
      "Epoch 598/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4046e-04 - my_r2: 0.9013 - val_loss: 4.5149e-05 - val_my_r2: 0.9845\n",
      "Epoch 599/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5312e-04 - my_r2: 0.9096 - val_loss: 4.8736e-05 - val_my_r2: 0.9840\n",
      "Epoch 600/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2619e-04 - my_r2: 0.9312 - val_loss: 4.2393e-05 - val_my_r2: 0.9850\n",
      "Epoch 601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5907e-04 - my_r2: 0.9024 - val_loss: 4.1614e-05 - val_my_r2: 0.9848\n",
      "Epoch 602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3801e-04 - my_r2: 0.8675 - val_loss: 4.2247e-05 - val_my_r2: 0.9846\n",
      "Epoch 603/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6690e-04 - my_r2: 0.9153 - val_loss: 4.1034e-05 - val_my_r2: 0.9851\n",
      "Epoch 604/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2159e-04 - my_r2: 0.8863 - val_loss: 4.3123e-05 - val_my_r2: 0.9843\n",
      "Epoch 605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5164e-04 - my_r2: 0.9136 - val_loss: 4.1953e-05 - val_my_r2: 0.9855\n",
      "Epoch 606/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7692e-04 - my_r2: 0.9166 - val_loss: 4.8347e-05 - val_my_r2: 0.9839\n",
      "Epoch 607/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4500e-04 - my_r2: 0.8976 - val_loss: 4.0653e-05 - val_my_r2: 0.9859\n",
      "Epoch 608/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6201e-04 - my_r2: 0.9328 - val_loss: 4.1718e-05 - val_my_r2: 0.9848\n",
      "Epoch 609/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8548e-04 - my_r2: 0.8861 - val_loss: 4.2507e-05 - val_my_r2: 0.9847\n",
      "Epoch 610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5470e-04 - my_r2: 0.9345 - val_loss: 3.9271e-05 - val_my_r2: 0.9861\n",
      "Epoch 611/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7640e-04 - my_r2: 0.9430 - val_loss: 4.2181e-05 - val_my_r2: 0.9856\n",
      "Epoch 612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3290e-04 - my_r2: 0.9191 - val_loss: 3.9764e-05 - val_my_r2: 0.9861\n",
      "Epoch 613/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7039e-04 - my_r2: 0.9370 - val_loss: 4.1483e-05 - val_my_r2: 0.9854\n",
      "Epoch 614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6551e-04 - my_r2: 0.9327 - val_loss: 4.4671e-05 - val_my_r2: 0.9836\n",
      "Epoch 615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3557e-04 - my_r2: 0.8836 - val_loss: 5.0869e-05 - val_my_r2: 0.9805\n",
      "Epoch 616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0556e-04 - my_r2: 0.8895 - val_loss: 4.5381e-05 - val_my_r2: 0.9826\n",
      "Epoch 617/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9412e-04 - my_r2: 0.9122 - val_loss: 4.6266e-05 - val_my_r2: 0.9839\n",
      "Epoch 618/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2504e-04 - my_r2: 0.9285 - val_loss: 4.4857e-05 - val_my_r2: 0.9844\n",
      "Epoch 619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5147e-04 - my_r2: 0.9359 - val_loss: 4.5348e-05 - val_my_r2: 0.9843\n",
      "Epoch 620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3176e-04 - my_r2: 0.8548 - val_loss: 3.9019e-05 - val_my_r2: 0.9859\n",
      "Epoch 621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5476e-04 - my_r2: 0.9382 - val_loss: 3.8979e-05 - val_my_r2: 0.9866\n",
      "Epoch 622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1568e-04 - my_r2: 0.9077 - val_loss: 4.1389e-05 - val_my_r2: 0.9863\n",
      "Epoch 623/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3302e-04 - my_r2: 0.9220 - val_loss: 3.8364e-05 - val_my_r2: 0.9871\n",
      "Epoch 624/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3730e-04 - my_r2: 0.8997 - val_loss: 3.7018e-05 - val_my_r2: 0.9870\n",
      "Epoch 625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1138e-04 - my_r2: 0.8887 - val_loss: 3.9084e-05 - val_my_r2: 0.9867\n",
      "Epoch 626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6898e-04 - my_r2: 0.9162 - val_loss: 4.2398e-05 - val_my_r2: 0.9857\n",
      "Epoch 627/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3703e-04 - my_r2: 0.9400 - val_loss: 3.9305e-05 - val_my_r2: 0.9870\n",
      "Epoch 628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8443e-04 - my_r2: 0.9110 - val_loss: 3.9685e-05 - val_my_r2: 0.9863\n",
      "Epoch 629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6774e-04 - my_r2: 0.9399 - val_loss: 3.7946e-05 - val_my_r2: 0.9862\n",
      "Epoch 630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7150e-04 - my_r2: 0.9075 - val_loss: 3.7348e-05 - val_my_r2: 0.9868\n",
      "Epoch 631/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5191e-04 - my_r2: 0.9365 - val_loss: 3.8699e-05 - val_my_r2: 0.9863\n",
      "Epoch 632/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6589e-04 - my_r2: 0.9288 - val_loss: 3.8749e-05 - val_my_r2: 0.9859\n",
      "Epoch 633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4107e-04 - my_r2: 0.9095 - val_loss: 3.7209e-05 - val_my_r2: 0.9863\n",
      "Epoch 634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0267e-04 - my_r2: 0.9281 - val_loss: 3.6794e-05 - val_my_r2: 0.9865\n",
      "Epoch 635/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8635e-04 - my_r2: 0.8886 - val_loss: 4.0214e-05 - val_my_r2: 0.9858\n",
      "Epoch 636/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4830e-04 - my_r2: 0.8228 - val_loss: 3.5580e-05 - val_my_r2: 0.9865\n",
      "Epoch 637/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9425e-04 - my_r2: 0.8994 - val_loss: 3.6611e-05 - val_my_r2: 0.9862\n",
      "Epoch 638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0870e-04 - my_r2: 0.9116 - val_loss: 3.8354e-05 - val_my_r2: 0.9854\n",
      "Epoch 639/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3016e-04 - my_r2: 0.8124 - val_loss: 3.6956e-05 - val_my_r2: 0.9858\n",
      "Epoch 640/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.8264e-04 - my_r2: 0.9321 - val_loss: 3.6842e-05 - val_my_r2: 0.9861\n",
      "Epoch 641/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7073e-04 - my_r2: 0.9341 - val_loss: 3.5157e-05 - val_my_r2: 0.9865\n",
      "Epoch 642/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6681e-04 - my_r2: 0.9145 - val_loss: 3.7652e-05 - val_my_r2: 0.9859\n",
      "Epoch 643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7249e-04 - my_r2: 0.9438 - val_loss: 3.9466e-05 - val_my_r2: 0.9849\n",
      "Epoch 644/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6800e-04 - my_r2: 0.9330 - val_loss: 4.6052e-05 - val_my_r2: 0.9827\n",
      "Epoch 645/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2902e-04 - my_r2: 0.9184 - val_loss: 3.8637e-05 - val_my_r2: 0.9853\n",
      "Epoch 646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3406e-04 - my_r2: 0.9302 - val_loss: 3.6721e-05 - val_my_r2: 0.9864\n",
      "Epoch 647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5633e-04 - my_r2: 0.9244 - val_loss: 3.4181e-05 - val_my_r2: 0.9873\n",
      "Epoch 648/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9037e-04 - my_r2: 0.9369 - val_loss: 3.5284e-05 - val_my_r2: 0.9869\n",
      "Epoch 649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2192e-04 - my_r2: 0.9095 - val_loss: 4.2935e-05 - val_my_r2: 0.9848\n",
      "Epoch 650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7976e-04 - my_r2: 0.9210 - val_loss: 4.2035e-05 - val_my_r2: 0.9853\n",
      "Epoch 651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9405e-04 - my_r2: 0.9161 - val_loss: 3.7530e-05 - val_my_r2: 0.9871\n",
      "Epoch 652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8255e-04 - my_r2: 0.9408 - val_loss: 4.0186e-05 - val_my_r2: 0.9858\n",
      "Epoch 653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8252e-04 - my_r2: 0.9291 - val_loss: 3.7088e-05 - val_my_r2: 0.9866\n",
      "Epoch 654/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1179e-04 - my_r2: 0.9227 - val_loss: 3.7876e-05 - val_my_r2: 0.9865\n",
      "Epoch 655/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8033e-04 - my_r2: 0.9241 - val_loss: 3.6741e-05 - val_my_r2: 0.9865\n",
      "Epoch 656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5922e-04 - my_r2: 0.9407 - val_loss: 3.5632e-05 - val_my_r2: 0.9865\n",
      "Epoch 657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4338e-04 - my_r2: 0.8904 - val_loss: 3.7662e-05 - val_my_r2: 0.9857\n",
      "Epoch 658/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.2868e-04 - my_r2: 0.8777 - val_loss: 3.7109e-05 - val_my_r2: 0.9859\n",
      "Epoch 659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1314e-04 - my_r2: 0.9168 - val_loss: 3.5686e-05 - val_my_r2: 0.9864\n",
      "Epoch 660/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6871e-04 - my_r2: 0.8963 - val_loss: 3.5788e-05 - val_my_r2: 0.9870\n",
      "Epoch 661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0825e-04 - my_r2: 0.9277 - val_loss: 3.7724e-05 - val_my_r2: 0.9861\n",
      "Epoch 662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3205e-04 - my_r2: 0.8625 - val_loss: 4.0077e-05 - val_my_r2: 0.9849\n",
      "Epoch 663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2538e-04 - my_r2: 0.9115 - val_loss: 3.6665e-05 - val_my_r2: 0.9862\n",
      "Epoch 664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6589e-04 - my_r2: 0.9475 - val_loss: 3.5255e-05 - val_my_r2: 0.9868\n",
      "Epoch 665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8015e-04 - my_r2: 0.9161 - val_loss: 3.5139e-05 - val_my_r2: 0.9870\n",
      "Epoch 666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1029e-04 - my_r2: 0.9232 - val_loss: 3.8467e-05 - val_my_r2: 0.9858\n",
      "Epoch 667/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1839e-04 - my_r2: 0.9290 - val_loss: 3.7895e-05 - val_my_r2: 0.9853\n",
      "Epoch 668/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4048e-04 - my_r2: 0.9399 - val_loss: 3.7369e-05 - val_my_r2: 0.9857\n",
      "Epoch 669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9355e-04 - my_r2: 0.9276 - val_loss: 4.1262e-05 - val_my_r2: 0.9844\n",
      "Epoch 670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2823e-04 - my_r2: 0.8952 - val_loss: 3.7460e-05 - val_my_r2: 0.9859\n",
      "Epoch 671/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.4701e-04 - my_r2: 0.9326 - val_loss: 3.5013e-05 - val_my_r2: 0.9866\n",
      "Epoch 672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3392e-04 - my_r2: 0.9506 - val_loss: 3.5590e-05 - val_my_r2: 0.9865\n",
      "Epoch 673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4935e-04 - my_r2: 0.9099 - val_loss: 3.9004e-05 - val_my_r2: 0.9845\n",
      "Epoch 674/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6487e-04 - my_r2: 0.8981 - val_loss: 3.4728e-05 - val_my_r2: 0.9860\n",
      "Epoch 675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7463e-04 - my_r2: 0.9284 - val_loss: 3.3391e-05 - val_my_r2: 0.9868\n",
      "Epoch 676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5781e-04 - my_r2: 0.9363 - val_loss: 3.5149e-05 - val_my_r2: 0.9866\n",
      "Epoch 677/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0815e-04 - my_r2: 0.7601 - val_loss: 3.4498e-05 - val_my_r2: 0.9866\n",
      "Epoch 678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2661e-04 - my_r2: 0.8663 - val_loss: 3.2685e-05 - val_my_r2: 0.9875\n",
      "Epoch 679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3727e-04 - my_r2: 0.8988 - val_loss: 3.4303e-05 - val_my_r2: 0.9864\n",
      "Epoch 680/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9559e-04 - my_r2: 0.9322 - val_loss: 3.5104e-05 - val_my_r2: 0.9862\n",
      "Epoch 681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4543e-04 - my_r2: 0.9434 - val_loss: 3.5706e-05 - val_my_r2: 0.9866\n",
      "Epoch 682/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9527e-04 - my_r2: 0.9258 - val_loss: 4.3589e-05 - val_my_r2: 0.9842\n",
      "Epoch 683/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8970e-04 - my_r2: 0.8986 - val_loss: 5.5487e-05 - val_my_r2: 0.9793\n",
      "Epoch 684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7005e-04 - my_r2: 0.9295 - val_loss: 3.8510e-05 - val_my_r2: 0.9857\n",
      "Epoch 685/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.4545e-04 - my_r2: 0.8989 - val_loss: 3.1721e-05 - val_my_r2: 0.9886\n",
      "Epoch 686/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7042e-04 - my_r2: 0.9088 - val_loss: 3.1537e-05 - val_my_r2: 0.9884\n",
      "Epoch 687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2356e-04 - my_r2: 0.8885 - val_loss: 3.1047e-05 - val_my_r2: 0.9885\n",
      "Epoch 688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3987e-04 - my_r2: 0.9284 - val_loss: 3.2940e-05 - val_my_r2: 0.9882\n",
      "Epoch 689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5449e-04 - my_r2: 0.9033 - val_loss: 3.5064e-05 - val_my_r2: 0.9871\n",
      "Epoch 690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3458e-04 - my_r2: 0.9334 - val_loss: 3.3956e-05 - val_my_r2: 0.9875\n",
      "Epoch 691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0117e-04 - my_r2: 0.9079 - val_loss: 3.4415e-05 - val_my_r2: 0.9874\n",
      "Epoch 692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9694e-04 - my_r2: 0.9298 - val_loss: 3.2292e-05 - val_my_r2: 0.9886\n",
      "Epoch 693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2465e-04 - my_r2: 0.8992 - val_loss: 3.2514e-05 - val_my_r2: 0.9885\n",
      "Epoch 694/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4124e-04 - my_r2: 0.9133 - val_loss: 3.4496e-05 - val_my_r2: 0.9875\n",
      "Epoch 695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0137e-04 - my_r2: 0.6899 - val_loss: 3.6114e-05 - val_my_r2: 0.9875\n",
      "Epoch 696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4475e-04 - my_r2: 0.9259 - val_loss: 2.9998e-05 - val_my_r2: 0.9893\n",
      "Epoch 697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8367e-04 - my_r2: 0.7376 - val_loss: 3.3663e-05 - val_my_r2: 0.9870\n",
      "Epoch 698/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1818e-04 - my_r2: 0.9167 - val_loss: 3.0969e-05 - val_my_r2: 0.9887\n",
      "Epoch 699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3242e-04 - my_r2: 0.4652 - val_loss: 3.5946e-05 - val_my_r2: 0.9876\n",
      "Epoch 700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8122e-04 - my_r2: 0.9170 - val_loss: 3.1917e-05 - val_my_r2: 0.9887\n",
      "Epoch 701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7428e-04 - my_r2: 0.9142 - val_loss: 3.3695e-05 - val_my_r2: 0.9877\n",
      "Epoch 702/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8724e-04 - my_r2: 0.9107 - val_loss: 3.5756e-05 - val_my_r2: 0.9869\n",
      "Epoch 703/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6658e-04 - my_r2: 0.8517 - val_loss: 3.2660e-05 - val_my_r2: 0.9876\n",
      "Epoch 704/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9493e-04 - my_r2: 0.9215 - val_loss: 3.4504e-05 - val_my_r2: 0.9867\n",
      "Epoch 705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3706e-04 - my_r2: 0.8647 - val_loss: 3.1474e-05 - val_my_r2: 0.9882\n",
      "Epoch 706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0852e-04 - my_r2: 0.9139 - val_loss: 3.0734e-05 - val_my_r2: 0.9887\n",
      "Epoch 707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8505e-04 - my_r2: 0.9213 - val_loss: 3.0031e-05 - val_my_r2: 0.9892\n",
      "Epoch 708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7251e-04 - my_r2: 0.9493 - val_loss: 3.3426e-05 - val_my_r2: 0.9883\n",
      "Epoch 709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0489e-04 - my_r2: 0.9350 - val_loss: 3.1462e-05 - val_my_r2: 0.9891\n",
      "Epoch 710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2164e-04 - my_r2: 0.8098 - val_loss: 2.9889e-05 - val_my_r2: 0.9895\n",
      "Epoch 711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6733e-04 - my_r2: 0.7996 - val_loss: 3.2658e-05 - val_my_r2: 0.9892\n",
      "Epoch 712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5480e-04 - my_r2: 0.9203 - val_loss: 2.9098e-05 - val_my_r2: 0.9899\n",
      "Epoch 713/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8795e-04 - my_r2: 0.9360 - val_loss: 3.1583e-05 - val_my_r2: 0.9895\n",
      "Epoch 714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9048e-04 - my_r2: 0.9093 - val_loss: 3.3160e-05 - val_my_r2: 0.9894\n",
      "Epoch 715/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3446e-04 - my_r2: 0.9625 - val_loss: 3.5402e-05 - val_my_r2: 0.9891\n",
      "Epoch 716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9814e-04 - my_r2: 0.7679 - val_loss: 3.1781e-05 - val_my_r2: 0.9895\n",
      "Epoch 717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7112e-04 - my_r2: 0.9488 - val_loss: 3.5889e-05 - val_my_r2: 0.9888\n",
      "Epoch 718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3022e-04 - my_r2: 0.8627 - val_loss: 3.0919e-05 - val_my_r2: 0.9899\n",
      "Epoch 719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0394e-04 - my_r2: 0.9292 - val_loss: 2.8761e-05 - val_my_r2: 0.9904\n",
      "Epoch 720/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3762e-04 - my_r2: 0.8380 - val_loss: 3.0445e-05 - val_my_r2: 0.9899\n",
      "Epoch 721/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8010e-04 - my_r2: 0.8422 - val_loss: 3.2784e-05 - val_my_r2: 0.9895\n",
      "Epoch 722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6656e-04 - my_r2: 0.9160 - val_loss: 2.9088e-05 - val_my_r2: 0.9901\n",
      "Epoch 723/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2776e-04 - my_r2: 0.9114 - val_loss: 2.8834e-05 - val_my_r2: 0.9902\n",
      "Epoch 724/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2595e-04 - my_r2: 0.9136 - val_loss: 2.8076e-05 - val_my_r2: 0.9898\n",
      "Epoch 725/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4256e-04 - my_r2: 0.9477 - val_loss: 3.1135e-05 - val_my_r2: 0.9889\n",
      "Epoch 726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8724e-04 - my_r2: 0.9344 - val_loss: 3.1096e-05 - val_my_r2: 0.9892\n",
      "Epoch 727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9754e-04 - my_r2: 0.9372 - val_loss: 2.9108e-05 - val_my_r2: 0.9891\n",
      "Epoch 728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9436e-04 - my_r2: 0.9285 - val_loss: 2.9226e-05 - val_my_r2: 0.9889\n",
      "Epoch 729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7891e-04 - my_r2: 0.9111 - val_loss: 2.9769e-05 - val_my_r2: 0.9888\n",
      "Epoch 730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2127e-04 - my_r2: 0.9275 - val_loss: 3.1801e-05 - val_my_r2: 0.9876\n",
      "Epoch 731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4173e-04 - my_r2: 0.9211 - val_loss: 3.0566e-05 - val_my_r2: 0.9888\n",
      "Epoch 732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4635e-04 - my_r2: 0.9426 - val_loss: 2.7959e-05 - val_my_r2: 0.9896\n",
      "Epoch 733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4603e-04 - my_r2: 0.9259 - val_loss: 2.7522e-05 - val_my_r2: 0.9897\n",
      "Epoch 734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4937e-04 - my_r2: 0.9532 - val_loss: 3.0931e-05 - val_my_r2: 0.9887\n",
      "Epoch 735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5265e-04 - my_r2: 0.9296 - val_loss: 2.8919e-05 - val_my_r2: 0.9895\n",
      "Epoch 736/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1860e-04 - my_r2: 0.9270 - val_loss: 2.8295e-05 - val_my_r2: 0.9895\n",
      "Epoch 737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5351e-04 - my_r2: 0.9405 - val_loss: 3.0366e-05 - val_my_r2: 0.9888\n",
      "Epoch 738/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2871e-04 - my_r2: 0.8859 - val_loss: 2.9198e-05 - val_my_r2: 0.9890\n",
      "Epoch 739/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0575e-04 - my_r2: 0.9350 - val_loss: 3.1915e-05 - val_my_r2: 0.9883\n",
      "Epoch 740/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9947e-04 - my_r2: 0.9242 - val_loss: 3.0071e-05 - val_my_r2: 0.9885\n",
      "Epoch 741/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0036e-04 - my_r2: 0.9060 - val_loss: 3.2121e-05 - val_my_r2: 0.9872\n",
      "Epoch 742/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5860e-04 - my_r2: 0.9276 - val_loss: 2.9232e-05 - val_my_r2: 0.9888\n",
      "Epoch 743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6616e-04 - my_r2: 0.9085 - val_loss: 3.1392e-05 - val_my_r2: 0.9880\n",
      "Epoch 744/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2579e-04 - my_r2: 0.9482 - val_loss: 3.5730e-05 - val_my_r2: 0.9867\n",
      "Epoch 745/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8816e-04 - my_r2: 0.9145 - val_loss: 2.8188e-05 - val_my_r2: 0.9891\n",
      "Epoch 746/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0418e-04 - my_r2: 0.9001 - val_loss: 2.6082e-05 - val_my_r2: 0.9899\n",
      "Epoch 747/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.1058e-04 - my_r2: 0.9003 - val_loss: 2.5790e-05 - val_my_r2: 0.9900\n",
      "Epoch 748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5163e-04 - my_r2: 0.9202 - val_loss: 2.7585e-05 - val_my_r2: 0.9899\n",
      "Epoch 749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4335e-04 - my_r2: 0.9095 - val_loss: 3.7529e-05 - val_my_r2: 0.9868\n",
      "Epoch 750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8805e-04 - my_r2: 0.9150 - val_loss: 2.6690e-05 - val_my_r2: 0.9906\n",
      "Epoch 751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7744e-04 - my_r2: 0.9224 - val_loss: 2.5623e-05 - val_my_r2: 0.9903\n",
      "Epoch 752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8569e-04 - my_r2: 0.9313 - val_loss: 2.4470e-05 - val_my_r2: 0.9913\n",
      "Epoch 753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0204e-04 - my_r2: 0.9149 - val_loss: 2.4674e-05 - val_my_r2: 0.9915\n",
      "Epoch 754/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.2491e-04 - my_r2: 0.9232 - val_loss: 2.5600e-05 - val_my_r2: 0.9910\n",
      "Epoch 755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3884e-04 - my_r2: 0.8726 - val_loss: 2.7203e-05 - val_my_r2: 0.9904\n",
      "Epoch 756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1719e-04 - my_r2: 0.9168 - val_loss: 2.6629e-05 - val_my_r2: 0.9902\n",
      "Epoch 757/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2814e-04 - my_r2: 0.8846 - val_loss: 2.9497e-05 - val_my_r2: 0.9887\n",
      "Epoch 758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1055e-04 - my_r2: 0.9331 - val_loss: 2.9009e-05 - val_my_r2: 0.9888\n",
      "Epoch 759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8069e-04 - my_r2: 0.8999 - val_loss: 2.5016e-05 - val_my_r2: 0.9912\n",
      "Epoch 760/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8132e-04 - my_r2: 0.9345 - val_loss: 2.5758e-05 - val_my_r2: 0.9904\n",
      "Epoch 761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4280e-04 - my_r2: 0.8388 - val_loss: 2.6914e-05 - val_my_r2: 0.9894\n",
      "Epoch 762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1700e-04 - my_r2: 0.9333 - val_loss: 2.8078e-05 - val_my_r2: 0.9887\n",
      "Epoch 763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1550e-04 - my_r2: 0.9279 - val_loss: 2.7302e-05 - val_my_r2: 0.9903\n",
      "Epoch 764/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2546e-04 - my_r2: 0.9529 - val_loss: 3.1024e-05 - val_my_r2: 0.9897\n",
      "Epoch 765/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3193e-04 - my_r2: 0.9227 - val_loss: 2.9067e-05 - val_my_r2: 0.9897\n",
      "Epoch 766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4286e-04 - my_r2: 0.8823 - val_loss: 3.0727e-05 - val_my_r2: 0.9892\n",
      "Epoch 767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9572e-04 - my_r2: 0.9278 - val_loss: 3.1300e-05 - val_my_r2: 0.9885\n",
      "Epoch 768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4416e-04 - my_r2: 0.9474 - val_loss: 3.0841e-05 - val_my_r2: 0.9891\n",
      "Epoch 769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6803e-04 - my_r2: 0.9018 - val_loss: 3.7054e-05 - val_my_r2: 0.9875\n",
      "Epoch 770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1818e-04 - my_r2: 0.9335 - val_loss: 2.7555e-05 - val_my_r2: 0.9899\n",
      "Epoch 771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4315e-04 - my_r2: 0.8466 - val_loss: 2.5415e-05 - val_my_r2: 0.9908\n",
      "Epoch 772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6500e-04 - my_r2: 0.8677 - val_loss: 2.4613e-05 - val_my_r2: 0.9914\n",
      "Epoch 773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8619e-04 - my_r2: 0.9041 - val_loss: 2.8933e-05 - val_my_r2: 0.9901\n",
      "Epoch 774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5322e-04 - my_r2: 0.9100 - val_loss: 2.9534e-05 - val_my_r2: 0.9898\n",
      "Epoch 775/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7138e-04 - my_r2: 0.9298 - val_loss: 2.8049e-05 - val_my_r2: 0.9903\n",
      "Epoch 776/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5744e-04 - my_r2: 0.9436 - val_loss: 2.5766e-05 - val_my_r2: 0.9910\n",
      "Epoch 777/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8528e-04 - my_r2: 0.9190 - val_loss: 2.5590e-05 - val_my_r2: 0.9912\n",
      "Epoch 778/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4198e-04 - my_r2: 0.9355 - val_loss: 2.6908e-05 - val_my_r2: 0.9902\n",
      "Epoch 779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8391e-04 - my_r2: 0.9277 - val_loss: 2.8073e-05 - val_my_r2: 0.9895\n",
      "Epoch 780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0834e-04 - my_r2: 0.9175 - val_loss: 3.0159e-05 - val_my_r2: 0.9893\n",
      "Epoch 781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2172e-04 - my_r2: 0.9531 - val_loss: 2.8055e-05 - val_my_r2: 0.9900\n",
      "Epoch 782/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5387e-04 - my_r2: 0.8201 - val_loss: 3.5023e-05 - val_my_r2: 0.9873\n",
      "Epoch 783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2809e-04 - my_r2: 0.8202 - val_loss: 2.8330e-05 - val_my_r2: 0.9897\n",
      "Epoch 784/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5271e-04 - my_r2: 0.8894 - val_loss: 2.4617e-05 - val_my_r2: 0.9907\n",
      "Epoch 785/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.4110e-04 - my_r2: 0.9354 - val_loss: 2.4015e-05 - val_my_r2: 0.9906\n",
      "Epoch 786/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8497e-04 - my_r2: 0.9244 - val_loss: 3.0372e-05 - val_my_r2: 0.9893\n",
      "Epoch 787/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1082e-04 - my_r2: 0.9247 - val_loss: 2.5516e-05 - val_my_r2: 0.9907\n",
      "Epoch 788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5756e-04 - my_r2: 0.8505 - val_loss: 2.5584e-05 - val_my_r2: 0.9907\n",
      "Epoch 789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9848e-04 - my_r2: 0.8835 - val_loss: 2.6953e-05 - val_my_r2: 0.9899\n",
      "Epoch 790/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.1496e-04 - my_r2: 0.9525 - val_loss: 2.7518e-05 - val_my_r2: 0.9897\n",
      "Epoch 791/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6785e-04 - my_r2: 0.9176 - val_loss: 3.0591e-05 - val_my_r2: 0.9887\n",
      "Epoch 792/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0362e-04 - my_r2: 0.9075 - val_loss: 2.6684e-05 - val_my_r2: 0.9906\n",
      "Epoch 793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6372e-04 - my_r2: 0.8906 - val_loss: 2.5533e-05 - val_my_r2: 0.9912\n",
      "Epoch 794/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4214e-04 - my_r2: 0.9413 - val_loss: 2.8212e-05 - val_my_r2: 0.9901\n",
      "Epoch 795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8395e-04 - my_r2: 0.9331 - val_loss: 2.5138e-05 - val_my_r2: 0.9912\n",
      "Epoch 796/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3813e-04 - my_r2: 0.9510 - val_loss: 2.5828e-05 - val_my_r2: 0.9908\n",
      "Epoch 797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6910e-04 - my_r2: 0.9296 - val_loss: 2.3995e-05 - val_my_r2: 0.9915\n",
      "Epoch 798/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4558e-04 - my_r2: 0.8541 - val_loss: 2.5893e-05 - val_my_r2: 0.9908\n",
      "Epoch 799/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.6532e-04 - my_r2: 0.9121 - val_loss: 2.6049e-05 - val_my_r2: 0.9905\n",
      "Epoch 800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0168e-04 - my_r2: 0.8948 - val_loss: 3.0059e-05 - val_my_r2: 0.9900\n",
      "Epoch 801/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0791e-04 - my_r2: 0.9494 - val_loss: 3.3459e-05 - val_my_r2: 0.9891\n",
      "Epoch 802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5904e-04 - my_r2: 0.9357 - val_loss: 2.3342e-05 - val_my_r2: 0.9920\n",
      "Epoch 803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9130e-04 - my_r2: 0.9251 - val_loss: 2.5268e-05 - val_my_r2: 0.9917\n",
      "Epoch 804/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1998e-04 - my_r2: 0.9208 - val_loss: 2.5226e-05 - val_my_r2: 0.9916\n",
      "Epoch 805/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3444e-04 - my_r2: 0.9485 - val_loss: 2.5648e-05 - val_my_r2: 0.9917\n",
      "Epoch 806/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7248e-04 - my_r2: 0.9455 - val_loss: 2.5475e-05 - val_my_r2: 0.9915\n",
      "Epoch 807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1542e-04 - my_r2: 0.9321 - val_loss: 2.7194e-05 - val_my_r2: 0.9909\n",
      "Epoch 808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8549e-04 - my_r2: 0.9447 - val_loss: 2.3674e-05 - val_my_r2: 0.9915\n",
      "Epoch 809/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4578e-04 - my_r2: 0.9227 - val_loss: 2.4329e-05 - val_my_r2: 0.9910\n",
      "Epoch 810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1524e-04 - my_r2: 0.8865 - val_loss: 2.8468e-05 - val_my_r2: 0.9903\n",
      "Epoch 811/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1714e-04 - my_r2: 0.9269 - val_loss: 2.2213e-05 - val_my_r2: 0.9923\n",
      "Epoch 812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5158e-04 - my_r2: 0.9286 - val_loss: 2.1970e-05 - val_my_r2: 0.9922\n",
      "Epoch 813/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.7515e-04 - my_r2: 0.9384 - val_loss: 2.2249e-05 - val_my_r2: 0.9921\n",
      "Epoch 814/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5216e-04 - my_r2: 0.9278 - val_loss: 2.1085e-05 - val_my_r2: 0.9922\n",
      "Epoch 815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7033e-04 - my_r2: 0.9364 - val_loss: 2.3242e-05 - val_my_r2: 0.9914\n",
      "Epoch 816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8671e-04 - my_r2: 0.9029 - val_loss: 2.5502e-05 - val_my_r2: 0.9907\n",
      "Epoch 817/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6081e-04 - my_r2: 0.9111 - val_loss: 2.4548e-05 - val_my_r2: 0.9910\n",
      "Epoch 818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2283e-04 - my_r2: 0.9537 - val_loss: 2.1216e-05 - val_my_r2: 0.9922\n",
      "Epoch 819/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2411e-04 - my_r2: 0.8983 - val_loss: 2.0946e-05 - val_my_r2: 0.9925\n",
      "Epoch 820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1627e-04 - my_r2: 0.9118 - val_loss: 2.0664e-05 - val_my_r2: 0.9926\n",
      "Epoch 821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2489e-04 - my_r2: 0.9303 - val_loss: 2.4724e-05 - val_my_r2: 0.9918\n",
      "Epoch 822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5290e-04 - my_r2: 0.9130 - val_loss: 2.9156e-05 - val_my_r2: 0.9908\n",
      "Epoch 823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1159e-04 - my_r2: 0.9566 - val_loss: 2.5724e-05 - val_my_r2: 0.9917\n",
      "Epoch 824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1198e-04 - my_r2: 0.9157 - val_loss: 2.5658e-05 - val_my_r2: 0.9912\n",
      "Epoch 825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6573e-04 - my_r2: 0.9291 - val_loss: 2.5458e-05 - val_my_r2: 0.9911\n",
      "Epoch 826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5703e-04 - my_r2: 0.9398 - val_loss: 2.7971e-05 - val_my_r2: 0.9909\n",
      "Epoch 827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3958e-04 - my_r2: 0.9380 - val_loss: 2.6492e-05 - val_my_r2: 0.9916\n",
      "Epoch 828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4634e-04 - my_r2: 0.9367 - val_loss: 2.6615e-05 - val_my_r2: 0.9920\n",
      "Epoch 829/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5392e-04 - my_r2: 0.9353 - val_loss: 2.6230e-05 - val_my_r2: 0.9919\n",
      "Epoch 830/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1153e-04 - my_r2: 0.9185 - val_loss: 2.6200e-05 - val_my_r2: 0.9915\n",
      "Epoch 831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9259e-04 - my_r2: 0.8888 - val_loss: 2.2450e-05 - val_my_r2: 0.9923\n",
      "Epoch 832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9512e-04 - my_r2: 0.9375 - val_loss: 2.0593e-05 - val_my_r2: 0.9927\n",
      "Epoch 833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2976e-04 - my_r2: 0.9305 - val_loss: 2.4472e-05 - val_my_r2: 0.9915\n",
      "Epoch 834/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9458e-04 - my_r2: 0.9316 - val_loss: 2.5258e-05 - val_my_r2: 0.9912\n",
      "Epoch 835/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6358e-04 - my_r2: 0.9490 - val_loss: 2.5178e-05 - val_my_r2: 0.9902\n",
      "Epoch 836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0248e-04 - my_r2: 0.8800 - val_loss: 2.4572e-05 - val_my_r2: 0.9903\n",
      "Epoch 837/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7523e-04 - my_r2: 0.9123 - val_loss: 2.1777e-05 - val_my_r2: 0.9913\n",
      "Epoch 838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9887e-04 - my_r2: 0.9045 - val_loss: 2.4807e-05 - val_my_r2: 0.9909\n",
      "Epoch 839/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4001e-04 - my_r2: 0.9058 - val_loss: 2.3167e-05 - val_my_r2: 0.9911\n",
      "Epoch 840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0073e-04 - my_r2: 0.9112 - val_loss: 2.2169e-05 - val_my_r2: 0.9914\n",
      "Epoch 841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0568e-04 - my_r2: 0.8321 - val_loss: 2.2264e-05 - val_my_r2: 0.9914\n",
      "Epoch 842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9776e-04 - my_r2: 0.9355 - val_loss: 2.8752e-05 - val_my_r2: 0.9891\n",
      "Epoch 843/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1301e-04 - my_r2: 0.9073 - val_loss: 2.2992e-05 - val_my_r2: 0.9912\n",
      "Epoch 844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0163e-04 - my_r2: 0.9251 - val_loss: 2.1922e-05 - val_my_r2: 0.9915\n",
      "Epoch 845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4948e-04 - my_r2: 0.8751 - val_loss: 2.5563e-05 - val_my_r2: 0.9902\n",
      "Epoch 846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9123e-04 - my_r2: 0.8933 - val_loss: 2.7528e-05 - val_my_r2: 0.9893\n",
      "Epoch 847/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7949e-04 - my_r2: 0.9336 - val_loss: 2.3536e-05 - val_my_r2: 0.9906\n",
      "Epoch 848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5519e-04 - my_r2: 0.8740 - val_loss: 2.3617e-05 - val_my_r2: 0.9912\n",
      "Epoch 849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8628e-04 - my_r2: 0.9209 - val_loss: 2.6166e-05 - val_my_r2: 0.9905\n",
      "Epoch 850/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0267e-04 - my_r2: 0.9200 - val_loss: 2.3078e-05 - val_my_r2: 0.9908\n",
      "Epoch 851/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2843e-04 - my_r2: 0.9543 - val_loss: 2.4391e-05 - val_my_r2: 0.9905\n",
      "Epoch 852/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4429e-04 - my_r2: 0.8757 - val_loss: 2.1778e-05 - val_my_r2: 0.9916\n",
      "Epoch 853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4445e-04 - my_r2: 0.8246 - val_loss: 2.6979e-05 - val_my_r2: 0.9903\n",
      "Epoch 854/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8409e-04 - my_r2: 0.9261 - val_loss: 2.9007e-05 - val_my_r2: 0.9893\n",
      "Epoch 855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5265e-04 - my_r2: 0.9180 - val_loss: 2.3504e-05 - val_my_r2: 0.9907\n",
      "Epoch 856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7694e-04 - my_r2: 0.9092 - val_loss: 2.2711e-05 - val_my_r2: 0.9914\n",
      "Epoch 857/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7003e-04 - my_r2: 0.9050 - val_loss: 2.5921e-05 - val_my_r2: 0.9903\n",
      "Epoch 858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4879e-04 - my_r2: 0.9346 - val_loss: 2.4820e-05 - val_my_r2: 0.9906\n",
      "Epoch 859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9275e-04 - my_r2: 0.9155 - val_loss: 2.1710e-05 - val_my_r2: 0.9920\n",
      "Epoch 860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2830e-04 - my_r2: 0.9464 - val_loss: 2.3223e-05 - val_my_r2: 0.9911\n",
      "Epoch 861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3702e-04 - my_r2: 0.9201 - val_loss: 2.6560e-05 - val_my_r2: 0.9899\n",
      "Epoch 862/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5327e-04 - my_r2: 0.8719 - val_loss: 3.5292e-05 - val_my_r2: 0.9869\n",
      "Epoch 863/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6110e-04 - my_r2: 0.9463 - val_loss: 2.6905e-05 - val_my_r2: 0.9897\n",
      "Epoch 864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6061e-04 - my_r2: 0.9307 - val_loss: 2.2178e-05 - val_my_r2: 0.9915\n",
      "Epoch 865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0425e-04 - my_r2: 0.9200 - val_loss: 2.5344e-05 - val_my_r2: 0.9908\n",
      "Epoch 866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5825e-04 - my_r2: 0.9009 - val_loss: 2.6123e-05 - val_my_r2: 0.9903\n",
      "Epoch 867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6121e-04 - my_r2: 0.8670 - val_loss: 2.5809e-05 - val_my_r2: 0.9906\n",
      "Epoch 868/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2508e-04 - my_r2: 0.9224 - val_loss: 2.5179e-05 - val_my_r2: 0.9914\n",
      "Epoch 869/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0265e-04 - my_r2: 0.8985 - val_loss: 3.5799e-05 - val_my_r2: 0.9885\n",
      "Epoch 870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4287e-04 - my_r2: 0.9154 - val_loss: 2.5238e-05 - val_my_r2: 0.9919\n",
      "Epoch 871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8736e-04 - my_r2: 0.9107 - val_loss: 2.0618e-05 - val_my_r2: 0.9928\n",
      "Epoch 872/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6121e-04 - my_r2: 0.9179 - val_loss: 2.2316e-05 - val_my_r2: 0.9919\n",
      "Epoch 873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7005e-04 - my_r2: 0.9180 - val_loss: 2.7370e-05 - val_my_r2: 0.9894\n",
      "Epoch 874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3950e-04 - my_r2: 0.9420 - val_loss: 2.6738e-05 - val_my_r2: 0.9899\n",
      "Epoch 875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8888e-04 - my_r2: 0.9195 - val_loss: 3.0903e-05 - val_my_r2: 0.9881\n",
      "Epoch 876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6516e-04 - my_r2: 0.9329 - val_loss: 2.8155e-05 - val_my_r2: 0.9892\n",
      "Epoch 877/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4920e-04 - my_r2: 0.9482 - val_loss: 2.5729e-05 - val_my_r2: 0.9896\n",
      "Epoch 878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8644e-04 - my_r2: 0.9239 - val_loss: 2.5667e-05 - val_my_r2: 0.9907\n",
      "Epoch 879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4011e-04 - my_r2: 0.9065 - val_loss: 2.3970e-05 - val_my_r2: 0.9913\n",
      "Epoch 880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7740e-04 - my_r2: 0.9232 - val_loss: 2.3921e-05 - val_my_r2: 0.9914\n",
      "Epoch 881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5350e-04 - my_r2: 0.9504 - val_loss: 2.7191e-05 - val_my_r2: 0.9901\n",
      "Epoch 882/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.3313e-04 - my_r2: 0.8883 - val_loss: 2.6086e-05 - val_my_r2: 0.9906\n",
      "Epoch 883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5148e-04 - my_r2: 0.9205 - val_loss: 2.3593e-05 - val_my_r2: 0.9913\n",
      "Epoch 884/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4261e-04 - my_r2: 0.8918 - val_loss: 2.3402e-05 - val_my_r2: 0.9915\n",
      "Epoch 885/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3452e-04 - my_r2: 0.9158 - val_loss: 2.4627e-05 - val_my_r2: 0.9909\n",
      "Epoch 886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8856e-04 - my_r2: 0.8991 - val_loss: 2.6467e-05 - val_my_r2: 0.9898\n",
      "Epoch 887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7268e-04 - my_r2: 0.9387 - val_loss: 2.5152e-05 - val_my_r2: 0.9902\n",
      "Epoch 888/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6597e-04 - my_r2: 0.8159 - val_loss: 2.1697e-05 - val_my_r2: 0.9915\n",
      "Epoch 889/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6987e-04 - my_r2: 0.9333 - val_loss: 2.1366e-05 - val_my_r2: 0.9916\n",
      "Epoch 890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3368e-04 - my_r2: 0.9174 - val_loss: 2.0267e-05 - val_my_r2: 0.9919\n",
      "Epoch 891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0002e-04 - my_r2: 0.8403 - val_loss: 1.9745e-05 - val_my_r2: 0.9926\n",
      "Epoch 892/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9348e-04 - my_r2: 0.9386 - val_loss: 1.9329e-05 - val_my_r2: 0.9927\n",
      "Epoch 893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4133e-04 - my_r2: 0.9219 - val_loss: 2.1873e-05 - val_my_r2: 0.9919\n",
      "Epoch 894/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.3632e-04 - my_r2: 0.9274 - val_loss: 2.2471e-05 - val_my_r2: 0.9919\n",
      "Epoch 895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3946e-04 - my_r2: 0.9362 - val_loss: 1.9724e-05 - val_my_r2: 0.9929\n",
      "Epoch 896/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5815e-04 - my_r2: 0.9380 - val_loss: 2.2420e-05 - val_my_r2: 0.9925\n",
      "Epoch 897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8008e-04 - my_r2: 0.9014 - val_loss: 2.0186e-05 - val_my_r2: 0.9929\n",
      "Epoch 898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5019e-04 - my_r2: 0.9208 - val_loss: 1.8441e-05 - val_my_r2: 0.9937\n",
      "Epoch 899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1695e-04 - my_r2: 0.9340 - val_loss: 1.8839e-05 - val_my_r2: 0.9935\n",
      "Epoch 900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1730e-04 - my_r2: 0.9276 - val_loss: 2.0352e-05 - val_my_r2: 0.9928\n",
      "Epoch 901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3420e-04 - my_r2: 0.9107 - val_loss: 2.0990e-05 - val_my_r2: 0.9926\n",
      "Epoch 902/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8582e-04 - my_r2: 0.9250 - val_loss: 2.1276e-05 - val_my_r2: 0.9924\n",
      "Epoch 903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2317e-04 - my_r2: 0.9396 - val_loss: 2.2152e-05 - val_my_r2: 0.9920\n",
      "Epoch 904/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7008e-04 - my_r2: 0.9405 - val_loss: 2.3279e-05 - val_my_r2: 0.9918\n",
      "Epoch 905/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1463e-04 - my_r2: 0.9388 - val_loss: 2.7917e-05 - val_my_r2: 0.9904\n",
      "Epoch 906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1288e-04 - my_r2: 0.9511 - val_loss: 2.1783e-05 - val_my_r2: 0.9922\n",
      "Epoch 907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5509e-04 - my_r2: 0.9152 - val_loss: 2.1869e-05 - val_my_r2: 0.9920\n",
      "Epoch 908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8786e-04 - my_r2: 0.9183 - val_loss: 2.1500e-05 - val_my_r2: 0.9926\n",
      "Epoch 909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6948e-04 - my_r2: 0.8968 - val_loss: 2.6052e-05 - val_my_r2: 0.9910\n",
      "Epoch 910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6841e-04 - my_r2: 0.9450 - val_loss: 2.6537e-05 - val_my_r2: 0.9906\n",
      "Epoch 911/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4903e-04 - my_r2: 0.9251 - val_loss: 2.2952e-05 - val_my_r2: 0.9915\n",
      "Epoch 912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7777e-04 - my_r2: 0.9382 - val_loss: 2.7623e-05 - val_my_r2: 0.9903\n",
      "Epoch 913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9845e-04 - my_r2: 0.9271 - val_loss: 2.0407e-05 - val_my_r2: 0.9921\n",
      "Epoch 914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5194e-04 - my_r2: 0.9378 - val_loss: 2.0668e-05 - val_my_r2: 0.9919\n",
      "Epoch 915/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2497e-04 - my_r2: 0.9227 - val_loss: 2.0774e-05 - val_my_r2: 0.9919\n",
      "Epoch 916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6965e-04 - my_r2: 0.9293 - val_loss: 2.2820e-05 - val_my_r2: 0.9905\n",
      "Epoch 917/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8183e-04 - my_r2: 0.9329 - val_loss: 2.4668e-05 - val_my_r2: 0.9903\n",
      "Epoch 918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8342e-04 - my_r2: 0.9342 - val_loss: 3.2794e-05 - val_my_r2: 0.9880\n",
      "Epoch 919/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0417e-04 - my_r2: 0.9055 - val_loss: 2.1286e-05 - val_my_r2: 0.9922\n",
      "Epoch 920/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7477e-04 - my_r2: 0.9259 - val_loss: 1.8555e-05 - val_my_r2: 0.9932\n",
      "Epoch 921/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9633e-04 - my_r2: 0.9119 - val_loss: 1.9740e-05 - val_my_r2: 0.9929\n",
      "Epoch 922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4635e-04 - my_r2: 0.8938 - val_loss: 2.1110e-05 - val_my_r2: 0.9920\n",
      "Epoch 923/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8179e-04 - my_r2: 0.8989 - val_loss: 2.2829e-05 - val_my_r2: 0.9912\n",
      "Epoch 924/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4379e-04 - my_r2: 0.9469 - val_loss: 1.7657e-05 - val_my_r2: 0.9931\n",
      "Epoch 925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9144e-04 - my_r2: 0.9166 - val_loss: 2.7547e-05 - val_my_r2: 0.9901\n",
      "Epoch 926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5280e-04 - my_r2: 0.9394 - val_loss: 2.2477e-05 - val_my_r2: 0.9920\n",
      "Epoch 927/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5079e-04 - my_r2: 0.8484 - val_loss: 1.8276e-05 - val_my_r2: 0.9935\n",
      "Epoch 928/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1492e-04 - my_r2: 0.9479 - val_loss: 1.6787e-05 - val_my_r2: 0.9938\n",
      "Epoch 929/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9887e-04 - my_r2: 0.9279 - val_loss: 1.9489e-05 - val_my_r2: 0.9923\n",
      "Epoch 930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7934e-04 - my_r2: 0.9166 - val_loss: 1.9899e-05 - val_my_r2: 0.9922\n",
      "Epoch 931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1762e-04 - my_r2: 0.9383 - val_loss: 1.7096e-05 - val_my_r2: 0.9934\n",
      "Epoch 932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6658e-04 - my_r2: 0.9155 - val_loss: 1.9762e-05 - val_my_r2: 0.9924\n",
      "Epoch 933/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6573e-04 - my_r2: 0.9083 - val_loss: 2.1838e-05 - val_my_r2: 0.9922\n",
      "Epoch 934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6143e-04 - my_r2: 0.8994 - val_loss: 1.9362e-05 - val_my_r2: 0.9932\n",
      "Epoch 935/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2144e-04 - my_r2: 0.9108 - val_loss: 1.8779e-05 - val_my_r2: 0.9934\n",
      "Epoch 936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3111e-04 - my_r2: 0.9369 - val_loss: 2.4226e-05 - val_my_r2: 0.9913\n",
      "Epoch 937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9974e-04 - my_r2: 0.9203 - val_loss: 2.2878e-05 - val_my_r2: 0.9923\n",
      "Epoch 938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2888e-04 - my_r2: 0.9384 - val_loss: 2.1186e-05 - val_my_r2: 0.9929\n",
      "Epoch 939/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.9402e-04 - my_r2: 0.9429 - val_loss: 2.0757e-05 - val_my_r2: 0.9926\n",
      "Epoch 940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2572e-04 - my_r2: 0.9544 - val_loss: 1.8938e-05 - val_my_r2: 0.9936\n",
      "Epoch 941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3621e-04 - my_r2: 0.9396 - val_loss: 2.0229e-05 - val_my_r2: 0.9931\n",
      "Epoch 942/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8481e-04 - my_r2: 0.9338 - val_loss: 1.9850e-05 - val_my_r2: 0.9930\n",
      "Epoch 943/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1737e-04 - my_r2: 0.9531 - val_loss: 2.2320e-05 - val_my_r2: 0.9919\n",
      "Epoch 944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3578e-04 - my_r2: 0.9211 - val_loss: 2.2567e-05 - val_my_r2: 0.9915\n",
      "Epoch 945/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4451e-04 - my_r2: 0.9376 - val_loss: 1.8487e-05 - val_my_r2: 0.9930\n",
      "Epoch 946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0246e-04 - my_r2: 0.9339 - val_loss: 1.7994e-05 - val_my_r2: 0.9932\n",
      "Epoch 947/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7482e-04 - my_r2: 0.6117 - val_loss: 2.2413e-05 - val_my_r2: 0.9924\n",
      "Epoch 948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7270e-04 - my_r2: 0.9295 - val_loss: 2.2459e-05 - val_my_r2: 0.9926\n",
      "Epoch 949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2487e-04 - my_r2: 0.9473 - val_loss: 1.9270e-05 - val_my_r2: 0.9935\n",
      "Epoch 950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9368e-04 - my_r2: 0.9094 - val_loss: 1.7823e-05 - val_my_r2: 0.9938\n",
      "Epoch 951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7780e-04 - my_r2: 0.9231 - val_loss: 2.0301e-05 - val_my_r2: 0.9927\n",
      "Epoch 952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5573e-04 - my_r2: 0.9398 - val_loss: 2.7467e-05 - val_my_r2: 0.9906\n",
      "Epoch 953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2693e-04 - my_r2: 0.8964 - val_loss: 1.8190e-05 - val_my_r2: 0.9938\n",
      "Epoch 954/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7558e-04 - my_r2: 0.9478 - val_loss: 1.7610e-05 - val_my_r2: 0.9938\n",
      "Epoch 955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6453e-04 - my_r2: 0.9384 - val_loss: 1.7165e-05 - val_my_r2: 0.9939\n",
      "Epoch 956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8819e-04 - my_r2: 0.8779 - val_loss: 1.9882e-05 - val_my_r2: 0.9929\n",
      "Epoch 957/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.7237e-04 - my_r2: 0.9305 - val_loss: 2.0933e-05 - val_my_r2: 0.9923\n",
      "Epoch 958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2885e-04 - my_r2: 0.9549 - val_loss: 1.8195e-05 - val_my_r2: 0.9928\n",
      "Epoch 959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0928e-04 - my_r2: 0.9217 - val_loss: 2.3593e-05 - val_my_r2: 0.9900\n",
      "Epoch 960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6493e-04 - my_r2: 0.9379 - val_loss: 1.7181e-05 - val_my_r2: 0.9933\n",
      "Epoch 961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5502e-04 - my_r2: 0.9205 - val_loss: 1.8234e-05 - val_my_r2: 0.9931\n",
      "Epoch 962/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7485e-04 - my_r2: 0.9039 - val_loss: 1.6696e-05 - val_my_r2: 0.9942\n",
      "Epoch 963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9329e-04 - my_r2: 0.9232 - val_loss: 1.5890e-05 - val_my_r2: 0.9946\n",
      "Epoch 964/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7424e-04 - my_r2: 0.9344 - val_loss: 1.6763e-05 - val_my_r2: 0.9945\n",
      "Epoch 965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0644e-04 - my_r2: 0.9376 - val_loss: 1.5609e-05 - val_my_r2: 0.9945\n",
      "Epoch 966/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0234e-04 - my_r2: 0.9330 - val_loss: 1.6031e-05 - val_my_r2: 0.9941\n",
      "Epoch 967/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.6205e-04 - my_r2: 0.9361 - val_loss: 1.7416e-05 - val_my_r2: 0.9938\n",
      "Epoch 968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0353e-04 - my_r2: 0.9250 - val_loss: 1.7490e-05 - val_my_r2: 0.9938\n",
      "Epoch 969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4539e-04 - my_r2: 0.9223 - val_loss: 1.9744e-05 - val_my_r2: 0.9928\n",
      "Epoch 970/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3438e-04 - my_r2: 0.9386 - val_loss: 2.1706e-05 - val_my_r2: 0.9919\n",
      "Epoch 971/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7195e-04 - my_r2: 0.9218 - val_loss: 2.3256e-05 - val_my_r2: 0.9914\n",
      "Epoch 972/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0877e-04 - my_r2: 0.9250 - val_loss: 1.8084e-05 - val_my_r2: 0.9937\n",
      "Epoch 973/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.1008e-04 - my_r2: 0.9271 - val_loss: 1.8036e-05 - val_my_r2: 0.9937\n",
      "Epoch 974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4949e-04 - my_r2: 0.9470 - val_loss: 2.0247e-05 - val_my_r2: 0.9929\n",
      "Epoch 975/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4936e-04 - my_r2: 0.9125 - val_loss: 2.4626e-05 - val_my_r2: 0.9909\n",
      "Epoch 976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0445e-04 - my_r2: 0.8928 - val_loss: 1.7502e-05 - val_my_r2: 0.9936\n",
      "Epoch 977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1411e-04 - my_r2: 0.9290 - val_loss: 1.5906e-05 - val_my_r2: 0.9941\n",
      "Epoch 978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5054e-04 - my_r2: 0.8998 - val_loss: 1.7070e-05 - val_my_r2: 0.9939\n",
      "Epoch 979/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.5371e-04 - my_r2: 0.9414 - val_loss: 1.4141e-05 - val_my_r2: 0.9951\n",
      "Epoch 980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6792e-04 - my_r2: 0.9043 - val_loss: 1.4328e-05 - val_my_r2: 0.9952\n",
      "Epoch 981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0947e-04 - my_r2: 0.9222 - val_loss: 1.9222e-05 - val_my_r2: 0.9934\n",
      "Epoch 982/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6685e-04 - my_r2: 0.8408 - val_loss: 2.1668e-05 - val_my_r2: 0.9919\n",
      "Epoch 983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4519e-04 - my_r2: 0.8465 - val_loss: 1.6709e-05 - val_my_r2: 0.9939\n",
      "Epoch 984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2171e-04 - my_r2: 0.9134 - val_loss: 1.8827e-05 - val_my_r2: 0.9930\n",
      "Epoch 985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4401e-04 - my_r2: 0.9283 - val_loss: 1.7197e-05 - val_my_r2: 0.9938\n",
      "Epoch 986/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4106e-04 - my_r2: 0.9419 - val_loss: 1.5837e-05 - val_my_r2: 0.9946\n",
      "Epoch 987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0491e-04 - my_r2: 0.9178 - val_loss: 1.6122e-05 - val_my_r2: 0.9944\n",
      "Epoch 988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4748e-04 - my_r2: 0.9008 - val_loss: 1.8567e-05 - val_my_r2: 0.9933\n",
      "Epoch 989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5342e-04 - my_r2: 0.9165 - val_loss: 1.8548e-05 - val_my_r2: 0.9941\n",
      "Epoch 990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5652e-04 - my_r2: 0.9465 - val_loss: 1.7759e-05 - val_my_r2: 0.9947\n",
      "Epoch 991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9676e-04 - my_r2: 0.9372 - val_loss: 1.5823e-05 - val_my_r2: 0.9949\n",
      "Epoch 992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1869e-04 - my_r2: 0.9234 - val_loss: 1.7159e-05 - val_my_r2: 0.9939\n",
      "Epoch 993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8684e-04 - my_r2: 0.9558 - val_loss: 1.6629e-05 - val_my_r2: 0.9939\n",
      "Epoch 994/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8323e-04 - my_r2: 0.9196 - val_loss: 1.6150e-05 - val_my_r2: 0.9943\n",
      "Epoch 995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3291e-04 - my_r2: 0.9425 - val_loss: 1.8703e-05 - val_my_r2: 0.9936\n",
      "Epoch 996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4554e-04 - my_r2: 0.9295 - val_loss: 1.9750e-05 - val_my_r2: 0.9936\n",
      "Epoch 997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9528e-04 - my_r2: 0.9262 - val_loss: 1.6034e-05 - val_my_r2: 0.9948\n",
      "Epoch 998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0131e-04 - my_r2: 0.9326 - val_loss: 1.7719e-05 - val_my_r2: 0.9942\n",
      "Epoch 999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4496e-04 - my_r2: 0.9302 - val_loss: 2.2107e-05 - val_my_r2: 0.9929\n",
      "Epoch 1000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0181e-04 - my_r2: 0.8855 - val_loss: 2.0846e-05 - val_my_r2: 0.9931\n",
      "Epoch 1001/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1588e-04 - my_r2: 0.9293 - val_loss: 1.6012e-05 - val_my_r2: 0.9947\n",
      "Epoch 1002/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1776e-04 - my_r2: 0.9078 - val_loss: 1.6463e-05 - val_my_r2: 0.9945\n",
      "Epoch 1003/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8957e-04 - my_r2: 0.9336 - val_loss: 1.7029e-05 - val_my_r2: 0.9939\n",
      "Epoch 1004/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6744e-04 - my_r2: 0.8922 - val_loss: 1.6149e-05 - val_my_r2: 0.9941\n",
      "Epoch 1005/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6292e-04 - my_r2: 0.9309 - val_loss: 1.7297e-05 - val_my_r2: 0.9942\n",
      "Epoch 1006/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9213e-04 - my_r2: 0.9357 - val_loss: 1.8907e-05 - val_my_r2: 0.9940\n",
      "Epoch 1007/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4802e-04 - my_r2: 0.9441 - val_loss: 1.8868e-05 - val_my_r2: 0.9939\n",
      "Epoch 1008/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9662e-04 - my_r2: 0.9154 - val_loss: 1.7455e-05 - val_my_r2: 0.9943\n",
      "Epoch 1009/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3174e-04 - my_r2: 0.9419 - val_loss: 1.9571e-05 - val_my_r2: 0.9936\n",
      "Epoch 1010/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3914e-04 - my_r2: 0.9381 - val_loss: 2.0342e-05 - val_my_r2: 0.9933\n",
      "Epoch 1011/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1191e-04 - my_r2: 0.9500 - val_loss: 1.7095e-05 - val_my_r2: 0.9945\n",
      "Epoch 1012/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2192e-04 - my_r2: 0.8893 - val_loss: 1.8355e-05 - val_my_r2: 0.9936\n",
      "Epoch 1013/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0257e-04 - my_r2: 0.8903 - val_loss: 1.9346e-05 - val_my_r2: 0.9928\n",
      "Epoch 1014/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0130e-04 - my_r2: 0.9429 - val_loss: 1.8383e-05 - val_my_r2: 0.9931\n",
      "Epoch 1015/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3295e-04 - my_r2: 0.9218 - val_loss: 2.5305e-05 - val_my_r2: 0.9908\n",
      "Epoch 1016/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3667e-04 - my_r2: 0.9403 - val_loss: 1.6001e-05 - val_my_r2: 0.9942\n",
      "Epoch 1017/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1386e-04 - my_r2: 0.8984 - val_loss: 1.5466e-05 - val_my_r2: 0.9943\n",
      "Epoch 1018/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4979e-04 - my_r2: 0.9253 - val_loss: 1.5808e-05 - val_my_r2: 0.9943\n",
      "Epoch 1019/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7107e-04 - my_r2: 0.9386 - val_loss: 1.8340e-05 - val_my_r2: 0.9932\n",
      "Epoch 1020/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5145e-04 - my_r2: 0.9281 - val_loss: 1.7130e-05 - val_my_r2: 0.9936\n",
      "Epoch 1021/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6432e-04 - my_r2: 0.9104 - val_loss: 2.0534e-05 - val_my_r2: 0.9928\n",
      "Epoch 1022/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2782e-04 - my_r2: 0.9202 - val_loss: 2.1589e-05 - val_my_r2: 0.9920\n",
      "Epoch 1023/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8804e-04 - my_r2: 0.9008 - val_loss: 2.1314e-05 - val_my_r2: 0.9915\n",
      "Epoch 1024/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8551e-04 - my_r2: 0.9322 - val_loss: 1.9502e-05 - val_my_r2: 0.9920\n",
      "Epoch 1025/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8653e-04 - my_r2: 0.9046 - val_loss: 1.6079e-05 - val_my_r2: 0.9938\n",
      "Epoch 1026/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4107e-04 - my_r2: 0.9382 - val_loss: 1.8646e-05 - val_my_r2: 0.9930\n",
      "Epoch 1027/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3158e-04 - my_r2: 0.9339 - val_loss: 1.9616e-05 - val_my_r2: 0.9930\n",
      "Epoch 1028/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5524e-04 - my_r2: 0.9267 - val_loss: 1.5883e-05 - val_my_r2: 0.9944\n",
      "Epoch 1029/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6561e-04 - my_r2: 0.9225 - val_loss: 1.6988e-05 - val_my_r2: 0.9943\n",
      "Epoch 1030/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8223e-04 - my_r2: 0.9374 - val_loss: 2.8679e-05 - val_my_r2: 0.9899\n",
      "Epoch 1031/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6448e-04 - my_r2: 0.8953 - val_loss: 1.6732e-05 - val_my_r2: 0.9942\n",
      "Epoch 1032/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9652e-04 - my_r2: 0.9212 - val_loss: 1.8766e-05 - val_my_r2: 0.9935\n",
      "Epoch 1033/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4401e-04 - my_r2: 0.9460 - val_loss: 1.6441e-05 - val_my_r2: 0.9943\n",
      "Epoch 1034/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2300e-04 - my_r2: 0.9269 - val_loss: 1.5107e-05 - val_my_r2: 0.9945\n",
      "Epoch 1035/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4529e-04 - my_r2: 0.9367 - val_loss: 1.3198e-05 - val_my_r2: 0.9945\n",
      "Epoch 1036/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6007e-04 - my_r2: 0.9455 - val_loss: 1.3213e-05 - val_my_r2: 0.9947\n",
      "Epoch 1037/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4155e-04 - my_r2: 0.9444 - val_loss: 1.5760e-05 - val_my_r2: 0.9939\n",
      "Epoch 1038/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4380e-04 - my_r2: 0.9124 - val_loss: 1.9013e-05 - val_my_r2: 0.9927\n",
      "Epoch 1039/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6159e-04 - my_r2: 0.9272 - val_loss: 1.8727e-05 - val_my_r2: 0.9926\n",
      "Epoch 1040/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3185e-04 - my_r2: 0.9240 - val_loss: 1.6936e-05 - val_my_r2: 0.9934\n",
      "Epoch 1041/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3059e-04 - my_r2: 0.8693 - val_loss: 1.5082e-05 - val_my_r2: 0.9941\n",
      "Epoch 1042/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6473e-04 - my_r2: 0.9314 - val_loss: 1.5065e-05 - val_my_r2: 0.9941\n",
      "Epoch 1043/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0602e-04 - my_r2: 0.9037 - val_loss: 1.8785e-05 - val_my_r2: 0.9929\n",
      "Epoch 1044/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4307e-04 - my_r2: 0.8732 - val_loss: 2.1372e-05 - val_my_r2: 0.9923\n",
      "Epoch 1045/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9580e-04 - my_r2: 0.9232 - val_loss: 1.9456e-05 - val_my_r2: 0.9928\n",
      "Epoch 1046/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0659e-04 - my_r2: 0.9060 - val_loss: 1.8964e-05 - val_my_r2: 0.9927\n",
      "Epoch 1047/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2145e-04 - my_r2: 0.9290 - val_loss: 1.4970e-05 - val_my_r2: 0.9943\n",
      "Epoch 1048/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3917e-04 - my_r2: 0.9315 - val_loss: 1.9246e-05 - val_my_r2: 0.9928\n",
      "Epoch 1049/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2652e-04 - my_r2: 0.9302 - val_loss: 1.6411e-05 - val_my_r2: 0.9936\n",
      "Epoch 1050/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3166e-04 - my_r2: 0.9449 - val_loss: 1.6498e-05 - val_my_r2: 0.9938\n",
      "Epoch 1051/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4451e-04 - my_r2: 0.9498 - val_loss: 2.4708e-05 - val_my_r2: 0.9900\n",
      "Epoch 1052/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9264e-04 - my_r2: 0.8956 - val_loss: 2.0754e-05 - val_my_r2: 0.9916\n",
      "Epoch 1053/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1038e-04 - my_r2: 0.9321 - val_loss: 2.5538e-05 - val_my_r2: 0.9901\n",
      "Epoch 1054/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3642e-04 - my_r2: 0.9129 - val_loss: 2.1865e-05 - val_my_r2: 0.9922\n",
      "Epoch 1055/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9775e-04 - my_r2: 0.9147 - val_loss: 1.4703e-05 - val_my_r2: 0.9947\n",
      "Epoch 1056/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8344e-04 - my_r2: 0.9246 - val_loss: 1.8004e-05 - val_my_r2: 0.9937\n",
      "Epoch 1057/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6516e-04 - my_r2: 0.9095 - val_loss: 1.6833e-05 - val_my_r2: 0.9936\n",
      "Epoch 1058/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0520e-04 - my_r2: 0.9160 - val_loss: 1.4059e-05 - val_my_r2: 0.9946\n",
      "Epoch 1059/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9057e-04 - my_r2: 0.9165 - val_loss: 1.5796e-05 - val_my_r2: 0.9941\n",
      "Epoch 1060/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8445e-04 - my_r2: 0.8766 - val_loss: 1.4286e-05 - val_my_r2: 0.9949\n",
      "Epoch 1061/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2524e-04 - my_r2: 0.9299 - val_loss: 2.0551e-05 - val_my_r2: 0.9934\n",
      "Epoch 1062/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8703e-04 - my_r2: 0.8569 - val_loss: 1.3761e-05 - val_my_r2: 0.9949\n",
      "Epoch 1063/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3197e-04 - my_r2: 0.9208 - val_loss: 1.3597e-05 - val_my_r2: 0.9950\n",
      "Epoch 1064/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0873e-04 - my_r2: 0.9285 - val_loss: 1.3727e-05 - val_my_r2: 0.9954\n",
      "Epoch 1065/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1132e-04 - my_r2: 0.9453 - val_loss: 1.5629e-05 - val_my_r2: 0.9949\n",
      "Epoch 1066/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9265e-04 - my_r2: 0.9126 - val_loss: 1.3914e-05 - val_my_r2: 0.9953\n",
      "Epoch 1067/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0898e-04 - my_r2: 0.9290 - val_loss: 1.4077e-05 - val_my_r2: 0.9950\n",
      "Epoch 1068/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2789e-04 - my_r2: 0.9170 - val_loss: 1.5147e-05 - val_my_r2: 0.9949\n",
      "Epoch 1069/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7724e-04 - my_r2: 0.9453 - val_loss: 1.7175e-05 - val_my_r2: 0.9943\n",
      "Epoch 1070/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6064e-04 - my_r2: 0.9203 - val_loss: 1.5734e-05 - val_my_r2: 0.9943\n",
      "Epoch 1071/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4472e-04 - my_r2: 0.9194 - val_loss: 2.0563e-05 - val_my_r2: 0.9925\n",
      "Epoch 1072/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7953e-04 - my_r2: 0.8759 - val_loss: 1.4164e-05 - val_my_r2: 0.9945\n",
      "Epoch 1073/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1172e-04 - my_r2: 0.9337 - val_loss: 1.4273e-05 - val_my_r2: 0.9944\n",
      "Epoch 1074/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2100e-04 - my_r2: 0.9179 - val_loss: 1.5589e-05 - val_my_r2: 0.9944\n",
      "Epoch 1075/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8550e-04 - my_r2: 0.9354 - val_loss: 1.6954e-05 - val_my_r2: 0.9941\n",
      "Epoch 1076/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5106e-04 - my_r2: 0.9364 - val_loss: 1.8032e-05 - val_my_r2: 0.9939\n",
      "Epoch 1077/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4845e-04 - my_r2: 0.9393 - val_loss: 1.7255e-05 - val_my_r2: 0.9942\n",
      "Epoch 1078/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3213e-04 - my_r2: 0.8958 - val_loss: 1.7336e-05 - val_my_r2: 0.9940\n",
      "Epoch 1079/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4688e-04 - my_r2: 0.9050 - val_loss: 1.7191e-05 - val_my_r2: 0.9937\n",
      "Epoch 1080/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4217e-04 - my_r2: 0.9460 - val_loss: 1.7266e-05 - val_my_r2: 0.9938\n",
      "Epoch 1081/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4094e-04 - my_r2: 0.9482 - val_loss: 1.6285e-05 - val_my_r2: 0.9934\n",
      "Epoch 1082/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8459e-04 - my_r2: 0.8935 - val_loss: 1.4564e-05 - val_my_r2: 0.9944\n",
      "Epoch 1083/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4295e-04 - my_r2: 0.9352 - val_loss: 1.3733e-05 - val_my_r2: 0.9952\n",
      "Epoch 1084/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2483e-04 - my_r2: 0.9432 - val_loss: 1.3185e-05 - val_my_r2: 0.9955\n",
      "Epoch 1085/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7386e-04 - my_r2: 0.9401 - val_loss: 1.3746e-05 - val_my_r2: 0.9951\n",
      "Epoch 1086/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4445e-04 - my_r2: 0.9231 - val_loss: 1.3768e-05 - val_my_r2: 0.9950\n",
      "Epoch 1087/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1500e-04 - my_r2: 0.8989 - val_loss: 1.4696e-05 - val_my_r2: 0.9947\n",
      "Epoch 1088/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7410e-04 - my_r2: 0.9491 - val_loss: 1.4961e-05 - val_my_r2: 0.9947\n",
      "Epoch 1089/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8008e-04 - my_r2: 0.9023 - val_loss: 1.4469e-05 - val_my_r2: 0.9950\n",
      "Epoch 1090/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1136e-04 - my_r2: 0.9267 - val_loss: 1.5995e-05 - val_my_r2: 0.9947\n",
      "Epoch 1091/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9972e-04 - my_r2: 0.9366 - val_loss: 1.7011e-05 - val_my_r2: 0.9938\n",
      "Epoch 1092/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7070e-04 - my_r2: 0.9628 - val_loss: 1.8960e-05 - val_my_r2: 0.9928\n",
      "Epoch 1093/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6762e-04 - my_r2: 0.9379 - val_loss: 2.2224e-05 - val_my_r2: 0.9919\n",
      "Epoch 1094/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7742e-04 - my_r2: 0.9013 - val_loss: 1.8218e-05 - val_my_r2: 0.9935\n",
      "Epoch 1095/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0269e-04 - my_r2: 0.8872 - val_loss: 1.8530e-05 - val_my_r2: 0.9931\n",
      "Epoch 1096/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6948e-04 - my_r2: 0.9455 - val_loss: 1.6114e-05 - val_my_r2: 0.9937\n",
      "Epoch 1097/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6008e-04 - my_r2: 0.9113 - val_loss: 1.5401e-05 - val_my_r2: 0.9937\n",
      "Epoch 1098/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9513e-04 - my_r2: 0.9049 - val_loss: 1.5494e-05 - val_my_r2: 0.9939\n",
      "Epoch 1099/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3408e-04 - my_r2: 0.9246 - val_loss: 1.8728e-05 - val_my_r2: 0.9927\n",
      "Epoch 1100/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5134e-04 - my_r2: 0.9297 - val_loss: 1.5449e-05 - val_my_r2: 0.9940\n",
      "Epoch 1101/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.2243e-04 - my_r2: 0.9315 - val_loss: 1.5934e-05 - val_my_r2: 0.9942\n",
      "Epoch 1102/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4180e-04 - my_r2: 0.8510 - val_loss: 1.9994e-05 - val_my_r2: 0.9932\n",
      "Epoch 1103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7640e-04 - my_r2: 0.9094 - val_loss: 1.8994e-05 - val_my_r2: 0.9937\n",
      "Epoch 1104/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5929e-04 - my_r2: 0.8941 - val_loss: 1.6080e-05 - val_my_r2: 0.9946\n",
      "Epoch 1105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7338e-04 - my_r2: 0.9105 - val_loss: 1.4590e-05 - val_my_r2: 0.9951\n",
      "Epoch 1106/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5501e-04 - my_r2: 0.8843 - val_loss: 1.4897e-05 - val_my_r2: 0.9946\n",
      "Epoch 1107/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4658e-04 - my_r2: 0.9168 - val_loss: 1.7700e-05 - val_my_r2: 0.9940\n",
      "Epoch 1108/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7029e-04 - my_r2: 0.8809 - val_loss: 1.5999e-05 - val_my_r2: 0.9944\n",
      "Epoch 1109/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9447e-04 - my_r2: 0.9560 - val_loss: 1.5211e-05 - val_my_r2: 0.9945\n",
      "Epoch 1110/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5102e-04 - my_r2: 0.8702 - val_loss: 1.3180e-05 - val_my_r2: 0.9953\n",
      "Epoch 1111/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8423e-04 - my_r2: 0.7744 - val_loss: 1.4318e-05 - val_my_r2: 0.9947\n",
      "Epoch 1112/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8015e-04 - my_r2: 0.9483 - val_loss: 1.5143e-05 - val_my_r2: 0.9943\n",
      "Epoch 1113/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5976e-04 - my_r2: 0.9387 - val_loss: 1.6328e-05 - val_my_r2: 0.9949\n",
      "Epoch 1114/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6682e-04 - my_r2: 0.9367 - val_loss: 1.7566e-05 - val_my_r2: 0.9943\n",
      "Epoch 1115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6784e-04 - my_r2: 0.9343 - val_loss: 2.0453e-05 - val_my_r2: 0.9933\n",
      "Epoch 1116/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2325e-04 - my_r2: 0.9480 - val_loss: 2.0144e-05 - val_my_r2: 0.9928\n",
      "Epoch 1117/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9740e-04 - my_r2: 0.9326 - val_loss: 1.8847e-05 - val_my_r2: 0.9932\n",
      "Epoch 1118/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6307e-04 - my_r2: 0.9107 - val_loss: 2.0352e-05 - val_my_r2: 0.9927\n",
      "Epoch 1119/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6598e-04 - my_r2: 0.8977 - val_loss: 2.3665e-05 - val_my_r2: 0.9910\n",
      "Epoch 1120/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2487e-04 - my_r2: 0.9203 - val_loss: 2.0499e-05 - val_my_r2: 0.9923\n",
      "Epoch 1121/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6646e-04 - my_r2: 0.9455 - val_loss: 1.5085e-05 - val_my_r2: 0.9943\n",
      "Epoch 1122/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6438e-04 - my_r2: 0.9307 - val_loss: 2.2869e-05 - val_my_r2: 0.9918\n",
      "Epoch 1123/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7541e-04 - my_r2: 0.9194 - val_loss: 1.6596e-05 - val_my_r2: 0.9941\n",
      "Epoch 1124/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6779e-04 - my_r2: 0.9120 - val_loss: 1.7414e-05 - val_my_r2: 0.9936\n",
      "Epoch 1125/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2248e-04 - my_r2: 0.8469 - val_loss: 1.5372e-05 - val_my_r2: 0.9938\n",
      "Epoch 1126/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4551e-04 - my_r2: 0.9308 - val_loss: 1.8648e-05 - val_my_r2: 0.9926\n",
      "Epoch 1127/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4448e-04 - my_r2: 0.9166 - val_loss: 1.8440e-05 - val_my_r2: 0.9930\n",
      "Epoch 1128/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3635e-04 - my_r2: 0.9366 - val_loss: 1.7928e-05 - val_my_r2: 0.9932\n",
      "Epoch 1129/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5083e-04 - my_r2: 0.8735 - val_loss: 1.8090e-05 - val_my_r2: 0.9926\n",
      "Epoch 1130/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1041e-04 - my_r2: 0.9274 - val_loss: 1.8751e-05 - val_my_r2: 0.9914\n",
      "Epoch 1131/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0066e-04 - my_r2: 0.9171 - val_loss: 2.0213e-05 - val_my_r2: 0.9907\n",
      "Epoch 1132/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2928e-04 - my_r2: 0.9232 - val_loss: 1.7356e-05 - val_my_r2: 0.9933\n",
      "Epoch 1133/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2708e-04 - my_r2: 0.9511 - val_loss: 1.6701e-05 - val_my_r2: 0.9941\n",
      "Epoch 1134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8130e-04 - my_r2: 0.9481 - val_loss: 1.6747e-05 - val_my_r2: 0.9931\n",
      "Epoch 1135/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9261e-04 - my_r2: 0.9131 - val_loss: 1.6027e-05 - val_my_r2: 0.9930\n",
      "Epoch 1136/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0591e-04 - my_r2: 0.9432 - val_loss: 1.7093e-05 - val_my_r2: 0.9926\n",
      "Epoch 1137/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2336e-04 - my_r2: 0.9148 - val_loss: 2.0852e-05 - val_my_r2: 0.9923\n",
      "Epoch 1138/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8097e-04 - my_r2: 0.9069 - val_loss: 2.0203e-05 - val_my_r2: 0.9926\n",
      "Epoch 1139/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3836e-04 - my_r2: 0.8897 - val_loss: 1.8852e-05 - val_my_r2: 0.9933\n",
      "Epoch 1140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0592e-04 - my_r2: 0.9341 - val_loss: 1.5718e-05 - val_my_r2: 0.9940\n",
      "Epoch 1141/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5456e-04 - my_r2: 0.9351 - val_loss: 1.5460e-05 - val_my_r2: 0.9940\n",
      "Epoch 1142/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7008e-04 - my_r2: 0.9116 - val_loss: 2.1362e-05 - val_my_r2: 0.9917\n",
      "Epoch 1143/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7854e-04 - my_r2: 0.7915 - val_loss: 1.3764e-05 - val_my_r2: 0.9950\n",
      "Epoch 1144/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1519e-04 - my_r2: 0.8590 - val_loss: 1.7851e-05 - val_my_r2: 0.9937\n",
      "Epoch 1145/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9681e-04 - my_r2: 0.9256 - val_loss: 1.6345e-05 - val_my_r2: 0.9939\n",
      "Epoch 1146/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5287e-04 - my_r2: 0.9290 - val_loss: 1.8149e-05 - val_my_r2: 0.9938\n",
      "Epoch 1147/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1692e-04 - my_r2: 0.9398 - val_loss: 1.8343e-05 - val_my_r2: 0.9934\n",
      "Epoch 1148/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5861e-04 - my_r2: 0.9310 - val_loss: 1.6479e-05 - val_my_r2: 0.9943\n",
      "Epoch 1149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7020e-04 - my_r2: 0.9387 - val_loss: 1.7330e-05 - val_my_r2: 0.9933\n",
      "Epoch 1150/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6676e-04 - my_r2: 0.8973 - val_loss: 1.5702e-05 - val_my_r2: 0.9931\n",
      "Epoch 1151/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0020e-04 - my_r2: 0.9400 - val_loss: 1.6092e-05 - val_my_r2: 0.9932\n",
      "Epoch 1152/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8141e-04 - my_r2: 0.8679 - val_loss: 1.4270e-05 - val_my_r2: 0.9943\n",
      "Epoch 1153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6028e-04 - my_r2: 0.9315 - val_loss: 1.6571e-05 - val_my_r2: 0.9943\n",
      "Epoch 1154/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1330e-04 - my_r2: 0.9381 - val_loss: 1.7941e-05 - val_my_r2: 0.9940\n",
      "Epoch 1155/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1563e-04 - my_r2: 0.9351 - val_loss: 1.6287e-05 - val_my_r2: 0.9944\n",
      "Epoch 1156/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9870e-04 - my_r2: 0.9277 - val_loss: 1.7435e-05 - val_my_r2: 0.9943\n",
      "Epoch 1157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7703e-04 - my_r2: 0.9345 - val_loss: 1.3423e-05 - val_my_r2: 0.9954\n",
      "Epoch 1158/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4935e-04 - my_r2: 0.9322 - val_loss: 1.4549e-05 - val_my_r2: 0.9955\n",
      "Epoch 1159/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6064e-04 - my_r2: 0.9319 - val_loss: 1.3476e-05 - val_my_r2: 0.9959\n",
      "Epoch 1160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0989e-04 - my_r2: 0.9561 - val_loss: 1.4210e-05 - val_my_r2: 0.9956\n",
      "Epoch 1161/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6845e-04 - my_r2: 0.9399 - val_loss: 1.4027e-05 - val_my_r2: 0.9958\n",
      "Epoch 1162/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5517e-04 - my_r2: 0.9132 - val_loss: 1.2979e-05 - val_my_r2: 0.9958\n",
      "Epoch 1163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8511e-04 - my_r2: 0.9393 - val_loss: 1.4203e-05 - val_my_r2: 0.9955\n",
      "Epoch 1164/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6079e-04 - my_r2: 0.9148 - val_loss: 1.3440e-05 - val_my_r2: 0.9956\n",
      "Epoch 1165/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7734e-04 - my_r2: 0.9196 - val_loss: 1.6815e-05 - val_my_r2: 0.9943\n",
      "Epoch 1166/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3482e-04 - my_r2: 0.8259 - val_loss: 1.5766e-05 - val_my_r2: 0.9947\n",
      "Epoch 1167/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7839e-04 - my_r2: 0.8627 - val_loss: 1.6553e-05 - val_my_r2: 0.9945\n",
      "Epoch 1168/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3838e-04 - my_r2: 0.9369 - val_loss: 1.5622e-05 - val_my_r2: 0.9947\n",
      "Epoch 1169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5131e-04 - my_r2: 0.9170 - val_loss: 1.3940e-05 - val_my_r2: 0.9953\n",
      "Epoch 1170/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8229e-04 - my_r2: 0.9336 - val_loss: 1.4008e-05 - val_my_r2: 0.9951\n",
      "Epoch 1171/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.3426e-04 - my_r2: 0.9466 - val_loss: 1.3136e-05 - val_my_r2: 0.9953\n",
      "Epoch 1172/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4785e-04 - my_r2: 0.9401 - val_loss: 1.4697e-05 - val_my_r2: 0.9948\n",
      "Epoch 1173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5302e-04 - my_r2: 0.9536 - val_loss: 1.3191e-05 - val_my_r2: 0.9953\n",
      "Epoch 1174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6347e-04 - my_r2: 0.9276 - val_loss: 1.5248e-05 - val_my_r2: 0.9944\n",
      "Epoch 1175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0692e-04 - my_r2: 0.9527 - val_loss: 1.8264e-05 - val_my_r2: 0.9935\n",
      "Epoch 1176/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9681e-04 - my_r2: 0.9423 - val_loss: 1.3876e-05 - val_my_r2: 0.9950\n",
      "Epoch 1177/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0377e-04 - my_r2: 0.9550 - val_loss: 1.1694e-05 - val_my_r2: 0.9954\n",
      "Epoch 1178/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6232e-04 - my_r2: 0.9311 - val_loss: 1.1628e-05 - val_my_r2: 0.9958\n",
      "Epoch 1179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9324e-04 - my_r2: 0.9232 - val_loss: 1.4911e-05 - val_my_r2: 0.9947\n",
      "Epoch 1180/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5819e-04 - my_r2: 0.9083 - val_loss: 1.3431e-05 - val_my_r2: 0.9954\n",
      "Epoch 1181/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5510e-04 - my_r2: 0.9279 - val_loss: 1.4060e-05 - val_my_r2: 0.9952\n",
      "Epoch 1182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7249e-04 - my_r2: 0.9108 - val_loss: 1.3362e-05 - val_my_r2: 0.9949\n",
      "Epoch 1183/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6155e-04 - my_r2: 0.9207 - val_loss: 1.3220e-05 - val_my_r2: 0.9948\n",
      "Epoch 1184/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7786e-04 - my_r2: 0.9032 - val_loss: 1.2258e-05 - val_my_r2: 0.9949\n",
      "Epoch 1185/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3064e-04 - my_r2: 0.9205 - val_loss: 1.5027e-05 - val_my_r2: 0.9942\n",
      "Epoch 1186/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7889e-04 - my_r2: 0.9323 - val_loss: 1.4732e-05 - val_my_r2: 0.9944\n",
      "Epoch 1187/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5322e-04 - my_r2: 0.8905 - val_loss: 2.0225e-05 - val_my_r2: 0.9926\n",
      "Epoch 1188/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5047e-04 - my_r2: 0.7323 - val_loss: 2.0235e-05 - val_my_r2: 0.9925\n",
      "Epoch 1189/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1529e-04 - my_r2: 0.9193 - val_loss: 1.3467e-05 - val_my_r2: 0.9950\n",
      "Epoch 1190/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0742e-04 - my_r2: 0.9504 - val_loss: 1.2342e-05 - val_my_r2: 0.9955\n",
      "Epoch 1191/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.9554e-04 - my_r2: 0.8885 - val_loss: 1.2033e-05 - val_my_r2: 0.9957\n",
      "Epoch 1192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1702e-04 - my_r2: 0.9542 - val_loss: 1.3608e-05 - val_my_r2: 0.9952\n",
      "Epoch 1193/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6410e-04 - my_r2: 0.9346 - val_loss: 1.7253e-05 - val_my_r2: 0.9945\n",
      "Epoch 1194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6256e-04 - my_r2: 0.9151 - val_loss: 1.7078e-05 - val_my_r2: 0.9946\n",
      "Epoch 1195/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7082e-04 - my_r2: 0.9235 - val_loss: 1.8382e-05 - val_my_r2: 0.9940\n",
      "Epoch 1196/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5561e-04 - my_r2: 0.9325 - val_loss: 2.0851e-05 - val_my_r2: 0.9927\n",
      "Epoch 1197/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2749e-04 - my_r2: 0.8383 - val_loss: 2.1985e-05 - val_my_r2: 0.9928\n",
      "Epoch 1198/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8518e-04 - my_r2: 0.9326 - val_loss: 1.4818e-05 - val_my_r2: 0.9953\n",
      "Epoch 1199/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.4096e-04 - my_r2: 0.9287 - val_loss: 1.2291e-05 - val_my_r2: 0.9958\n",
      "Epoch 1200/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5774e-04 - my_r2: 0.9409 - val_loss: 1.5127e-05 - val_my_r2: 0.9945\n",
      "Epoch 1201/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3878e-04 - my_r2: 0.8605 - val_loss: 1.5564e-05 - val_my_r2: 0.9940\n",
      "Epoch 1202/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4835e-04 - my_r2: 0.9391 - val_loss: 1.3645e-05 - val_my_r2: 0.9950\n",
      "Epoch 1203/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1766e-04 - my_r2: 0.9489 - val_loss: 1.2696e-05 - val_my_r2: 0.9957\n",
      "Epoch 1204/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.4704e-04 - my_r2: 0.9502 - val_loss: 1.4949e-05 - val_my_r2: 0.9947\n",
      "Epoch 1205/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8620e-04 - my_r2: 0.9413 - val_loss: 1.8509e-05 - val_my_r2: 0.9934\n",
      "Epoch 1206/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2460e-04 - my_r2: 0.9384 - val_loss: 1.1974e-05 - val_my_r2: 0.9959\n",
      "Epoch 1207/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0579e-04 - my_r2: 0.9179 - val_loss: 1.1272e-05 - val_my_r2: 0.9960\n",
      "Epoch 1208/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0273e-04 - my_r2: 0.9420 - val_loss: 1.2174e-05 - val_my_r2: 0.9960\n",
      "Epoch 1209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4052e-04 - my_r2: 0.9555 - val_loss: 1.2752e-05 - val_my_r2: 0.9956\n",
      "Epoch 1210/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4879e-04 - my_r2: 0.9128 - val_loss: 1.3809e-05 - val_my_r2: 0.9952\n",
      "Epoch 1211/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7722e-04 - my_r2: 0.9358 - val_loss: 1.7085e-05 - val_my_r2: 0.9944\n",
      "Epoch 1212/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7759e-04 - my_r2: 0.9158 - val_loss: 2.0083e-05 - val_my_r2: 0.9934\n",
      "Epoch 1213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2480e-04 - my_r2: 0.9278 - val_loss: 1.5771e-05 - val_my_r2: 0.9945\n",
      "Epoch 1214/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7414e-04 - my_r2: 0.9034 - val_loss: 1.2720e-05 - val_my_r2: 0.9956\n",
      "Epoch 1215/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6637e-04 - my_r2: 0.9405 - val_loss: 1.4452e-05 - val_my_r2: 0.9954\n",
      "Epoch 1216/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9935e-04 - my_r2: 0.9286 - val_loss: 1.7354e-05 - val_my_r2: 0.9943\n",
      "Epoch 1217/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1249e-04 - my_r2: 0.9593 - val_loss: 1.4807e-05 - val_my_r2: 0.9950\n",
      "Epoch 1218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1852e-04 - my_r2: 0.9526 - val_loss: 1.4250e-05 - val_my_r2: 0.9949\n",
      "Epoch 1219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7268e-04 - my_r2: 0.9154 - val_loss: 1.3851e-05 - val_my_r2: 0.9950\n",
      "Epoch 1220/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2279e-04 - my_r2: 0.9234 - val_loss: 1.3990e-05 - val_my_r2: 0.9949\n",
      "Epoch 1221/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2131e-04 - my_r2: 0.9533 - val_loss: 1.3642e-05 - val_my_r2: 0.9950\n",
      "Epoch 1222/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8706e-04 - my_r2: 0.9296 - val_loss: 1.3864e-05 - val_my_r2: 0.9951\n",
      "Epoch 1223/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8751e-04 - my_r2: 0.9404 - val_loss: 1.5605e-05 - val_my_r2: 0.9941\n",
      "Epoch 1224/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7414e-04 - my_r2: 0.9050 - val_loss: 1.6497e-05 - val_my_r2: 0.9938\n",
      "Epoch 1225/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3118e-04 - my_r2: 0.9339 - val_loss: 1.6669e-05 - val_my_r2: 0.9940\n",
      "Epoch 1226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1701e-04 - my_r2: 0.9318 - val_loss: 1.5173e-05 - val_my_r2: 0.9949\n",
      "Epoch 1227/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0291e-04 - my_r2: 0.9405 - val_loss: 1.2985e-05 - val_my_r2: 0.9955\n",
      "Epoch 1228/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5849e-04 - my_r2: 0.9051 - val_loss: 1.3411e-05 - val_my_r2: 0.9956\n",
      "Epoch 1229/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9702e-04 - my_r2: 0.9226 - val_loss: 1.2117e-05 - val_my_r2: 0.9960\n",
      "Epoch 1230/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.7209e-04 - my_r2: 0.9376 - val_loss: 1.2835e-05 - val_my_r2: 0.9956\n",
      "Epoch 1231/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9813e-04 - my_r2: 0.9202 - val_loss: 1.3403e-05 - val_my_r2: 0.9955\n",
      "Epoch 1232/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5571e-04 - my_r2: 0.9425 - val_loss: 1.2706e-05 - val_my_r2: 0.9956\n",
      "Epoch 1233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3311e-04 - my_r2: 0.9218 - val_loss: 1.0191e-05 - val_my_r2: 0.9964\n",
      "Epoch 1234/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6070e-04 - my_r2: 0.9445 - val_loss: 1.2625e-05 - val_my_r2: 0.9956\n",
      "Epoch 1235/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3580e-04 - my_r2: 0.9417 - val_loss: 1.3626e-05 - val_my_r2: 0.9953\n",
      "Epoch 1236/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4043e-04 - my_r2: 0.8975 - val_loss: 1.2955e-05 - val_my_r2: 0.9954\n",
      "Epoch 1237/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5753e-04 - my_r2: 0.9460 - val_loss: 1.1566e-05 - val_my_r2: 0.9962\n",
      "Epoch 1238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5437e-04 - my_r2: 0.9475 - val_loss: 1.2803e-05 - val_my_r2: 0.9959\n",
      "Epoch 1239/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2977e-04 - my_r2: 0.9379 - val_loss: 1.4026e-05 - val_my_r2: 0.9951\n",
      "Epoch 1240/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1862e-04 - my_r2: 0.3852 - val_loss: 1.5995e-05 - val_my_r2: 0.9944\n",
      "Epoch 1241/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4963e-04 - my_r2: 0.9407 - val_loss: 1.6654e-05 - val_my_r2: 0.9939\n",
      "Epoch 1242/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3481e-04 - my_r2: 0.9411 - val_loss: 1.5273e-05 - val_my_r2: 0.9942\n",
      "Epoch 1243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7793e-04 - my_r2: 0.9042 - val_loss: 1.8434e-05 - val_my_r2: 0.9933\n",
      "Epoch 1244/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9017e-04 - my_r2: 0.9135 - val_loss: 1.6747e-05 - val_my_r2: 0.9939\n",
      "Epoch 1245/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9077e-04 - my_r2: 0.9298 - val_loss: 1.1009e-05 - val_my_r2: 0.9960\n",
      "Epoch 1246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8103e-04 - my_r2: 0.9373 - val_loss: 1.3173e-05 - val_my_r2: 0.9952\n",
      "Epoch 1247/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1883e-04 - my_r2: 0.8708 - val_loss: 1.0434e-05 - val_my_r2: 0.9965\n",
      "Epoch 1248/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1150e-04 - my_r2: 0.9288 - val_loss: 1.0418e-05 - val_my_r2: 0.9962\n",
      "Epoch 1249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5459e-04 - my_r2: 0.9268 - val_loss: 1.3170e-05 - val_my_r2: 0.9954\n",
      "Epoch 1250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7407e-04 - my_r2: 0.9011 - val_loss: 1.1564e-05 - val_my_r2: 0.9959\n",
      "Epoch 1251/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9393e-04 - my_r2: 0.8937 - val_loss: 2.0299e-05 - val_my_r2: 0.9930\n",
      "Epoch 1252/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7727e-04 - my_r2: 0.9378 - val_loss: 2.9278e-05 - val_my_r2: 0.9902\n",
      "Epoch 1253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9970e-04 - my_r2: 0.9414 - val_loss: 1.5530e-05 - val_my_r2: 0.9941\n",
      "Epoch 1254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3238e-04 - my_r2: 0.9292 - val_loss: 1.8476e-05 - val_my_r2: 0.9936\n",
      "Epoch 1255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6844e-04 - my_r2: 0.9340 - val_loss: 1.4563e-05 - val_my_r2: 0.9955\n",
      "Epoch 1256/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8857e-04 - my_r2: 0.9363 - val_loss: 1.3940e-05 - val_my_r2: 0.9958\n",
      "Epoch 1257/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2298e-04 - my_r2: 0.9213 - val_loss: 1.4824e-05 - val_my_r2: 0.9957\n",
      "Epoch 1258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4229e-04 - my_r2: 0.9270 - val_loss: 1.2121e-05 - val_my_r2: 0.9961\n",
      "Epoch 1259/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8082e-04 - my_r2: 0.9406 - val_loss: 1.1707e-05 - val_my_r2: 0.9961\n",
      "Epoch 1260/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6713e-04 - my_r2: 0.9389 - val_loss: 1.3274e-05 - val_my_r2: 0.9956\n",
      "Epoch 1261/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9279e-04 - my_r2: 0.9436 - val_loss: 1.3934e-05 - val_my_r2: 0.9955\n",
      "Epoch 1262/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3054e-04 - my_r2: 0.9423 - val_loss: 1.4783e-05 - val_my_r2: 0.9947\n",
      "Epoch 1263/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.7226e-04 - my_r2: 0.9110 - val_loss: 1.3578e-05 - val_my_r2: 0.9953\n",
      "Epoch 1264/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2538e-04 - my_r2: 0.9232 - val_loss: 1.4071e-05 - val_my_r2: 0.9957\n",
      "Epoch 1265/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9637e-04 - my_r2: 0.9348 - val_loss: 1.1607e-05 - val_my_r2: 0.9961\n",
      "Epoch 1266/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3367e-04 - my_r2: 0.9234 - val_loss: 1.0981e-05 - val_my_r2: 0.9962\n",
      "Epoch 1267/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4076e-04 - my_r2: 0.9401 - val_loss: 1.0220e-05 - val_my_r2: 0.9963\n",
      "Epoch 1268/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4810e-04 - my_r2: 0.9425 - val_loss: 1.0336e-05 - val_my_r2: 0.9966\n",
      "Epoch 1269/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5687e-04 - my_r2: 0.9339 - val_loss: 1.1830e-05 - val_my_r2: 0.9963\n",
      "Epoch 1270/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7005e-04 - my_r2: 0.8852 - val_loss: 1.4275e-05 - val_my_r2: 0.9958\n",
      "Epoch 1271/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3667e-04 - my_r2: 0.9500 - val_loss: 1.4414e-05 - val_my_r2: 0.9954\n",
      "Epoch 1272/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4694e-04 - my_r2: 0.9153 - val_loss: 1.6310e-05 - val_my_r2: 0.9952\n",
      "Epoch 1273/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6681e-04 - my_r2: 0.8766 - val_loss: 1.4015e-05 - val_my_r2: 0.9954\n",
      "Epoch 1274/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0147e-04 - my_r2: 0.9364 - val_loss: 1.2610e-05 - val_my_r2: 0.9956\n",
      "Epoch 1275/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0504e-04 - my_r2: 0.9129 - val_loss: 1.8145e-05 - val_my_r2: 0.9943\n",
      "Epoch 1276/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1667e-04 - my_r2: 0.9357 - val_loss: 1.4523e-05 - val_my_r2: 0.9955\n",
      "Epoch 1277/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8659e-04 - my_r2: 0.9308 - val_loss: 1.4653e-05 - val_my_r2: 0.9953\n",
      "Epoch 1278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5861e-04 - my_r2: 0.9478 - val_loss: 1.3742e-05 - val_my_r2: 0.9956\n",
      "Epoch 1279/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3760e-04 - my_r2: 0.9092 - val_loss: 1.3784e-05 - val_my_r2: 0.9955\n",
      "Epoch 1280/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4434e-04 - my_r2: 0.9380 - val_loss: 1.5676e-05 - val_my_r2: 0.9949\n",
      "Epoch 1281/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.9712e-04 - my_r2: 0.9289 - val_loss: 1.5133e-05 - val_my_r2: 0.9951\n",
      "Epoch 1282/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0637e-04 - my_r2: 0.9468 - val_loss: 1.8137e-05 - val_my_r2: 0.9944\n",
      "Epoch 1283/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8867e-04 - my_r2: 0.9010 - val_loss: 1.4540e-05 - val_my_r2: 0.9957\n",
      "Epoch 1284/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8320e-04 - my_r2: 0.9378 - val_loss: 1.1682e-05 - val_my_r2: 0.9961\n",
      "Epoch 1285/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1224e-04 - my_r2: 0.9402 - val_loss: 1.6829e-05 - val_my_r2: 0.9941\n",
      "Epoch 1286/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7744e-04 - my_r2: 0.9331 - val_loss: 1.5819e-05 - val_my_r2: 0.9945\n",
      "Epoch 1287/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3762e-04 - my_r2: 0.9387 - val_loss: 1.3811e-05 - val_my_r2: 0.9956\n",
      "Epoch 1288/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9565e-04 - my_r2: 0.9212 - val_loss: 1.8323e-05 - val_my_r2: 0.9941\n",
      "Epoch 1289/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.1297e-04 - my_r2: 0.9153 - val_loss: 1.5410e-05 - val_my_r2: 0.9950\n",
      "Epoch 1290/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1521e-04 - my_r2: 0.9491 - val_loss: 1.3023e-05 - val_my_r2: 0.9959\n",
      "Epoch 1291/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6110e-04 - my_r2: 0.9160 - val_loss: 1.2163e-05 - val_my_r2: 0.9963\n",
      "Epoch 1292/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2111e-04 - my_r2: 0.9423 - val_loss: 1.1849e-05 - val_my_r2: 0.9962\n",
      "Epoch 1293/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1877e-04 - my_r2: 0.9314 - val_loss: 1.3217e-05 - val_my_r2: 0.9961\n",
      "Epoch 1294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2789e-04 - my_r2: 0.9266 - val_loss: 1.6689e-05 - val_my_r2: 0.9932\n",
      "Epoch 1295/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3811e-04 - my_r2: 0.8707 - val_loss: 1.5108e-05 - val_my_r2: 0.9936\n",
      "Epoch 1296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9229e-04 - my_r2: 0.9356 - val_loss: 1.1478e-05 - val_my_r2: 0.9955\n",
      "Epoch 1297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1394e-04 - my_r2: 0.9506 - val_loss: 1.1709e-05 - val_my_r2: 0.9958\n",
      "Epoch 1298/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2731e-04 - my_r2: 0.9099 - val_loss: 1.4895e-05 - val_my_r2: 0.9949\n",
      "Epoch 1299/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1757e-04 - my_r2: 0.9319 - val_loss: 1.7054e-05 - val_my_r2: 0.9943\n",
      "Epoch 1300/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3754e-04 - my_r2: 0.9412 - val_loss: 1.4276e-05 - val_my_r2: 0.9954\n",
      "Epoch 1301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5773e-04 - my_r2: 0.9143 - val_loss: 1.2824e-05 - val_my_r2: 0.9961\n",
      "Epoch 1302/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3488e-04 - my_r2: 0.9087 - val_loss: 2.2401e-05 - val_my_r2: 0.9927\n",
      "Epoch 1303/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8810e-04 - my_r2: 0.8966 - val_loss: 1.2849e-05 - val_my_r2: 0.9952\n",
      "Epoch 1304/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1843e-04 - my_r2: 0.9283 - val_loss: 1.5840e-05 - val_my_r2: 0.9945\n",
      "Epoch 1305/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4578e-04 - my_r2: 0.9448 - val_loss: 1.3589e-05 - val_my_r2: 0.9953\n",
      "Epoch 1306/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4389e-04 - my_r2: 0.9425 - val_loss: 9.9960e-06 - val_my_r2: 0.9964\n",
      "Epoch 1307/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4611e-04 - my_r2: 0.9397 - val_loss: 9.3040e-06 - val_my_r2: 0.9968\n",
      "Epoch 1308/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4432e-04 - my_r2: 0.9207 - val_loss: 1.1497e-05 - val_my_r2: 0.9955\n",
      "Epoch 1309/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5481e-04 - my_r2: 0.8992 - val_loss: 9.8878e-06 - val_my_r2: 0.9965\n",
      "Epoch 1310/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5251e-04 - my_r2: 0.9414 - val_loss: 1.0791e-05 - val_my_r2: 0.9963\n",
      "Epoch 1311/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4101e-04 - my_r2: 0.9469 - val_loss: 1.1070e-05 - val_my_r2: 0.9963\n",
      "Epoch 1312/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5700e-04 - my_r2: 0.9136 - val_loss: 1.3952e-05 - val_my_r2: 0.9948\n",
      "Epoch 1313/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6091e-04 - my_r2: 0.9351 - val_loss: 1.3635e-05 - val_my_r2: 0.9950\n",
      "Epoch 1314/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5872e-04 - my_r2: 0.9450 - val_loss: 1.2649e-05 - val_my_r2: 0.9955\n",
      "Epoch 1315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1165e-04 - my_r2: 0.9569 - val_loss: 1.4064e-05 - val_my_r2: 0.9950\n",
      "Epoch 1316/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8263e-04 - my_r2: 0.9332 - val_loss: 1.1313e-05 - val_my_r2: 0.9960\n",
      "Epoch 1317/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6739e-04 - my_r2: 0.9201 - val_loss: 1.4850e-05 - val_my_r2: 0.9948\n",
      "Epoch 1318/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3504e-04 - my_r2: 0.9265 - val_loss: 1.1868e-05 - val_my_r2: 0.9959\n",
      "Epoch 1319/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.5995e-04 - my_r2: 0.9166 - val_loss: 9.7040e-06 - val_my_r2: 0.9967\n",
      "Epoch 1320/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4769e-04 - my_r2: 0.8679 - val_loss: 1.8711e-05 - val_my_r2: 0.9937\n",
      "Epoch 1321/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5941e-04 - my_r2: 0.9480 - val_loss: 1.8852e-05 - val_my_r2: 0.9932\n",
      "Epoch 1322/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6489e-04 - my_r2: 0.9265 - val_loss: 1.3008e-05 - val_my_r2: 0.9951\n",
      "Epoch 1323/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2958e-04 - my_r2: 0.9067 - val_loss: 1.2827e-05 - val_my_r2: 0.9949\n",
      "Epoch 1324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8777e-04 - my_r2: 0.9202 - val_loss: 1.6914e-05 - val_my_r2: 0.9932\n",
      "Epoch 1325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8206e-04 - my_r2: 0.7674 - val_loss: 1.8729e-05 - val_my_r2: 0.9929\n",
      "Epoch 1326/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2486e-04 - my_r2: 0.9153 - val_loss: 1.3137e-05 - val_my_r2: 0.9951\n",
      "Epoch 1327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3452e-04 - my_r2: 0.9343 - val_loss: 1.2835e-05 - val_my_r2: 0.9950\n",
      "Epoch 1328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6707e-04 - my_r2: 0.9278 - val_loss: 1.3517e-05 - val_my_r2: 0.9948\n",
      "Epoch 1329/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3880e-04 - my_r2: 0.8959 - val_loss: 1.3776e-05 - val_my_r2: 0.9951\n",
      "Epoch 1330/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4001e-04 - my_r2: 0.9234 - val_loss: 1.1703e-05 - val_my_r2: 0.9963\n",
      "Epoch 1331/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1637e-04 - my_r2: 0.9402 - val_loss: 1.3328e-05 - val_my_r2: 0.9960\n",
      "Epoch 1332/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3055e-04 - my_r2: 0.9473 - val_loss: 1.4463e-05 - val_my_r2: 0.9957\n",
      "Epoch 1333/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6872e-04 - my_r2: 0.9472 - val_loss: 1.5890e-05 - val_my_r2: 0.9951\n",
      "Epoch 1334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5423e-04 - my_r2: 0.9059 - val_loss: 1.1849e-05 - val_my_r2: 0.9964\n",
      "Epoch 1335/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6345e-04 - my_r2: 0.9007 - val_loss: 1.8320e-05 - val_my_r2: 0.9944\n",
      "Epoch 1336/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6917e-04 - my_r2: 0.9297 - val_loss: 2.5691e-05 - val_my_r2: 0.9921\n",
      "Epoch 1337/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5090e-04 - my_r2: 0.8945 - val_loss: 1.5631e-05 - val_my_r2: 0.9949\n",
      "Epoch 1338/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8356e-04 - my_r2: 0.9395 - val_loss: 1.1633e-05 - val_my_r2: 0.9960\n",
      "Epoch 1339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5189e-04 - my_r2: 0.9496 - val_loss: 9.1776e-06 - val_my_r2: 0.9969\n",
      "Epoch 1340/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3509e-04 - my_r2: 0.9091 - val_loss: 9.6657e-06 - val_my_r2: 0.9966\n",
      "Epoch 1341/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7041e-04 - my_r2: 0.9382 - val_loss: 1.4685e-05 - val_my_r2: 0.9945\n",
      "Epoch 1342/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4448e-04 - my_r2: 0.9188 - val_loss: 1.1377e-05 - val_my_r2: 0.9960\n",
      "Epoch 1343/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7294e-04 - my_r2: 0.9153 - val_loss: 1.0590e-05 - val_my_r2: 0.9961\n",
      "Epoch 1344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9055e-04 - my_r2: 0.8853 - val_loss: 1.2400e-05 - val_my_r2: 0.9954\n",
      "Epoch 1345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1717e-04 - my_r2: 0.9502 - val_loss: 1.7107e-05 - val_my_r2: 0.9936\n",
      "Epoch 1346/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3495e-04 - my_r2: 0.8781 - val_loss: 1.5196e-05 - val_my_r2: 0.9946\n",
      "Epoch 1347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0694e-04 - my_r2: 0.9132 - val_loss: 1.4707e-05 - val_my_r2: 0.9950\n",
      "Epoch 1348/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5835e-04 - my_r2: 0.9504 - val_loss: 1.0871e-05 - val_my_r2: 0.9964\n",
      "Epoch 1349/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0537e-04 - my_r2: 0.8795 - val_loss: 1.2099e-05 - val_my_r2: 0.9958\n",
      "Epoch 1350/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8651e-04 - my_r2: 0.9010 - val_loss: 1.1391e-05 - val_my_r2: 0.9960\n",
      "Epoch 1351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3507e-04 - my_r2: 0.9543 - val_loss: 1.3240e-05 - val_my_r2: 0.9955\n",
      "Epoch 1352/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6246e-04 - my_r2: 0.9273 - val_loss: 1.2309e-05 - val_my_r2: 0.9960\n",
      "Epoch 1353/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2030e-04 - my_r2: 0.9476 - val_loss: 1.6843e-05 - val_my_r2: 0.9945\n",
      "Epoch 1354/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3934e-04 - my_r2: 0.9433 - val_loss: 1.5278e-05 - val_my_r2: 0.9948\n",
      "Epoch 1355/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0087e-04 - my_r2: 0.9077 - val_loss: 1.3410e-05 - val_my_r2: 0.9955\n",
      "Epoch 1356/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1594e-04 - my_r2: 0.9407 - val_loss: 1.1705e-05 - val_my_r2: 0.9959\n",
      "Epoch 1357/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3318e-04 - my_r2: 0.9467 - val_loss: 1.2331e-05 - val_my_r2: 0.9958\n",
      "Epoch 1358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0124e-04 - my_r2: 0.8934 - val_loss: 1.1255e-05 - val_my_r2: 0.9961\n",
      "Epoch 1359/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1724e-04 - my_r2: 0.9207 - val_loss: 1.5471e-05 - val_my_r2: 0.9947\n",
      "Epoch 1360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3503e-04 - my_r2: 0.9377 - val_loss: 1.5380e-05 - val_my_r2: 0.9945\n",
      "Epoch 1361/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3399e-04 - my_r2: 0.8557 - val_loss: 1.2767e-05 - val_my_r2: 0.9954\n",
      "Epoch 1362/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8968e-04 - my_r2: 0.9556 - val_loss: 1.6200e-05 - val_my_r2: 0.9943\n",
      "Epoch 1363/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6113e-04 - my_r2: 0.9003 - val_loss: 1.1685e-05 - val_my_r2: 0.9962\n",
      "Epoch 1364/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1137e-04 - my_r2: 0.9530 - val_loss: 1.2199e-05 - val_my_r2: 0.9963\n",
      "Epoch 1365/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3577e-04 - my_r2: 0.9450 - val_loss: 1.1015e-05 - val_my_r2: 0.9964\n",
      "Epoch 1366/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4637e-04 - my_r2: 0.9288 - val_loss: 1.1459e-05 - val_my_r2: 0.9960\n",
      "Epoch 1367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3461e-04 - my_r2: 0.9425 - val_loss: 1.3107e-05 - val_my_r2: 0.9952\n",
      "Epoch 1368/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3922e-04 - my_r2: 0.9452 - val_loss: 1.0692e-05 - val_my_r2: 0.9964\n",
      "Epoch 1369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4794e-04 - my_r2: 0.9449 - val_loss: 1.0358e-05 - val_my_r2: 0.9967\n",
      "Epoch 1370/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9129e-04 - my_r2: 0.9304 - val_loss: 9.0293e-06 - val_my_r2: 0.9972\n",
      "Epoch 1371/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5088e-04 - my_r2: 0.9223 - val_loss: 1.2458e-05 - val_my_r2: 0.9960\n",
      "Epoch 1372/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5425e-04 - my_r2: 0.9293 - val_loss: 1.7884e-05 - val_my_r2: 0.9945\n",
      "Epoch 1373/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.0868e-04 - my_r2: 0.9493 - val_loss: 1.3580e-05 - val_my_r2: 0.9958\n",
      "Epoch 1374/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7051e-04 - my_r2: 0.9165 - val_loss: 1.9344e-05 - val_my_r2: 0.9934\n",
      "Epoch 1375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7014e-04 - my_r2: 0.9254 - val_loss: 1.2193e-05 - val_my_r2: 0.9960\n",
      "Epoch 1376/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5778e-04 - my_r2: 0.9194 - val_loss: 1.2104e-05 - val_my_r2: 0.9961\n",
      "Epoch 1377/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.4067e-04 - my_r2: 0.8897 - val_loss: 1.3299e-05 - val_my_r2: 0.9956\n",
      "Epoch 1378/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8502e-04 - my_r2: 0.8980 - val_loss: 1.3799e-05 - val_my_r2: 0.9953\n",
      "Epoch 1379/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7598e-04 - my_r2: 0.9113 - val_loss: 1.2607e-05 - val_my_r2: 0.9959\n",
      "Epoch 1380/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3681e-04 - my_r2: 0.9415 - val_loss: 1.1954e-05 - val_my_r2: 0.9959\n",
      "Epoch 1381/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3369e-04 - my_r2: 0.9120 - val_loss: 1.1081e-05 - val_my_r2: 0.9961\n",
      "Epoch 1382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4338e-04 - my_r2: 0.9419 - val_loss: 1.2596e-05 - val_my_r2: 0.9955\n",
      "Epoch 1383/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3445e-04 - my_r2: 0.9224 - val_loss: 1.1604e-05 - val_my_r2: 0.9959\n",
      "Epoch 1384/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3341e-04 - my_r2: 0.9402 - val_loss: 1.1947e-05 - val_my_r2: 0.9958\n",
      "Epoch 1385/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 1.8521e-04 - my_r2: 0.9564 - val_loss: 1.2287e-05 - val_my_r2: 0.9956\n",
      "Epoch 1386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4288e-04 - my_r2: 0.9476 - val_loss: 1.6406e-05 - val_my_r2: 0.9943\n",
      "Epoch 1387/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6434e-04 - my_r2: 0.9150 - val_loss: 1.3229e-05 - val_my_r2: 0.9956\n",
      "Epoch 1388/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1318e-04 - my_r2: 0.9133 - val_loss: 1.3897e-05 - val_my_r2: 0.9955\n",
      "Epoch 1389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9654e-04 - my_r2: 0.9414 - val_loss: 1.4037e-05 - val_my_r2: 0.9956\n",
      "Epoch 1390/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6676e-04 - my_r2: 0.8500 - val_loss: 1.4535e-05 - val_my_r2: 0.9955\n",
      "Epoch 1391/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5530e-04 - my_r2: 0.9215 - val_loss: 1.3353e-05 - val_my_r2: 0.9958\n",
      "Epoch 1392/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5465e-04 - my_r2: 0.9271 - val_loss: 1.8559e-05 - val_my_r2: 0.9943\n",
      "Epoch 1393/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2243e-04 - my_r2: 0.9518 - val_loss: 1.8659e-05 - val_my_r2: 0.9940\n",
      "Epoch 1394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7439e-04 - my_r2: 0.9146 - val_loss: 1.3959e-05 - val_my_r2: 0.9956\n",
      "Epoch 1395/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.8766e-04 - my_r2: 0.9293 - val_loss: 1.6422e-05 - val_my_r2: 0.9947\n",
      "Epoch 1396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0785e-04 - my_r2: 0.9504 - val_loss: 2.0043e-05 - val_my_r2: 0.9937\n",
      "Epoch 1397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9917e-04 - my_r2: 0.9525 - val_loss: 2.0078e-05 - val_my_r2: 0.9936\n",
      "Epoch 1398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7742e-04 - my_r2: 0.9089 - val_loss: 1.4938e-05 - val_my_r2: 0.9950\n",
      "Epoch 1399/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6825e-04 - my_r2: 0.9447 - val_loss: 1.1310e-05 - val_my_r2: 0.9963\n",
      "Epoch 1400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7903e-04 - my_r2: 0.9241 - val_loss: 1.1877e-05 - val_my_r2: 0.9961\n",
      "Epoch 1401/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3969e-04 - my_r2: 0.9142 - val_loss: 1.1192e-05 - val_my_r2: 0.9963\n",
      "Epoch 1402/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0880e-04 - my_r2: 0.9296 - val_loss: 1.1350e-05 - val_my_r2: 0.9960\n",
      "Epoch 1403/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7411e-04 - my_r2: 0.8656 - val_loss: 1.1637e-05 - val_my_r2: 0.9958\n",
      "Epoch 1404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5077e-04 - my_r2: 0.9446 - val_loss: 1.3336e-05 - val_my_r2: 0.9954\n",
      "Epoch 1405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9730e-04 - my_r2: 0.9344 - val_loss: 1.2770e-05 - val_my_r2: 0.9957\n",
      "Epoch 1406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1843e-04 - my_r2: 0.9467 - val_loss: 1.4036e-05 - val_my_r2: 0.9949\n",
      "Epoch 1407/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9693e-04 - my_r2: 0.9434 - val_loss: 1.5256e-05 - val_my_r2: 0.9942\n",
      "Epoch 1408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6722e-04 - my_r2: 0.9684 - val_loss: 1.3900e-05 - val_my_r2: 0.9951\n",
      "Epoch 1409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5468e-04 - my_r2: 0.8442 - val_loss: 2.1371e-05 - val_my_r2: 0.9924\n",
      "Epoch 1410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1123e-04 - my_r2: 0.9274 - val_loss: 2.1354e-05 - val_my_r2: 0.9928\n",
      "Epoch 1411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7782e-04 - my_r2: 0.9241 - val_loss: 1.7342e-05 - val_my_r2: 0.9943\n",
      "Epoch 1412/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5226e-04 - my_r2: 0.9437 - val_loss: 1.3799e-05 - val_my_r2: 0.9954\n",
      "Epoch 1413/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3367e-04 - my_r2: 0.8977 - val_loss: 1.3792e-05 - val_my_r2: 0.9953\n",
      "Epoch 1414/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.8586e-04 - my_r2: 0.9296 - val_loss: 1.5107e-05 - val_my_r2: 0.9947\n",
      "Epoch 1415/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1279e-04 - my_r2: 0.9335 - val_loss: 1.3850e-05 - val_my_r2: 0.9949\n",
      "Epoch 1416/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6601e-04 - my_r2: 0.9058 - val_loss: 1.3636e-05 - val_my_r2: 0.9949\n",
      "Epoch 1417/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3913e-04 - my_r2: 0.9387 - val_loss: 1.4107e-05 - val_my_r2: 0.9948\n",
      "Epoch 1418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0622e-04 - my_r2: 0.9044 - val_loss: 1.5024e-05 - val_my_r2: 0.9948\n",
      "Epoch 1419/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9432e-04 - my_r2: 0.9383 - val_loss: 1.3791e-05 - val_my_r2: 0.9954\n",
      "Epoch 1420/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3679e-04 - my_r2: 0.9418 - val_loss: 1.2348e-05 - val_my_r2: 0.9956\n",
      "Epoch 1421/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4059e-04 - my_r2: 0.9527 - val_loss: 1.0880e-05 - val_my_r2: 0.9962\n",
      "Epoch 1422/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1714e-04 - my_r2: 0.9397 - val_loss: 1.0651e-05 - val_my_r2: 0.9964\n",
      "Epoch 1423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3014e-04 - my_r2: 0.7833 - val_loss: 1.2715e-05 - val_my_r2: 0.9956\n",
      "Epoch 1424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3607e-04 - my_r2: 0.9199 - val_loss: 1.1358e-05 - val_my_r2: 0.9962\n",
      "Epoch 1425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6834e-04 - my_r2: 0.9282 - val_loss: 1.0867e-05 - val_my_r2: 0.9963\n",
      "Epoch 1426/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2158e-04 - my_r2: 0.8947 - val_loss: 9.8398e-06 - val_my_r2: 0.9967\n",
      "Epoch 1427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4344e-04 - my_r2: 0.9334 - val_loss: 9.4685e-06 - val_my_r2: 0.9968\n",
      "Epoch 1428/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4746e-04 - my_r2: 0.8896 - val_loss: 1.1575e-05 - val_my_r2: 0.9958\n",
      "Epoch 1429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7448e-04 - my_r2: 0.8907 - val_loss: 1.1145e-05 - val_my_r2: 0.9963\n",
      "Epoch 1430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1477e-04 - my_r2: 0.9388 - val_loss: 9.7799e-06 - val_my_r2: 0.9969\n",
      "Epoch 1431/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8946e-04 - my_r2: 0.9579 - val_loss: 1.0672e-05 - val_my_r2: 0.9967\n",
      "Epoch 1432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3328e-04 - my_r2: 0.9374 - val_loss: 1.2337e-05 - val_my_r2: 0.9963\n",
      "Epoch 1433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3415e-04 - my_r2: 0.9355 - val_loss: 1.2286e-05 - val_my_r2: 0.9963\n",
      "Epoch 1434/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3407e-04 - my_r2: 0.8800 - val_loss: 1.3369e-05 - val_my_r2: 0.9961\n",
      "Epoch 1435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6381e-04 - my_r2: 0.9333 - val_loss: 1.4907e-05 - val_my_r2: 0.9958\n",
      "Epoch 1436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4725e-04 - my_r2: 0.9373 - val_loss: 2.0475e-05 - val_my_r2: 0.9938\n",
      "Epoch 1437/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6754e-04 - my_r2: 0.9122 - val_loss: 1.2804e-05 - val_my_r2: 0.9962\n",
      "Epoch 1438/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6091e-04 - my_r2: 0.9347 - val_loss: 1.7780e-05 - val_my_r2: 0.9939\n",
      "Epoch 1439/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1853e-04 - my_r2: 0.9520 - val_loss: 2.1786e-05 - val_my_r2: 0.9920\n",
      "Epoch 1440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4592e-04 - my_r2: 0.9381 - val_loss: 1.9278e-05 - val_my_r2: 0.9929\n",
      "Epoch 1441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2059e-04 - my_r2: 0.9306 - val_loss: 1.5557e-05 - val_my_r2: 0.9946\n",
      "Epoch 1442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9417e-04 - my_r2: 0.9213 - val_loss: 9.9955e-06 - val_my_r2: 0.9969\n",
      "Epoch 1443/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0630e-04 - my_r2: 0.9155 - val_loss: 1.1212e-05 - val_my_r2: 0.9964\n",
      "Epoch 1444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2544e-04 - my_r2: 0.9302 - val_loss: 9.6818e-06 - val_my_r2: 0.9970\n",
      "Epoch 1445/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4933e-04 - my_r2: 0.9300 - val_loss: 8.5964e-06 - val_my_r2: 0.9973\n",
      "Epoch 1446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4675e-04 - my_r2: 0.9436 - val_loss: 1.1638e-05 - val_my_r2: 0.9959\n",
      "Epoch 1447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2619e-04 - my_r2: 0.9085 - val_loss: 9.1392e-06 - val_my_r2: 0.9970\n",
      "Epoch 1448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1112e-04 - my_r2: 0.9517 - val_loss: 1.0641e-05 - val_my_r2: 0.9965\n",
      "Epoch 1449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0441e-04 - my_r2: 0.9384 - val_loss: 9.8546e-06 - val_my_r2: 0.9965\n",
      "Epoch 1450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3434e-04 - my_r2: 0.9330 - val_loss: 1.0225e-05 - val_my_r2: 0.9966\n",
      "Epoch 1451/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4913e-04 - my_r2: 0.8976 - val_loss: 2.0089e-05 - val_my_r2: 0.9926\n",
      "Epoch 1452/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5073e-04 - my_r2: 0.8972 - val_loss: 1.5737e-05 - val_my_r2: 0.9943\n",
      "Epoch 1453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9874e-04 - my_r2: 0.9288 - val_loss: 1.3523e-05 - val_my_r2: 0.9951\n",
      "Epoch 1454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5531e-04 - my_r2: 0.8763 - val_loss: 1.5077e-05 - val_my_r2: 0.9945\n",
      "Epoch 1455/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0091e-04 - my_r2: 0.9486 - val_loss: 1.6396e-05 - val_my_r2: 0.9941\n",
      "Epoch 1456/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4496e-04 - my_r2: 0.9380 - val_loss: 1.5894e-05 - val_my_r2: 0.9948\n",
      "Epoch 1457/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8164e-04 - my_r2: 0.8993 - val_loss: 1.4430e-05 - val_my_r2: 0.9955\n",
      "Epoch 1458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5106e-04 - my_r2: 0.9148 - val_loss: 1.3243e-05 - val_my_r2: 0.9954\n",
      "Epoch 1459/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1534e-04 - my_r2: 0.9350 - val_loss: 1.5892e-05 - val_my_r2: 0.9943\n",
      "Epoch 1460/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2807e-04 - my_r2: 0.9521 - val_loss: 1.0566e-05 - val_my_r2: 0.9962\n",
      "Epoch 1461/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8700e-04 - my_r2: 0.9362 - val_loss: 1.0782e-05 - val_my_r2: 0.9961\n",
      "Epoch 1462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7316e-04 - my_r2: 0.9193 - val_loss: 1.1705e-05 - val_my_r2: 0.9959\n",
      "Epoch 1463/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1345e-04 - my_r2: 0.9409 - val_loss: 1.4641e-05 - val_my_r2: 0.9947\n",
      "Epoch 1464/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1419e-04 - my_r2: 0.9359 - val_loss: 1.1148e-05 - val_my_r2: 0.9960\n",
      "Epoch 1465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4922e-04 - my_r2: 0.9045 - val_loss: 9.3362e-06 - val_my_r2: 0.9968\n",
      "Epoch 1466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9751e-04 - my_r2: 0.9405 - val_loss: 1.0172e-05 - val_my_r2: 0.9966\n",
      "Epoch 1467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7080e-04 - my_r2: 0.7686 - val_loss: 1.1659e-05 - val_my_r2: 0.9960\n",
      "Epoch 1468/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4621e-04 - my_r2: 0.9435 - val_loss: 1.0613e-05 - val_my_r2: 0.9964\n",
      "Epoch 1469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7896e-04 - my_r2: 0.9522 - val_loss: 1.1811e-05 - val_my_r2: 0.9962\n",
      "Epoch 1470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3685e-04 - my_r2: 0.9114 - val_loss: 1.1196e-05 - val_my_r2: 0.9963\n",
      "Epoch 1471/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2674e-04 - my_r2: 0.9444 - val_loss: 1.0248e-05 - val_my_r2: 0.9967\n",
      "Epoch 1472/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8878e-04 - my_r2: 0.9260 - val_loss: 1.2117e-05 - val_my_r2: 0.9957\n",
      "Epoch 1473/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4488e-04 - my_r2: 0.9226 - val_loss: 1.1418e-05 - val_my_r2: 0.9960\n",
      "Epoch 1474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5250e-04 - my_r2: 0.9225 - val_loss: 1.1478e-05 - val_my_r2: 0.9963\n",
      "Epoch 1475/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7697e-04 - my_r2: 0.9629 - val_loss: 1.3421e-05 - val_my_r2: 0.9958\n",
      "Epoch 1476/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.1766e-04 - my_r2: 0.9562 - val_loss: 1.1272e-05 - val_my_r2: 0.9965\n",
      "Epoch 1477/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8303e-04 - my_r2: 0.9152 - val_loss: 1.4304e-05 - val_my_r2: 0.9953\n",
      "Epoch 1478/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0384e-04 - my_r2: 0.9311 - val_loss: 1.4844e-05 - val_my_r2: 0.9950\n",
      "Epoch 1479/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6688e-04 - my_r2: 0.9012 - val_loss: 1.4450e-05 - val_my_r2: 0.9954\n",
      "Epoch 1480/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6775e-04 - my_r2: 0.9309 - val_loss: 1.5946e-05 - val_my_r2: 0.9951\n",
      "Epoch 1481/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0107e-04 - my_r2: 0.9125 - val_loss: 1.2468e-05 - val_my_r2: 0.9961\n",
      "Epoch 1482/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7485e-04 - my_r2: 0.9476 - val_loss: 1.3424e-05 - val_my_r2: 0.9961\n",
      "Epoch 1483/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9827e-04 - my_r2: 0.9395 - val_loss: 1.4311e-05 - val_my_r2: 0.9955\n",
      "Epoch 1484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4652e-04 - my_r2: 0.9095 - val_loss: 1.1009e-05 - val_my_r2: 0.9965\n",
      "Epoch 1485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3451e-04 - my_r2: 0.9338 - val_loss: 1.0434e-05 - val_my_r2: 0.9966\n",
      "Epoch 1486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4221e-04 - my_r2: 0.9342 - val_loss: 1.2521e-05 - val_my_r2: 0.9960\n",
      "Epoch 1487/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8473e-04 - my_r2: 0.9396 - val_loss: 1.4255e-05 - val_my_r2: 0.9955\n",
      "Epoch 1488/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1739e-04 - my_r2: 0.9247 - val_loss: 1.0989e-05 - val_my_r2: 0.9963\n",
      "Epoch 1489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1130e-04 - my_r2: 0.9410 - val_loss: 1.2034e-05 - val_my_r2: 0.9957\n",
      "Epoch 1490/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1423e-04 - my_r2: 0.9387 - val_loss: 1.4415e-05 - val_my_r2: 0.9951\n",
      "Epoch 1491/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9223e-04 - my_r2: 0.9119 - val_loss: 1.4423e-05 - val_my_r2: 0.9951\n",
      "Epoch 1492/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6841e-04 - my_r2: 0.9138 - val_loss: 1.4101e-05 - val_my_r2: 0.9950\n",
      "Epoch 1493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3731e-04 - my_r2: 0.9196 - val_loss: 1.0243e-05 - val_my_r2: 0.9965\n",
      "Epoch 1494/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.1768e-04 - my_r2: 0.9303 - val_loss: 1.2348e-05 - val_my_r2: 0.9961\n",
      "Epoch 1495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5164e-04 - my_r2: 0.9417 - val_loss: 1.1922e-05 - val_my_r2: 0.9963\n",
      "Epoch 1496/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7458e-04 - my_r2: 0.9254 - val_loss: 1.4369e-05 - val_my_r2: 0.9954\n",
      "Epoch 1497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6100e-04 - my_r2: 0.9041 - val_loss: 1.3005e-05 - val_my_r2: 0.9956\n",
      "Epoch 1498/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4281e-04 - my_r2: 0.9405 - val_loss: 1.4496e-05 - val_my_r2: 0.9946\n",
      "Epoch 1499/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5068e-04 - my_r2: 0.9065 - val_loss: 1.0788e-05 - val_my_r2: 0.9961\n",
      "Epoch 1500/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5054e-04 - my_r2: 0.9432 - val_loss: 1.1326e-05 - val_my_r2: 0.9961\n",
      "Epoch 1501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2080e-04 - my_r2: 0.8990 - val_loss: 1.3601e-05 - val_my_r2: 0.9951\n",
      "Epoch 1502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2453e-04 - my_r2: 0.9141 - val_loss: 1.7753e-05 - val_my_r2: 0.9936\n",
      "Epoch 1503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8166e-04 - my_r2: 0.9381 - val_loss: 1.1300e-05 - val_my_r2: 0.9960\n",
      "Epoch 1504/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5747e-04 - my_r2: 0.9469 - val_loss: 1.3332e-05 - val_my_r2: 0.9949\n",
      "Epoch 1505/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4254e-04 - my_r2: 0.9281 - val_loss: 8.8076e-06 - val_my_r2: 0.9969\n",
      "Epoch 1506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0143e-04 - my_r2: 0.9611 - val_loss: 1.0176e-05 - val_my_r2: 0.9965\n",
      "Epoch 1507/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4253e-04 - my_r2: 0.9466 - val_loss: 8.9165e-06 - val_my_r2: 0.9967\n",
      "Epoch 1508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1630e-04 - my_r2: 0.9213 - val_loss: 1.0363e-05 - val_my_r2: 0.9965\n",
      "Epoch 1509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3027e-04 - my_r2: 0.8795 - val_loss: 1.0434e-05 - val_my_r2: 0.9961\n",
      "Epoch 1510/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4817e-04 - my_r2: 0.9372 - val_loss: 2.2665e-05 - val_my_r2: 0.9913\n",
      "Epoch 1511/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2823e-04 - my_r2: 0.9505 - val_loss: 1.9478e-05 - val_my_r2: 0.9924\n",
      "Epoch 1512/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9367e-04 - my_r2: 0.9049 - val_loss: 1.2937e-05 - val_my_r2: 0.9952\n",
      "Epoch 1513/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8509e-04 - my_r2: 0.9159 - val_loss: 1.1182e-05 - val_my_r2: 0.9963\n",
      "Epoch 1514/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4410e-04 - my_r2: 0.9313 - val_loss: 1.2094e-05 - val_my_r2: 0.9962\n",
      "Epoch 1515/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.4559e-04 - my_r2: 0.9418 - val_loss: 1.0418e-05 - val_my_r2: 0.9967\n",
      "Epoch 1516/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6603e-04 - my_r2: 0.8737 - val_loss: 8.5567e-06 - val_my_r2: 0.9973\n",
      "Epoch 1517/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6726e-04 - my_r2: 0.9265 - val_loss: 1.5264e-05 - val_my_r2: 0.9943\n",
      "Epoch 1518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2206e-04 - my_r2: 0.9394 - val_loss: 9.8567e-06 - val_my_r2: 0.9967\n",
      "Epoch 1519/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1977e-04 - my_r2: 0.9128 - val_loss: 1.1375e-05 - val_my_r2: 0.9960\n",
      "Epoch 1520/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1526e-04 - my_r2: 0.9405 - val_loss: 9.7700e-06 - val_my_r2: 0.9966\n",
      "Epoch 1521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3972e-04 - my_r2: 0.9220 - val_loss: 1.0787e-05 - val_my_r2: 0.9966\n",
      "Epoch 1522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9163e-04 - my_r2: 0.9319 - val_loss: 1.0008e-05 - val_my_r2: 0.9968\n",
      "Epoch 1523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4247e-04 - my_r2: 0.9500 - val_loss: 1.1500e-05 - val_my_r2: 0.9963\n",
      "Epoch 1524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3810e-04 - my_r2: 0.9238 - val_loss: 1.3512e-05 - val_my_r2: 0.9952\n",
      "Epoch 1525/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9118e-04 - my_r2: 0.9293 - val_loss: 1.2554e-05 - val_my_r2: 0.9953\n",
      "Epoch 1526/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2378e-04 - my_r2: 0.9204 - val_loss: 8.6647e-06 - val_my_r2: 0.9969\n",
      "Epoch 1527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4196e-04 - my_r2: 0.9281 - val_loss: 7.5856e-06 - val_my_r2: 0.9973\n",
      "Epoch 1528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3388e-04 - my_r2: 0.9366 - val_loss: 1.0304e-05 - val_my_r2: 0.9962\n",
      "Epoch 1529/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1138e-04 - my_r2: 0.9515 - val_loss: 8.9100e-06 - val_my_r2: 0.9969\n",
      "Epoch 1530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5698e-04 - my_r2: 0.9466 - val_loss: 7.8990e-06 - val_my_r2: 0.9970\n",
      "Epoch 1531/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8800e-04 - my_r2: 0.9360 - val_loss: 8.5469e-06 - val_my_r2: 0.9967\n",
      "Epoch 1532/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4576e-04 - my_r2: 0.9362 - val_loss: 8.5592e-06 - val_my_r2: 0.9970\n",
      "Epoch 1533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6124e-04 - my_r2: 0.9164 - val_loss: 1.4127e-05 - val_my_r2: 0.9948\n",
      "Epoch 1534/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6361e-04 - my_r2: 0.9466 - val_loss: 1.4875e-05 - val_my_r2: 0.9943\n",
      "Epoch 1535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1969e-04 - my_r2: 0.9359 - val_loss: 9.5980e-06 - val_my_r2: 0.9964\n",
      "Epoch 1536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4834e-04 - my_r2: 0.9164 - val_loss: 9.4602e-06 - val_my_r2: 0.9967\n",
      "Epoch 1537/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3675e-04 - my_r2: 0.9289 - val_loss: 8.2652e-06 - val_my_r2: 0.9972\n",
      "Epoch 1538/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6033e-04 - my_r2: 0.8839 - val_loss: 8.7040e-06 - val_my_r2: 0.9968\n",
      "Epoch 1539/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8358e-04 - my_r2: 0.9285 - val_loss: 7.8900e-06 - val_my_r2: 0.9971\n",
      "Epoch 1540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4793e-04 - my_r2: 0.9303 - val_loss: 9.1402e-06 - val_my_r2: 0.9967\n",
      "Epoch 1541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9474e-04 - my_r2: 0.9513 - val_loss: 1.0568e-05 - val_my_r2: 0.9963\n",
      "Epoch 1542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1582e-04 - my_r2: 0.9231 - val_loss: 1.2084e-05 - val_my_r2: 0.9959\n",
      "Epoch 1543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7224e-04 - my_r2: 0.9016 - val_loss: 1.0111e-05 - val_my_r2: 0.9963\n",
      "Epoch 1544/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9251e-04 - my_r2: 0.9307 - val_loss: 1.1134e-05 - val_my_r2: 0.9962\n",
      "Epoch 1545/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5315e-04 - my_r2: 0.9284 - val_loss: 1.0340e-05 - val_my_r2: 0.9967\n",
      "Epoch 1546/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9652e-04 - my_r2: 0.9307 - val_loss: 1.2751e-05 - val_my_r2: 0.9958\n",
      "Epoch 1547/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2096e-04 - my_r2: 0.9520 - val_loss: 1.4892e-05 - val_my_r2: 0.9946\n",
      "Epoch 1548/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4053e-04 - my_r2: 0.8958 - val_loss: 1.4416e-05 - val_my_r2: 0.9948\n",
      "Epoch 1549/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1085e-04 - my_r2: 0.8582 - val_loss: 1.3978e-05 - val_my_r2: 0.9952\n",
      "Epoch 1550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1456e-04 - my_r2: 0.9434 - val_loss: 1.2036e-05 - val_my_r2: 0.9961\n",
      "Epoch 1551/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3583e-04 - my_r2: 0.9194 - val_loss: 9.0271e-06 - val_my_r2: 0.9969\n",
      "Epoch 1552/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5749e-04 - my_r2: 0.9180 - val_loss: 1.1848e-05 - val_my_r2: 0.9959\n",
      "Epoch 1553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8198e-04 - my_r2: 0.8800 - val_loss: 1.1681e-05 - val_my_r2: 0.9962\n",
      "Epoch 1554/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7950e-04 - my_r2: 0.9448 - val_loss: 1.1605e-05 - val_my_r2: 0.9962\n",
      "Epoch 1555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8787e-04 - my_r2: 0.9263 - val_loss: 1.1364e-05 - val_my_r2: 0.9961\n",
      "Epoch 1556/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6234e-04 - my_r2: 0.9296 - val_loss: 2.0148e-05 - val_my_r2: 0.9929\n",
      "Epoch 1557/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3662e-04 - my_r2: 0.9364 - val_loss: 1.2917e-05 - val_my_r2: 0.9958\n",
      "Epoch 1558/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0003e-04 - my_r2: 0.9431 - val_loss: 1.3792e-05 - val_my_r2: 0.9955\n",
      "Epoch 1559/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7795e-04 - my_r2: 0.9372 - val_loss: 1.3160e-05 - val_my_r2: 0.9957\n",
      "Epoch 1560/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6848e-04 - my_r2: 0.9343 - val_loss: 1.2456e-05 - val_my_r2: 0.9952\n",
      "Epoch 1561/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5078e-04 - my_r2: 0.9309 - val_loss: 1.6574e-05 - val_my_r2: 0.9933\n",
      "Epoch 1562/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9563e-04 - my_r2: 0.9212 - val_loss: 1.3888e-05 - val_my_r2: 0.9949\n",
      "Epoch 1563/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9728e-04 - my_r2: 0.9091 - val_loss: 1.2767e-05 - val_my_r2: 0.9956\n",
      "Epoch 1564/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3003e-04 - my_r2: 0.9313 - val_loss: 1.0150e-05 - val_my_r2: 0.9970\n",
      "Epoch 1565/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2959e-04 - my_r2: 0.9349 - val_loss: 1.1203e-05 - val_my_r2: 0.9967\n",
      "Epoch 1566/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7355e-04 - my_r2: 0.8969 - val_loss: 9.9761e-06 - val_my_r2: 0.9970\n",
      "Epoch 1567/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7033e-04 - my_r2: 0.9383 - val_loss: 1.0927e-05 - val_my_r2: 0.9964\n",
      "Epoch 1568/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7192e-04 - my_r2: 0.9233 - val_loss: 1.1798e-05 - val_my_r2: 0.9962\n",
      "Epoch 1569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5393e-04 - my_r2: 0.9464 - val_loss: 1.1429e-05 - val_my_r2: 0.9962\n",
      "Epoch 1570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3162e-04 - my_r2: 0.9360 - val_loss: 9.8644e-06 - val_my_r2: 0.9968\n",
      "Epoch 1571/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5545e-04 - my_r2: 0.9512 - val_loss: 1.0922e-05 - val_my_r2: 0.9964\n",
      "Epoch 1572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9395e-04 - my_r2: 0.9009 - val_loss: 1.3975e-05 - val_my_r2: 0.9953\n",
      "Epoch 1573/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2615e-04 - my_r2: 0.9574 - val_loss: 1.2237e-05 - val_my_r2: 0.9960\n",
      "Epoch 1574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6157e-04 - my_r2: 0.9156 - val_loss: 1.0915e-05 - val_my_r2: 0.9963\n",
      "Epoch 1575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0746e-04 - my_r2: 0.9219 - val_loss: 9.2886e-06 - val_my_r2: 0.9966\n",
      "Epoch 1576/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8010e-04 - my_r2: 0.9281 - val_loss: 1.0904e-05 - val_my_r2: 0.9964\n",
      "Epoch 1577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0739e-04 - my_r2: 0.9300 - val_loss: 9.9300e-06 - val_my_r2: 0.9966\n",
      "Epoch 1578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8071e-04 - my_r2: 0.9500 - val_loss: 1.0281e-05 - val_my_r2: 0.9964\n",
      "Epoch 1579/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6136e-04 - my_r2: 0.9388 - val_loss: 1.2309e-05 - val_my_r2: 0.9959\n",
      "Epoch 1580/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8557e-04 - my_r2: 0.9477 - val_loss: 1.1506e-05 - val_my_r2: 0.9963\n",
      "Epoch 1581/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6192e-04 - my_r2: 0.9359 - val_loss: 1.1479e-05 - val_my_r2: 0.9964\n",
      "Epoch 1582/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1818e-04 - my_r2: 0.9579 - val_loss: 1.2931e-05 - val_my_r2: 0.9956\n",
      "Epoch 1583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6702e-04 - my_r2: 0.9283 - val_loss: 1.2070e-05 - val_my_r2: 0.9957\n",
      "Epoch 1584/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3227e-04 - my_r2: 0.9516 - val_loss: 9.9599e-06 - val_my_r2: 0.9967\n",
      "Epoch 1585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1275e-04 - my_r2: 0.9335 - val_loss: 1.0390e-05 - val_my_r2: 0.9967\n",
      "Epoch 1586/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8612e-04 - my_r2: 0.8990 - val_loss: 1.0789e-05 - val_my_r2: 0.9966\n",
      "Epoch 1587/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1880e-04 - my_r2: 0.9463 - val_loss: 1.5277e-05 - val_my_r2: 0.9952\n",
      "Epoch 1588/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4567e-04 - my_r2: 0.9516 - val_loss: 1.3897e-05 - val_my_r2: 0.9957\n",
      "Epoch 1589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1236e-04 - my_r2: 0.9424 - val_loss: 1.6662e-05 - val_my_r2: 0.9943\n",
      "Epoch 1590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9766e-04 - my_r2: 0.9061 - val_loss: 1.2957e-05 - val_my_r2: 0.9957\n",
      "Epoch 1591/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8581e-04 - my_r2: 0.9476 - val_loss: 1.7307e-05 - val_my_r2: 0.9937\n",
      "Epoch 1592/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4002e-04 - my_r2: 0.9224 - val_loss: 1.6083e-05 - val_my_r2: 0.9942\n",
      "Epoch 1593/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7083e-04 - my_r2: 0.9258 - val_loss: 1.3304e-05 - val_my_r2: 0.9948\n",
      "Epoch 1594/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5900e-04 - my_r2: 0.9351 - val_loss: 1.2465e-05 - val_my_r2: 0.9955\n",
      "Epoch 1595/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9525e-04 - my_r2: 0.9482 - val_loss: 1.3988e-05 - val_my_r2: 0.9952\n",
      "Epoch 1596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2461e-04 - my_r2: 0.9381 - val_loss: 1.3955e-05 - val_my_r2: 0.9952\n",
      "Epoch 1597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3277e-04 - my_r2: 0.9329 - val_loss: 1.5948e-05 - val_my_r2: 0.9947\n",
      "Epoch 1598/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9237e-04 - my_r2: 0.8852 - val_loss: 1.3685e-05 - val_my_r2: 0.9957\n",
      "Epoch 1599/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1165e-04 - my_r2: 0.9390 - val_loss: 1.1130e-05 - val_my_r2: 0.9964\n",
      "Epoch 1600/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2035e-04 - my_r2: 0.9327 - val_loss: 9.1901e-06 - val_my_r2: 0.9968\n",
      "Epoch 1601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1343e-04 - my_r2: 0.9582 - val_loss: 9.4520e-06 - val_my_r2: 0.9964\n",
      "Epoch 1602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3672e-04 - my_r2: 0.9204 - val_loss: 1.0746e-05 - val_my_r2: 0.9968\n",
      "Epoch 1603/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2528e-04 - my_r2: 0.9432 - val_loss: 8.0352e-06 - val_my_r2: 0.9978\n",
      "Epoch 1604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4563e-04 - my_r2: 0.9364 - val_loss: 1.1260e-05 - val_my_r2: 0.9964\n",
      "Epoch 1605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7308e-04 - my_r2: 0.9279 - val_loss: 9.4601e-06 - val_my_r2: 0.9969\n",
      "Epoch 1606/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8203e-04 - my_r2: 0.9209 - val_loss: 1.0229e-05 - val_my_r2: 0.9965\n",
      "Epoch 1607/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.1421e-04 - my_r2: 0.9182 - val_loss: 1.0097e-05 - val_my_r2: 0.9965\n",
      "Epoch 1608/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3883e-04 - my_r2: 0.8674 - val_loss: 1.1145e-05 - val_my_r2: 0.9963\n",
      "Epoch 1609/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6879e-04 - my_r2: 0.9058 - val_loss: 1.1652e-05 - val_my_r2: 0.9959\n",
      "Epoch 1610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4355e-04 - my_r2: 0.9543 - val_loss: 1.2967e-05 - val_my_r2: 0.9956\n",
      "Epoch 1611/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0619e-04 - my_r2: 0.9537 - val_loss: 9.9862e-06 - val_my_r2: 0.9967\n",
      "Epoch 1612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8649e-04 - my_r2: 0.9255 - val_loss: 1.1059e-05 - val_my_r2: 0.9962\n",
      "Epoch 1613/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7198e-04 - my_r2: 0.9451 - val_loss: 1.0728e-05 - val_my_r2: 0.9965\n",
      "Epoch 1614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2003e-04 - my_r2: 0.9361 - val_loss: 9.9637e-06 - val_my_r2: 0.9968\n",
      "Epoch 1615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1759e-04 - my_r2: 0.9221 - val_loss: 1.4370e-05 - val_my_r2: 0.9950\n",
      "Epoch 1616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8389e-04 - my_r2: 0.9084 - val_loss: 1.2779e-05 - val_my_r2: 0.9959\n",
      "Epoch 1617/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6556e-04 - my_r2: 0.9344 - val_loss: 1.2053e-05 - val_my_r2: 0.9964\n",
      "Epoch 1618/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.5411e-04 - my_r2: 0.9395 - val_loss: 8.7791e-06 - val_my_r2: 0.9973\n",
      "Epoch 1619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1265e-04 - my_r2: 0.9093 - val_loss: 9.9609e-06 - val_my_r2: 0.9969\n",
      "Epoch 1620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5152e-04 - my_r2: 0.9494 - val_loss: 1.6975e-05 - val_my_r2: 0.9944\n",
      "Epoch 1621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6581e-04 - my_r2: 0.9234 - val_loss: 1.3428e-05 - val_my_r2: 0.9952\n",
      "Epoch 1622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2486e-04 - my_r2: 0.9238 - val_loss: 9.7797e-06 - val_my_r2: 0.9963\n",
      "Epoch 1623/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7412e-04 - my_r2: 0.9357 - val_loss: 1.2289e-05 - val_my_r2: 0.9948\n",
      "Epoch 1624/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2625e-04 - my_r2: 0.9425 - val_loss: 1.2004e-05 - val_my_r2: 0.9948\n",
      "Epoch 1625/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2408e-04 - my_r2: 0.9407 - val_loss: 1.1996e-05 - val_my_r2: 0.9952\n",
      "Epoch 1626/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2112e-04 - my_r2: 0.9516 - val_loss: 1.2764e-05 - val_my_r2: 0.9951\n",
      "Epoch 1627/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7386e-04 - my_r2: 0.9055 - val_loss: 1.2804e-05 - val_my_r2: 0.9949\n",
      "Epoch 1628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5863e-04 - my_r2: 0.9501 - val_loss: 1.6937e-05 - val_my_r2: 0.9939\n",
      "Epoch 1629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8480e-04 - my_r2: 0.9197 - val_loss: 1.7112e-05 - val_my_r2: 0.9937\n",
      "Epoch 1630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9764e-04 - my_r2: 0.9186 - val_loss: 8.8732e-06 - val_my_r2: 0.9969\n",
      "Epoch 1631/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9516e-04 - my_r2: 0.9468 - val_loss: 9.6821e-06 - val_my_r2: 0.9969\n",
      "Epoch 1632/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6707e-04 - my_r2: 0.9344 - val_loss: 1.0373e-05 - val_my_r2: 0.9967\n",
      "Epoch 1633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1639e-04 - my_r2: 0.9423 - val_loss: 1.2994e-05 - val_my_r2: 0.9959\n",
      "Epoch 1634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3915e-04 - my_r2: 0.9445 - val_loss: 1.0377e-05 - val_my_r2: 0.9970\n",
      "Epoch 1635/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5795e-04 - my_r2: 0.9207 - val_loss: 1.1769e-05 - val_my_r2: 0.9966\n",
      "Epoch 1636/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4789e-04 - my_r2: 0.9282 - val_loss: 1.1636e-05 - val_my_r2: 0.9961\n",
      "Epoch 1637/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4740e-04 - my_r2: 0.9396 - val_loss: 1.2873e-05 - val_my_r2: 0.9956\n",
      "Epoch 1638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2828e-04 - my_r2: 0.9446 - val_loss: 1.0755e-05 - val_my_r2: 0.9964\n",
      "Epoch 1639/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.0561e-04 - my_r2: 0.8966 - val_loss: 7.9698e-06 - val_my_r2: 0.9973\n",
      "Epoch 1640/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5127e-04 - my_r2: 0.9240 - val_loss: 9.9998e-06 - val_my_r2: 0.9964\n",
      "Epoch 1641/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3505e-04 - my_r2: 0.7867 - val_loss: 7.9810e-06 - val_my_r2: 0.9973\n",
      "Epoch 1642/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4456e-04 - my_r2: 0.9331 - val_loss: 6.9173e-06 - val_my_r2: 0.9975\n",
      "Epoch 1643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1380e-04 - my_r2: 0.9402 - val_loss: 7.8731e-06 - val_my_r2: 0.9971\n",
      "Epoch 1644/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8592e-04 - my_r2: 0.9515 - val_loss: 1.0193e-05 - val_my_r2: 0.9961\n",
      "Epoch 1645/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6241e-04 - my_r2: 0.9344 - val_loss: 1.1035e-05 - val_my_r2: 0.9961\n",
      "Epoch 1646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3588e-04 - my_r2: 0.9115 - val_loss: 1.4915e-05 - val_my_r2: 0.9947\n",
      "Epoch 1647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7089e-04 - my_r2: 0.9289 - val_loss: 1.2542e-05 - val_my_r2: 0.9954\n",
      "Epoch 1648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8268e-04 - my_r2: 0.9473 - val_loss: 1.0081e-05 - val_my_r2: 0.9963\n",
      "Epoch 1649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4057e-04 - my_r2: 0.9338 - val_loss: 1.1425e-05 - val_my_r2: 0.9961\n",
      "Epoch 1650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2935e-04 - my_r2: 0.8585 - val_loss: 9.9592e-06 - val_my_r2: 0.9966\n",
      "Epoch 1651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4340e-04 - my_r2: 0.9234 - val_loss: 1.1901e-05 - val_my_r2: 0.9957\n",
      "Epoch 1652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9027e-04 - my_r2: 0.9240 - val_loss: 1.4158e-05 - val_my_r2: 0.9945\n",
      "Epoch 1653/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3798e-04 - my_r2: 0.9321 - val_loss: 1.2321e-05 - val_my_r2: 0.9954\n",
      "Epoch 1654/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8954e-04 - my_r2: 0.9458 - val_loss: 1.0416e-05 - val_my_r2: 0.9963\n",
      "Epoch 1655/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6961e-04 - my_r2: 0.9120 - val_loss: 1.1269e-05 - val_my_r2: 0.9959\n",
      "Epoch 1656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6473e-04 - my_r2: 0.9248 - val_loss: 8.5519e-06 - val_my_r2: 0.9969\n",
      "Epoch 1657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6392e-04 - my_r2: 0.9192 - val_loss: 9.7805e-06 - val_my_r2: 0.9964\n",
      "Epoch 1658/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5156e-04 - my_r2: 0.9395 - val_loss: 1.1760e-05 - val_my_r2: 0.9955\n",
      "Epoch 1659/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4806e-04 - my_r2: 0.9337 - val_loss: 1.0805e-05 - val_my_r2: 0.9955\n",
      "Epoch 1660/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9579e-04 - my_r2: 0.7380 - val_loss: 1.0532e-05 - val_my_r2: 0.9957\n",
      "Epoch 1661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5357e-04 - my_r2: 0.9358 - val_loss: 1.3515e-05 - val_my_r2: 0.9949\n",
      "Epoch 1662/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6517e-04 - my_r2: 0.9378 - val_loss: 1.1851e-05 - val_my_r2: 0.9958\n",
      "Epoch 1663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3304e-04 - my_r2: 0.9282 - val_loss: 1.0971e-05 - val_my_r2: 0.9963\n",
      "Epoch 1664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2473e-04 - my_r2: 0.9405 - val_loss: 1.1564e-05 - val_my_r2: 0.9961\n",
      "Epoch 1665/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 1.7694e-04 - my_r2: 0.9501 - val_loss: 1.0327e-05 - val_my_r2: 0.9964\n",
      "Epoch 1666/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.8751e-04 - my_r2: 0.9314 - val_loss: 9.2932e-06 - val_my_r2: 0.9967\n",
      "Epoch 1667/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0731e-04 - my_r2: 0.9391 - val_loss: 1.0697e-05 - val_my_r2: 0.9961\n",
      "Epoch 1668/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0693e-04 - my_r2: 0.9483 - val_loss: 8.7301e-06 - val_my_r2: 0.9971\n",
      "Epoch 1669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0840e-04 - my_r2: 0.9336 - val_loss: 1.0790e-05 - val_my_r2: 0.9964\n",
      "Epoch 1670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6748e-04 - my_r2: 0.8912 - val_loss: 9.4271e-06 - val_my_r2: 0.9970\n",
      "Epoch 1671/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9814e-04 - my_r2: 0.9315 - val_loss: 1.2979e-05 - val_my_r2: 0.9960\n",
      "Epoch 1672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6752e-04 - my_r2: 0.9378 - val_loss: 1.1832e-05 - val_my_r2: 0.9967\n",
      "Epoch 1673/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1520e-04 - my_r2: 0.9121 - val_loss: 1.4103e-05 - val_my_r2: 0.9956\n",
      "Epoch 1674/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9857e-04 - my_r2: 0.9320 - val_loss: 1.7607e-05 - val_my_r2: 0.9943\n",
      "Epoch 1675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5087e-04 - my_r2: 0.9510 - val_loss: 1.2740e-05 - val_my_r2: 0.9958\n",
      "Epoch 1676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2211e-04 - my_r2: 0.9337 - val_loss: 1.1733e-05 - val_my_r2: 0.9961\n",
      "Epoch 1677/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4056e-04 - my_r2: 0.9358 - val_loss: 1.0157e-05 - val_my_r2: 0.9966\n",
      "Epoch 1678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1350e-04 - my_r2: 0.9138 - val_loss: 9.1356e-06 - val_my_r2: 0.9970\n",
      "Epoch 1679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4777e-04 - my_r2: 0.9233 - val_loss: 1.4601e-05 - val_my_r2: 0.9944\n",
      "Epoch 1680/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7271e-04 - my_r2: 0.9341 - val_loss: 1.4287e-05 - val_my_r2: 0.9946\n",
      "Epoch 1681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5944e-04 - my_r2: 0.9266 - val_loss: 9.5075e-06 - val_my_r2: 0.9967\n",
      "Epoch 1682/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5962e-04 - my_r2: 0.9276 - val_loss: 7.9224e-06 - val_my_r2: 0.9972\n",
      "Epoch 1683/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6837e-04 - my_r2: 0.9203 - val_loss: 8.7206e-06 - val_my_r2: 0.9966\n",
      "Epoch 1684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7468e-04 - my_r2: 0.9319 - val_loss: 1.0028e-05 - val_my_r2: 0.9960\n",
      "Epoch 1685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8105e-04 - my_r2: 0.9028 - val_loss: 1.0558e-05 - val_my_r2: 0.9963\n",
      "Epoch 1686/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5932e-04 - my_r2: 0.9351 - val_loss: 1.4206e-05 - val_my_r2: 0.9945\n",
      "Epoch 1687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9272e-04 - my_r2: 0.9592 - val_loss: 1.2583e-05 - val_my_r2: 0.9954\n",
      "Epoch 1688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5691e-04 - my_r2: 0.9233 - val_loss: 9.5358e-06 - val_my_r2: 0.9964\n",
      "Epoch 1689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1708e-04 - my_r2: 0.9152 - val_loss: 1.0221e-05 - val_my_r2: 0.9961\n",
      "Epoch 1690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6092e-04 - my_r2: 0.9311 - val_loss: 1.3359e-05 - val_my_r2: 0.9954\n",
      "Epoch 1691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1652e-04 - my_r2: 0.9372 - val_loss: 1.2030e-05 - val_my_r2: 0.9952\n",
      "Epoch 1692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5808e-04 - my_r2: 0.9279 - val_loss: 1.3427e-05 - val_my_r2: 0.9945\n",
      "Epoch 1693/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.2100e-04 - my_r2: 0.9512 - val_loss: 1.0232e-05 - val_my_r2: 0.9959\n",
      "Epoch 1694/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6119e-04 - my_r2: 0.9049 - val_loss: 1.0884e-05 - val_my_r2: 0.9960\n",
      "Epoch 1695/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5559e-04 - my_r2: 0.9364 - val_loss: 1.1176e-05 - val_my_r2: 0.9958\n",
      "Epoch 1696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6172e-04 - my_r2: 0.9034 - val_loss: 1.1385e-05 - val_my_r2: 0.9956\n",
      "Epoch 1697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1872e-04 - my_r2: 0.8553 - val_loss: 1.1991e-05 - val_my_r2: 0.9954\n",
      "Epoch 1698/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9622e-04 - my_r2: 0.9395 - val_loss: 1.3093e-05 - val_my_r2: 0.9951\n",
      "Epoch 1699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3326e-04 - my_r2: 0.9092 - val_loss: 1.3743e-05 - val_my_r2: 0.9949\n",
      "Epoch 1700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1733e-04 - my_r2: 0.9351 - val_loss: 1.2565e-05 - val_my_r2: 0.9955\n",
      "Epoch 1701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4938e-04 - my_r2: 0.8886 - val_loss: 1.2657e-05 - val_my_r2: 0.9951\n",
      "Epoch 1702/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7312e-04 - my_r2: 0.9348 - val_loss: 1.3986e-05 - val_my_r2: 0.9944\n",
      "Epoch 1703/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6047e-04 - my_r2: 0.9328 - val_loss: 1.4060e-05 - val_my_r2: 0.9945\n",
      "Epoch 1704/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1973e-04 - my_r2: 0.9503 - val_loss: 1.1292e-05 - val_my_r2: 0.9961\n",
      "Epoch 1705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7886e-04 - my_r2: 0.9530 - val_loss: 1.0263e-05 - val_my_r2: 0.9966\n",
      "Epoch 1706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0607e-04 - my_r2: 0.9519 - val_loss: 1.0340e-05 - val_my_r2: 0.9968\n",
      "Epoch 1707/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6628e-04 - my_r2: 0.8925 - val_loss: 1.0154e-05 - val_my_r2: 0.9965\n",
      "Epoch 1708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2211e-04 - my_r2: 0.8917 - val_loss: 1.1884e-05 - val_my_r2: 0.9954\n",
      "Epoch 1709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9337e-04 - my_r2: 0.8589 - val_loss: 2.0663e-05 - val_my_r2: 0.9927\n",
      "Epoch 1710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7041e-04 - my_r2: 0.8846 - val_loss: 8.1807e-06 - val_my_r2: 0.9969\n",
      "Epoch 1711/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6402e-04 - my_r2: 0.9313 - val_loss: 8.4714e-06 - val_my_r2: 0.9969\n",
      "Epoch 1712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8104e-04 - my_r2: 0.9031 - val_loss: 1.2677e-05 - val_my_r2: 0.9958\n",
      "Epoch 1713/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3771e-04 - my_r2: 0.9477 - val_loss: 9.3334e-06 - val_my_r2: 0.9969\n",
      "Epoch 1714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3311e-04 - my_r2: 0.9338 - val_loss: 8.9089e-06 - val_my_r2: 0.9970\n",
      "Epoch 1715/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7059e-04 - my_r2: 0.9305 - val_loss: 1.3699e-05 - val_my_r2: 0.9952\n",
      "Epoch 1716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7014e-04 - my_r2: 0.9345 - val_loss: 8.6524e-06 - val_my_r2: 0.9972\n",
      "Epoch 1717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1580e-04 - my_r2: 0.9297 - val_loss: 8.4569e-06 - val_my_r2: 0.9972\n",
      "Epoch 1718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4967e-04 - my_r2: 0.8983 - val_loss: 1.1799e-05 - val_my_r2: 0.9960\n",
      "Epoch 1719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8007e-04 - my_r2: 0.9128 - val_loss: 1.1851e-05 - val_my_r2: 0.9961\n",
      "Epoch 1720/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3301e-04 - my_r2: 0.9398 - val_loss: 9.2816e-06 - val_my_r2: 0.9970\n",
      "Epoch 1721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3628e-04 - my_r2: 0.9260 - val_loss: 1.0413e-05 - val_my_r2: 0.9965\n",
      "Epoch 1722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8739e-04 - my_r2: 0.9048 - val_loss: 1.3042e-05 - val_my_r2: 0.9955\n",
      "Epoch 1723/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3572e-04 - my_r2: 0.9485 - val_loss: 1.4418e-05 - val_my_r2: 0.9953\n",
      "Epoch 1724/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1801e-04 - my_r2: 0.9520 - val_loss: 1.0788e-05 - val_my_r2: 0.9966\n",
      "Epoch 1725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3918e-04 - my_r2: 0.9388 - val_loss: 1.0131e-05 - val_my_r2: 0.9966\n",
      "Epoch 1726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0854e-04 - my_r2: 0.9480 - val_loss: 1.0935e-05 - val_my_r2: 0.9964\n",
      "Epoch 1727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3558e-04 - my_r2: 0.7287 - val_loss: 1.3068e-05 - val_my_r2: 0.9958\n",
      "Epoch 1728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1994e-04 - my_r2: 0.9358 - val_loss: 1.2887e-05 - val_my_r2: 0.9958\n",
      "Epoch 1729/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8762e-04 - my_r2: 0.9527 - val_loss: 1.1139e-05 - val_my_r2: 0.9963\n",
      "Epoch 1730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9729e-04 - my_r2: 0.8756 - val_loss: 1.0878e-05 - val_my_r2: 0.9967\n",
      "Epoch 1731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6244e-04 - my_r2: 0.9255 - val_loss: 8.0604e-06 - val_my_r2: 0.9974\n",
      "Epoch 1732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4625e-04 - my_r2: 0.9407 - val_loss: 9.1332e-06 - val_my_r2: 0.9964\n",
      "Epoch 1733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4138e-04 - my_r2: 0.9409 - val_loss: 9.7420e-06 - val_my_r2: 0.9964\n",
      "Epoch 1734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6288e-04 - my_r2: 0.8808 - val_loss: 1.1447e-05 - val_my_r2: 0.9963\n",
      "Epoch 1735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2843e-04 - my_r2: 0.9453 - val_loss: 1.2010e-05 - val_my_r2: 0.9962\n",
      "Epoch 1736/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5487e-04 - my_r2: 0.9397 - val_loss: 1.1665e-05 - val_my_r2: 0.9962\n",
      "Epoch 1737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6968e-04 - my_r2: 0.9382 - val_loss: 1.1697e-05 - val_my_r2: 0.9962\n",
      "Epoch 1738/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4660e-04 - my_r2: 0.9283 - val_loss: 1.1753e-05 - val_my_r2: 0.9965\n",
      "Epoch 1739/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1072e-04 - my_r2: 0.9427 - val_loss: 9.7586e-06 - val_my_r2: 0.9971\n",
      "Epoch 1740/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0126e-04 - my_r2: 0.9364 - val_loss: 1.1192e-05 - val_my_r2: 0.9965\n",
      "Epoch 1741/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3716e-04 - my_r2: 0.9419 - val_loss: 1.3340e-05 - val_my_r2: 0.9960\n",
      "Epoch 1742/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6717e-04 - my_r2: 0.9407 - val_loss: 1.1833e-05 - val_my_r2: 0.9965\n",
      "Epoch 1743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3123e-04 - my_r2: 0.9301 - val_loss: 1.1818e-05 - val_my_r2: 0.9965\n",
      "Epoch 1744/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2134e-04 - my_r2: 0.9323 - val_loss: 1.2393e-05 - val_my_r2: 0.9962\n",
      "Epoch 1745/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7958e-04 - my_r2: 0.8375 - val_loss: 1.2264e-05 - val_my_r2: 0.9961\n",
      "Epoch 1746/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1053e-04 - my_r2: 0.9547 - val_loss: 1.4901e-05 - val_my_r2: 0.9951\n",
      "Epoch 1747/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2391e-04 - my_r2: 0.9341 - val_loss: 1.2216e-05 - val_my_r2: 0.9960\n",
      "Epoch 1748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1932e-04 - my_r2: 0.9632 - val_loss: 9.5168e-06 - val_my_r2: 0.9971\n",
      "Epoch 1749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1521e-04 - my_r2: 0.9309 - val_loss: 9.1695e-06 - val_my_r2: 0.9971\n",
      "Epoch 1750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8435e-04 - my_r2: 0.9237 - val_loss: 8.9050e-06 - val_my_r2: 0.9969\n",
      "Epoch 1751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9257e-04 - my_r2: 0.9157 - val_loss: 7.9963e-06 - val_my_r2: 0.9973\n",
      "Epoch 1752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6162e-04 - my_r2: 0.8938 - val_loss: 1.0516e-05 - val_my_r2: 0.9966\n",
      "Epoch 1753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5072e-04 - my_r2: 0.9403 - val_loss: 8.8520e-06 - val_my_r2: 0.9972\n",
      "Epoch 1754/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8786e-04 - my_r2: 0.9536 - val_loss: 1.0644e-05 - val_my_r2: 0.9966\n",
      "Epoch 1755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3281e-04 - my_r2: 0.9370 - val_loss: 1.1384e-05 - val_my_r2: 0.9961\n",
      "Epoch 1756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2606e-04 - my_r2: 0.9373 - val_loss: 1.3048e-05 - val_my_r2: 0.9956\n",
      "Epoch 1757/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5163e-04 - my_r2: 0.9313 - val_loss: 1.0381e-05 - val_my_r2: 0.9966\n",
      "Epoch 1758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0907e-04 - my_r2: 0.9406 - val_loss: 8.7623e-06 - val_my_r2: 0.9970\n",
      "Epoch 1759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0130e-04 - my_r2: 0.9170 - val_loss: 8.3467e-06 - val_my_r2: 0.9972\n",
      "Epoch 1760/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2369e-04 - my_r2: 0.9459 - val_loss: 1.2076e-05 - val_my_r2: 0.9959\n",
      "Epoch 1761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0731e-04 - my_r2: 0.9368 - val_loss: 1.0174e-05 - val_my_r2: 0.9966\n",
      "Epoch 1762/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4027e-04 - my_r2: 0.9382 - val_loss: 1.1578e-05 - val_my_r2: 0.9956\n",
      "Epoch 1763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4507e-04 - my_r2: 0.9361 - val_loss: 9.8790e-06 - val_my_r2: 0.9963\n",
      "Epoch 1764/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2210e-04 - my_r2: 0.9487 - val_loss: 7.3164e-06 - val_my_r2: 0.9975\n",
      "Epoch 1765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8329e-04 - my_r2: 0.9113 - val_loss: 9.6237e-06 - val_my_r2: 0.9966\n",
      "Epoch 1766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8425e-04 - my_r2: 0.9327 - val_loss: 1.1114e-05 - val_my_r2: 0.9962\n",
      "Epoch 1767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2601e-04 - my_r2: 0.9432 - val_loss: 8.0693e-06 - val_my_r2: 0.9974\n",
      "Epoch 1768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4459e-04 - my_r2: 0.9417 - val_loss: 8.2537e-06 - val_my_r2: 0.9974\n",
      "Epoch 1769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8866e-04 - my_r2: 0.9231 - val_loss: 1.0203e-05 - val_my_r2: 0.9967\n",
      "Epoch 1770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2599e-04 - my_r2: 0.9430 - val_loss: 1.1165e-05 - val_my_r2: 0.9963\n",
      "Epoch 1771/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0844e-04 - my_r2: 0.9486 - val_loss: 1.1213e-05 - val_my_r2: 0.9966\n",
      "Epoch 1772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5868e-04 - my_r2: 0.9401 - val_loss: 1.0389e-05 - val_my_r2: 0.9965\n",
      "Epoch 1773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6103e-04 - my_r2: 0.9362 - val_loss: 8.8013e-06 - val_my_r2: 0.9964\n",
      "Epoch 1774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2607e-04 - my_r2: 0.9452 - val_loss: 1.1381e-05 - val_my_r2: 0.9955\n",
      "Epoch 1775/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1976e-04 - my_r2: 0.9262 - val_loss: 9.0840e-06 - val_my_r2: 0.9969\n",
      "Epoch 1776/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3008e-04 - my_r2: 0.9065 - val_loss: 1.0369e-05 - val_my_r2: 0.9967\n",
      "Epoch 1777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8173e-04 - my_r2: 0.9557 - val_loss: 1.0114e-05 - val_my_r2: 0.9967\n",
      "Epoch 1778/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.7332e-04 - my_r2: 0.9308 - val_loss: 1.2055e-05 - val_my_r2: 0.9960\n",
      "Epoch 1779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6149e-04 - my_r2: 0.9146 - val_loss: 1.2359e-05 - val_my_r2: 0.9959\n",
      "Epoch 1780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6043e-04 - my_r2: 0.9051 - val_loss: 1.0938e-05 - val_my_r2: 0.9963\n",
      "Epoch 1781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5842e-04 - my_r2: 0.9308 - val_loss: 1.5012e-05 - val_my_r2: 0.9946\n",
      "Epoch 1782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3152e-04 - my_r2: 0.9098 - val_loss: 1.8505e-05 - val_my_r2: 0.9930\n",
      "Epoch 1783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2181e-04 - my_r2: 0.9004 - val_loss: 1.5651e-05 - val_my_r2: 0.9943\n",
      "Epoch 1784/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4927e-04 - my_r2: 0.9279 - val_loss: 1.3106e-05 - val_my_r2: 0.9955\n",
      "Epoch 1785/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1321e-04 - my_r2: 0.9311 - val_loss: 1.2084e-05 - val_my_r2: 0.9959\n",
      "Epoch 1786/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4230e-04 - my_r2: 0.9459 - val_loss: 1.2457e-05 - val_my_r2: 0.9956\n",
      "Epoch 1787/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6359e-04 - my_r2: 0.9182 - val_loss: 1.0763e-05 - val_my_r2: 0.9962\n",
      "Epoch 1788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7965e-04 - my_r2: 0.9130 - val_loss: 8.9405e-06 - val_my_r2: 0.9968\n",
      "Epoch 1789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2929e-04 - my_r2: 0.8604 - val_loss: 9.0386e-06 - val_my_r2: 0.9970\n",
      "Epoch 1790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3417e-04 - my_r2: 0.9485 - val_loss: 7.7942e-06 - val_my_r2: 0.9972\n",
      "Epoch 1791/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3749e-04 - my_r2: 0.9487 - val_loss: 1.0244e-05 - val_my_r2: 0.9964\n",
      "Epoch 1792/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8030e-04 - my_r2: 0.9150 - val_loss: 8.0026e-06 - val_my_r2: 0.9972\n",
      "Epoch 1793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8810e-04 - my_r2: 0.9528 - val_loss: 1.6318e-05 - val_my_r2: 0.9944\n",
      "Epoch 1794/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5463e-04 - my_r2: 0.9357 - val_loss: 8.0728e-06 - val_my_r2: 0.9972\n",
      "Epoch 1795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1438e-04 - my_r2: 0.9340 - val_loss: 8.7003e-06 - val_my_r2: 0.9969\n",
      "Epoch 1796/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0074e-04 - my_r2: 0.8932 - val_loss: 1.0634e-05 - val_my_r2: 0.9967\n",
      "Epoch 1797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5974e-04 - my_r2: 0.9489 - val_loss: 1.1506e-05 - val_my_r2: 0.9958\n",
      "Epoch 1798/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0936e-04 - my_r2: 0.8925 - val_loss: 1.2667e-05 - val_my_r2: 0.9952\n",
      "Epoch 1799/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2939e-04 - my_r2: 0.9307 - val_loss: 9.2257e-06 - val_my_r2: 0.9968\n",
      "Epoch 1800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6529e-04 - my_r2: 0.8847 - val_loss: 9.0533e-06 - val_my_r2: 0.9969\n",
      "Epoch 1801/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1065e-04 - my_r2: 0.9475 - val_loss: 7.7101e-06 - val_my_r2: 0.9977\n",
      "Epoch 1802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8628e-04 - my_r2: 0.9281 - val_loss: 9.3688e-06 - val_my_r2: 0.9968\n",
      "Epoch 1803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6873e-04 - my_r2: 0.9119 - val_loss: 7.1805e-06 - val_my_r2: 0.9973\n",
      "Epoch 1804/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1850e-04 - my_r2: 0.9264 - val_loss: 9.2671e-06 - val_my_r2: 0.9968\n",
      "Epoch 1805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8969e-04 - my_r2: 0.9436 - val_loss: 1.1023e-05 - val_my_r2: 0.9961\n",
      "Epoch 1806/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6616e-04 - my_r2: 0.9127 - val_loss: 1.6446e-05 - val_my_r2: 0.9938\n",
      "Epoch 1807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7683e-04 - my_r2: 0.9407 - val_loss: 8.9092e-06 - val_my_r2: 0.9970\n",
      "Epoch 1808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5512e-04 - my_r2: 0.9404 - val_loss: 9.8257e-06 - val_my_r2: 0.9971\n",
      "Epoch 1809/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9098e-04 - my_r2: 0.9147 - val_loss: 1.0227e-05 - val_my_r2: 0.9968\n",
      "Epoch 1810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4773e-04 - my_r2: 0.9253 - val_loss: 1.0968e-05 - val_my_r2: 0.9967\n",
      "Epoch 1811/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8025e-04 - my_r2: 0.9338 - val_loss: 1.5869e-05 - val_my_r2: 0.9945\n",
      "Epoch 1812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5173e-04 - my_r2: 0.9155 - val_loss: 8.4519e-06 - val_my_r2: 0.9972\n",
      "Epoch 1813/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3178e-04 - my_r2: 0.9411 - val_loss: 9.0458e-06 - val_my_r2: 0.9972\n",
      "Epoch 1814/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1752e-04 - my_r2: 0.9347 - val_loss: 1.1681e-05 - val_my_r2: 0.9964\n",
      "Epoch 1815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5744e-04 - my_r2: 0.9177 - val_loss: 1.0430e-05 - val_my_r2: 0.9967\n",
      "Epoch 1816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2288e-04 - my_r2: 0.9243 - val_loss: 1.0842e-05 - val_my_r2: 0.9965\n",
      "Epoch 1817/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4535e-04 - my_r2: 0.9411 - val_loss: 1.2492e-05 - val_my_r2: 0.9959\n",
      "Epoch 1818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0615e-04 - my_r2: 0.9556 - val_loss: 1.4342e-05 - val_my_r2: 0.9947\n",
      "Epoch 1819/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1517e-04 - my_r2: 0.9390 - val_loss: 1.2530e-05 - val_my_r2: 0.9955\n",
      "Epoch 1820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1618e-04 - my_r2: 0.9423 - val_loss: 1.2173e-05 - val_my_r2: 0.9960\n",
      "Epoch 1821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0626e-04 - my_r2: 0.9185 - val_loss: 1.1426e-05 - val_my_r2: 0.9963\n",
      "Epoch 1822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6274e-04 - my_r2: 0.9371 - val_loss: 1.1243e-05 - val_my_r2: 0.9963\n",
      "Epoch 1823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1371e-04 - my_r2: 0.9513 - val_loss: 9.2144e-06 - val_my_r2: 0.9971\n",
      "Epoch 1824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5738e-04 - my_r2: 0.9064 - val_loss: 7.9625e-06 - val_my_r2: 0.9974\n",
      "Epoch 1825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1880e-04 - my_r2: 0.9559 - val_loss: 9.6365e-06 - val_my_r2: 0.9970\n",
      "Epoch 1826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8076e-04 - my_r2: 0.9421 - val_loss: 1.2104e-05 - val_my_r2: 0.9963\n",
      "Epoch 1827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2936e-04 - my_r2: 0.8842 - val_loss: 1.1800e-05 - val_my_r2: 0.9961\n",
      "Epoch 1828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1285e-04 - my_r2: 0.8845 - val_loss: 1.0556e-05 - val_my_r2: 0.9967\n",
      "Epoch 1829/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1254e-04 - my_r2: 0.9175 - val_loss: 1.0720e-05 - val_my_r2: 0.9968\n",
      "Epoch 1830/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2350e-04 - my_r2: 0.9095 - val_loss: 1.0828e-05 - val_my_r2: 0.9966\n",
      "Epoch 1831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2040e-04 - my_r2: 0.9333 - val_loss: 9.4209e-06 - val_my_r2: 0.9969\n",
      "Epoch 1832/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.6005e-04 - my_r2: 0.9446 - val_loss: 8.3315e-06 - val_my_r2: 0.9974\n",
      "Epoch 1833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6000e-04 - my_r2: 0.9097 - val_loss: 8.5376e-06 - val_my_r2: 0.9972\n",
      "Epoch 1834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6133e-04 - my_r2: 0.9380 - val_loss: 8.0406e-06 - val_my_r2: 0.9975\n",
      "Epoch 1835/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5701e-04 - my_r2: 0.9379 - val_loss: 8.4871e-06 - val_my_r2: 0.9971\n",
      "Epoch 1836/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6587e-04 - my_r2: 0.9257 - val_loss: 9.4981e-06 - val_my_r2: 0.9963\n",
      "Epoch 1837/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3956e-04 - my_r2: 0.9300 - val_loss: 8.7528e-06 - val_my_r2: 0.9971\n",
      "Epoch 1838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2890e-04 - my_r2: 0.9173 - val_loss: 8.7046e-06 - val_my_r2: 0.9972\n",
      "Epoch 1839/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7793e-04 - my_r2: 0.9173 - val_loss: 8.6263e-06 - val_my_r2: 0.9971\n",
      "Epoch 1840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6914e-04 - my_r2: 0.9227 - val_loss: 8.9851e-06 - val_my_r2: 0.9969\n",
      "Epoch 1841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6227e-04 - my_r2: 0.9026 - val_loss: 1.1528e-05 - val_my_r2: 0.9958\n",
      "Epoch 1842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1063e-04 - my_r2: 0.9408 - val_loss: 1.0088e-05 - val_my_r2: 0.9963\n",
      "Epoch 1843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3020e-04 - my_r2: 0.9249 - val_loss: 1.0999e-05 - val_my_r2: 0.9962\n",
      "Epoch 1844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1387e-04 - my_r2: 0.9340 - val_loss: 1.2598e-05 - val_my_r2: 0.9958\n",
      "Epoch 1845/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5482e-04 - my_r2: 0.9326 - val_loss: 1.1040e-05 - val_my_r2: 0.9962\n",
      "Epoch 1846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2586e-04 - my_r2: 0.9511 - val_loss: 9.6541e-06 - val_my_r2: 0.9964\n",
      "Epoch 1847/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0895e-04 - my_r2: 0.9346 - val_loss: 1.3548e-05 - val_my_r2: 0.9950\n",
      "Epoch 1848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3693e-04 - my_r2: 0.9228 - val_loss: 1.2202e-05 - val_my_r2: 0.9960\n",
      "Epoch 1849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2359e-04 - my_r2: 0.8705 - val_loss: 9.3660e-06 - val_my_r2: 0.9969\n",
      "Epoch 1850/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6300e-04 - my_r2: 0.9243 - val_loss: 8.9948e-06 - val_my_r2: 0.9969\n",
      "Epoch 1851/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1101e-04 - my_r2: 0.9088 - val_loss: 9.2403e-06 - val_my_r2: 0.9969\n",
      "Epoch 1852/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6489e-04 - my_r2: 0.9345 - val_loss: 1.5272e-05 - val_my_r2: 0.9948\n",
      "Epoch 1853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1409e-04 - my_r2: 0.9364 - val_loss: 1.3469e-05 - val_my_r2: 0.9954\n",
      "Epoch 1854/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0750e-04 - my_r2: 0.9447 - val_loss: 1.0845e-05 - val_my_r2: 0.9966\n",
      "Epoch 1855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2979e-04 - my_r2: 0.9520 - val_loss: 1.0614e-05 - val_my_r2: 0.9968\n",
      "Epoch 1856/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9676e-04 - my_r2: 0.9258 - val_loss: 1.0550e-05 - val_my_r2: 0.9966\n",
      "Epoch 1857/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7127e-04 - my_r2: 0.9495 - val_loss: 8.1890e-06 - val_my_r2: 0.9974\n",
      "Epoch 1858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9630e-04 - my_r2: 0.9130 - val_loss: 9.2058e-06 - val_my_r2: 0.9970\n",
      "Epoch 1859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6940e-04 - my_r2: 0.9417 - val_loss: 8.0940e-06 - val_my_r2: 0.9972\n",
      "Epoch 1860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3796e-04 - my_r2: 0.9382 - val_loss: 7.4539e-06 - val_my_r2: 0.9972\n",
      "Epoch 1861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2397e-04 - my_r2: 0.9465 - val_loss: 1.0254e-05 - val_my_r2: 0.9962\n",
      "Epoch 1862/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1106e-04 - my_r2: 0.9509 - val_loss: 1.0262e-05 - val_my_r2: 0.9965\n",
      "Epoch 1863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1434e-04 - my_r2: 0.9512 - val_loss: 1.3628e-05 - val_my_r2: 0.9955\n",
      "Epoch 1864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7480e-04 - my_r2: 0.9088 - val_loss: 9.4819e-06 - val_my_r2: 0.9971\n",
      "Epoch 1865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3142e-04 - my_r2: 0.9543 - val_loss: 9.0712e-06 - val_my_r2: 0.9974\n",
      "Epoch 1866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7412e-04 - my_r2: 0.9207 - val_loss: 9.5940e-06 - val_my_r2: 0.9967\n",
      "Epoch 1867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3858e-04 - my_r2: 0.9400 - val_loss: 9.1592e-06 - val_my_r2: 0.9970\n",
      "Epoch 1868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8812e-04 - my_r2: 0.9526 - val_loss: 9.5767e-06 - val_my_r2: 0.9969\n",
      "Epoch 1869/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3546e-04 - my_r2: 0.9349 - val_loss: 1.2196e-05 - val_my_r2: 0.9958\n",
      "Epoch 1870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8055e-04 - my_r2: 0.9087 - val_loss: 8.9856e-06 - val_my_r2: 0.9968\n",
      "Epoch 1871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3667e-04 - my_r2: 0.9288 - val_loss: 1.2295e-05 - val_my_r2: 0.9957\n",
      "Epoch 1872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6768e-04 - my_r2: 0.9064 - val_loss: 1.1717e-05 - val_my_r2: 0.9958\n",
      "Epoch 1873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5606e-04 - my_r2: 0.9111 - val_loss: 1.2076e-05 - val_my_r2: 0.9958\n",
      "Epoch 1874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4563e-04 - my_r2: 0.9323 - val_loss: 9.7640e-06 - val_my_r2: 0.9965\n",
      "Epoch 1875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6830e-04 - my_r2: 0.9239 - val_loss: 7.6933e-06 - val_my_r2: 0.9972\n",
      "Epoch 1876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3133e-04 - my_r2: 0.9058 - val_loss: 8.7858e-06 - val_my_r2: 0.9965\n",
      "Epoch 1877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8160e-04 - my_r2: 0.9401 - val_loss: 6.7103e-06 - val_my_r2: 0.9974\n",
      "Epoch 1878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3119e-04 - my_r2: 0.9485 - val_loss: 7.9471e-06 - val_my_r2: 0.9971\n",
      "Epoch 1879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7231e-04 - my_r2: 0.9287 - val_loss: 9.8567e-06 - val_my_r2: 0.9963\n",
      "Epoch 1880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2404e-04 - my_r2: 0.9536 - val_loss: 8.8320e-06 - val_my_r2: 0.9965\n",
      "Epoch 1881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9637e-04 - my_r2: 0.9398 - val_loss: 9.2650e-06 - val_my_r2: 0.9966\n",
      "Epoch 1882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3237e-04 - my_r2: 0.8633 - val_loss: 9.0850e-06 - val_my_r2: 0.9969\n",
      "Epoch 1883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1190e-04 - my_r2: 0.9583 - val_loss: 1.1144e-05 - val_my_r2: 0.9964\n",
      "Epoch 1884/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4005e-04 - my_r2: 0.9296 - val_loss: 9.0280e-06 - val_my_r2: 0.9969\n",
      "Epoch 1885/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9021e-04 - my_r2: 0.9301 - val_loss: 7.1452e-06 - val_my_r2: 0.9974\n",
      "Epoch 1886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0448e-04 - my_r2: 0.9369 - val_loss: 7.1437e-06 - val_my_r2: 0.9972\n",
      "Epoch 1887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4131e-04 - my_r2: 0.9441 - val_loss: 7.8887e-06 - val_my_r2: 0.9973\n",
      "Epoch 1888/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6007e-04 - my_r2: 0.9063 - val_loss: 8.2819e-06 - val_my_r2: 0.9966\n",
      "Epoch 1889/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3328e-04 - my_r2: 0.9343 - val_loss: 8.7295e-06 - val_my_r2: 0.9964\n",
      "Epoch 1890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3145e-04 - my_r2: 0.9378 - val_loss: 1.0030e-05 - val_my_r2: 0.9958\n",
      "Epoch 1891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4970e-04 - my_r2: 0.9409 - val_loss: 1.2328e-05 - val_my_r2: 0.9949\n",
      "Epoch 1892/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6677e-04 - my_r2: 0.9236 - val_loss: 1.3418e-05 - val_my_r2: 0.9947\n",
      "Epoch 1893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2139e-04 - my_r2: 0.9217 - val_loss: 1.2201e-05 - val_my_r2: 0.9950\n",
      "Epoch 1894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1068e-04 - my_r2: 0.9535 - val_loss: 1.2618e-05 - val_my_r2: 0.9951\n",
      "Epoch 1895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4101e-04 - my_r2: 0.9392 - val_loss: 1.1257e-05 - val_my_r2: 0.9960\n",
      "Epoch 1896/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7590e-04 - my_r2: 0.8657 - val_loss: 1.0552e-05 - val_my_r2: 0.9961\n",
      "Epoch 1897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3971e-04 - my_r2: 0.9303 - val_loss: 1.0656e-05 - val_my_r2: 0.9963\n",
      "Epoch 1898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8265e-04 - my_r2: 0.9087 - val_loss: 9.3121e-06 - val_my_r2: 0.9967\n",
      "Epoch 1899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5219e-04 - my_r2: 0.9414 - val_loss: 1.2209e-05 - val_my_r2: 0.9957\n",
      "Epoch 1900/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0526e-04 - my_r2: 0.9592 - val_loss: 1.0905e-05 - val_my_r2: 0.9956\n",
      "Epoch 1901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1344e-04 - my_r2: 0.9185 - val_loss: 1.2104e-05 - val_my_r2: 0.9952\n",
      "Epoch 1902/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9051e-04 - my_r2: 0.8855 - val_loss: 9.3729e-06 - val_my_r2: 0.9961\n",
      "Epoch 1903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6724e-04 - my_r2: 0.8250 - val_loss: 8.1447e-06 - val_my_r2: 0.9967\n",
      "Epoch 1904/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3271e-04 - my_r2: 0.9270 - val_loss: 9.0296e-06 - val_my_r2: 0.9962\n",
      "Epoch 1905/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2989e-04 - my_r2: 0.9071 - val_loss: 9.2549e-06 - val_my_r2: 0.9966\n",
      "Epoch 1906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1799e-04 - my_r2: 0.9057 - val_loss: 1.1609e-05 - val_my_r2: 0.9964\n",
      "Epoch 1907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1600e-04 - my_r2: 0.7817 - val_loss: 1.1912e-05 - val_my_r2: 0.9963\n",
      "Epoch 1908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9382e-04 - my_r2: 0.9609 - val_loss: 1.3727e-05 - val_my_r2: 0.9955\n",
      "Epoch 1909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4401e-04 - my_r2: 0.9006 - val_loss: 1.4468e-05 - val_my_r2: 0.9952\n",
      "Epoch 1910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9074e-04 - my_r2: 0.9075 - val_loss: 1.0142e-05 - val_my_r2: 0.9961\n",
      "Epoch 1911/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7261e-04 - my_r2: 0.9055 - val_loss: 1.2013e-05 - val_my_r2: 0.9952\n",
      "Epoch 1912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9931e-04 - my_r2: 0.9531 - val_loss: 1.0325e-05 - val_my_r2: 0.9958\n",
      "Epoch 1913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8114e-04 - my_r2: 0.9305 - val_loss: 8.1448e-06 - val_my_r2: 0.9972\n",
      "Epoch 1914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5983e-04 - my_r2: 0.9412 - val_loss: 9.6940e-06 - val_my_r2: 0.9961\n",
      "Epoch 1915/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.6915e-04 - my_r2: 0.9490 - val_loss: 9.1510e-06 - val_my_r2: 0.9967\n",
      "Epoch 1916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2758e-04 - my_r2: 0.9192 - val_loss: 9.5974e-06 - val_my_r2: 0.9973\n",
      "Epoch 1917/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8225e-04 - my_r2: 0.9503 - val_loss: 7.8747e-06 - val_my_r2: 0.9976\n",
      "Epoch 1918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9663e-04 - my_r2: 0.9601 - val_loss: 8.3458e-06 - val_my_r2: 0.9975\n",
      "Epoch 1919/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1421e-04 - my_r2: 0.9455 - val_loss: 1.1038e-05 - val_my_r2: 0.9966\n",
      "Epoch 1920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9597e-04 - my_r2: 0.9369 - val_loss: 1.4003e-05 - val_my_r2: 0.9957\n",
      "Epoch 1921/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7174e-04 - my_r2: 0.9361 - val_loss: 1.2458e-05 - val_my_r2: 0.9962\n",
      "Epoch 1922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7590e-04 - my_r2: 0.8492 - val_loss: 1.2926e-05 - val_my_r2: 0.9958\n",
      "Epoch 1923/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7441e-04 - my_r2: 0.9154 - val_loss: 1.3918e-05 - val_my_r2: 0.9954\n",
      "Epoch 1924/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3931e-04 - my_r2: 0.9505 - val_loss: 1.0956e-05 - val_my_r2: 0.9963\n",
      "Epoch 1925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3132e-04 - my_r2: 0.9466 - val_loss: 1.3141e-05 - val_my_r2: 0.9953\n",
      "Epoch 1926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2021e-04 - my_r2: 0.9207 - val_loss: 1.2995e-05 - val_my_r2: 0.9950\n",
      "Epoch 1927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2473e-04 - my_r2: 0.9480 - val_loss: 1.1848e-05 - val_my_r2: 0.9949\n",
      "Epoch 1928/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4787e-04 - my_r2: 0.9237 - val_loss: 1.2822e-05 - val_my_r2: 0.9948\n",
      "Epoch 1929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0012e-04 - my_r2: 0.9291 - val_loss: 1.0310e-05 - val_my_r2: 0.9965\n",
      "Epoch 1930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0271e-04 - my_r2: 0.9261 - val_loss: 1.0642e-05 - val_my_r2: 0.9966\n",
      "Epoch 1931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3502e-04 - my_r2: 0.9466 - val_loss: 1.0076e-05 - val_my_r2: 0.9969\n",
      "Epoch 1932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0462e-04 - my_r2: 0.9427 - val_loss: 1.0648e-05 - val_my_r2: 0.9964\n",
      "Epoch 1933/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7814e-04 - my_r2: 0.9184 - val_loss: 8.0779e-06 - val_my_r2: 0.9974\n",
      "Epoch 1934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5736e-04 - my_r2: 0.9286 - val_loss: 8.1818e-06 - val_my_r2: 0.9971\n",
      "Epoch 1935/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1648e-04 - my_r2: 0.9085 - val_loss: 9.2452e-06 - val_my_r2: 0.9973\n",
      "Epoch 1936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4590e-04 - my_r2: 0.9095 - val_loss: 9.3137e-06 - val_my_r2: 0.9973\n",
      "Epoch 1937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4858e-04 - my_r2: 0.9450 - val_loss: 1.0446e-05 - val_my_r2: 0.9965\n",
      "Epoch 1938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6240e-04 - my_r2: 0.9173 - val_loss: 8.3686e-06 - val_my_r2: 0.9977\n",
      "Epoch 1939/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.6373e-04 - my_r2: 0.9291 - val_loss: 6.9910e-06 - val_my_r2: 0.9980\n",
      "Epoch 1940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5108e-04 - my_r2: 0.9414 - val_loss: 6.7521e-06 - val_my_r2: 0.9980\n",
      "Epoch 1941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8745e-04 - my_r2: 0.9309 - val_loss: 9.3506e-06 - val_my_r2: 0.9972\n",
      "Epoch 1942/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1233e-04 - my_r2: 0.9313 - val_loss: 8.0855e-06 - val_my_r2: 0.9972\n",
      "Epoch 1943/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3174e-04 - my_r2: 0.9407 - val_loss: 9.4217e-06 - val_my_r2: 0.9970\n",
      "Epoch 1944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7363e-04 - my_r2: 0.9497 - val_loss: 8.3179e-06 - val_my_r2: 0.9974\n",
      "Epoch 1945/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4808e-04 - my_r2: 0.9312 - val_loss: 1.1319e-05 - val_my_r2: 0.9963\n",
      "Epoch 1946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8673e-04 - my_r2: 0.8063 - val_loss: 1.4182e-05 - val_my_r2: 0.9956\n",
      "Epoch 1947/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6533e-04 - my_r2: 0.9303 - val_loss: 1.5648e-05 - val_my_r2: 0.9951\n",
      "Epoch 1948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1924e-04 - my_r2: 0.9519 - val_loss: 1.6648e-05 - val_my_r2: 0.9942\n",
      "Epoch 1949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9982e-04 - my_r2: 0.8985 - val_loss: 1.8738e-05 - val_my_r2: 0.9936\n",
      "Epoch 1950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2271e-04 - my_r2: 0.9035 - val_loss: 1.4564e-05 - val_my_r2: 0.9952\n",
      "Epoch 1951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5642e-04 - my_r2: 0.9339 - val_loss: 1.1783e-05 - val_my_r2: 0.9961\n",
      "Epoch 1952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4049e-04 - my_r2: 0.9208 - val_loss: 1.0863e-05 - val_my_r2: 0.9964\n",
      "Epoch 1953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1034e-04 - my_r2: 0.9219 - val_loss: 1.0722e-05 - val_my_r2: 0.9963\n",
      "Epoch 1954/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7967e-04 - my_r2: 0.9269 - val_loss: 1.3585e-05 - val_my_r2: 0.9950\n",
      "Epoch 1955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5482e-04 - my_r2: 0.9397 - val_loss: 1.8591e-05 - val_my_r2: 0.9932\n",
      "Epoch 1956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5161e-04 - my_r2: 0.9288 - val_loss: 7.7448e-06 - val_my_r2: 0.9971\n",
      "Epoch 1957/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4555e-04 - my_r2: 0.8861 - val_loss: 8.7822e-06 - val_my_r2: 0.9969\n",
      "Epoch 1958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8389e-04 - my_r2: 0.9115 - val_loss: 1.0910e-05 - val_my_r2: 0.9961\n",
      "Epoch 1959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6997e-04 - my_r2: 0.8926 - val_loss: 1.0663e-05 - val_my_r2: 0.9962\n",
      "Epoch 1960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6540e-04 - my_r2: 0.7892 - val_loss: 8.6264e-06 - val_my_r2: 0.9965\n",
      "Epoch 1961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2728e-04 - my_r2: 0.9207 - val_loss: 1.0161e-05 - val_my_r2: 0.9961\n",
      "Epoch 1962/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8386e-04 - my_r2: 0.9056 - val_loss: 1.0043e-05 - val_my_r2: 0.9960\n",
      "Epoch 1963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2597e-04 - my_r2: 0.9436 - val_loss: 8.7374e-06 - val_my_r2: 0.9965\n",
      "Epoch 1964/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2748e-04 - my_r2: 0.9258 - val_loss: 1.0520e-05 - val_my_r2: 0.9956\n",
      "Epoch 1965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3319e-04 - my_r2: 0.9327 - val_loss: 7.8133e-06 - val_my_r2: 0.9970\n",
      "Epoch 1966/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0338e-04 - my_r2: 0.9386 - val_loss: 7.1442e-06 - val_my_r2: 0.9973\n",
      "Epoch 1967/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5880e-04 - my_r2: 0.9317 - val_loss: 7.1188e-06 - val_my_r2: 0.9976\n",
      "Epoch 1968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0611e-04 - my_r2: 0.9448 - val_loss: 1.1572e-05 - val_my_r2: 0.9963\n",
      "Epoch 1969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5960e-04 - my_r2: 0.8604 - val_loss: 1.2774e-05 - val_my_r2: 0.9961\n",
      "Epoch 1970/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6863e-04 - my_r2: 0.9323 - val_loss: 8.1647e-06 - val_my_r2: 0.9974\n",
      "Epoch 1971/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3608e-04 - my_r2: 0.9509 - val_loss: 6.9619e-06 - val_my_r2: 0.9976\n",
      "Epoch 1972/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5929e-04 - my_r2: 0.9037 - val_loss: 8.2709e-06 - val_my_r2: 0.9972\n",
      "Epoch 1973/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6989e-04 - my_r2: 0.9052 - val_loss: 8.8255e-06 - val_my_r2: 0.9970\n",
      "Epoch 1974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5240e-04 - my_r2: 0.8852 - val_loss: 7.2860e-06 - val_my_r2: 0.9976\n",
      "Epoch 1975/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9330e-04 - my_r2: 0.8831 - val_loss: 9.2096e-06 - val_my_r2: 0.9971\n",
      "Epoch 1976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.5433e-04 - my_r2: 0.9586 - val_loss: 6.6251e-06 - val_my_r2: 0.9979\n",
      "Epoch 1977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2145e-04 - my_r2: 0.9402 - val_loss: 6.9425e-06 - val_my_r2: 0.9976\n",
      "Epoch 1978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2384e-04 - my_r2: 0.9173 - val_loss: 1.0806e-05 - val_my_r2: 0.9961\n",
      "Epoch 1979/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5208e-04 - my_r2: 0.9084 - val_loss: 1.0079e-05 - val_my_r2: 0.9963\n",
      "Epoch 1980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6435e-04 - my_r2: 0.9372 - val_loss: 1.0849e-05 - val_my_r2: 0.9961\n",
      "Epoch 1981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4887e-04 - my_r2: 0.9492 - val_loss: 1.1127e-05 - val_my_r2: 0.9960\n",
      "Epoch 1982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6312e-04 - my_r2: 0.9319 - val_loss: 8.4682e-06 - val_my_r2: 0.9970\n",
      "Epoch 1983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3916e-04 - my_r2: 0.9472 - val_loss: 1.0251e-05 - val_my_r2: 0.9963\n",
      "Epoch 1984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8005e-04 - my_r2: 0.9055 - val_loss: 1.1955e-05 - val_my_r2: 0.9958\n",
      "Epoch 1985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1411e-04 - my_r2: 0.9113 - val_loss: 1.5649e-05 - val_my_r2: 0.9945\n",
      "Epoch 1986/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2647e-04 - my_r2: 0.9255 - val_loss: 1.3278e-05 - val_my_r2: 0.9956\n",
      "Epoch 1987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8202e-04 - my_r2: 0.9170 - val_loss: 1.3336e-05 - val_my_r2: 0.9952\n",
      "Epoch 1988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5282e-04 - my_r2: 0.9158 - val_loss: 1.1009e-05 - val_my_r2: 0.9959\n",
      "Epoch 1989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2057e-04 - my_r2: 0.9454 - val_loss: 8.0383e-06 - val_my_r2: 0.9970\n",
      "Epoch 1990/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5363e-04 - my_r2: 0.9245 - val_loss: 9.6287e-06 - val_my_r2: 0.9966\n",
      "Epoch 1991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3268e-04 - my_r2: 0.9289 - val_loss: 8.6627e-06 - val_my_r2: 0.9971\n",
      "Epoch 1992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8628e-04 - my_r2: 0.9279 - val_loss: 1.1508e-05 - val_my_r2: 0.9959\n",
      "Epoch 1993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4801e-04 - my_r2: 0.8438 - val_loss: 9.0144e-06 - val_my_r2: 0.9966\n",
      "Epoch 1994/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6489e-04 - my_r2: 0.9486 - val_loss: 8.1194e-06 - val_my_r2: 0.9968\n",
      "Epoch 1995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4745e-04 - my_r2: 0.9418 - val_loss: 9.3058e-06 - val_my_r2: 0.9962\n",
      "Epoch 1996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4084e-04 - my_r2: 0.9334 - val_loss: 9.2679e-06 - val_my_r2: 0.9962\n",
      "Epoch 1997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7623e-04 - my_r2: 0.9297 - val_loss: 8.9468e-06 - val_my_r2: 0.9961\n",
      "Epoch 1998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6918e-04 - my_r2: 0.9573 - val_loss: 7.1249e-06 - val_my_r2: 0.9972\n",
      "Epoch 1999/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8523e-04 - my_r2: 0.9411 - val_loss: 7.1416e-06 - val_my_r2: 0.9973\n",
      "Epoch 2000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2831e-04 - my_r2: 0.9324 - val_loss: 7.4623e-06 - val_my_r2: 0.9973\n",
      "---------- 1\n",
      "---------- 1\n",
      "train = 1.00 test = 1.00 loss-train = -1.000000 loss-test = -1.000000 iter=0\n",
      "Stats for iML1515_ec6_UB_AMN_QP CPU-time 1393.9358\n",
      "R2 = 0.9990 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Q2 = 0.9990 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Iter 3 Collated Q2 0.9989505525576107\n",
      "number of reactions:  1186 1186\n",
      "number of metabolites:  2084\n",
      "filtered measurements size:  1\n",
      "RC reservoir file: ./Reservoir/iML1515_ec6_UB_AMN_QP\n",
      "RC model type: RC\n",
      "RC scaler: 0.0\n",
      "RC model input dim: 38\n",
      "RC model output dim: 1\n",
      "RC model medium bound: UB\n",
      "training set size (110, 38) (110, 1)\n",
      "reservoir S, Pin, Pout matrices (2084, 1186) (38, 1186) (1, 1186)\n",
      "RC training epochs: 2000\n",
      "RC training regression: True\n",
      "RC training learn rate: 0.0001\n",
      "RC training dropout: 0.25\n",
      "RC training batch size: 5\n",
      "RC training validation iter: 0\n",
      "RC training xfold: 0\n",
      "RC training early stopping: False\n",
      "--------prior network --------\n",
      "training file: None\n",
      "model type: ANN_Dense\n",
      "model scaler: 0.0\n",
      "model input dim: 10\n",
      "model output dim: 10\n",
      "model medium bound: \n",
      "timestep: 0\n",
      "no training set provided\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "--------reservoir network-----\n",
      "training file: ./Dataset_model/iML1515_ec6_UB\n",
      "model type: AMN_QP\n",
      "model scaler: 7.95\n",
      "model input dim: 38\n",
      "model output dim: 2376\n",
      "model medium bound: UB\n",
      "timestep: 4\n",
      "training set size (11000, 38) (11000, 1)\n",
      "nbr hidden layer: 1\n",
      "hidden layer size: 500\n",
      "activation function: relu\n",
      "gradient learn rate: 0.001\n",
      "gradient decay rate: 0.9\n",
      "training epochs: 20\n",
      "training regression: True\n",
      "training learn rate: 0.001\n",
      "training dropout: 0.25\n",
      "training batch size: 100\n",
      "training validation iter: 0\n",
      "training xfold: 5\n",
      "training early stopping: False\n",
      "AMN scaler 0.0\n",
      "RC input shape (110, 38) (110, 1)\n",
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "Physical devices cannot be modified after being initialized\n",
      "----------------------------------- RC\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 10 relu True\n",
      "Prior inputs and outputs (None, 10) (None, 10)\n",
      "Res inputs added to Prior_outputs 28\n",
      "Res inputs (final) (None, 38)\n",
      "Dense layer n_hidden, hidden_dim, output_dim, activation, trainable: 1 500 1186 relu False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:33:24.573663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMN output shapes for PoutV, SV, PinV, Vpos, V, outputs (None, 1) (None, 1) (None, 1) (None, 1) (None, 1186) (None, 2376)\n",
      "=======================\n",
      "PoutV: (None, 1)\n",
      "SV: (None, 1)\n",
      "PinV: (None, 1)\n",
      "Vpos: (None, 1)\n",
      "V: (None, 1186)\n",
      "V0: KerasTensor(type_spec=TensorSpec(shape=(None, 1186), dtype=tf.float32, name=None), name='tf.__operators__.add_71/AddV2:0', description=\"created by layer 'tf.__operators__.add_71'\")\n",
      "Vin: KerasTensor(type_spec=TensorSpec(shape=(None, 38), dtype=tf.float32, name=None), name='tf.math.truediv_134/truediv:0', description=\"created by layer 'tf.math.truediv_134'\")\n",
      "Vout: tf.Tensor([], shape=(0, 0), dtype=float32)\n",
      "Res_outputs-------------------- (None, 2376)\n",
      "SV, PinV, Vpos, V-------------- (None, 1) (None, 1) (None, 1) (None, 1186)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " lambda_29 (Lambda)             (None, 10)           0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 500)          5500        ['lambda_29[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 500)          0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_28 (Lambda)             (None, 28)           0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 10)           5010        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 38)           0           ['lambda_28[0][0]',              \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_65 (TFOp  (None, 38)          0           ['input_5[0][0]',                \n",
      " Lambda)                                                          'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_136 (TFOpLamb  (None, 38)          0           ['concatenate_12[0][0]',         \n",
      " da)                                                              'tf.math.divide_no_nan_65[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_134 (TFOpLambd  (None, 38)          0           ['tf.math.multiply_136[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 500)          19500       ['tf.math.truediv_134[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 500)          0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 1186)         594186      ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_100 (TFOpLamb  (None, 1186)        0           ['tf.math.truediv_134[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_55 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_100[0][0]',   \n",
      " a)                                                               'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.relu_55 (TFOpLambda)     (None, 1186)         0           ['tf.math.subtract_55[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_66 (TFOp  (None, 1186)        0           ['tf.nn.relu_55[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_55[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract_56 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_66[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_137 (TFOpLamb  (None, 1186)        0           ['tf.math.divide_no_nan_66[0][0]'\n",
      " da)                                                             , 'dense_19[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply_138 (TFOpLamb  (None, 1186)        0           ['tf.math.subtract_56[0][0]',    \n",
      " da)                                                              'tf.linalg.matmul_100[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.multiply_139 (TFOpLamb  (None, 1186)        0           ['dense_19[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_70 (TFOpL  (None, 1186)        0           ['tf.math.multiply_137[0][0]',   \n",
      " ambda)                                                           'tf.math.multiply_138[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_71 (TFOpL  (None, 1186)        0           ['tf.math.multiply_139[0][0]',   \n",
      " ambda)                                                           'tf.__operators__.add_70[0][0]']\n",
      "                                                                                                  \n",
      " tf.linalg.matmul_103 (TFOpLamb  (None, 38)          0           ['tf.__operators__.add_71[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_57 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_103[0][0]',   \n",
      " a)                                                               'tf.math.truediv_134[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_56 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_57[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_45 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_71[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_101 (TFOpLamb  (None, 2084)        0           ['tf.__operators__.add_71[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_67 (TFOp  (None, 38)          0           ['tf.nn.relu_56[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_56[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_57 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_45[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_102 (TFOpLamb  (None, 1186)        0           ['tf.linalg.matmul_101[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_141 (TFOpLamb  (None, 38)          0           ['tf.nn.relu_56[0][0]',          \n",
      " da)                                                              'tf.math.divide_no_nan_67[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_68 (TFOp  (None, 1186)        0           ['tf.nn.relu_57[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_57[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_138 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_102[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_104 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_141[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.negative_46 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_68[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_141 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_138[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_145 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_104[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_142 (TFOpLamb  (None, 1186)        0           ['tf.nn.relu_57[0][0]',          \n",
      " da)                                                              'tf.math.negative_46[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_72 (TFOpL  (None, 1186)        0           ['tf.math.truediv_141[0][0]',    \n",
      " ambda)                                                           'tf.math.truediv_145[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_150 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_142[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_73 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_72[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_150[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_140 (TFOpLamb  (None, 1186)        0           ['tf.__operators__.add_71[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_143 (TFOpLamb  (None, 1186)        0           ['tf.__operators__.add_73[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_144 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_140[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_145 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_143[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_58 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_144[0][0]',   \n",
      " a)                                                               'tf.math.multiply_145[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_74 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_71[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_58[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_107 (TFOpLamb  (None, 38)          0           ['tf.__operators__.add_74[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_59 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_107[0][0]',   \n",
      " a)                                                               'tf.math.truediv_134[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_58 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_59[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_47 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_74[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_105 (TFOpLamb  (None, 2084)        0           ['tf.__operators__.add_74[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_70 (TFOp  (None, 38)          0           ['tf.nn.relu_58[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_58[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_59 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_47[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_106 (TFOpLamb  (None, 1186)        0           ['tf.linalg.matmul_105[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_146 (TFOpLamb  (None, 38)          0           ['tf.nn.relu_58[0][0]',          \n",
      " da)                                                              'tf.math.divide_no_nan_70[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_71 (TFOp  (None, 1186)        0           ['tf.nn.relu_59[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_59[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_156 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_106[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_108 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_146[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.negative_48 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_71[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_157 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_156[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_159 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_108[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_147 (TFOpLamb  (None, 1186)        0           ['tf.nn.relu_59[0][0]',          \n",
      " da)                                                              'tf.math.negative_48[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_75 (TFOpL  (None, 1186)        0           ['tf.math.truediv_157[0][0]',    \n",
      " ambda)                                                           'tf.math.truediv_159[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_161 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_147[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_76 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_75[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_161[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_148 (TFOpLamb  (None, 1186)        0           ['tf.__operators__.add_76[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_149 (TFOpLamb  (None, 1186)        0           ['tf.math.subtract_58[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_150 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_148[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_60 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_149[0][0]',   \n",
      " a)                                                               'tf.math.multiply_150[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_77 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_74[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_60[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_111 (TFOpLamb  (None, 38)          0           ['tf.__operators__.add_77[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_61 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_111[0][0]',   \n",
      " a)                                                               'tf.math.truediv_134[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_60 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_61[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_49 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_77[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_109 (TFOpLamb  (None, 2084)        0           ['tf.__operators__.add_77[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_73 (TFOp  (None, 38)          0           ['tf.nn.relu_60[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_60[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_61 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_49[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_110 (TFOpLamb  (None, 1186)        0           ['tf.linalg.matmul_109[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_151 (TFOpLamb  (None, 38)          0           ['tf.nn.relu_60[0][0]',          \n",
      " da)                                                              'tf.math.divide_no_nan_73[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_74 (TFOp  (None, 1186)        0           ['tf.nn.relu_61[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_61[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_163 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_110[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_112 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_151[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.negative_50 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_74[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_164 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_163[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_166 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_112[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_152 (TFOpLamb  (None, 1186)        0           ['tf.nn.relu_61[0][0]',          \n",
      " da)                                                              'tf.math.negative_50[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_78 (TFOpL  (None, 1186)        0           ['tf.math.truediv_164[0][0]',    \n",
      " ambda)                                                           'tf.math.truediv_166[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_168 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_152[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_79 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_78[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_168[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_153 (TFOpLamb  (None, 1186)        0           ['tf.__operators__.add_79[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_154 (TFOpLamb  (None, 1186)        0           ['tf.math.subtract_60[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_155 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_153[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_62 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_154[0][0]',   \n",
      " a)                                                               'tf.math.multiply_155[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_80 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_77[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_62[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_115 (TFOpLamb  (None, 38)          0           ['tf.__operators__.add_80[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_63 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_115[0][0]',   \n",
      " a)                                                               'tf.math.truediv_134[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_62 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_63[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_51 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_80[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_113 (TFOpLamb  (None, 2084)        0           ['tf.__operators__.add_80[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_76 (TFOp  (None, 38)          0           ['tf.nn.relu_62[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_62[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.relu_63 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_51[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_114 (TFOpLamb  (None, 1186)        0           ['tf.linalg.matmul_113[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_156 (TFOpLamb  (None, 38)          0           ['tf.nn.relu_62[0][0]',          \n",
      " da)                                                              'tf.math.divide_no_nan_76[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.divide_no_nan_77 (TFOp  (None, 1186)        0           ['tf.nn.relu_63[0][0]',          \n",
      " Lambda)                                                          'tf.nn.relu_63[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.truediv_170 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_114[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_116 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_156[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.negative_52 (TFOpLambd  (None, 1186)        0           ['tf.math.divide_no_nan_77[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_171 (TFOpLambd  (None, 1186)        0           ['tf.math.truediv_170[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_173 (TFOpLambd  (None, 1186)        0           ['tf.linalg.matmul_116[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_157 (TFOpLamb  (None, 1186)        0           ['tf.nn.relu_63[0][0]',          \n",
      " da)                                                              'tf.math.negative_52[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_81 (TFOpL  (None, 1186)        0           ['tf.math.truediv_171[0][0]',    \n",
      " ambda)                                                           'tf.math.truediv_173[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.truediv_175 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_157[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_82 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_81[0][0]',\n",
      " ambda)                                                           'tf.math.truediv_175[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_158 (TFOpLamb  (None, 1186)        0           ['tf.__operators__.add_82[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_159 (TFOpLamb  (None, 1186)        0           ['tf.math.subtract_62[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_160 (TFOpLamb  (None, 1186)        0           ['tf.math.multiply_158[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_64 (TFOpLambd  (None, 1186)        0           ['tf.math.multiply_159[0][0]',   \n",
      " a)                                                               'tf.math.multiply_160[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add_83 (TFOpL  (None, 1186)        0           ['tf.__operators__.add_80[0][0]',\n",
      " ambda)                                                           'tf.math.subtract_64[0][0]']    \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_119 (TFOpLamb  (None, 38)          0           ['tf.__operators__.add_83[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_65 (TFOpLambd  (None, 38)          0           ['tf.linalg.matmul_119[0][0]',   \n",
      " a)                                                               'tf.math.truediv_134[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.negative_53 (TFOpLambd  (None, 1186)        0           ['tf.__operators__.add_83[0][0]']\n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_118 (TFOpLamb  (None, 2084)        0           ['tf.__operators__.add_83[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.relu_64 (TFOpLambda)     (None, 38)           0           ['tf.math.subtract_65[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.relu_65 (TFOpLambda)     (None, 1186)         0           ['tf.math.negative_53[0][0]']    \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_75 (TFOpLamb  (None, 1)           0           ['tf.linalg.matmul_118[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_76 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_64[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_77 (TFOpLamb  (None, 1)           0           ['tf.nn.relu_65[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_117 (TFOpLamb  (None, 1)           0           ['tf.__operators__.add_83[0][0]']\n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.truediv_176 (TFOpLambd  (None, 1)           0           ['tf.compat.v1.norm_75[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_177 (TFOpLambd  (None, 1)           0           ['tf.compat.v1.norm_76[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.truediv_178 (TFOpLambd  (None, 1)           0           ['tf.compat.v1.norm_77[0][0]']   \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 2376)         0           ['tf.linalg.matmul_117[0][0]',   \n",
      "                                                                  'tf.math.truediv_176[0][0]',    \n",
      "                                                                  'tf.math.truediv_177[0][0]',    \n",
      "                                                                  'tf.math.truediv_178[0][0]',    \n",
      "                                                                  'tf.__operators__.add_83[0][0]',\n",
      "                                                                  'tf.__operators__.add_71[0][0]']\n",
      "                                                                                                  \n",
      " lambda_30 (Lambda)             (None, 1)            0           ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_31 (Lambda)             (None, 1)            0           ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_32 (Lambda)             (None, 1)            0           ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_33 (Lambda)             (None, 1)            0           ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_34 (Lambda)             (None, 1186)         0           ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 1228)         0           ['lambda_30[0][0]',              \n",
      "                                                                  'lambda_31[0][0]',              \n",
      "                                                                  'lambda_32[0][0]',              \n",
      "                                                                  'lambda_33[0][0]',              \n",
      "                                                                  'lambda_34[0][0]',              \n",
      "                                                                  'tf.math.truediv_134[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 624,196\n",
      "Trainable params: 10,510\n",
      "Non-trainable params: 613,686\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "nbr parameters: 624196\n",
      "Epoch 1/2000\n",
      "22/22 [==============================] - 2s 48ms/step - loss: 0.0398 - my_r2: -9.9053 - val_loss: 0.0379 - val_my_r2: -10.5469\n",
      "Epoch 2/2000\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.0352 - my_r2: -8.8558 - val_loss: 0.0339 - val_my_r2: -9.3128\n",
      "Epoch 3/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0330 - my_r2: -6.4829 - val_loss: 0.0298 - val_my_r2: -8.0585\n",
      "Epoch 4/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0283 - my_r2: -4.6841 - val_loss: 0.0248 - val_my_r2: -6.5404\n",
      "Epoch 5/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0226 - my_r2: -4.7876 - val_loss: 0.0205 - val_my_r2: -5.2121\n",
      "Epoch 6/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0200 - my_r2: -18.3792 - val_loss: 0.0166 - val_my_r2: -4.0329\n",
      "Epoch 7/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0150 - my_r2: -3.2213 - val_loss: 0.0137 - val_my_r2: -3.1504\n",
      "Epoch 8/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0131 - my_r2: -3.0579 - val_loss: 0.0112 - val_my_r2: -2.3840\n",
      "Epoch 9/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0107 - my_r2: -1.5088 - val_loss: 0.0095 - val_my_r2: -1.8557\n",
      "Epoch 10/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0094 - my_r2: -2.4363 - val_loss: 0.0080 - val_my_r2: -1.4030\n",
      "Epoch 11/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0080 - my_r2: -0.8359 - val_loss: 0.0069 - val_my_r2: -1.0676\n",
      "Epoch 12/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0071 - my_r2: -1.2439 - val_loss: 0.0061 - val_my_r2: -0.8186\n",
      "Epoch 13/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0060 - my_r2: -0.3442 - val_loss: 0.0054 - val_my_r2: -0.6193\n",
      "Epoch 14/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0054 - my_r2: -0.3433 - val_loss: 0.0049 - val_my_r2: -0.4694\n",
      "Epoch 15/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0050 - my_r2: -0.6483 - val_loss: 0.0045 - val_my_r2: -0.3419\n",
      "Epoch 16/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0045 - my_r2: -0.0039 - val_loss: 0.0041 - val_my_r2: -0.2442\n",
      "Epoch 17/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0042 - my_r2: 0.1241 - val_loss: 0.0038 - val_my_r2: -0.1574\n",
      "Epoch 18/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0038 - my_r2: -0.0967 - val_loss: 0.0036 - val_my_r2: -0.0889\n",
      "Epoch 19/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0038 - my_r2: 0.1481 - val_loss: 0.0034 - val_my_r2: -0.0299\n",
      "Epoch 20/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0035 - my_r2: 0.2778 - val_loss: 0.0032 - val_my_r2: 0.0230\n",
      "Epoch 21/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0033 - my_r2: -0.0198 - val_loss: 0.0031 - val_my_r2: 0.0657\n",
      "Epoch 22/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0035 - my_r2: 0.2883 - val_loss: 0.0029 - val_my_r2: 0.1048\n",
      "Epoch 23/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0032 - my_r2: 0.1864 - val_loss: 0.0028 - val_my_r2: 0.1389\n",
      "Epoch 24/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0030 - my_r2: 0.2354 - val_loss: 0.0027 - val_my_r2: 0.1741\n",
      "Epoch 25/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0027 - my_r2: 0.3239 - val_loss: 0.0026 - val_my_r2: 0.2042\n",
      "Epoch 26/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0027 - my_r2: 0.3400 - val_loss: 0.0025 - val_my_r2: 0.2321\n",
      "Epoch 27/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0027 - my_r2: 0.4553 - val_loss: 0.0024 - val_my_r2: 0.2563\n",
      "Epoch 28/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0026 - my_r2: 0.3032 - val_loss: 0.0023 - val_my_r2: 0.2818\n",
      "Epoch 29/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0023 - my_r2: 0.4729 - val_loss: 0.0022 - val_my_r2: 0.3030\n",
      "Epoch 30/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0024 - my_r2: 0.3474 - val_loss: 0.0022 - val_my_r2: 0.3241\n",
      "Epoch 31/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0023 - my_r2: 0.4620 - val_loss: 0.0021 - val_my_r2: 0.3432\n",
      "Epoch 32/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0024 - my_r2: 0.4698 - val_loss: 0.0020 - val_my_r2: 0.3640\n",
      "Epoch 33/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0021 - my_r2: 0.3020 - val_loss: 0.0020 - val_my_r2: 0.3794\n",
      "Epoch 34/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0023 - my_r2: -0.0165 - val_loss: 0.0019 - val_my_r2: 0.3982\n",
      "Epoch 35/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0020 - my_r2: 0.2952 - val_loss: 0.0019 - val_my_r2: 0.4147\n",
      "Epoch 36/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0020 - my_r2: 0.6258 - val_loss: 0.0018 - val_my_r2: 0.4286\n",
      "Epoch 37/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0024 - my_r2: 0.4394 - val_loss: 0.0018 - val_my_r2: 0.4440\n",
      "Epoch 38/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0022 - my_r2: 0.4085 - val_loss: 0.0017 - val_my_r2: 0.4577\n",
      "Epoch 39/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0021 - my_r2: 0.4597 - val_loss: 0.0017 - val_my_r2: 0.4715\n",
      "Epoch 40/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0019 - my_r2: 0.5753 - val_loss: 0.0016 - val_my_r2: 0.4850\n",
      "Epoch 41/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0016 - my_r2: 0.6535 - val_loss: 0.0016 - val_my_r2: 0.4961\n",
      "Epoch 42/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0018 - my_r2: 0.6500 - val_loss: 0.0015 - val_my_r2: 0.5072\n",
      "Epoch 43/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0017 - my_r2: 0.1479 - val_loss: 0.0015 - val_my_r2: 0.5159\n",
      "Epoch 44/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.5371 - val_loss: 0.0015 - val_my_r2: 0.5261\n",
      "Epoch 45/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0017 - my_r2: 0.4245 - val_loss: 0.0014 - val_my_r2: 0.5362\n",
      "Epoch 46/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0019 - my_r2: -0.3216 - val_loss: 0.0014 - val_my_r2: 0.5457\n",
      "Epoch 47/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0016 - my_r2: 0.4695 - val_loss: 0.0014 - val_my_r2: 0.5537\n",
      "Epoch 48/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.4596 - val_loss: 0.0014 - val_my_r2: 0.5619\n",
      "Epoch 49/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0016 - my_r2: 0.4692 - val_loss: 0.0013 - val_my_r2: 0.5686\n",
      "Epoch 50/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.6195 - val_loss: 0.0013 - val_my_r2: 0.5777\n",
      "Epoch 51/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.6584 - val_loss: 0.0013 - val_my_r2: 0.5839\n",
      "Epoch 52/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.6962 - val_loss: 0.0013 - val_my_r2: 0.5899\n",
      "Epoch 53/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0017 - my_r2: 0.6540 - val_loss: 0.0012 - val_my_r2: 0.5981\n",
      "Epoch 54/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.0013 - my_r2: 0.5820 - val_loss: 0.0012 - val_my_r2: 0.6043\n",
      "Epoch 55/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0014 - my_r2: 0.6150 - val_loss: 0.0012 - val_my_r2: 0.6096\n",
      "Epoch 56/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0015 - my_r2: 0.6120 - val_loss: 0.0012 - val_my_r2: 0.6166\n",
      "Epoch 57/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.7065 - val_loss: 0.0012 - val_my_r2: 0.6218\n",
      "Epoch 58/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0016 - my_r2: 0.7024 - val_loss: 0.0011 - val_my_r2: 0.6278\n",
      "Epoch 59/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.6682 - val_loss: 0.0011 - val_my_r2: 0.6316\n",
      "Epoch 60/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0012 - my_r2: 0.6840 - val_loss: 0.0011 - val_my_r2: 0.6373\n",
      "Epoch 61/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.5667 - val_loss: 0.0011 - val_my_r2: 0.6396\n",
      "Epoch 62/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6580 - val_loss: 0.0011 - val_my_r2: 0.6428\n",
      "Epoch 63/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0014 - my_r2: 0.5529 - val_loss: 0.0011 - val_my_r2: 0.6487\n",
      "Epoch 64/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6604 - val_loss: 0.0010 - val_my_r2: 0.6541\n",
      "Epoch 65/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0011 - my_r2: 0.6954 - val_loss: 0.0010 - val_my_r2: 0.6582\n",
      "Epoch 66/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0012 - my_r2: 0.7030 - val_loss: 0.0010 - val_my_r2: 0.6623\n",
      "Epoch 67/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7298 - val_loss: 0.0010 - val_my_r2: 0.6674\n",
      "Epoch 68/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7102 - val_loss: 9.9030e-04 - val_my_r2: 0.6712\n",
      "Epoch 69/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0013 - my_r2: 0.6262 - val_loss: 9.7789e-04 - val_my_r2: 0.6748\n",
      "Epoch 70/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0014 - my_r2: 0.6034 - val_loss: 9.6510e-04 - val_my_r2: 0.6796\n",
      "Epoch 71/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7608 - val_loss: 9.5510e-04 - val_my_r2: 0.6834\n",
      "Epoch 72/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6857 - val_loss: 9.4316e-04 - val_my_r2: 0.6871\n",
      "Epoch 73/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.6229 - val_loss: 9.3114e-04 - val_my_r2: 0.6912\n",
      "Epoch 74/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.6415 - val_loss: 9.1697e-04 - val_my_r2: 0.6957\n",
      "Epoch 75/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.5297 - val_loss: 9.0488e-04 - val_my_r2: 0.7016\n",
      "Epoch 76/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0013 - my_r2: 0.5109 - val_loss: 8.9229e-04 - val_my_r2: 0.7056\n",
      "Epoch 77/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.6460 - val_loss: 8.8109e-04 - val_my_r2: 0.7104\n",
      "Epoch 78/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.8502e-04 - my_r2: 0.7167 - val_loss: 8.6861e-04 - val_my_r2: 0.7123\n",
      "Epoch 79/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7382 - val_loss: 8.5817e-04 - val_my_r2: 0.7163\n",
      "Epoch 80/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7028 - val_loss: 8.5001e-04 - val_my_r2: 0.7174\n",
      "Epoch 81/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.9587e-04 - my_r2: 0.6555 - val_loss: 8.3946e-04 - val_my_r2: 0.7202\n",
      "Epoch 82/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.3617e-04 - my_r2: 0.7678 - val_loss: 8.3168e-04 - val_my_r2: 0.7220\n",
      "Epoch 83/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0012 - my_r2: 0.7063 - val_loss: 8.2417e-04 - val_my_r2: 0.7253\n",
      "Epoch 84/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0010 - my_r2: 0.6885 - val_loss: 8.1564e-04 - val_my_r2: 0.7273\n",
      "Epoch 85/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.6209 - val_loss: 8.0446e-04 - val_my_r2: 0.7316\n",
      "Epoch 86/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7365 - val_loss: 7.9593e-04 - val_my_r2: 0.7362\n",
      "Epoch 87/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.0011 - my_r2: 0.6765 - val_loss: 7.8851e-04 - val_my_r2: 0.7407\n",
      "Epoch 88/2000\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 8.8825e-04 - my_r2: 0.7641 - val_loss: 7.7738e-04 - val_my_r2: 0.7438\n",
      "Epoch 89/2000\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.0010 - my_r2: 0.7822 - val_loss: 7.7000e-04 - val_my_r2: 0.7456\n",
      "Epoch 90/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7122 - val_loss: 7.5969e-04 - val_my_r2: 0.7483\n",
      "Epoch 91/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0011 - my_r2: 0.7179 - val_loss: 7.4881e-04 - val_my_r2: 0.7523\n",
      "Epoch 92/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.0010 - my_r2: 0.5208 - val_loss: 7.4080e-04 - val_my_r2: 0.7549\n",
      "Epoch 93/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.0740e-04 - my_r2: 0.7901 - val_loss: 7.2970e-04 - val_my_r2: 0.7562\n",
      "Epoch 94/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 9.4615e-04 - my_r2: 0.7767 - val_loss: 7.2063e-04 - val_my_r2: 0.7583\n",
      "Epoch 95/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.0010 - my_r2: 0.7064 - val_loss: 7.1287e-04 - val_my_r2: 0.7614\n",
      "Epoch 96/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.3696e-04 - my_r2: 0.6718 - val_loss: 7.0627e-04 - val_my_r2: 0.7640\n",
      "Epoch 97/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0010 - my_r2: 0.7128 - val_loss: 7.0011e-04 - val_my_r2: 0.7653\n",
      "Epoch 98/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.0011 - my_r2: 0.7244 - val_loss: 6.8932e-04 - val_my_r2: 0.7681\n",
      "Epoch 99/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.3802e-04 - my_r2: 0.7409 - val_loss: 6.8054e-04 - val_my_r2: 0.7705\n",
      "Epoch 100/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.7608e-04 - my_r2: 0.8205 - val_loss: 6.7304e-04 - val_my_r2: 0.7713\n",
      "Epoch 101/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 8.2845e-04 - my_r2: 0.7202 - val_loss: 6.6416e-04 - val_my_r2: 0.7731\n",
      "Epoch 102/2000\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.0010 - my_r2: 0.7473 - val_loss: 6.5726e-04 - val_my_r2: 0.7752\n",
      "Epoch 103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.7175e-04 - my_r2: 0.6088 - val_loss: 6.4989e-04 - val_my_r2: 0.7765\n",
      "Epoch 104/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.0417e-04 - my_r2: 0.5933 - val_loss: 6.4184e-04 - val_my_r2: 0.7795\n",
      "Epoch 105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.1544e-04 - my_r2: 0.8105 - val_loss: 6.3515e-04 - val_my_r2: 0.7843\n",
      "Epoch 106/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 7.9553e-04 - my_r2: 0.8073 - val_loss: 6.2875e-04 - val_my_r2: 0.7851\n",
      "Epoch 107/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.3625e-04 - my_r2: 0.7966 - val_loss: 6.2300e-04 - val_my_r2: 0.7880\n",
      "Epoch 108/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.5507e-04 - my_r2: 0.8403 - val_loss: 6.1516e-04 - val_my_r2: 0.7901\n",
      "Epoch 109/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.2721e-04 - my_r2: 0.7332 - val_loss: 6.0810e-04 - val_my_r2: 0.7929\n",
      "Epoch 110/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.9178e-04 - my_r2: 0.7732 - val_loss: 6.0307e-04 - val_my_r2: 0.7956\n",
      "Epoch 111/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.5115e-04 - my_r2: 0.6153 - val_loss: 5.9786e-04 - val_my_r2: 0.7966\n",
      "Epoch 112/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.1014e-04 - my_r2: 0.7836 - val_loss: 5.9057e-04 - val_my_r2: 0.7986\n",
      "Epoch 113/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.9354e-04 - my_r2: 0.8084 - val_loss: 5.8413e-04 - val_my_r2: 0.8004\n",
      "Epoch 114/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 9.7811e-04 - my_r2: 0.6642 - val_loss: 5.7712e-04 - val_my_r2: 0.8027\n",
      "Epoch 115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.2386e-04 - my_r2: 0.8139 - val_loss: 5.7096e-04 - val_my_r2: 0.8051\n",
      "Epoch 116/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.4967e-04 - my_r2: 0.8121 - val_loss: 5.6847e-04 - val_my_r2: 0.8058\n",
      "Epoch 117/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.6983e-04 - my_r2: 0.7574 - val_loss: 5.6697e-04 - val_my_r2: 0.8062\n",
      "Epoch 118/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.3599e-04 - my_r2: 0.7819 - val_loss: 5.5552e-04 - val_my_r2: 0.8082\n",
      "Epoch 119/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.9059e-04 - my_r2: 0.5690 - val_loss: 5.4928e-04 - val_my_r2: 0.8085\n",
      "Epoch 120/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.6507e-04 - my_r2: 0.7666 - val_loss: 5.4171e-04 - val_my_r2: 0.8108\n",
      "Epoch 121/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 8.6410e-04 - my_r2: 0.7804 - val_loss: 5.3495e-04 - val_my_r2: 0.8139\n",
      "Epoch 122/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.9496e-04 - my_r2: 0.8202 - val_loss: 5.3039e-04 - val_my_r2: 0.8158\n",
      "Epoch 123/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.8015e-04 - my_r2: 0.7779 - val_loss: 5.2457e-04 - val_my_r2: 0.8180\n",
      "Epoch 124/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.2492e-04 - my_r2: 0.5860 - val_loss: 5.1979e-04 - val_my_r2: 0.8177\n",
      "Epoch 125/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.6554e-04 - my_r2: 0.7346 - val_loss: 5.1518e-04 - val_my_r2: 0.8200\n",
      "Epoch 126/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.5774e-04 - my_r2: 0.7464 - val_loss: 5.1080e-04 - val_my_r2: 0.8220\n",
      "Epoch 127/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.5208e-04 - my_r2: 0.8270 - val_loss: 5.0475e-04 - val_my_r2: 0.8252\n",
      "Epoch 128/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.1973e-04 - my_r2: 0.7452 - val_loss: 5.0119e-04 - val_my_r2: 0.8256\n",
      "Epoch 129/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.9031e-04 - my_r2: 0.8586 - val_loss: 4.9578e-04 - val_my_r2: 0.8265\n",
      "Epoch 130/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 9.1036e-04 - my_r2: 0.6484 - val_loss: 4.9389e-04 - val_my_r2: 0.8256\n",
      "Epoch 131/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3411e-04 - my_r2: 0.7736 - val_loss: 4.8968e-04 - val_my_r2: 0.8281\n",
      "Epoch 132/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.8592e-04 - my_r2: 0.8176 - val_loss: 4.8201e-04 - val_my_r2: 0.8333\n",
      "Epoch 133/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8268e-04 - my_r2: 0.8200 - val_loss: 4.7857e-04 - val_my_r2: 0.8348\n",
      "Epoch 134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.7434e-04 - my_r2: 0.7803 - val_loss: 4.7786e-04 - val_my_r2: 0.8362\n",
      "Epoch 135/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.6875e-04 - my_r2: 0.7814 - val_loss: 4.7332e-04 - val_my_r2: 0.8371\n",
      "Epoch 136/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.6623e-04 - my_r2: 0.8086 - val_loss: 4.6728e-04 - val_my_r2: 0.8381\n",
      "Epoch 137/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.5712e-04 - my_r2: 0.8259 - val_loss: 4.6343e-04 - val_my_r2: 0.8367\n",
      "Epoch 138/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.4153e-04 - my_r2: 0.7568 - val_loss: 4.5957e-04 - val_my_r2: 0.8369\n",
      "Epoch 139/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.2115e-04 - my_r2: 0.7872 - val_loss: 4.5656e-04 - val_my_r2: 0.8369\n",
      "Epoch 140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.9993e-04 - my_r2: 0.8141 - val_loss: 4.5602e-04 - val_my_r2: 0.8375\n",
      "Epoch 141/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.7631e-04 - my_r2: 0.8465 - val_loss: 4.5146e-04 - val_my_r2: 0.8387\n",
      "Epoch 142/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7265e-04 - my_r2: 0.8126 - val_loss: 4.4584e-04 - val_my_r2: 0.8400\n",
      "Epoch 143/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.4656e-04 - my_r2: 0.8297 - val_loss: 4.4183e-04 - val_my_r2: 0.8424\n",
      "Epoch 144/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 8.0104e-04 - my_r2: 0.8037 - val_loss: 4.3975e-04 - val_my_r2: 0.8436\n",
      "Epoch 145/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.4749e-04 - my_r2: 0.8128 - val_loss: 4.3481e-04 - val_my_r2: 0.8466\n",
      "Epoch 146/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2579e-04 - my_r2: 0.8586 - val_loss: 4.3077e-04 - val_my_r2: 0.8491\n",
      "Epoch 147/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.2511e-04 - my_r2: 0.8276 - val_loss: 4.2682e-04 - val_my_r2: 0.8495\n",
      "Epoch 148/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 8.0022e-04 - my_r2: 0.8321 - val_loss: 4.2432e-04 - val_my_r2: 0.8492\n",
      "Epoch 149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.6016e-04 - my_r2: 0.6859 - val_loss: 4.2140e-04 - val_my_r2: 0.8507\n",
      "Epoch 150/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4832e-04 - my_r2: 0.8668 - val_loss: 4.1851e-04 - val_my_r2: 0.8526\n",
      "Epoch 151/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.5368e-04 - my_r2: 0.8056 - val_loss: 4.1752e-04 - val_my_r2: 0.8543\n",
      "Epoch 152/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8504e-04 - my_r2: 0.8245 - val_loss: 4.1392e-04 - val_my_r2: 0.8557\n",
      "Epoch 153/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7197e-04 - my_r2: 0.8243 - val_loss: 4.1018e-04 - val_my_r2: 0.8567\n",
      "Epoch 154/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.0856e-04 - my_r2: 0.8287 - val_loss: 4.0727e-04 - val_my_r2: 0.8576\n",
      "Epoch 155/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 7.3041e-04 - my_r2: 0.7504 - val_loss: 4.0268e-04 - val_my_r2: 0.8583\n",
      "Epoch 156/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.2245e-04 - my_r2: 0.7621 - val_loss: 3.9978e-04 - val_my_r2: 0.8607\n",
      "Epoch 157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.2801e-04 - my_r2: 0.8287 - val_loss: 3.9663e-04 - val_my_r2: 0.8618\n",
      "Epoch 158/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8699e-04 - my_r2: 0.8695 - val_loss: 3.9358e-04 - val_my_r2: 0.8623\n",
      "Epoch 159/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.4464e-04 - my_r2: 0.8158 - val_loss: 3.9000e-04 - val_my_r2: 0.8633\n",
      "Epoch 160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3026e-04 - my_r2: 0.8487 - val_loss: 3.8757e-04 - val_my_r2: 0.8633\n",
      "Epoch 161/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5624e-04 - my_r2: 0.8720 - val_loss: 3.8513e-04 - val_my_r2: 0.8649\n",
      "Epoch 162/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8262e-04 - my_r2: 0.8749 - val_loss: 3.8142e-04 - val_my_r2: 0.8657\n",
      "Epoch 163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9942e-04 - my_r2: 0.8536 - val_loss: 3.7856e-04 - val_my_r2: 0.8672\n",
      "Epoch 164/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6968e-04 - my_r2: 0.8453 - val_loss: 3.7476e-04 - val_my_r2: 0.8679\n",
      "Epoch 165/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.4046e-04 - my_r2: 0.8666 - val_loss: 3.7300e-04 - val_my_r2: 0.8694\n",
      "Epoch 166/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3128e-04 - my_r2: 0.8684 - val_loss: 3.7480e-04 - val_my_r2: 0.8695\n",
      "Epoch 167/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.5010e-04 - my_r2: 0.8452 - val_loss: 3.7034e-04 - val_my_r2: 0.8718\n",
      "Epoch 168/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.7028e-04 - my_r2: 0.7625 - val_loss: 3.7009e-04 - val_my_r2: 0.8726\n",
      "Epoch 169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0664e-04 - my_r2: 0.8088 - val_loss: 3.6603e-04 - val_my_r2: 0.8742\n",
      "Epoch 170/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.4428e-04 - my_r2: 0.7733 - val_loss: 3.6184e-04 - val_my_r2: 0.8752\n",
      "Epoch 171/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7784e-04 - my_r2: 0.8859 - val_loss: 3.6016e-04 - val_my_r2: 0.8752\n",
      "Epoch 172/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0470e-04 - my_r2: 0.7375 - val_loss: 3.5667e-04 - val_my_r2: 0.8776\n",
      "Epoch 173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3037e-04 - my_r2: 0.8435 - val_loss: 3.5232e-04 - val_my_r2: 0.8776\n",
      "Epoch 174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.0385e-04 - my_r2: 0.8454 - val_loss: 3.5048e-04 - val_my_r2: 0.8773\n",
      "Epoch 175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8156e-04 - my_r2: 0.8571 - val_loss: 3.4778e-04 - val_my_r2: 0.8784\n",
      "Epoch 176/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.8631e-04 - my_r2: 0.8869 - val_loss: 3.4535e-04 - val_my_r2: 0.8798\n",
      "Epoch 177/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8430e-04 - my_r2: 0.7887 - val_loss: 3.4799e-04 - val_my_r2: 0.8793\n",
      "Epoch 178/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3679e-04 - my_r2: 0.8709 - val_loss: 3.4233e-04 - val_my_r2: 0.8817\n",
      "Epoch 179/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.3571e-04 - my_r2: 0.8319 - val_loss: 3.3796e-04 - val_my_r2: 0.8829\n",
      "Epoch 180/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.1703e-04 - my_r2: 0.8663 - val_loss: 3.3181e-04 - val_my_r2: 0.8836\n",
      "Epoch 181/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.9020e-04 - my_r2: 0.8576 - val_loss: 3.3239e-04 - val_my_r2: 0.8846\n",
      "Epoch 182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3774e-04 - my_r2: 0.7978 - val_loss: 3.2852e-04 - val_my_r2: 0.8852\n",
      "Epoch 183/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.1062e-04 - my_r2: 0.8522 - val_loss: 3.3329e-04 - val_my_r2: 0.8834\n",
      "Epoch 184/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.0202e-04 - my_r2: 0.8496 - val_loss: 3.3180e-04 - val_my_r2: 0.8852\n",
      "Epoch 185/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.2707e-04 - my_r2: 0.7171 - val_loss: 3.2666e-04 - val_my_r2: 0.8877\n",
      "Epoch 186/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.4501e-04 - my_r2: 0.8733 - val_loss: 3.2150e-04 - val_my_r2: 0.8869\n",
      "Epoch 187/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.2920e-04 - my_r2: 0.8675 - val_loss: 3.1988e-04 - val_my_r2: 0.8867\n",
      "Epoch 188/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9200e-04 - my_r2: 0.5556 - val_loss: 3.1570e-04 - val_my_r2: 0.8890\n",
      "Epoch 189/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.7704e-04 - my_r2: 0.8341 - val_loss: 3.1186e-04 - val_my_r2: 0.8912\n",
      "Epoch 190/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.8568e-04 - my_r2: 0.8561 - val_loss: 3.1359e-04 - val_my_r2: 0.8899\n",
      "Epoch 191/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 6.2186e-04 - my_r2: 0.7901 - val_loss: 3.0616e-04 - val_my_r2: 0.8939\n",
      "Epoch 192/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 5.5734e-04 - my_r2: 0.8600 - val_loss: 3.0921e-04 - val_my_r2: 0.8922\n",
      "Epoch 193/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7771e-04 - my_r2: 0.8674 - val_loss: 3.0664e-04 - val_my_r2: 0.8918\n",
      "Epoch 194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 7.3283e-04 - my_r2: 0.7957 - val_loss: 3.0187e-04 - val_my_r2: 0.8942\n",
      "Epoch 195/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7184e-04 - my_r2: 0.8829 - val_loss: 3.0073e-04 - val_my_r2: 0.8955\n",
      "Epoch 196/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4134e-04 - my_r2: 0.8572 - val_loss: 2.9953e-04 - val_my_r2: 0.8964\n",
      "Epoch 197/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7969e-04 - my_r2: 0.8265 - val_loss: 3.0005e-04 - val_my_r2: 0.8976\n",
      "Epoch 198/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8312e-04 - my_r2: 0.8391 - val_loss: 3.0044e-04 - val_my_r2: 0.8954\n",
      "Epoch 199/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9628e-04 - my_r2: 0.8704 - val_loss: 2.9630e-04 - val_my_r2: 0.8969\n",
      "Epoch 200/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3177e-04 - my_r2: 0.7461 - val_loss: 2.9259e-04 - val_my_r2: 0.8985\n",
      "Epoch 201/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6468e-04 - my_r2: 0.8559 - val_loss: 2.8940e-04 - val_my_r2: 0.9003\n",
      "Epoch 202/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1766e-04 - my_r2: 0.8738 - val_loss: 2.8707e-04 - val_my_r2: 0.9006\n",
      "Epoch 203/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.9623e-04 - my_r2: 0.8223 - val_loss: 2.8697e-04 - val_my_r2: 0.9003\n",
      "Epoch 204/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4996e-04 - my_r2: 0.3495 - val_loss: 2.8508e-04 - val_my_r2: 0.8997\n",
      "Epoch 205/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1918e-04 - my_r2: 0.8062 - val_loss: 2.8501e-04 - val_my_r2: 0.8995\n",
      "Epoch 206/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3523e-04 - my_r2: 0.8590 - val_loss: 2.8266e-04 - val_my_r2: 0.8985\n",
      "Epoch 207/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.6152e-04 - my_r2: 0.8558 - val_loss: 2.7740e-04 - val_my_r2: 0.9019\n",
      "Epoch 208/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9678e-04 - my_r2: 0.8540 - val_loss: 2.7717e-04 - val_my_r2: 0.9025\n",
      "Epoch 209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5549e-04 - my_r2: 0.8765 - val_loss: 2.7563e-04 - val_my_r2: 0.9049\n",
      "Epoch 210/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.8013e-04 - my_r2: 0.8636 - val_loss: 2.7595e-04 - val_my_r2: 0.9053\n",
      "Epoch 211/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.0062e-04 - my_r2: 0.8350 - val_loss: 2.7954e-04 - val_my_r2: 0.9045\n",
      "Epoch 212/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2873e-04 - my_r2: 0.8394 - val_loss: 2.7440e-04 - val_my_r2: 0.9052\n",
      "Epoch 213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3728e-04 - my_r2: 0.8565 - val_loss: 2.6989e-04 - val_my_r2: 0.9066\n",
      "Epoch 214/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.5581e-04 - my_r2: 0.8452 - val_loss: 2.6842e-04 - val_my_r2: 0.9074\n",
      "Epoch 215/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2500e-04 - my_r2: 0.8540 - val_loss: 2.6780e-04 - val_my_r2: 0.9073\n",
      "Epoch 216/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.3738e-04 - my_r2: 0.8733 - val_loss: 2.6220e-04 - val_my_r2: 0.9083\n",
      "Epoch 217/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0442e-04 - my_r2: 0.8989 - val_loss: 2.6014e-04 - val_my_r2: 0.9096\n",
      "Epoch 218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1058e-04 - my_r2: 0.8603 - val_loss: 2.5991e-04 - val_my_r2: 0.9104\n",
      "Epoch 219/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8731e-04 - my_r2: 0.8259 - val_loss: 2.5691e-04 - val_my_r2: 0.9107\n",
      "Epoch 220/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0659e-04 - my_r2: 0.8549 - val_loss: 2.5513e-04 - val_my_r2: 0.9102\n",
      "Epoch 221/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5667e-04 - my_r2: 0.9034 - val_loss: 2.5288e-04 - val_my_r2: 0.9099\n",
      "Epoch 222/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6971e-04 - my_r2: 0.8143 - val_loss: 2.5047e-04 - val_my_r2: 0.9108\n",
      "Epoch 223/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.0658e-04 - my_r2: 0.8630 - val_loss: 2.5117e-04 - val_my_r2: 0.9106\n",
      "Epoch 224/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.9587e-04 - my_r2: 0.7684 - val_loss: 2.4936e-04 - val_my_r2: 0.9106\n",
      "Epoch 225/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3789e-04 - my_r2: 0.8652 - val_loss: 2.4736e-04 - val_my_r2: 0.9112\n",
      "Epoch 226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0271e-04 - my_r2: 0.8923 - val_loss: 2.4638e-04 - val_my_r2: 0.9127\n",
      "Epoch 227/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5290e-04 - my_r2: 0.8138 - val_loss: 2.4461e-04 - val_my_r2: 0.9127\n",
      "Epoch 228/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.9245e-04 - my_r2: 0.8778 - val_loss: 2.4755e-04 - val_my_r2: 0.9118\n",
      "Epoch 229/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.1285e-04 - my_r2: 0.8438 - val_loss: 2.4292e-04 - val_my_r2: 0.9135\n",
      "Epoch 230/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0951e-04 - my_r2: 0.8896 - val_loss: 2.4263e-04 - val_my_r2: 0.9128\n",
      "Epoch 231/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6559e-04 - my_r2: 0.8642 - val_loss: 2.4545e-04 - val_my_r2: 0.9114\n",
      "Epoch 232/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1005e-04 - my_r2: 0.7794 - val_loss: 2.4265e-04 - val_my_r2: 0.9106\n",
      "Epoch 233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7831e-04 - my_r2: 0.8773 - val_loss: 2.4234e-04 - val_my_r2: 0.9106\n",
      "Epoch 234/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2407e-04 - my_r2: 0.8814 - val_loss: 2.4100e-04 - val_my_r2: 0.9117\n",
      "Epoch 235/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.7254e-04 - my_r2: 0.8971 - val_loss: 2.3533e-04 - val_my_r2: 0.9139\n",
      "Epoch 236/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 6.1247e-04 - my_r2: 0.8223 - val_loss: 2.3401e-04 - val_my_r2: 0.9141\n",
      "Epoch 237/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7288e-04 - my_r2: 0.8283 - val_loss: 2.3455e-04 - val_my_r2: 0.9132\n",
      "Epoch 238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4907e-04 - my_r2: 0.8243 - val_loss: 2.3520e-04 - val_my_r2: 0.9128\n",
      "Epoch 239/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5731e-04 - my_r2: 0.7738 - val_loss: 2.3024e-04 - val_my_r2: 0.9160\n",
      "Epoch 240/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6062e-04 - my_r2: 0.8849 - val_loss: 2.2619e-04 - val_my_r2: 0.9179\n",
      "Epoch 241/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6318e-04 - my_r2: 0.8887 - val_loss: 2.2714e-04 - val_my_r2: 0.9178\n",
      "Epoch 242/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.9468e-04 - my_r2: 0.8793 - val_loss: 2.2792e-04 - val_my_r2: 0.9180\n",
      "Epoch 243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7445e-04 - my_r2: 0.8803 - val_loss: 2.2319e-04 - val_my_r2: 0.9199\n",
      "Epoch 244/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5433e-04 - my_r2: 0.8844 - val_loss: 2.1902e-04 - val_my_r2: 0.9223\n",
      "Epoch 245/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.2339e-04 - my_r2: 0.8499 - val_loss: 2.1777e-04 - val_my_r2: 0.9234\n",
      "Epoch 246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9615e-04 - my_r2: 0.9034 - val_loss: 2.1741e-04 - val_my_r2: 0.9237\n",
      "Epoch 247/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.6228e-04 - my_r2: 0.8875 - val_loss: 2.1982e-04 - val_my_r2: 0.9210\n",
      "Epoch 248/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6974e-04 - my_r2: 0.8862 - val_loss: 2.1539e-04 - val_my_r2: 0.9223\n",
      "Epoch 249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9211e-04 - my_r2: 0.8997 - val_loss: 2.1088e-04 - val_my_r2: 0.9253\n",
      "Epoch 250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1516e-04 - my_r2: 0.8656 - val_loss: 2.1356e-04 - val_my_r2: 0.9257\n",
      "Epoch 251/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 6.8189e-04 - my_r2: 0.7864 - val_loss: 2.1057e-04 - val_my_r2: 0.9252\n",
      "Epoch 252/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.6427e-04 - my_r2: 0.8735 - val_loss: 2.1110e-04 - val_my_r2: 0.9253\n",
      "Epoch 253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9827e-04 - my_r2: 0.8413 - val_loss: 2.0808e-04 - val_my_r2: 0.9259\n",
      "Epoch 254/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.3988e-04 - my_r2: 0.7841 - val_loss: 2.0800e-04 - val_my_r2: 0.9263\n",
      "Epoch 255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8831e-04 - my_r2: 0.8782 - val_loss: 2.0524e-04 - val_my_r2: 0.9276\n",
      "Epoch 256/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5089e-04 - my_r2: 0.9015 - val_loss: 2.0343e-04 - val_my_r2: 0.9274\n",
      "Epoch 257/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5594e-04 - my_r2: 0.7954 - val_loss: 2.0290e-04 - val_my_r2: 0.9290\n",
      "Epoch 258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8889e-04 - my_r2: 0.8943 - val_loss: 2.0720e-04 - val_my_r2: 0.9283\n",
      "Epoch 259/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.7245e-04 - my_r2: 0.8321 - val_loss: 2.0299e-04 - val_my_r2: 0.9290\n",
      "Epoch 260/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4720e-04 - my_r2: 0.8918 - val_loss: 1.9976e-04 - val_my_r2: 0.9307\n",
      "Epoch 261/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2617e-04 - my_r2: 0.9108 - val_loss: 1.9823e-04 - val_my_r2: 0.9323\n",
      "Epoch 262/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8578e-04 - my_r2: 0.8250 - val_loss: 2.0192e-04 - val_my_r2: 0.9307\n",
      "Epoch 263/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1275e-04 - my_r2: 0.9148 - val_loss: 2.0019e-04 - val_my_r2: 0.9313\n",
      "Epoch 264/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6827e-04 - my_r2: 0.8745 - val_loss: 1.9648e-04 - val_my_r2: 0.9327\n",
      "Epoch 265/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.2025e-04 - my_r2: 0.9003 - val_loss: 1.9545e-04 - val_my_r2: 0.9322\n",
      "Epoch 266/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9639e-04 - my_r2: 0.8685 - val_loss: 1.9327e-04 - val_my_r2: 0.9325\n",
      "Epoch 267/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4591e-04 - my_r2: 0.8134 - val_loss: 1.9617e-04 - val_my_r2: 0.9319\n",
      "Epoch 268/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8330e-04 - my_r2: 0.8654 - val_loss: 1.9383e-04 - val_my_r2: 0.9325\n",
      "Epoch 269/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1172e-04 - my_r2: 0.8607 - val_loss: 1.9703e-04 - val_my_r2: 0.9333\n",
      "Epoch 270/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3981e-04 - my_r2: 0.9105 - val_loss: 1.9097e-04 - val_my_r2: 0.9342\n",
      "Epoch 271/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2702e-04 - my_r2: 0.8596 - val_loss: 1.8918e-04 - val_my_r2: 0.9344\n",
      "Epoch 272/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8599e-04 - my_r2: 0.8845 - val_loss: 1.9501e-04 - val_my_r2: 0.9310\n",
      "Epoch 273/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.4512e-04 - my_r2: 0.8263 - val_loss: 1.9185e-04 - val_my_r2: 0.9304\n",
      "Epoch 274/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.8591e-04 - my_r2: 0.8551 - val_loss: 1.9378e-04 - val_my_r2: 0.9284\n",
      "Epoch 275/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 5.5980e-04 - my_r2: 0.8230 - val_loss: 1.9340e-04 - val_my_r2: 0.9289\n",
      "Epoch 276/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.9944e-04 - my_r2: 0.8830 - val_loss: 1.8926e-04 - val_my_r2: 0.9328\n",
      "Epoch 277/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6741e-04 - my_r2: 0.7614 - val_loss: 1.8493e-04 - val_my_r2: 0.9331\n",
      "Epoch 278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9044e-04 - my_r2: 0.8786 - val_loss: 1.8083e-04 - val_my_r2: 0.9367\n",
      "Epoch 279/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5605e-04 - my_r2: 0.9033 - val_loss: 1.8306e-04 - val_my_r2: 0.9372\n",
      "Epoch 280/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.0367e-04 - my_r2: 0.8927 - val_loss: 1.7955e-04 - val_my_r2: 0.9383\n",
      "Epoch 281/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5680e-04 - my_r2: 0.8897 - val_loss: 1.7526e-04 - val_my_r2: 0.9399\n",
      "Epoch 282/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2715e-04 - my_r2: 0.8949 - val_loss: 1.7442e-04 - val_my_r2: 0.9402\n",
      "Epoch 283/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.1222e-04 - my_r2: 0.8383 - val_loss: 1.7445e-04 - val_my_r2: 0.9401\n",
      "Epoch 284/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.8307e-04 - my_r2: 0.8948 - val_loss: 1.7347e-04 - val_my_r2: 0.9399\n",
      "Epoch 285/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 4.4591e-04 - my_r2: 0.8558 - val_loss: 1.7055e-04 - val_my_r2: 0.9400\n",
      "Epoch 286/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5407e-04 - my_r2: 0.8371 - val_loss: 1.7201e-04 - val_my_r2: 0.9390\n",
      "Epoch 287/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5665e-04 - my_r2: 0.8970 - val_loss: 1.7033e-04 - val_my_r2: 0.9405\n",
      "Epoch 288/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.8252e-04 - my_r2: 0.8426 - val_loss: 1.6995e-04 - val_my_r2: 0.9409\n",
      "Epoch 289/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.3612e-04 - my_r2: 0.8830 - val_loss: 1.6905e-04 - val_my_r2: 0.9410\n",
      "Epoch 290/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9109e-04 - my_r2: 0.8260 - val_loss: 1.6703e-04 - val_my_r2: 0.9420\n",
      "Epoch 291/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4798e-04 - my_r2: 0.8758 - val_loss: 1.6849e-04 - val_my_r2: 0.9418\n",
      "Epoch 292/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1847e-04 - my_r2: 0.9131 - val_loss: 1.6957e-04 - val_my_r2: 0.9397\n",
      "Epoch 293/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4439e-04 - my_r2: 0.7223 - val_loss: 1.6691e-04 - val_my_r2: 0.9404\n",
      "Epoch 294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6238e-04 - my_r2: 0.9045 - val_loss: 1.6477e-04 - val_my_r2: 0.9432\n",
      "Epoch 295/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9963e-04 - my_r2: 0.8870 - val_loss: 1.6330e-04 - val_my_r2: 0.9435\n",
      "Epoch 296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3199e-04 - my_r2: 0.8862 - val_loss: 1.6582e-04 - val_my_r2: 0.9431\n",
      "Epoch 297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0723e-04 - my_r2: 0.8925 - val_loss: 1.6139e-04 - val_my_r2: 0.9440\n",
      "Epoch 298/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6359e-04 - my_r2: 0.8727 - val_loss: 1.5991e-04 - val_my_r2: 0.9446\n",
      "Epoch 299/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2528e-04 - my_r2: 0.9037 - val_loss: 1.6115e-04 - val_my_r2: 0.9445\n",
      "Epoch 300/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5325e-04 - my_r2: 0.8957 - val_loss: 1.5857e-04 - val_my_r2: 0.9443\n",
      "Epoch 301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4415e-04 - my_r2: 0.8919 - val_loss: 1.5708e-04 - val_my_r2: 0.9442\n",
      "Epoch 302/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8204e-04 - my_r2: 0.9065 - val_loss: 1.6191e-04 - val_my_r2: 0.9432\n",
      "Epoch 303/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4672e-04 - my_r2: 0.9082 - val_loss: 1.5661e-04 - val_my_r2: 0.9451\n",
      "Epoch 304/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8499e-04 - my_r2: 0.8439 - val_loss: 1.6898e-04 - val_my_r2: 0.9425\n",
      "Epoch 305/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9169e-04 - my_r2: 0.7771 - val_loss: 1.5750e-04 - val_my_r2: 0.9449\n",
      "Epoch 306/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1303e-04 - my_r2: 0.8928 - val_loss: 1.5434e-04 - val_my_r2: 0.9451\n",
      "Epoch 307/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7887e-04 - my_r2: 0.9156 - val_loss: 1.5793e-04 - val_my_r2: 0.9426\n",
      "Epoch 308/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2513e-04 - my_r2: 0.8190 - val_loss: 1.5878e-04 - val_my_r2: 0.9419\n",
      "Epoch 309/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.9547e-04 - my_r2: 0.8917 - val_loss: 1.5311e-04 - val_my_r2: 0.9460\n",
      "Epoch 310/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5308e-04 - my_r2: 0.8722 - val_loss: 1.5072e-04 - val_my_r2: 0.9463\n",
      "Epoch 311/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0846e-04 - my_r2: 0.8551 - val_loss: 1.5288e-04 - val_my_r2: 0.9448\n",
      "Epoch 312/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6363e-04 - my_r2: 0.9250 - val_loss: 1.5187e-04 - val_my_r2: 0.9467\n",
      "Epoch 313/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6721e-04 - my_r2: 0.9034 - val_loss: 1.5763e-04 - val_my_r2: 0.9469\n",
      "Epoch 314/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9057e-04 - my_r2: 0.8975 - val_loss: 1.5657e-04 - val_my_r2: 0.9475\n",
      "Epoch 315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5654e-04 - my_r2: 0.9007 - val_loss: 1.5303e-04 - val_my_r2: 0.9471\n",
      "Epoch 316/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3894e-04 - my_r2: 0.8694 - val_loss: 1.4877e-04 - val_my_r2: 0.9485\n",
      "Epoch 317/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2261e-04 - my_r2: 0.8946 - val_loss: 1.4794e-04 - val_my_r2: 0.9490\n",
      "Epoch 318/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0074e-04 - my_r2: 0.9193 - val_loss: 1.4900e-04 - val_my_r2: 0.9478\n",
      "Epoch 319/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.3827e-04 - my_r2: 0.8899 - val_loss: 1.4730e-04 - val_my_r2: 0.9474\n",
      "Epoch 320/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5482e-04 - my_r2: 0.8679 - val_loss: 1.4711e-04 - val_my_r2: 0.9471\n",
      "Epoch 321/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6352e-04 - my_r2: 0.8662 - val_loss: 1.4286e-04 - val_my_r2: 0.9490\n",
      "Epoch 322/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4063e-04 - my_r2: 0.9292 - val_loss: 1.4335e-04 - val_my_r2: 0.9489\n",
      "Epoch 323/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.6112e-04 - my_r2: 0.8769 - val_loss: 1.4800e-04 - val_my_r2: 0.9481\n",
      "Epoch 324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5789e-04 - my_r2: 0.8131 - val_loss: 1.4176e-04 - val_my_r2: 0.9494\n",
      "Epoch 325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9043e-04 - my_r2: 0.9014 - val_loss: 1.3981e-04 - val_my_r2: 0.9492\n",
      "Epoch 326/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.8873e-04 - my_r2: 0.8316 - val_loss: 1.4367e-04 - val_my_r2: 0.9466\n",
      "Epoch 327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1896e-04 - my_r2: 0.8977 - val_loss: 1.4292e-04 - val_my_r2: 0.9467\n",
      "Epoch 328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9547e-04 - my_r2: 0.8681 - val_loss: 1.4036e-04 - val_my_r2: 0.9482\n",
      "Epoch 329/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2541e-04 - my_r2: 0.8800 - val_loss: 1.3989e-04 - val_my_r2: 0.9481\n",
      "Epoch 330/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1950e-04 - my_r2: 0.9154 - val_loss: 1.3643e-04 - val_my_r2: 0.9506\n",
      "Epoch 331/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4562e-04 - my_r2: 0.8805 - val_loss: 1.3983e-04 - val_my_r2: 0.9509\n",
      "Epoch 332/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9545e-04 - my_r2: 0.8778 - val_loss: 1.3568e-04 - val_my_r2: 0.9515\n",
      "Epoch 333/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1231e-04 - my_r2: 0.8954 - val_loss: 1.3418e-04 - val_my_r2: 0.9513\n",
      "Epoch 334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4110e-04 - my_r2: 0.9000 - val_loss: 1.3450e-04 - val_my_r2: 0.9526\n",
      "Epoch 335/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5636e-04 - my_r2: 0.9168 - val_loss: 1.3425e-04 - val_my_r2: 0.9535\n",
      "Epoch 336/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9185e-04 - my_r2: 0.9251 - val_loss: 1.4491e-04 - val_my_r2: 0.9513\n",
      "Epoch 337/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 4.1912e-04 - my_r2: 0.8947 - val_loss: 1.3689e-04 - val_my_r2: 0.9529\n",
      "Epoch 338/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0245e-04 - my_r2: 0.8931 - val_loss: 1.3083e-04 - val_my_r2: 0.9541\n",
      "Epoch 339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2049e-04 - my_r2: 0.8950 - val_loss: 1.2893e-04 - val_my_r2: 0.9545\n",
      "Epoch 340/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1223e-04 - my_r2: 0.9101 - val_loss: 1.3380e-04 - val_my_r2: 0.9536\n",
      "Epoch 341/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6282e-04 - my_r2: 0.8577 - val_loss: 1.3583e-04 - val_my_r2: 0.9525\n",
      "Epoch 342/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8769e-04 - my_r2: 0.7471 - val_loss: 1.3010e-04 - val_my_r2: 0.9543\n",
      "Epoch 343/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4203e-04 - my_r2: 0.9208 - val_loss: 1.2917e-04 - val_my_r2: 0.9553\n",
      "Epoch 344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8152e-04 - my_r2: 0.9121 - val_loss: 1.3478e-04 - val_my_r2: 0.9544\n",
      "Epoch 345/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0400e-04 - my_r2: 0.9041 - val_loss: 1.3043e-04 - val_my_r2: 0.9556\n",
      "Epoch 346/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2691e-04 - my_r2: 0.9108 - val_loss: 1.2719e-04 - val_my_r2: 0.9561\n",
      "Epoch 347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1219e-04 - my_r2: 0.9177 - val_loss: 1.2755e-04 - val_my_r2: 0.9559\n",
      "Epoch 348/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1165e-04 - my_r2: 0.9377 - val_loss: 1.2400e-04 - val_my_r2: 0.9573\n",
      "Epoch 349/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.4464e-04 - my_r2: 0.8790 - val_loss: 1.2468e-04 - val_my_r2: 0.9566\n",
      "Epoch 350/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9724e-04 - my_r2: 0.9168 - val_loss: 1.2720e-04 - val_my_r2: 0.9565\n",
      "Epoch 351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9697e-04 - my_r2: 0.8993 - val_loss: 1.2690e-04 - val_my_r2: 0.9569\n",
      "Epoch 352/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4068e-04 - my_r2: 0.8976 - val_loss: 1.2616e-04 - val_my_r2: 0.9569\n",
      "Epoch 353/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2588e-04 - my_r2: 0.8555 - val_loss: 1.2743e-04 - val_my_r2: 0.9566\n",
      "Epoch 354/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9738e-04 - my_r2: 0.9064 - val_loss: 1.2598e-04 - val_my_r2: 0.9567\n",
      "Epoch 355/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7766e-04 - my_r2: 0.9115 - val_loss: 1.2119e-04 - val_my_r2: 0.9580\n",
      "Epoch 356/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3340e-04 - my_r2: 0.8975 - val_loss: 1.2004e-04 - val_my_r2: 0.9572\n",
      "Epoch 357/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4514e-04 - my_r2: 0.9080 - val_loss: 1.2692e-04 - val_my_r2: 0.9555\n",
      "Epoch 358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9398e-04 - my_r2: 0.9330 - val_loss: 1.2385e-04 - val_my_r2: 0.9559\n",
      "Epoch 359/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0475e-04 - my_r2: 0.8914 - val_loss: 1.2376e-04 - val_my_r2: 0.9556\n",
      "Epoch 360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5683e-04 - my_r2: 0.8694 - val_loss: 1.2085e-04 - val_my_r2: 0.9561\n",
      "Epoch 361/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0286e-04 - my_r2: 0.8742 - val_loss: 1.1881e-04 - val_my_r2: 0.9582\n",
      "Epoch 362/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3597e-04 - my_r2: 0.9073 - val_loss: 1.1508e-04 - val_my_r2: 0.9598\n",
      "Epoch 363/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7804e-04 - my_r2: 0.9020 - val_loss: 1.2681e-04 - val_my_r2: 0.9565\n",
      "Epoch 364/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.9109e-04 - my_r2: 0.9415 - val_loss: 1.2018e-04 - val_my_r2: 0.9586\n",
      "Epoch 365/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.7477e-04 - my_r2: 0.8624 - val_loss: 1.1344e-04 - val_my_r2: 0.9603\n",
      "Epoch 366/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7693e-04 - my_r2: 0.9118 - val_loss: 1.1440e-04 - val_my_r2: 0.9595\n",
      "Epoch 367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7767e-04 - my_r2: 0.9017 - val_loss: 1.1432e-04 - val_my_r2: 0.9591\n",
      "Epoch 368/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4981e-04 - my_r2: 0.9240 - val_loss: 1.1386e-04 - val_my_r2: 0.9597\n",
      "Epoch 369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7866e-04 - my_r2: 0.9011 - val_loss: 1.1252e-04 - val_my_r2: 0.9604\n",
      "Epoch 370/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0731e-04 - my_r2: 0.8887 - val_loss: 1.1138e-04 - val_my_r2: 0.9611\n",
      "Epoch 371/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4683e-04 - my_r2: 0.7977 - val_loss: 1.1076e-04 - val_my_r2: 0.9610\n",
      "Epoch 372/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1615e-04 - my_r2: 0.8950 - val_loss: 1.1093e-04 - val_my_r2: 0.9604\n",
      "Epoch 373/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.5052e-04 - my_r2: 0.8896 - val_loss: 1.0801e-04 - val_my_r2: 0.9623\n",
      "Epoch 374/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 5.3211e-04 - my_r2: 0.8048 - val_loss: 1.1124e-04 - val_my_r2: 0.9600\n",
      "Epoch 375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6062e-04 - my_r2: 0.8686 - val_loss: 1.0737e-04 - val_my_r2: 0.9629\n",
      "Epoch 376/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6403e-04 - my_r2: 0.9194 - val_loss: 1.0823e-04 - val_my_r2: 0.9628\n",
      "Epoch 377/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9363e-04 - my_r2: 0.8995 - val_loss: 1.1109e-04 - val_my_r2: 0.9593\n",
      "Epoch 378/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0063e-04 - my_r2: 0.8990 - val_loss: 1.0798e-04 - val_my_r2: 0.9616\n",
      "Epoch 379/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8415e-04 - my_r2: 0.9250 - val_loss: 1.0810e-04 - val_my_r2: 0.9625\n",
      "Epoch 380/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7092e-04 - my_r2: 0.9264 - val_loss: 1.1325e-04 - val_my_r2: 0.9624\n",
      "Epoch 381/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6675e-04 - my_r2: 0.9367 - val_loss: 1.1338e-04 - val_my_r2: 0.9622\n",
      "Epoch 382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6476e-04 - my_r2: 0.9001 - val_loss: 1.1537e-04 - val_my_r2: 0.9610\n",
      "Epoch 383/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4434e-04 - my_r2: 0.8559 - val_loss: 1.0445e-04 - val_my_r2: 0.9632\n",
      "Epoch 384/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7453e-04 - my_r2: 0.9044 - val_loss: 1.0578e-04 - val_my_r2: 0.9616\n",
      "Epoch 385/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2471e-04 - my_r2: 0.9120 - val_loss: 1.0812e-04 - val_my_r2: 0.9612\n",
      "Epoch 386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2122e-04 - my_r2: 0.8549 - val_loss: 1.0755e-04 - val_my_r2: 0.9618\n",
      "Epoch 387/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6166e-04 - my_r2: 0.9043 - val_loss: 1.0293e-04 - val_my_r2: 0.9636\n",
      "Epoch 388/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8213e-04 - my_r2: 0.9192 - val_loss: 1.0337e-04 - val_my_r2: 0.9632\n",
      "Epoch 389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.2269e-04 - my_r2: 0.8895 - val_loss: 1.0405e-04 - val_my_r2: 0.9637\n",
      "Epoch 390/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.2154e-04 - my_r2: 0.8129 - val_loss: 1.0562e-04 - val_my_r2: 0.9634\n",
      "Epoch 391/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.6185e-04 - my_r2: 0.9039 - val_loss: 1.0479e-04 - val_my_r2: 0.9633\n",
      "Epoch 392/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7787e-04 - my_r2: 0.8104 - val_loss: 1.0313e-04 - val_my_r2: 0.9636\n",
      "Epoch 393/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2687e-04 - my_r2: 0.9051 - val_loss: 1.0130e-04 - val_my_r2: 0.9645\n",
      "Epoch 394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9613e-04 - my_r2: 0.8452 - val_loss: 1.0416e-04 - val_my_r2: 0.9637\n",
      "Epoch 395/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5130e-04 - my_r2: 0.8728 - val_loss: 9.7743e-05 - val_my_r2: 0.9656\n",
      "Epoch 396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2817e-04 - my_r2: 0.9115 - val_loss: 9.6261e-05 - val_my_r2: 0.9663\n",
      "Epoch 397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1853e-04 - my_r2: 0.9249 - val_loss: 9.6237e-05 - val_my_r2: 0.9663\n",
      "Epoch 398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7816e-04 - my_r2: 0.9206 - val_loss: 9.6673e-05 - val_my_r2: 0.9663\n",
      "Epoch 399/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.2053e-04 - my_r2: 0.8757 - val_loss: 9.8391e-05 - val_my_r2: 0.9652\n",
      "Epoch 400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5460e-04 - my_r2: 0.9373 - val_loss: 9.5033e-05 - val_my_r2: 0.9663\n",
      "Epoch 401/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3198e-04 - my_r2: 0.8900 - val_loss: 9.5683e-05 - val_my_r2: 0.9651\n",
      "Epoch 402/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5068e-04 - my_r2: 0.8812 - val_loss: 9.4646e-05 - val_my_r2: 0.9656\n",
      "Epoch 403/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4339e-04 - my_r2: 0.9286 - val_loss: 9.4586e-05 - val_my_r2: 0.9669\n",
      "Epoch 404/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0130e-04 - my_r2: 0.9003 - val_loss: 9.3653e-05 - val_my_r2: 0.9669\n",
      "Epoch 405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8463e-04 - my_r2: 0.8678 - val_loss: 9.5392e-05 - val_my_r2: 0.9681\n",
      "Epoch 406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6868e-04 - my_r2: 0.9084 - val_loss: 9.3038e-05 - val_my_r2: 0.9686\n",
      "Epoch 407/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4800e-04 - my_r2: 0.9159 - val_loss: 9.2354e-05 - val_my_r2: 0.9678\n",
      "Epoch 408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8872e-04 - my_r2: 0.8657 - val_loss: 9.5960e-05 - val_my_r2: 0.9653\n",
      "Epoch 409/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1812e-04 - my_r2: 0.9152 - val_loss: 9.2702e-05 - val_my_r2: 0.9666\n",
      "Epoch 410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5183e-04 - my_r2: 0.9054 - val_loss: 9.1609e-05 - val_my_r2: 0.9669\n",
      "Epoch 411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9665e-04 - my_r2: 0.9392 - val_loss: 9.0460e-05 - val_my_r2: 0.9681\n",
      "Epoch 412/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0767e-04 - my_r2: 0.8801 - val_loss: 8.9885e-05 - val_my_r2: 0.9681\n",
      "Epoch 413/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5557e-04 - my_r2: 0.9130 - val_loss: 8.7038e-05 - val_my_r2: 0.9696\n",
      "Epoch 414/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.4001e-04 - my_r2: 0.8977 - val_loss: 9.4089e-05 - val_my_r2: 0.9683\n",
      "Epoch 415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.1811e-04 - my_r2: 0.9118 - val_loss: 9.0500e-05 - val_my_r2: 0.9691\n",
      "Epoch 416/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3530e-04 - my_r2: 0.9220 - val_loss: 8.5348e-05 - val_my_r2: 0.9707\n",
      "Epoch 417/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7418e-04 - my_r2: 0.9014 - val_loss: 8.9084e-05 - val_my_r2: 0.9698\n",
      "Epoch 418/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1948e-04 - my_r2: 0.7370 - val_loss: 9.1979e-05 - val_my_r2: 0.9683\n",
      "Epoch 419/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9568e-04 - my_r2: 0.9212 - val_loss: 9.0096e-05 - val_my_r2: 0.9673\n",
      "Epoch 420/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4319e-04 - my_r2: 0.8844 - val_loss: 9.0355e-05 - val_my_r2: 0.9650\n",
      "Epoch 421/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8490e-04 - my_r2: 0.8482 - val_loss: 8.6562e-05 - val_my_r2: 0.9681\n",
      "Epoch 422/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9605e-04 - my_r2: 0.8791 - val_loss: 8.3564e-05 - val_my_r2: 0.9694\n",
      "Epoch 423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2990e-04 - my_r2: 0.9135 - val_loss: 8.9539e-05 - val_my_r2: 0.9676\n",
      "Epoch 424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9298e-04 - my_r2: 0.9315 - val_loss: 8.6369e-05 - val_my_r2: 0.9694\n",
      "Epoch 425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5782e-04 - my_r2: 0.8260 - val_loss: 8.6786e-05 - val_my_r2: 0.9702\n",
      "Epoch 426/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9358e-04 - my_r2: 0.8551 - val_loss: 8.2595e-05 - val_my_r2: 0.9712\n",
      "Epoch 427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9125e-04 - my_r2: 0.9011 - val_loss: 8.5527e-05 - val_my_r2: 0.9705\n",
      "Epoch 428/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9097e-04 - my_r2: 0.8871 - val_loss: 8.3049e-05 - val_my_r2: 0.9714\n",
      "Epoch 429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1203e-04 - my_r2: 0.9071 - val_loss: 8.6503e-05 - val_my_r2: 0.9707\n",
      "Epoch 430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6375e-04 - my_r2: 0.9206 - val_loss: 8.6830e-05 - val_my_r2: 0.9711\n",
      "Epoch 431/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0191e-04 - my_r2: 0.9268 - val_loss: 8.0770e-05 - val_my_r2: 0.9716\n",
      "Epoch 432/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3610e-04 - my_r2: 0.9233 - val_loss: 7.8753e-05 - val_my_r2: 0.9722\n",
      "Epoch 433/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0384e-04 - my_r2: 0.9294 - val_loss: 7.9065e-05 - val_my_r2: 0.9714\n",
      "Epoch 434/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8285e-04 - my_r2: 0.8836 - val_loss: 7.8551e-05 - val_my_r2: 0.9716\n",
      "Epoch 435/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6038e-04 - my_r2: 0.8746 - val_loss: 8.0380e-05 - val_my_r2: 0.9716\n",
      "Epoch 436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6240e-04 - my_r2: 0.9057 - val_loss: 7.9102e-05 - val_my_r2: 0.9715\n",
      "Epoch 437/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4927e-04 - my_r2: 0.9118 - val_loss: 8.1617e-05 - val_my_r2: 0.9701\n",
      "Epoch 438/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0598e-04 - my_r2: 0.9351 - val_loss: 8.4508e-05 - val_my_r2: 0.9691\n",
      "Epoch 439/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1090e-04 - my_r2: 0.9179 - val_loss: 7.9935e-05 - val_my_r2: 0.9706\n",
      "Epoch 440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3676e-04 - my_r2: 0.9089 - val_loss: 8.0210e-05 - val_my_r2: 0.9704\n",
      "Epoch 441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6935e-04 - my_r2: 0.9016 - val_loss: 7.6552e-05 - val_my_r2: 0.9717\n",
      "Epoch 442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3891e-04 - my_r2: 0.9150 - val_loss: 7.6523e-05 - val_my_r2: 0.9723\n",
      "Epoch 443/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4679e-04 - my_r2: 0.9241 - val_loss: 8.0800e-05 - val_my_r2: 0.9710\n",
      "Epoch 444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3992e-04 - my_r2: 0.9345 - val_loss: 7.5145e-05 - val_my_r2: 0.9732\n",
      "Epoch 445/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9852e-04 - my_r2: 0.9208 - val_loss: 7.4501e-05 - val_my_r2: 0.9731\n",
      "Epoch 446/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5585e-04 - my_r2: 0.9123 - val_loss: 7.6729e-05 - val_my_r2: 0.9730\n",
      "Epoch 447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0351e-04 - my_r2: 0.8415 - val_loss: 7.4946e-05 - val_my_r2: 0.9730\n",
      "Epoch 448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0817e-04 - my_r2: 0.9003 - val_loss: 7.7930e-05 - val_my_r2: 0.9717\n",
      "Epoch 449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7785e-04 - my_r2: 0.9470 - val_loss: 7.9317e-05 - val_my_r2: 0.9715\n",
      "Epoch 450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6750e-04 - my_r2: 0.8477 - val_loss: 7.8509e-05 - val_my_r2: 0.9720\n",
      "Epoch 451/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8240e-04 - my_r2: 0.9317 - val_loss: 7.1642e-05 - val_my_r2: 0.9742\n",
      "Epoch 452/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5758e-04 - my_r2: 0.9003 - val_loss: 7.0869e-05 - val_my_r2: 0.9741\n",
      "Epoch 453/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2180e-04 - my_r2: 0.9446 - val_loss: 7.4982e-05 - val_my_r2: 0.9726\n",
      "Epoch 454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 4.0520e-04 - my_r2: 0.9086 - val_loss: 7.5507e-05 - val_my_r2: 0.9722\n",
      "Epoch 455/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6626e-04 - my_r2: 0.9047 - val_loss: 7.2644e-05 - val_my_r2: 0.9735\n",
      "Epoch 456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8189e-04 - my_r2: 0.8950 - val_loss: 7.3063e-05 - val_my_r2: 0.9733\n",
      "Epoch 457/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9047e-04 - my_r2: 0.9211 - val_loss: 6.9737e-05 - val_my_r2: 0.9746\n",
      "Epoch 458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1207e-04 - my_r2: 0.9112 - val_loss: 6.9764e-05 - val_my_r2: 0.9742\n",
      "Epoch 459/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2971e-04 - my_r2: 0.9294 - val_loss: 7.1988e-05 - val_my_r2: 0.9733\n",
      "Epoch 460/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6031e-04 - my_r2: 0.9350 - val_loss: 7.5149e-05 - val_my_r2: 0.9731\n",
      "Epoch 461/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0321e-04 - my_r2: 0.8660 - val_loss: 7.2134e-05 - val_my_r2: 0.9736\n",
      "Epoch 462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7526e-04 - my_r2: 0.8020 - val_loss: 6.9840e-05 - val_my_r2: 0.9741\n",
      "Epoch 463/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5417e-04 - my_r2: 0.9117 - val_loss: 6.9829e-05 - val_my_r2: 0.9742\n",
      "Epoch 464/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5757e-04 - my_r2: 0.8941 - val_loss: 6.8633e-05 - val_my_r2: 0.9744\n",
      "Epoch 465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2660e-04 - my_r2: 0.9361 - val_loss: 6.9649e-05 - val_my_r2: 0.9741\n",
      "Epoch 466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7724e-04 - my_r2: 0.9304 - val_loss: 6.8650e-05 - val_my_r2: 0.9747\n",
      "Epoch 467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9255e-04 - my_r2: 0.9140 - val_loss: 6.6535e-05 - val_my_r2: 0.9751\n",
      "Epoch 468/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.5142e-04 - my_r2: 0.8714 - val_loss: 6.4882e-05 - val_my_r2: 0.9764\n",
      "Epoch 469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0714e-04 - my_r2: 0.9258 - val_loss: 7.1445e-05 - val_my_r2: 0.9755\n",
      "Epoch 470/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6185e-04 - my_r2: 0.9400 - val_loss: 7.6066e-05 - val_my_r2: 0.9747\n",
      "Epoch 471/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3621e-04 - my_r2: 0.8926 - val_loss: 7.4016e-05 - val_my_r2: 0.9752\n",
      "Epoch 472/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3432e-04 - my_r2: 0.9241 - val_loss: 7.0406e-05 - val_my_r2: 0.9755\n",
      "Epoch 473/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8933e-04 - my_r2: 0.9097 - val_loss: 6.6561e-05 - val_my_r2: 0.9762\n",
      "Epoch 474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7247e-04 - my_r2: 0.8994 - val_loss: 6.3554e-05 - val_my_r2: 0.9775\n",
      "Epoch 475/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9994e-04 - my_r2: 0.9350 - val_loss: 6.5584e-05 - val_my_r2: 0.9775\n",
      "Epoch 476/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9526e-04 - my_r2: 0.9288 - val_loss: 7.3162e-05 - val_my_r2: 0.9752\n",
      "Epoch 477/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6947e-04 - my_r2: 0.9076 - val_loss: 7.2040e-05 - val_my_r2: 0.9755\n",
      "Epoch 478/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6032e-04 - my_r2: 0.9411 - val_loss: 6.5736e-05 - val_my_r2: 0.9764\n",
      "Epoch 479/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0973e-04 - my_r2: 0.9187 - val_loss: 6.7341e-05 - val_my_r2: 0.9759\n",
      "Epoch 480/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6780e-04 - my_r2: 0.8295 - val_loss: 6.1573e-05 - val_my_r2: 0.9771\n",
      "Epoch 481/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.3407e-04 - my_r2: 0.9207 - val_loss: 6.0766e-05 - val_my_r2: 0.9784\n",
      "Epoch 482/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4534e-04 - my_r2: 0.9370 - val_loss: 6.2939e-05 - val_my_r2: 0.9782\n",
      "Epoch 483/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7840e-04 - my_r2: 0.9251 - val_loss: 6.3915e-05 - val_my_r2: 0.9780\n",
      "Epoch 484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6054e-04 - my_r2: 0.8992 - val_loss: 6.1159e-05 - val_my_r2: 0.9781\n",
      "Epoch 485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5641e-04 - my_r2: 0.9153 - val_loss: 6.0864e-05 - val_my_r2: 0.9782\n",
      "Epoch 486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9779e-04 - my_r2: 0.9390 - val_loss: 6.3553e-05 - val_my_r2: 0.9767\n",
      "Epoch 487/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1359e-04 - my_r2: 0.9133 - val_loss: 6.2973e-05 - val_my_r2: 0.9778\n",
      "Epoch 488/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3833e-04 - my_r2: 0.9372 - val_loss: 6.9647e-05 - val_my_r2: 0.9759\n",
      "Epoch 489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3980e-04 - my_r2: 0.9484 - val_loss: 6.8545e-05 - val_my_r2: 0.9761\n",
      "Epoch 490/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1417e-04 - my_r2: 0.8860 - val_loss: 6.4382e-05 - val_my_r2: 0.9776\n",
      "Epoch 491/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8575e-04 - my_r2: 0.8886 - val_loss: 6.3869e-05 - val_my_r2: 0.9765\n",
      "Epoch 492/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6826e-04 - my_r2: 0.9005 - val_loss: 6.2068e-05 - val_my_r2: 0.9770\n",
      "Epoch 493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8714e-04 - my_r2: 0.8926 - val_loss: 6.2576e-05 - val_my_r2: 0.9776\n",
      "Epoch 494/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6580e-04 - my_r2: 0.9319 - val_loss: 5.8325e-05 - val_my_r2: 0.9785\n",
      "Epoch 495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7412e-04 - my_r2: 0.9373 - val_loss: 5.6749e-05 - val_my_r2: 0.9788\n",
      "Epoch 496/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8473e-04 - my_r2: 0.9191 - val_loss: 5.8383e-05 - val_my_r2: 0.9778\n",
      "Epoch 497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2326e-04 - my_r2: 0.9128 - val_loss: 5.9296e-05 - val_my_r2: 0.9776\n",
      "Epoch 498/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5232e-04 - my_r2: 0.9165 - val_loss: 5.6523e-05 - val_my_r2: 0.9790\n",
      "Epoch 499/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3406e-04 - my_r2: 0.9190 - val_loss: 5.7899e-05 - val_my_r2: 0.9788\n",
      "Epoch 500/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7532e-04 - my_r2: 0.9379 - val_loss: 6.0189e-05 - val_my_r2: 0.9781\n",
      "Epoch 501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1151e-04 - my_r2: 0.9218 - val_loss: 6.6671e-05 - val_my_r2: 0.9760\n",
      "Epoch 502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2690e-04 - my_r2: 0.8758 - val_loss: 7.1429e-05 - val_my_r2: 0.9742\n",
      "Epoch 503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6642e-04 - my_r2: 0.8683 - val_loss: 5.8981e-05 - val_my_r2: 0.9781\n",
      "Epoch 504/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7040e-04 - my_r2: 0.7012 - val_loss: 5.6497e-05 - val_my_r2: 0.9788\n",
      "Epoch 505/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.8877e-04 - my_r2: 0.9041 - val_loss: 6.2445e-05 - val_my_r2: 0.9767\n",
      "Epoch 506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9070e-04 - my_r2: 0.9215 - val_loss: 5.7788e-05 - val_my_r2: 0.9784\n",
      "Epoch 507/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0346e-04 - my_r2: 0.9230 - val_loss: 5.8547e-05 - val_my_r2: 0.9787\n",
      "Epoch 508/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4782e-04 - my_r2: 0.8940 - val_loss: 5.6792e-05 - val_my_r2: 0.9791\n",
      "Epoch 509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8413e-04 - my_r2: 0.7463 - val_loss: 5.6265e-05 - val_my_r2: 0.9787\n",
      "Epoch 510/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1165e-04 - my_r2: 0.9185 - val_loss: 5.6328e-05 - val_my_r2: 0.9797\n",
      "Epoch 511/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1662e-04 - my_r2: 0.8796 - val_loss: 5.9320e-05 - val_my_r2: 0.9787\n",
      "Epoch 512/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0886e-04 - my_r2: 0.9234 - val_loss: 5.8178e-05 - val_my_r2: 0.9790\n",
      "Epoch 513/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8205e-04 - my_r2: 0.9316 - val_loss: 5.9455e-05 - val_my_r2: 0.9783\n",
      "Epoch 514/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8360e-04 - my_r2: 0.9168 - val_loss: 5.6435e-05 - val_my_r2: 0.9791\n",
      "Epoch 515/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6685e-04 - my_r2: 0.8698 - val_loss: 5.7919e-05 - val_my_r2: 0.9785\n",
      "Epoch 516/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8996e-04 - my_r2: 0.9301 - val_loss: 5.9802e-05 - val_my_r2: 0.9772\n",
      "Epoch 517/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.1077e-04 - my_r2: 0.9023 - val_loss: 5.5334e-05 - val_my_r2: 0.9783\n",
      "Epoch 518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9888e-04 - my_r2: 0.8876 - val_loss: 5.7546e-05 - val_my_r2: 0.9784\n",
      "Epoch 519/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3323e-04 - my_r2: 0.8899 - val_loss: 6.0492e-05 - val_my_r2: 0.9772\n",
      "Epoch 520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9921e-04 - my_r2: 0.8270 - val_loss: 5.8744e-05 - val_my_r2: 0.9788\n",
      "Epoch 521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7018e-04 - my_r2: 0.8594 - val_loss: 5.3417e-05 - val_my_r2: 0.9805\n",
      "Epoch 522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9845e-04 - my_r2: 0.9202 - val_loss: 5.2658e-05 - val_my_r2: 0.9811\n",
      "Epoch 523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8244e-04 - my_r2: 0.9419 - val_loss: 5.4816e-05 - val_my_r2: 0.9815\n",
      "Epoch 524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4619e-04 - my_r2: 0.9147 - val_loss: 5.3659e-05 - val_my_r2: 0.9813\n",
      "Epoch 525/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4926e-04 - my_r2: 0.9087 - val_loss: 5.6147e-05 - val_my_r2: 0.9791\n",
      "Epoch 526/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6449e-04 - my_r2: 0.9134 - val_loss: 6.0699e-05 - val_my_r2: 0.9793\n",
      "Epoch 527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2443e-04 - my_r2: 0.8747 - val_loss: 5.7520e-05 - val_my_r2: 0.9803\n",
      "Epoch 528/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2868e-04 - my_r2: 0.8795 - val_loss: 5.6690e-05 - val_my_r2: 0.9808\n",
      "Epoch 529/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8606e-04 - my_r2: 0.9078 - val_loss: 6.6176e-05 - val_my_r2: 0.9781\n",
      "Epoch 530/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3127e-04 - my_r2: 0.8893 - val_loss: 5.7386e-05 - val_my_r2: 0.9802\n",
      "Epoch 531/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9210e-04 - my_r2: 0.9209 - val_loss: 6.5152e-05 - val_my_r2: 0.9778\n",
      "Epoch 532/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3311e-04 - my_r2: 0.9306 - val_loss: 7.1729e-05 - val_my_r2: 0.9756\n",
      "Epoch 533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.9574e-04 - my_r2: 0.7149 - val_loss: 5.7528e-05 - val_my_r2: 0.9796\n",
      "Epoch 534/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5550e-04 - my_r2: 0.9154 - val_loss: 5.0270e-05 - val_my_r2: 0.9815\n",
      "Epoch 535/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2227e-04 - my_r2: 0.8871 - val_loss: 5.0227e-05 - val_my_r2: 0.9814\n",
      "Epoch 536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5312e-04 - my_r2: 0.9221 - val_loss: 4.9029e-05 - val_my_r2: 0.9817\n",
      "Epoch 537/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0221e-04 - my_r2: 0.8786 - val_loss: 5.0995e-05 - val_my_r2: 0.9808\n",
      "Epoch 538/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0023e-04 - my_r2: 0.9027 - val_loss: 6.1582e-05 - val_my_r2: 0.9777\n",
      "Epoch 539/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.7679e-04 - my_r2: 0.9189 - val_loss: 5.3300e-05 - val_my_r2: 0.9804\n",
      "Epoch 540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8204e-04 - my_r2: 0.9185 - val_loss: 4.8316e-05 - val_my_r2: 0.9819\n",
      "Epoch 541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9339e-04 - my_r2: 0.9182 - val_loss: 5.1822e-05 - val_my_r2: 0.9798\n",
      "Epoch 542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9734e-04 - my_r2: 0.9348 - val_loss: 5.2779e-05 - val_my_r2: 0.9794\n",
      "Epoch 543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6844e-04 - my_r2: 0.8318 - val_loss: 5.5298e-05 - val_my_r2: 0.9794\n",
      "Epoch 544/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3463e-04 - my_r2: 0.6933 - val_loss: 5.7324e-05 - val_my_r2: 0.9787\n",
      "Epoch 545/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1412e-04 - my_r2: 0.9224 - val_loss: 5.9509e-05 - val_my_r2: 0.9784\n",
      "Epoch 546/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4012e-04 - my_r2: 0.8942 - val_loss: 5.9815e-05 - val_my_r2: 0.9792\n",
      "Epoch 547/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1924e-04 - my_r2: 0.9269 - val_loss: 5.1577e-05 - val_my_r2: 0.9813\n",
      "Epoch 548/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1646e-04 - my_r2: 0.8346 - val_loss: 5.0106e-05 - val_my_r2: 0.9809\n",
      "Epoch 549/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0136e-04 - my_r2: 0.9220 - val_loss: 4.7916e-05 - val_my_r2: 0.9820\n",
      "Epoch 550/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4687e-04 - my_r2: 0.9145 - val_loss: 4.9349e-05 - val_my_r2: 0.9819\n",
      "Epoch 551/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 4.0047e-04 - my_r2: 0.8642 - val_loss: 5.9534e-05 - val_my_r2: 0.9800\n",
      "Epoch 552/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1228e-04 - my_r2: 0.8997 - val_loss: 6.6701e-05 - val_my_r2: 0.9785\n",
      "Epoch 553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8366e-04 - my_r2: 0.9218 - val_loss: 5.2445e-05 - val_my_r2: 0.9826\n",
      "Epoch 554/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3446e-04 - my_r2: 0.9038 - val_loss: 4.5837e-05 - val_my_r2: 0.9841\n",
      "Epoch 555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7198e-04 - my_r2: 0.9257 - val_loss: 4.5669e-05 - val_my_r2: 0.9841\n",
      "Epoch 556/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5531e-04 - my_r2: 0.8347 - val_loss: 5.1409e-05 - val_my_r2: 0.9826\n",
      "Epoch 557/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2121e-04 - my_r2: 0.9371 - val_loss: 4.9011e-05 - val_my_r2: 0.9829\n",
      "Epoch 558/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6345e-04 - my_r2: 0.9204 - val_loss: 5.0059e-05 - val_my_r2: 0.9822\n",
      "Epoch 559/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1053e-04 - my_r2: 0.9406 - val_loss: 4.7343e-05 - val_my_r2: 0.9822\n",
      "Epoch 560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4777e-04 - my_r2: 0.9208 - val_loss: 4.7352e-05 - val_my_r2: 0.9832\n",
      "Epoch 561/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1938e-04 - my_r2: 0.8979 - val_loss: 5.1465e-05 - val_my_r2: 0.9817\n",
      "Epoch 562/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0911e-04 - my_r2: 0.9269 - val_loss: 4.6828e-05 - val_my_r2: 0.9834\n",
      "Epoch 563/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6277e-04 - my_r2: 0.9360 - val_loss: 4.7673e-05 - val_my_r2: 0.9831\n",
      "Epoch 564/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5291e-04 - my_r2: 0.8879 - val_loss: 4.9174e-05 - val_my_r2: 0.9824\n",
      "Epoch 565/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1544e-04 - my_r2: 0.9173 - val_loss: 5.0236e-05 - val_my_r2: 0.9811\n",
      "Epoch 566/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9256e-04 - my_r2: 0.9277 - val_loss: 4.4617e-05 - val_my_r2: 0.9835\n",
      "Epoch 567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0646e-04 - my_r2: 0.9327 - val_loss: 4.2710e-05 - val_my_r2: 0.9842\n",
      "Epoch 568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5059e-04 - my_r2: 0.8923 - val_loss: 4.4423e-05 - val_my_r2: 0.9845\n",
      "Epoch 569/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4169e-04 - my_r2: 0.9487 - val_loss: 5.1549e-05 - val_my_r2: 0.9820\n",
      "Epoch 570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4283e-04 - my_r2: 0.9008 - val_loss: 4.6663e-05 - val_my_r2: 0.9830\n",
      "Epoch 571/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9304e-04 - my_r2: 0.9144 - val_loss: 5.0895e-05 - val_my_r2: 0.9822\n",
      "Epoch 572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8639e-04 - my_r2: 0.9183 - val_loss: 4.9981e-05 - val_my_r2: 0.9811\n",
      "Epoch 573/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3655e-04 - my_r2: 0.8956 - val_loss: 4.7379e-05 - val_my_r2: 0.9831\n",
      "Epoch 574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5238e-04 - my_r2: 0.9075 - val_loss: 4.4292e-05 - val_my_r2: 0.9845\n",
      "Epoch 575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2680e-04 - my_r2: 0.9080 - val_loss: 4.7916e-05 - val_my_r2: 0.9837\n",
      "Epoch 576/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0115e-04 - my_r2: 0.8995 - val_loss: 4.5041e-05 - val_my_r2: 0.9844\n",
      "Epoch 577/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3726e-04 - my_r2: 0.9275 - val_loss: 4.3424e-05 - val_my_r2: 0.9846\n",
      "Epoch 578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5881e-04 - my_r2: 0.9149 - val_loss: 4.3157e-05 - val_my_r2: 0.9848\n",
      "Epoch 579/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8851e-04 - my_r2: 0.9032 - val_loss: 4.5407e-05 - val_my_r2: 0.9849\n",
      "Epoch 580/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 3.0731e-04 - my_r2: 0.9067 - val_loss: 4.2208e-05 - val_my_r2: 0.9857\n",
      "Epoch 581/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2571e-04 - my_r2: 0.9195 - val_loss: 4.0884e-05 - val_my_r2: 0.9859\n",
      "Epoch 582/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7865e-04 - my_r2: 0.8598 - val_loss: 4.3614e-05 - val_my_r2: 0.9852\n",
      "Epoch 583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9230e-04 - my_r2: 0.9402 - val_loss: 4.4783e-05 - val_my_r2: 0.9850\n",
      "Epoch 584/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8218e-04 - my_r2: 0.9281 - val_loss: 4.8708e-05 - val_my_r2: 0.9836\n",
      "Epoch 585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2171e-04 - my_r2: 0.9489 - val_loss: 4.3335e-05 - val_my_r2: 0.9846\n",
      "Epoch 586/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5668e-04 - my_r2: 0.9118 - val_loss: 4.2139e-05 - val_my_r2: 0.9849\n",
      "Epoch 587/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3463e-04 - my_r2: 0.8929 - val_loss: 4.3309e-05 - val_my_r2: 0.9843\n",
      "Epoch 588/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9117e-04 - my_r2: 0.9128 - val_loss: 4.5197e-05 - val_my_r2: 0.9841\n",
      "Epoch 589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7932e-04 - my_r2: 0.9254 - val_loss: 4.5177e-05 - val_my_r2: 0.9842\n",
      "Epoch 590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6582e-04 - my_r2: 0.9324 - val_loss: 3.9184e-05 - val_my_r2: 0.9860\n",
      "Epoch 591/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3784e-04 - my_r2: 0.9080 - val_loss: 3.8303e-05 - val_my_r2: 0.9858\n",
      "Epoch 592/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4569e-04 - my_r2: 0.9012 - val_loss: 4.1185e-05 - val_my_r2: 0.9843\n",
      "Epoch 593/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8141e-04 - my_r2: 0.9356 - val_loss: 3.9316e-05 - val_my_r2: 0.9850\n",
      "Epoch 594/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9651e-04 - my_r2: 0.9003 - val_loss: 3.9295e-05 - val_my_r2: 0.9851\n",
      "Epoch 595/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1184e-04 - my_r2: 0.9285 - val_loss: 4.3335e-05 - val_my_r2: 0.9834\n",
      "Epoch 596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1687e-04 - my_r2: 0.8306 - val_loss: 4.4199e-05 - val_my_r2: 0.9830\n",
      "Epoch 597/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9923e-04 - my_r2: 0.9360 - val_loss: 4.3979e-05 - val_my_r2: 0.9828\n",
      "Epoch 598/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3082e-04 - my_r2: 0.9045 - val_loss: 4.5853e-05 - val_my_r2: 0.9819\n",
      "Epoch 599/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6210e-04 - my_r2: 0.8749 - val_loss: 4.1502e-05 - val_my_r2: 0.9846\n",
      "Epoch 600/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7767e-04 - my_r2: 0.9048 - val_loss: 3.8537e-05 - val_my_r2: 0.9854\n",
      "Epoch 601/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0325e-04 - my_r2: 0.9307 - val_loss: 3.8894e-05 - val_my_r2: 0.9852\n",
      "Epoch 602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7485e-04 - my_r2: 0.8972 - val_loss: 4.0555e-05 - val_my_r2: 0.9844\n",
      "Epoch 603/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1432e-04 - my_r2: 0.9217 - val_loss: 3.8859e-05 - val_my_r2: 0.9858\n",
      "Epoch 604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7656e-04 - my_r2: 0.9062 - val_loss: 3.9592e-05 - val_my_r2: 0.9851\n",
      "Epoch 605/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7668e-04 - my_r2: 0.9067 - val_loss: 4.0348e-05 - val_my_r2: 0.9838\n",
      "Epoch 606/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2678e-04 - my_r2: 0.9280 - val_loss: 4.3369e-05 - val_my_r2: 0.9824\n",
      "Epoch 607/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3751e-04 - my_r2: 0.9170 - val_loss: 4.2552e-05 - val_my_r2: 0.9826\n",
      "Epoch 608/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6788e-04 - my_r2: 0.9112 - val_loss: 4.0703e-05 - val_my_r2: 0.9843\n",
      "Epoch 609/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4307e-04 - my_r2: 0.9005 - val_loss: 3.7184e-05 - val_my_r2: 0.9862\n",
      "Epoch 610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1466e-04 - my_r2: 0.9126 - val_loss: 4.1660e-05 - val_my_r2: 0.9852\n",
      "Epoch 611/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7601e-04 - my_r2: 0.9307 - val_loss: 5.1341e-05 - val_my_r2: 0.9820\n",
      "Epoch 612/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0799e-04 - my_r2: 0.9042 - val_loss: 4.1106e-05 - val_my_r2: 0.9844\n",
      "Epoch 613/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.8116e-04 - my_r2: 0.8953 - val_loss: 3.8745e-05 - val_my_r2: 0.9861\n",
      "Epoch 614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8937e-04 - my_r2: 0.9009 - val_loss: 3.7393e-05 - val_my_r2: 0.9866\n",
      "Epoch 615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0698e-04 - my_r2: 0.9223 - val_loss: 3.8745e-05 - val_my_r2: 0.9858\n",
      "Epoch 616/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7494e-04 - my_r2: 0.9287 - val_loss: 3.6415e-05 - val_my_r2: 0.9869\n",
      "Epoch 617/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4382e-04 - my_r2: 0.9333 - val_loss: 3.5300e-05 - val_my_r2: 0.9873\n",
      "Epoch 618/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0334e-04 - my_r2: 0.8827 - val_loss: 3.5456e-05 - val_my_r2: 0.9870\n",
      "Epoch 619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4187e-04 - my_r2: 0.8183 - val_loss: 3.5457e-05 - val_my_r2: 0.9872\n",
      "Epoch 620/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1288e-04 - my_r2: 0.9003 - val_loss: 3.6397e-05 - val_my_r2: 0.9872\n",
      "Epoch 621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6167e-04 - my_r2: 0.9355 - val_loss: 3.6480e-05 - val_my_r2: 0.9874\n",
      "Epoch 622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4644e-04 - my_r2: 0.9079 - val_loss: 3.4486e-05 - val_my_r2: 0.9874\n",
      "Epoch 623/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2199e-04 - my_r2: 0.9053 - val_loss: 3.4960e-05 - val_my_r2: 0.9871\n",
      "Epoch 624/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4869e-04 - my_r2: 0.9358 - val_loss: 3.4854e-05 - val_my_r2: 0.9874\n",
      "Epoch 625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6808e-04 - my_r2: 0.9154 - val_loss: 3.3421e-05 - val_my_r2: 0.9878\n",
      "Epoch 626/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9702e-04 - my_r2: 0.9044 - val_loss: 3.3785e-05 - val_my_r2: 0.9875\n",
      "Epoch 627/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2837e-04 - my_r2: 0.9182 - val_loss: 5.0725e-05 - val_my_r2: 0.9817\n",
      "Epoch 628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.7491e-04 - my_r2: 0.9173 - val_loss: 3.6949e-05 - val_my_r2: 0.9862\n",
      "Epoch 629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7037e-04 - my_r2: 0.9235 - val_loss: 3.5704e-05 - val_my_r2: 0.9868\n",
      "Epoch 630/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4810e-04 - my_r2: 0.9280 - val_loss: 3.4600e-05 - val_my_r2: 0.9869\n",
      "Epoch 631/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8465e-04 - my_r2: 0.9106 - val_loss: 3.7142e-05 - val_my_r2: 0.9862\n",
      "Epoch 632/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5752e-04 - my_r2: 0.8896 - val_loss: 3.8707e-05 - val_my_r2: 0.9858\n",
      "Epoch 633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1392e-04 - my_r2: 0.8383 - val_loss: 3.6241e-05 - val_my_r2: 0.9866\n",
      "Epoch 634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8963e-04 - my_r2: 0.8978 - val_loss: 3.5010e-05 - val_my_r2: 0.9872\n",
      "Epoch 635/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2380e-04 - my_r2: 0.8990 - val_loss: 3.6417e-05 - val_my_r2: 0.9867\n",
      "Epoch 636/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1456e-04 - my_r2: 0.9317 - val_loss: 3.3404e-05 - val_my_r2: 0.9874\n",
      "Epoch 637/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1712e-04 - my_r2: 0.8927 - val_loss: 3.2126e-05 - val_my_r2: 0.9879\n",
      "Epoch 638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7612e-04 - my_r2: 0.9281 - val_loss: 3.3595e-05 - val_my_r2: 0.9874\n",
      "Epoch 639/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.9715e-04 - my_r2: 0.9113 - val_loss: 3.5560e-05 - val_my_r2: 0.9866\n",
      "Epoch 640/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9708e-04 - my_r2: 0.8869 - val_loss: 3.5032e-05 - val_my_r2: 0.9867\n",
      "Epoch 641/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5492e-04 - my_r2: 0.9302 - val_loss: 3.4553e-05 - val_my_r2: 0.9869\n",
      "Epoch 642/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5898e-04 - my_r2: 0.9251 - val_loss: 3.5983e-05 - val_my_r2: 0.9864\n",
      "Epoch 643/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5616e-04 - my_r2: 0.9148 - val_loss: 3.6015e-05 - val_my_r2: 0.9866\n",
      "Epoch 644/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7808e-04 - my_r2: 0.9315 - val_loss: 3.4166e-05 - val_my_r2: 0.9877\n",
      "Epoch 645/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.9169e-04 - my_r2: 0.9252 - val_loss: 3.6415e-05 - val_my_r2: 0.9871\n",
      "Epoch 646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5405e-04 - my_r2: 0.9354 - val_loss: 3.6365e-05 - val_my_r2: 0.9873\n",
      "Epoch 647/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5714e-04 - my_r2: 0.9396 - val_loss: 3.0743e-05 - val_my_r2: 0.9889\n",
      "Epoch 648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1812e-04 - my_r2: 0.9320 - val_loss: 3.5964e-05 - val_my_r2: 0.9859\n",
      "Epoch 649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3659e-04 - my_r2: 0.9023 - val_loss: 3.7460e-05 - val_my_r2: 0.9854\n",
      "Epoch 650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6799e-04 - my_r2: 0.9237 - val_loss: 3.3931e-05 - val_my_r2: 0.9873\n",
      "Epoch 651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1534e-04 - my_r2: 0.9323 - val_loss: 3.8179e-05 - val_my_r2: 0.9858\n",
      "Epoch 652/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8545e-04 - my_r2: 0.9205 - val_loss: 3.6184e-05 - val_my_r2: 0.9869\n",
      "Epoch 653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7963e-04 - my_r2: 0.9286 - val_loss: 3.2619e-05 - val_my_r2: 0.9881\n",
      "Epoch 654/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7682e-04 - my_r2: 0.9475 - val_loss: 3.5285e-05 - val_my_r2: 0.9871\n",
      "Epoch 655/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0303e-04 - my_r2: 0.9069 - val_loss: 3.2935e-05 - val_my_r2: 0.9877\n",
      "Epoch 656/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8692e-04 - my_r2: 0.9301 - val_loss: 3.2651e-05 - val_my_r2: 0.9879\n",
      "Epoch 657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6426e-04 - my_r2: 0.8714 - val_loss: 3.1777e-05 - val_my_r2: 0.9881\n",
      "Epoch 658/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9627e-04 - my_r2: 0.9028 - val_loss: 3.3264e-05 - val_my_r2: 0.9878\n",
      "Epoch 659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4313e-04 - my_r2: 0.8815 - val_loss: 3.5740e-05 - val_my_r2: 0.9867\n",
      "Epoch 660/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1484e-04 - my_r2: 0.9517 - val_loss: 3.7121e-05 - val_my_r2: 0.9861\n",
      "Epoch 661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1641e-04 - my_r2: 0.9365 - val_loss: 3.6031e-05 - val_my_r2: 0.9866\n",
      "Epoch 662/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4064e-04 - my_r2: 0.8913 - val_loss: 3.9070e-05 - val_my_r2: 0.9850\n",
      "Epoch 663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3832e-04 - my_r2: 0.9001 - val_loss: 3.5313e-05 - val_my_r2: 0.9871\n",
      "Epoch 664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8302e-04 - my_r2: 0.9417 - val_loss: 2.9624e-05 - val_my_r2: 0.9897\n",
      "Epoch 665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8686e-04 - my_r2: 0.9257 - val_loss: 2.9589e-05 - val_my_r2: 0.9896\n",
      "Epoch 666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0012e-04 - my_r2: 0.9380 - val_loss: 3.3183e-05 - val_my_r2: 0.9883\n",
      "Epoch 667/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4223e-04 - my_r2: 0.8543 - val_loss: 3.3673e-05 - val_my_r2: 0.9878\n",
      "Epoch 668/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4805e-04 - my_r2: 0.9196 - val_loss: 2.8900e-05 - val_my_r2: 0.9896\n",
      "Epoch 669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2881e-04 - my_r2: 0.9233 - val_loss: 3.3918e-05 - val_my_r2: 0.9882\n",
      "Epoch 670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3660e-04 - my_r2: 0.9002 - val_loss: 3.0975e-05 - val_my_r2: 0.9890\n",
      "Epoch 671/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3389e-04 - my_r2: 0.9270 - val_loss: 3.6761e-05 - val_my_r2: 0.9872\n",
      "Epoch 672/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2495e-04 - my_r2: 0.8890 - val_loss: 3.0679e-05 - val_my_r2: 0.9889\n",
      "Epoch 673/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3838e-04 - my_r2: 0.9171 - val_loss: 3.1473e-05 - val_my_r2: 0.9887\n",
      "Epoch 674/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8620e-04 - my_r2: 0.9307 - val_loss: 3.4558e-05 - val_my_r2: 0.9871\n",
      "Epoch 675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2540e-04 - my_r2: 0.9221 - val_loss: 3.3243e-05 - val_my_r2: 0.9874\n",
      "Epoch 676/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5591e-04 - my_r2: 0.9286 - val_loss: 3.8301e-05 - val_my_r2: 0.9855\n",
      "Epoch 677/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5542e-04 - my_r2: 0.9479 - val_loss: 3.3435e-05 - val_my_r2: 0.9877\n",
      "Epoch 678/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1261e-04 - my_r2: 0.9091 - val_loss: 3.1953e-05 - val_my_r2: 0.9885\n",
      "Epoch 679/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6088e-04 - my_r2: 0.8033 - val_loss: 2.6711e-05 - val_my_r2: 0.9904\n",
      "Epoch 680/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2958e-04 - my_r2: 0.9086 - val_loss: 2.8255e-05 - val_my_r2: 0.9901\n",
      "Epoch 681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6869e-04 - my_r2: 0.9300 - val_loss: 3.1909e-05 - val_my_r2: 0.9890\n",
      "Epoch 682/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5966e-04 - my_r2: 0.7801 - val_loss: 3.5360e-05 - val_my_r2: 0.9872\n",
      "Epoch 683/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6486e-04 - my_r2: 0.9236 - val_loss: 3.6233e-05 - val_my_r2: 0.9874\n",
      "Epoch 684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7446e-04 - my_r2: 0.9354 - val_loss: 3.2746e-05 - val_my_r2: 0.9888\n",
      "Epoch 685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1541e-04 - my_r2: 0.9512 - val_loss: 3.1461e-05 - val_my_r2: 0.9888\n",
      "Epoch 686/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6589e-04 - my_r2: 0.9149 - val_loss: 3.0440e-05 - val_my_r2: 0.9891\n",
      "Epoch 687/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4784e-04 - my_r2: 0.8400 - val_loss: 3.2068e-05 - val_my_r2: 0.9889\n",
      "Epoch 688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0639e-04 - my_r2: 0.9279 - val_loss: 3.5671e-05 - val_my_r2: 0.9879\n",
      "Epoch 689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7050e-04 - my_r2: 0.8922 - val_loss: 2.7728e-05 - val_my_r2: 0.9902\n",
      "Epoch 690/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3990e-04 - my_r2: 0.9370 - val_loss: 2.7584e-05 - val_my_r2: 0.9903\n",
      "Epoch 691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8645e-04 - my_r2: 0.8835 - val_loss: 3.3263e-05 - val_my_r2: 0.9880\n",
      "Epoch 692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4738e-04 - my_r2: 0.9385 - val_loss: 2.7304e-05 - val_my_r2: 0.9907\n",
      "Epoch 693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4574e-04 - my_r2: 0.9504 - val_loss: 2.8476e-05 - val_my_r2: 0.9898\n",
      "Epoch 694/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8441e-04 - my_r2: 0.9191 - val_loss: 3.1329e-05 - val_my_r2: 0.9889\n",
      "Epoch 695/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3988e-04 - my_r2: 0.9199 - val_loss: 3.1111e-05 - val_my_r2: 0.9892\n",
      "Epoch 696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6399e-04 - my_r2: 0.9286 - val_loss: 2.9566e-05 - val_my_r2: 0.9896\n",
      "Epoch 697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3507e-04 - my_r2: 0.8250 - val_loss: 3.0618e-05 - val_my_r2: 0.9899\n",
      "Epoch 698/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 3.0656e-04 - my_r2: 0.9132 - val_loss: 3.3109e-05 - val_my_r2: 0.9892\n",
      "Epoch 699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7896e-04 - my_r2: 0.8981 - val_loss: 2.9022e-05 - val_my_r2: 0.9898\n",
      "Epoch 700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4054e-04 - my_r2: 0.8969 - val_loss: 3.0091e-05 - val_my_r2: 0.9893\n",
      "Epoch 701/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5776e-04 - my_r2: 0.8262 - val_loss: 3.1428e-05 - val_my_r2: 0.9886\n",
      "Epoch 702/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8726e-04 - my_r2: 0.9342 - val_loss: 2.8547e-05 - val_my_r2: 0.9895\n",
      "Epoch 703/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1529e-04 - my_r2: 0.8753 - val_loss: 2.8077e-05 - val_my_r2: 0.9895\n",
      "Epoch 704/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7167e-04 - my_r2: 0.9336 - val_loss: 3.2677e-05 - val_my_r2: 0.9878\n",
      "Epoch 705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6299e-04 - my_r2: 0.8416 - val_loss: 3.1112e-05 - val_my_r2: 0.9875\n",
      "Epoch 706/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0189e-04 - my_r2: 0.9079 - val_loss: 2.9116e-05 - val_my_r2: 0.9885\n",
      "Epoch 707/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0259e-04 - my_r2: 0.9190 - val_loss: 2.9151e-05 - val_my_r2: 0.9885\n",
      "Epoch 708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1465e-04 - my_r2: 0.9203 - val_loss: 3.0568e-05 - val_my_r2: 0.9876\n",
      "Epoch 709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7563e-04 - my_r2: 0.9333 - val_loss: 2.8111e-05 - val_my_r2: 0.9888\n",
      "Epoch 710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3588e-04 - my_r2: 0.9278 - val_loss: 2.9711e-05 - val_my_r2: 0.9882\n",
      "Epoch 711/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6827e-04 - my_r2: 0.9281 - val_loss: 3.2161e-05 - val_my_r2: 0.9876\n",
      "Epoch 712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6802e-04 - my_r2: 0.9419 - val_loss: 3.3623e-05 - val_my_r2: 0.9869\n",
      "Epoch 713/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9603e-04 - my_r2: 0.9367 - val_loss: 2.8619e-05 - val_my_r2: 0.9889\n",
      "Epoch 714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5868e-04 - my_r2: 0.9350 - val_loss: 3.3301e-05 - val_my_r2: 0.9877\n",
      "Epoch 715/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0778e-04 - my_r2: 0.9191 - val_loss: 2.8767e-05 - val_my_r2: 0.9896\n",
      "Epoch 716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1725e-04 - my_r2: 0.9198 - val_loss: 2.7614e-05 - val_my_r2: 0.9901\n",
      "Epoch 717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6757e-04 - my_r2: 0.9382 - val_loss: 3.3292e-05 - val_my_r2: 0.9880\n",
      "Epoch 718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9690e-04 - my_r2: 0.9410 - val_loss: 2.9310e-05 - val_my_r2: 0.9891\n",
      "Epoch 719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9972e-04 - my_r2: 0.9321 - val_loss: 2.9478e-05 - val_my_r2: 0.9890\n",
      "Epoch 720/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1837e-04 - my_r2: 0.9330 - val_loss: 2.9185e-05 - val_my_r2: 0.9888\n",
      "Epoch 721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9878e-04 - my_r2: 0.9242 - val_loss: 2.9751e-05 - val_my_r2: 0.9889\n",
      "Epoch 722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3915e-04 - my_r2: 0.9060 - val_loss: 3.0185e-05 - val_my_r2: 0.9891\n",
      "Epoch 723/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5938e-04 - my_r2: 0.9232 - val_loss: 2.6300e-05 - val_my_r2: 0.9906\n",
      "Epoch 724/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3303e-04 - my_r2: 0.9138 - val_loss: 2.6226e-05 - val_my_r2: 0.9906\n",
      "Epoch 725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3938e-04 - my_r2: 0.8892 - val_loss: 2.8478e-05 - val_my_r2: 0.9896\n",
      "Epoch 726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0177e-04 - my_r2: 0.9512 - val_loss: 2.7005e-05 - val_my_r2: 0.9898\n",
      "Epoch 727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8964e-04 - my_r2: 0.8764 - val_loss: 2.7938e-05 - val_my_r2: 0.9889\n",
      "Epoch 728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6990e-04 - my_r2: 0.9108 - val_loss: 2.6107e-05 - val_my_r2: 0.9898\n",
      "Epoch 729/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7457e-04 - my_r2: 0.9514 - val_loss: 2.7181e-05 - val_my_r2: 0.9900\n",
      "Epoch 730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5294e-04 - my_r2: 0.9440 - val_loss: 2.7232e-05 - val_my_r2: 0.9897\n",
      "Epoch 731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4542e-04 - my_r2: 0.8714 - val_loss: 2.7185e-05 - val_my_r2: 0.9898\n",
      "Epoch 732/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0694e-04 - my_r2: 0.9261 - val_loss: 2.7604e-05 - val_my_r2: 0.9897\n",
      "Epoch 733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7194e-04 - my_r2: 0.9014 - val_loss: 2.8186e-05 - val_my_r2: 0.9899\n",
      "Epoch 734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1686e-04 - my_r2: 0.9027 - val_loss: 2.8234e-05 - val_my_r2: 0.9903\n",
      "Epoch 735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3494e-04 - my_r2: 0.9358 - val_loss: 2.6570e-05 - val_my_r2: 0.9909\n",
      "Epoch 736/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5432e-04 - my_r2: 0.8952 - val_loss: 2.5450e-05 - val_my_r2: 0.9909\n",
      "Epoch 737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9024e-04 - my_r2: 0.9056 - val_loss: 2.6126e-05 - val_my_r2: 0.9910\n",
      "Epoch 738/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1112e-04 - my_r2: 0.9291 - val_loss: 2.6426e-05 - val_my_r2: 0.9907\n",
      "Epoch 739/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6375e-04 - my_r2: 0.9302 - val_loss: 2.6216e-05 - val_my_r2: 0.9900\n",
      "Epoch 740/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4312e-04 - my_r2: 0.8921 - val_loss: 2.6767e-05 - val_my_r2: 0.9899\n",
      "Epoch 741/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8403e-04 - my_r2: 0.9220 - val_loss: 2.5926e-05 - val_my_r2: 0.9902\n",
      "Epoch 742/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3697e-04 - my_r2: 0.9171 - val_loss: 2.5839e-05 - val_my_r2: 0.9906\n",
      "Epoch 743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8423e-04 - my_r2: 0.8985 - val_loss: 2.6203e-05 - val_my_r2: 0.9906\n",
      "Epoch 744/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0387e-04 - my_r2: 0.8570 - val_loss: 2.4249e-05 - val_my_r2: 0.9910\n",
      "Epoch 745/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5455e-04 - my_r2: 0.9345 - val_loss: 2.7631e-05 - val_my_r2: 0.9896\n",
      "Epoch 746/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0645e-04 - my_r2: 0.9251 - val_loss: 2.9962e-05 - val_my_r2: 0.9894\n",
      "Epoch 747/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0659e-04 - my_r2: 0.9196 - val_loss: 3.1252e-05 - val_my_r2: 0.9888\n",
      "Epoch 748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4352e-04 - my_r2: 0.9403 - val_loss: 3.3967e-05 - val_my_r2: 0.9879\n",
      "Epoch 749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7142e-04 - my_r2: 0.8714 - val_loss: 2.9078e-05 - val_my_r2: 0.9892\n",
      "Epoch 750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0347e-04 - my_r2: 0.9182 - val_loss: 2.6483e-05 - val_my_r2: 0.9898\n",
      "Epoch 751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5462e-04 - my_r2: 0.9502 - val_loss: 2.5715e-05 - val_my_r2: 0.9906\n",
      "Epoch 752/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9501e-04 - my_r2: 0.9310 - val_loss: 2.6441e-05 - val_my_r2: 0.9911\n",
      "Epoch 753/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9738e-04 - my_r2: 0.8543 - val_loss: 3.1231e-05 - val_my_r2: 0.9894\n",
      "Epoch 754/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3191e-04 - my_r2: 0.9302 - val_loss: 2.9020e-05 - val_my_r2: 0.9893\n",
      "Epoch 755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5226e-04 - my_r2: 0.9317 - val_loss: 3.9298e-05 - val_my_r2: 0.9860\n",
      "Epoch 756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1535e-04 - my_r2: 0.9196 - val_loss: 3.1801e-05 - val_my_r2: 0.9892\n",
      "Epoch 757/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1408e-04 - my_r2: 0.8995 - val_loss: 2.5624e-05 - val_my_r2: 0.9913\n",
      "Epoch 758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7951e-04 - my_r2: 0.9186 - val_loss: 2.6016e-05 - val_my_r2: 0.9912\n",
      "Epoch 759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2939e-04 - my_r2: 0.9135 - val_loss: 2.6034e-05 - val_my_r2: 0.9918\n",
      "Epoch 760/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9950e-04 - my_r2: 0.9557 - val_loss: 2.5442e-05 - val_my_r2: 0.9919\n",
      "Epoch 761/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2066e-04 - my_r2: 0.9450 - val_loss: 2.5282e-05 - val_my_r2: 0.9918\n",
      "Epoch 762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0280e-04 - my_r2: 0.9094 - val_loss: 2.3172e-05 - val_my_r2: 0.9922\n",
      "Epoch 763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8915e-04 - my_r2: 0.9208 - val_loss: 2.4183e-05 - val_my_r2: 0.9918\n",
      "Epoch 764/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4248e-04 - my_r2: 0.9187 - val_loss: 2.5020e-05 - val_my_r2: 0.9912\n",
      "Epoch 765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9171e-04 - my_r2: 0.9024 - val_loss: 2.6733e-05 - val_my_r2: 0.9907\n",
      "Epoch 766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5284e-04 - my_r2: 0.9128 - val_loss: 3.0287e-05 - val_my_r2: 0.9898\n",
      "Epoch 767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2445e-04 - my_r2: 0.8362 - val_loss: 2.7218e-05 - val_my_r2: 0.9911\n",
      "Epoch 768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6129e-04 - my_r2: 0.8952 - val_loss: 2.6141e-05 - val_my_r2: 0.9914\n",
      "Epoch 769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0001e-04 - my_r2: 0.8581 - val_loss: 2.4910e-05 - val_my_r2: 0.9921\n",
      "Epoch 770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8446e-04 - my_r2: 0.9368 - val_loss: 2.3436e-05 - val_my_r2: 0.9925\n",
      "Epoch 771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9996e-04 - my_r2: 0.9234 - val_loss: 2.4385e-05 - val_my_r2: 0.9921\n",
      "Epoch 772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2485e-04 - my_r2: 0.9317 - val_loss: 2.8053e-05 - val_my_r2: 0.9914\n",
      "Epoch 773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4131e-04 - my_r2: 0.9189 - val_loss: 2.9839e-05 - val_my_r2: 0.9909\n",
      "Epoch 774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0997e-04 - my_r2: 0.9411 - val_loss: 3.5849e-05 - val_my_r2: 0.9890\n",
      "Epoch 775/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4810e-04 - my_r2: 0.9358 - val_loss: 3.4390e-05 - val_my_r2: 0.9892\n",
      "Epoch 776/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9304e-04 - my_r2: 0.9155 - val_loss: 2.9195e-05 - val_my_r2: 0.9911\n",
      "Epoch 777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1437e-04 - my_r2: 0.9198 - val_loss: 2.4457e-05 - val_my_r2: 0.9922\n",
      "Epoch 778/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5684e-04 - my_r2: 0.8983 - val_loss: 2.7897e-05 - val_my_r2: 0.9905\n",
      "Epoch 779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2248e-04 - my_r2: 0.9329 - val_loss: 2.3924e-05 - val_my_r2: 0.9911\n",
      "Epoch 780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4973e-04 - my_r2: 0.9244 - val_loss: 2.7279e-05 - val_my_r2: 0.9894\n",
      "Epoch 781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4825e-04 - my_r2: 0.9181 - val_loss: 2.6067e-05 - val_my_r2: 0.9900\n",
      "Epoch 782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6318e-04 - my_r2: 0.9529 - val_loss: 2.3177e-05 - val_my_r2: 0.9913\n",
      "Epoch 783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2721e-04 - my_r2: 0.8985 - val_loss: 3.0505e-05 - val_my_r2: 0.9885\n",
      "Epoch 784/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9331e-04 - my_r2: 0.8780 - val_loss: 2.4763e-05 - val_my_r2: 0.9910\n",
      "Epoch 785/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9944e-04 - my_r2: 0.9205 - val_loss: 3.3458e-05 - val_my_r2: 0.9875\n",
      "Epoch 786/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8106e-04 - my_r2: 0.9178 - val_loss: 2.8904e-05 - val_my_r2: 0.9889\n",
      "Epoch 787/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4316e-04 - my_r2: 0.9390 - val_loss: 2.0954e-05 - val_my_r2: 0.9921\n",
      "Epoch 788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5624e-04 - my_r2: 0.9116 - val_loss: 2.2302e-05 - val_my_r2: 0.9918\n",
      "Epoch 789/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2381e-04 - my_r2: 0.9018 - val_loss: 2.1807e-05 - val_my_r2: 0.9920\n",
      "Epoch 790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6057e-04 - my_r2: 0.9292 - val_loss: 2.2048e-05 - val_my_r2: 0.9921\n",
      "Epoch 791/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5738e-04 - my_r2: 0.9323 - val_loss: 2.3494e-05 - val_my_r2: 0.9916\n",
      "Epoch 792/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1778e-04 - my_r2: 0.9141 - val_loss: 2.7536e-05 - val_my_r2: 0.9909\n",
      "Epoch 793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8038e-04 - my_r2: 0.9163 - val_loss: 2.6354e-05 - val_my_r2: 0.9914\n",
      "Epoch 794/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8154e-04 - my_r2: 0.9299 - val_loss: 3.1071e-05 - val_my_r2: 0.9902\n",
      "Epoch 795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6880e-04 - my_r2: 0.9148 - val_loss: 3.3499e-05 - val_my_r2: 0.9897\n",
      "Epoch 796/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6823e-04 - my_r2: 0.8773 - val_loss: 2.1984e-05 - val_my_r2: 0.9931\n",
      "Epoch 797/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6592e-04 - my_r2: 0.9316 - val_loss: 2.4859e-05 - val_my_r2: 0.9914\n",
      "Epoch 798/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4948e-04 - my_r2: 0.9525 - val_loss: 2.2383e-05 - val_my_r2: 0.9923\n",
      "Epoch 799/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7994e-04 - my_r2: 0.9367 - val_loss: 2.3358e-05 - val_my_r2: 0.9919\n",
      "Epoch 800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3471e-04 - my_r2: 0.9440 - val_loss: 2.5604e-05 - val_my_r2: 0.9912\n",
      "Epoch 801/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.6023e-04 - my_r2: 0.8644 - val_loss: 2.4941e-05 - val_my_r2: 0.9922\n",
      "Epoch 802/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6073e-04 - my_r2: 0.9429 - val_loss: 2.3789e-05 - val_my_r2: 0.9925\n",
      "Epoch 803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9391e-04 - my_r2: 0.8441 - val_loss: 2.2106e-05 - val_my_r2: 0.9929\n",
      "Epoch 804/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8393e-04 - my_r2: 0.8992 - val_loss: 2.2303e-05 - val_my_r2: 0.9928\n",
      "Epoch 805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5607e-04 - my_r2: 0.9149 - val_loss: 1.9736e-05 - val_my_r2: 0.9932\n",
      "Epoch 806/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2254e-04 - my_r2: 0.9143 - val_loss: 1.9298e-05 - val_my_r2: 0.9937\n",
      "Epoch 807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6763e-04 - my_r2: 0.9361 - val_loss: 1.9410e-05 - val_my_r2: 0.9934\n",
      "Epoch 808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7537e-04 - my_r2: 0.8796 - val_loss: 1.8985e-05 - val_my_r2: 0.9934\n",
      "Epoch 809/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6742e-04 - my_r2: 0.9429 - val_loss: 2.3508e-05 - val_my_r2: 0.9922\n",
      "Epoch 810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1027e-04 - my_r2: 0.8771 - val_loss: 2.2620e-05 - val_my_r2: 0.9924\n",
      "Epoch 811/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5816e-04 - my_r2: 0.8923 - val_loss: 2.6591e-05 - val_my_r2: 0.9908\n",
      "Epoch 812/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8373e-04 - my_r2: 0.9057 - val_loss: 2.1428e-05 - val_my_r2: 0.9924\n",
      "Epoch 813/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7766e-04 - my_r2: 0.9554 - val_loss: 2.1953e-05 - val_my_r2: 0.9919\n",
      "Epoch 814/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6725e-04 - my_r2: 0.9440 - val_loss: 2.1160e-05 - val_my_r2: 0.9920\n",
      "Epoch 815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4023e-04 - my_r2: 0.9368 - val_loss: 2.3603e-05 - val_my_r2: 0.9913\n",
      "Epoch 816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8777e-04 - my_r2: 0.9115 - val_loss: 2.3075e-05 - val_my_r2: 0.9913\n",
      "Epoch 817/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8723e-04 - my_r2: 0.9128 - val_loss: 1.9618e-05 - val_my_r2: 0.9927\n",
      "Epoch 818/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5606e-04 - my_r2: 0.9253 - val_loss: 2.0894e-05 - val_my_r2: 0.9920\n",
      "Epoch 819/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3334e-04 - my_r2: 0.9289 - val_loss: 2.2809e-05 - val_my_r2: 0.9915\n",
      "Epoch 820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3639e-04 - my_r2: 0.9415 - val_loss: 2.5230e-05 - val_my_r2: 0.9910\n",
      "Epoch 821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7328e-04 - my_r2: 0.9364 - val_loss: 2.0319e-05 - val_my_r2: 0.9925\n",
      "Epoch 822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2858e-04 - my_r2: 0.9314 - val_loss: 2.2515e-05 - val_my_r2: 0.9921\n",
      "Epoch 823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9321e-04 - my_r2: 0.8849 - val_loss: 2.3328e-05 - val_my_r2: 0.9914\n",
      "Epoch 824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3746e-04 - my_r2: 0.8933 - val_loss: 1.9346e-05 - val_my_r2: 0.9927\n",
      "Epoch 825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8324e-04 - my_r2: 0.9287 - val_loss: 1.9900e-05 - val_my_r2: 0.9924\n",
      "Epoch 826/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1032e-04 - my_r2: 0.9251 - val_loss: 2.1147e-05 - val_my_r2: 0.9919\n",
      "Epoch 827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5524e-04 - my_r2: 0.9075 - val_loss: 2.4462e-05 - val_my_r2: 0.9908\n",
      "Epoch 828/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9316e-04 - my_r2: 0.9403 - val_loss: 1.9792e-05 - val_my_r2: 0.9926\n",
      "Epoch 829/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5527e-04 - my_r2: 0.9315 - val_loss: 2.2243e-05 - val_my_r2: 0.9917\n",
      "Epoch 830/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4177e-04 - my_r2: 0.6689 - val_loss: 2.2033e-05 - val_my_r2: 0.9917\n",
      "Epoch 831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6673e-04 - my_r2: 0.9391 - val_loss: 2.6184e-05 - val_my_r2: 0.9906\n",
      "Epoch 832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3677e-04 - my_r2: 0.9049 - val_loss: 2.9262e-05 - val_my_r2: 0.9890\n",
      "Epoch 833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5301e-04 - my_r2: 0.9305 - val_loss: 2.6290e-05 - val_my_r2: 0.9903\n",
      "Epoch 834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9074e-04 - my_r2: 0.9236 - val_loss: 3.2354e-05 - val_my_r2: 0.9887\n",
      "Epoch 835/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2593e-04 - my_r2: 0.9455 - val_loss: 3.3046e-05 - val_my_r2: 0.9881\n",
      "Epoch 836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2389e-04 - my_r2: 0.9288 - val_loss: 2.4694e-05 - val_my_r2: 0.9908\n",
      "Epoch 837/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1705e-04 - my_r2: 0.9434 - val_loss: 2.2566e-05 - val_my_r2: 0.9917\n",
      "Epoch 838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3254e-04 - my_r2: 0.9003 - val_loss: 2.2066e-05 - val_my_r2: 0.9921\n",
      "Epoch 839/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9666e-04 - my_r2: 0.9274 - val_loss: 2.1466e-05 - val_my_r2: 0.9922\n",
      "Epoch 840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2745e-04 - my_r2: 0.9299 - val_loss: 2.1651e-05 - val_my_r2: 0.9916\n",
      "Epoch 841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4455e-04 - my_r2: 0.9417 - val_loss: 2.3942e-05 - val_my_r2: 0.9909\n",
      "Epoch 842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7210e-04 - my_r2: 0.9220 - val_loss: 2.3606e-05 - val_my_r2: 0.9910\n",
      "Epoch 843/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2384e-04 - my_r2: 0.9401 - val_loss: 2.2212e-05 - val_my_r2: 0.9912\n",
      "Epoch 844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8773e-04 - my_r2: 0.9377 - val_loss: 2.0422e-05 - val_my_r2: 0.9923\n",
      "Epoch 845/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3902e-04 - my_r2: 0.9309 - val_loss: 1.9936e-05 - val_my_r2: 0.9926\n",
      "Epoch 846/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4832e-04 - my_r2: 0.9505 - val_loss: 1.9815e-05 - val_my_r2: 0.9926\n",
      "Epoch 847/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5030e-04 - my_r2: 0.9423 - val_loss: 1.8146e-05 - val_my_r2: 0.9928\n",
      "Epoch 848/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5285e-04 - my_r2: 0.8633 - val_loss: 2.0391e-05 - val_my_r2: 0.9918\n",
      "Epoch 849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7333e-04 - my_r2: 0.8951 - val_loss: 2.0908e-05 - val_my_r2: 0.9915\n",
      "Epoch 850/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4761e-04 - my_r2: 0.9410 - val_loss: 2.5260e-05 - val_my_r2: 0.9901\n",
      "Epoch 851/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5484e-04 - my_r2: 0.9280 - val_loss: 1.9408e-05 - val_my_r2: 0.9924\n",
      "Epoch 852/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0152e-04 - my_r2: 0.9138 - val_loss: 1.9738e-05 - val_my_r2: 0.9918\n",
      "Epoch 853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7180e-04 - my_r2: 0.9296 - val_loss: 2.1137e-05 - val_my_r2: 0.9913\n",
      "Epoch 854/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7662e-04 - my_r2: 0.7237 - val_loss: 2.0852e-05 - val_my_r2: 0.9917\n",
      "Epoch 855/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6444e-04 - my_r2: 0.9437 - val_loss: 2.2531e-05 - val_my_r2: 0.9911\n",
      "Epoch 856/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4156e-04 - my_r2: 0.9487 - val_loss: 2.2887e-05 - val_my_r2: 0.9910\n",
      "Epoch 857/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7163e-04 - my_r2: 0.9352 - val_loss: 2.0798e-05 - val_my_r2: 0.9915\n",
      "Epoch 858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0327e-04 - my_r2: 0.9235 - val_loss: 1.7852e-05 - val_my_r2: 0.9930\n",
      "Epoch 859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6233e-04 - my_r2: 0.9280 - val_loss: 2.0298e-05 - val_my_r2: 0.9922\n",
      "Epoch 860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0691e-04 - my_r2: 0.9132 - val_loss: 1.9265e-05 - val_my_r2: 0.9922\n",
      "Epoch 861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8717e-04 - my_r2: 0.9161 - val_loss: 1.8921e-05 - val_my_r2: 0.9922\n",
      "Epoch 862/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6453e-04 - my_r2: 0.8768 - val_loss: 1.8726e-05 - val_my_r2: 0.9926\n",
      "Epoch 863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8308e-04 - my_r2: 0.9113 - val_loss: 1.9060e-05 - val_my_r2: 0.9933\n",
      "Epoch 864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9036e-04 - my_r2: 0.9031 - val_loss: 2.5226e-05 - val_my_r2: 0.9914\n",
      "Epoch 865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2477e-04 - my_r2: 0.9428 - val_loss: 2.5816e-05 - val_my_r2: 0.9907\n",
      "Epoch 866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7360e-04 - my_r2: 0.9347 - val_loss: 2.1004e-05 - val_my_r2: 0.9923\n",
      "Epoch 867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3028e-04 - my_r2: 0.9309 - val_loss: 1.9120e-05 - val_my_r2: 0.9931\n",
      "Epoch 868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1619e-04 - my_r2: 0.9046 - val_loss: 2.0127e-05 - val_my_r2: 0.9932\n",
      "Epoch 869/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9222e-04 - my_r2: 0.9298 - val_loss: 2.0918e-05 - val_my_r2: 0.9934\n",
      "Epoch 870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8708e-04 - my_r2: 0.9351 - val_loss: 1.9343e-05 - val_my_r2: 0.9936\n",
      "Epoch 871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9348e-04 - my_r2: 0.9341 - val_loss: 2.5141e-05 - val_my_r2: 0.9920\n",
      "Epoch 872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6114e-04 - my_r2: 0.8472 - val_loss: 1.8336e-05 - val_my_r2: 0.9941\n",
      "Epoch 873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3396e-04 - my_r2: 0.9340 - val_loss: 1.9695e-05 - val_my_r2: 0.9935\n",
      "Epoch 874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3856e-04 - my_r2: 0.9464 - val_loss: 2.1793e-05 - val_my_r2: 0.9929\n",
      "Epoch 875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8796e-04 - my_r2: 0.8942 - val_loss: 1.9478e-05 - val_my_r2: 0.9936\n",
      "Epoch 876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8732e-04 - my_r2: 0.9311 - val_loss: 2.2186e-05 - val_my_r2: 0.9923\n",
      "Epoch 877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9240e-04 - my_r2: 0.8854 - val_loss: 2.0063e-05 - val_my_r2: 0.9935\n",
      "Epoch 878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6213e-04 - my_r2: 0.8494 - val_loss: 2.2493e-05 - val_my_r2: 0.9933\n",
      "Epoch 879/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8999e-04 - my_r2: 0.9234 - val_loss: 2.9189e-05 - val_my_r2: 0.9916\n",
      "Epoch 880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7658e-04 - my_r2: 0.8959 - val_loss: 2.3281e-05 - val_my_r2: 0.9934\n",
      "Epoch 881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5456e-04 - my_r2: 0.8623 - val_loss: 2.8012e-05 - val_my_r2: 0.9917\n",
      "Epoch 882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3889e-04 - my_r2: 0.9183 - val_loss: 2.2726e-05 - val_my_r2: 0.9928\n",
      "Epoch 883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0943e-04 - my_r2: 0.9047 - val_loss: 2.4071e-05 - val_my_r2: 0.9924\n",
      "Epoch 884/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8583e-04 - my_r2: 0.9270 - val_loss: 2.3442e-05 - val_my_r2: 0.9924\n",
      "Epoch 885/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9203e-04 - my_r2: 0.9371 - val_loss: 2.0334e-05 - val_my_r2: 0.9935\n",
      "Epoch 886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6351e-04 - my_r2: 0.9064 - val_loss: 2.4992e-05 - val_my_r2: 0.9916\n",
      "Epoch 887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9199e-04 - my_r2: 0.9237 - val_loss: 2.1845e-05 - val_my_r2: 0.9930\n",
      "Epoch 888/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4594e-04 - my_r2: 0.9397 - val_loss: 2.1512e-05 - val_my_r2: 0.9930\n",
      "Epoch 889/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3129e-04 - my_r2: 0.9344 - val_loss: 2.2351e-05 - val_my_r2: 0.9928\n",
      "Epoch 890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7400e-04 - my_r2: 0.9344 - val_loss: 2.0504e-05 - val_my_r2: 0.9929\n",
      "Epoch 891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7909e-04 - my_r2: 0.9348 - val_loss: 2.1774e-05 - val_my_r2: 0.9925\n",
      "Epoch 892/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6399e-04 - my_r2: 0.9336 - val_loss: 1.9128e-05 - val_my_r2: 0.9935\n",
      "Epoch 893/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5299e-04 - my_r2: 0.9094 - val_loss: 2.7961e-05 - val_my_r2: 0.9908\n",
      "Epoch 894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5265e-04 - my_r2: 0.8866 - val_loss: 2.0528e-05 - val_my_r2: 0.9929\n",
      "Epoch 895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6861e-04 - my_r2: 0.9407 - val_loss: 1.7468e-05 - val_my_r2: 0.9940\n",
      "Epoch 896/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0691e-04 - my_r2: 0.9319 - val_loss: 1.9547e-05 - val_my_r2: 0.9934\n",
      "Epoch 897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0733e-04 - my_r2: 0.8961 - val_loss: 2.0144e-05 - val_my_r2: 0.9930\n",
      "Epoch 898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5254e-04 - my_r2: 0.9214 - val_loss: 2.1516e-05 - val_my_r2: 0.9925\n",
      "Epoch 899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0171e-04 - my_r2: 0.9380 - val_loss: 2.3537e-05 - val_my_r2: 0.9920\n",
      "Epoch 900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4918e-04 - my_r2: 0.9268 - val_loss: 2.3860e-05 - val_my_r2: 0.9913\n",
      "Epoch 901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1713e-04 - my_r2: 0.9543 - val_loss: 2.0931e-05 - val_my_r2: 0.9923\n",
      "Epoch 902/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7722e-04 - my_r2: 0.9235 - val_loss: 2.4717e-05 - val_my_r2: 0.9913\n",
      "Epoch 903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2089e-04 - my_r2: 0.9100 - val_loss: 1.8027e-05 - val_my_r2: 0.9937\n",
      "Epoch 904/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1219e-04 - my_r2: 0.9497 - val_loss: 1.6960e-05 - val_my_r2: 0.9940\n",
      "Epoch 905/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8074e-04 - my_r2: 0.9062 - val_loss: 1.8637e-05 - val_my_r2: 0.9935\n",
      "Epoch 906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3732e-04 - my_r2: 0.9224 - val_loss: 1.6764e-05 - val_my_r2: 0.9941\n",
      "Epoch 907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8529e-04 - my_r2: 0.9390 - val_loss: 1.7783e-05 - val_my_r2: 0.9937\n",
      "Epoch 908/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5333e-04 - my_r2: 0.9243 - val_loss: 1.8791e-05 - val_my_r2: 0.9934\n",
      "Epoch 909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5023e-04 - my_r2: 0.9219 - val_loss: 2.0695e-05 - val_my_r2: 0.9931\n",
      "Epoch 910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2307e-04 - my_r2: 0.9295 - val_loss: 2.1983e-05 - val_my_r2: 0.9919\n",
      "Epoch 911/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2750e-04 - my_r2: 0.9259 - val_loss: 2.1813e-05 - val_my_r2: 0.9920\n",
      "Epoch 912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5329e-04 - my_r2: 0.9494 - val_loss: 2.1958e-05 - val_my_r2: 0.9920\n",
      "Epoch 913/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5957e-04 - my_r2: 0.9166 - val_loss: 2.0619e-05 - val_my_r2: 0.9926\n",
      "Epoch 914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6308e-04 - my_r2: 0.9423 - val_loss: 2.0455e-05 - val_my_r2: 0.9927\n",
      "Epoch 915/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8301e-04 - my_r2: 0.9619 - val_loss: 1.7755e-05 - val_my_r2: 0.9937\n",
      "Epoch 916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4147e-04 - my_r2: 0.9499 - val_loss: 1.7003e-05 - val_my_r2: 0.9939\n",
      "Epoch 917/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5404e-04 - my_r2: 0.9216 - val_loss: 2.0104e-05 - val_my_r2: 0.9931\n",
      "Epoch 918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0598e-04 - my_r2: 0.8948 - val_loss: 2.2073e-05 - val_my_r2: 0.9923\n",
      "Epoch 919/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1377e-04 - my_r2: 0.9046 - val_loss: 2.1287e-05 - val_my_r2: 0.9918\n",
      "Epoch 920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.5914e-04 - my_r2: 0.9185 - val_loss: 2.5281e-05 - val_my_r2: 0.9912\n",
      "Epoch 921/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0285e-04 - my_r2: 0.9147 - val_loss: 2.7943e-05 - val_my_r2: 0.9911\n",
      "Epoch 922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4461e-04 - my_r2: 0.9427 - val_loss: 3.1884e-05 - val_my_r2: 0.9900\n",
      "Epoch 923/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3138e-04 - my_r2: 0.9015 - val_loss: 1.8383e-05 - val_my_r2: 0.9938\n",
      "Epoch 924/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5966e-04 - my_r2: 0.9292 - val_loss: 1.6441e-05 - val_my_r2: 0.9944\n",
      "Epoch 925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7122e-04 - my_r2: 0.8862 - val_loss: 1.7979e-05 - val_my_r2: 0.9938\n",
      "Epoch 926/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2138e-04 - my_r2: 0.9444 - val_loss: 1.8981e-05 - val_my_r2: 0.9934\n",
      "Epoch 927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6372e-04 - my_r2: 0.9318 - val_loss: 1.8260e-05 - val_my_r2: 0.9934\n",
      "Epoch 928/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5052e-04 - my_r2: 0.9450 - val_loss: 1.8044e-05 - val_my_r2: 0.9931\n",
      "Epoch 929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5375e-04 - my_r2: 0.9360 - val_loss: 1.6427e-05 - val_my_r2: 0.9934\n",
      "Epoch 930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4737e-04 - my_r2: 0.9432 - val_loss: 1.7786e-05 - val_my_r2: 0.9932\n",
      "Epoch 931/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5029e-04 - my_r2: 0.9181 - val_loss: 2.1854e-05 - val_my_r2: 0.9917\n",
      "Epoch 932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5638e-04 - my_r2: 0.9337 - val_loss: 1.6739e-05 - val_my_r2: 0.9936\n",
      "Epoch 933/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3993e-04 - my_r2: 0.9304 - val_loss: 1.7116e-05 - val_my_r2: 0.9938\n",
      "Epoch 934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0991e-04 - my_r2: 0.9276 - val_loss: 2.1169e-05 - val_my_r2: 0.9920\n",
      "Epoch 935/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1273e-04 - my_r2: 0.9388 - val_loss: 1.9192e-05 - val_my_r2: 0.9926\n",
      "Epoch 936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7515e-04 - my_r2: 0.9272 - val_loss: 2.0217e-05 - val_my_r2: 0.9924\n",
      "Epoch 937/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6595e-04 - my_r2: 0.9057 - val_loss: 1.5303e-05 - val_my_r2: 0.9942\n",
      "Epoch 938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9487e-04 - my_r2: 0.9317 - val_loss: 1.7313e-05 - val_my_r2: 0.9935\n",
      "Epoch 939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4627e-04 - my_r2: 0.9259 - val_loss: 1.5707e-05 - val_my_r2: 0.9940\n",
      "Epoch 940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4034e-04 - my_r2: 0.9178 - val_loss: 1.7392e-05 - val_my_r2: 0.9933\n",
      "Epoch 941/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8848e-04 - my_r2: 0.9314 - val_loss: 2.3257e-05 - val_my_r2: 0.9914\n",
      "Epoch 942/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0711e-04 - my_r2: 0.9500 - val_loss: 2.0202e-05 - val_my_r2: 0.9925\n",
      "Epoch 943/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7777e-04 - my_r2: 0.9253 - val_loss: 1.9410e-05 - val_my_r2: 0.9931\n",
      "Epoch 944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5696e-04 - my_r2: 0.9401 - val_loss: 1.9306e-05 - val_my_r2: 0.9932\n",
      "Epoch 945/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0920e-04 - my_r2: 0.9463 - val_loss: 1.8870e-05 - val_my_r2: 0.9938\n",
      "Epoch 946/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5825e-04 - my_r2: 0.9484 - val_loss: 1.7962e-05 - val_my_r2: 0.9941\n",
      "Epoch 947/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2565e-04 - my_r2: 0.8914 - val_loss: 1.8958e-05 - val_my_r2: 0.9938\n",
      "Epoch 948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9059e-04 - my_r2: 0.9507 - val_loss: 1.7781e-05 - val_my_r2: 0.9940\n",
      "Epoch 949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1702e-04 - my_r2: 0.9179 - val_loss: 2.1728e-05 - val_my_r2: 0.9926\n",
      "Epoch 950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6957e-04 - my_r2: 0.9211 - val_loss: 1.7238e-05 - val_my_r2: 0.9942\n",
      "Epoch 951/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6561e-04 - my_r2: 0.9154 - val_loss: 1.8936e-05 - val_my_r2: 0.9938\n",
      "Epoch 952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0453e-04 - my_r2: 0.9435 - val_loss: 1.6977e-05 - val_my_r2: 0.9943\n",
      "Epoch 953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3708e-04 - my_r2: 0.9410 - val_loss: 1.6712e-05 - val_my_r2: 0.9943\n",
      "Epoch 954/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6959e-04 - my_r2: 0.9057 - val_loss: 1.6620e-05 - val_my_r2: 0.9944\n",
      "Epoch 955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5848e-04 - my_r2: 0.9287 - val_loss: 1.5861e-05 - val_my_r2: 0.9945\n",
      "Epoch 956/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2914e-04 - my_r2: 0.9394 - val_loss: 1.9473e-05 - val_my_r2: 0.9931\n",
      "Epoch 957/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9847e-04 - my_r2: 0.9081 - val_loss: 1.9108e-05 - val_my_r2: 0.9930\n",
      "Epoch 958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6806e-04 - my_r2: 0.9145 - val_loss: 1.7985e-05 - val_my_r2: 0.9931\n",
      "Epoch 959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1748e-04 - my_r2: 0.9103 - val_loss: 1.9762e-05 - val_my_r2: 0.9920\n",
      "Epoch 960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6228e-04 - my_r2: 0.9252 - val_loss: 2.6017e-05 - val_my_r2: 0.9906\n",
      "Epoch 961/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7559e-04 - my_r2: 0.9166 - val_loss: 2.2068e-05 - val_my_r2: 0.9919\n",
      "Epoch 962/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1020e-04 - my_r2: 0.9079 - val_loss: 2.9820e-05 - val_my_r2: 0.9887\n",
      "Epoch 963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1249e-04 - my_r2: 0.9417 - val_loss: 2.6837e-05 - val_my_r2: 0.9903\n",
      "Epoch 964/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8051e-04 - my_r2: 0.9126 - val_loss: 2.7509e-05 - val_my_r2: 0.9903\n",
      "Epoch 965/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2134e-04 - my_r2: 0.9098 - val_loss: 2.0209e-05 - val_my_r2: 0.9929\n",
      "Epoch 966/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9325e-04 - my_r2: 0.9453 - val_loss: 1.8332e-05 - val_my_r2: 0.9936\n",
      "Epoch 967/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1055e-04 - my_r2: 0.9388 - val_loss: 2.1286e-05 - val_my_r2: 0.9922\n",
      "Epoch 968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3744e-04 - my_r2: 0.9177 - val_loss: 1.7119e-05 - val_my_r2: 0.9936\n",
      "Epoch 969/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5511e-04 - my_r2: 0.9014 - val_loss: 1.6454e-05 - val_my_r2: 0.9940\n",
      "Epoch 970/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9394e-04 - my_r2: 0.9146 - val_loss: 2.1665e-05 - val_my_r2: 0.9922\n",
      "Epoch 971/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6619e-04 - my_r2: 0.8954 - val_loss: 2.3675e-05 - val_my_r2: 0.9913\n",
      "Epoch 972/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8984e-04 - my_r2: 0.9424 - val_loss: 1.7575e-05 - val_my_r2: 0.9936\n",
      "Epoch 973/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6183e-04 - my_r2: 0.9105 - val_loss: 1.9235e-05 - val_my_r2: 0.9927\n",
      "Epoch 974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6123e-04 - my_r2: 0.9415 - val_loss: 1.6004e-05 - val_my_r2: 0.9946\n",
      "Epoch 975/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7886e-04 - my_r2: 0.9286 - val_loss: 1.8139e-05 - val_my_r2: 0.9940\n",
      "Epoch 976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4698e-04 - my_r2: 0.9271 - val_loss: 1.6413e-05 - val_my_r2: 0.9945\n",
      "Epoch 977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5453e-04 - my_r2: 0.9098 - val_loss: 2.0334e-05 - val_my_r2: 0.9935\n",
      "Epoch 978/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7462e-04 - my_r2: 0.9083 - val_loss: 1.7613e-05 - val_my_r2: 0.9946\n",
      "Epoch 979/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9176e-04 - my_r2: 0.9364 - val_loss: 1.7866e-05 - val_my_r2: 0.9946\n",
      "Epoch 980/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5452e-04 - my_r2: 0.9341 - val_loss: 2.4596e-05 - val_my_r2: 0.9920\n",
      "Epoch 981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6999e-04 - my_r2: 0.9228 - val_loss: 2.8224e-05 - val_my_r2: 0.9902\n",
      "Epoch 982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1211e-04 - my_r2: 0.9073 - val_loss: 2.0947e-05 - val_my_r2: 0.9932\n",
      "Epoch 983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9793e-04 - my_r2: 0.9329 - val_loss: 2.1904e-05 - val_my_r2: 0.9931\n",
      "Epoch 984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3626e-04 - my_r2: 0.9224 - val_loss: 2.4044e-05 - val_my_r2: 0.9917\n",
      "Epoch 985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1131e-04 - my_r2: 0.8328 - val_loss: 1.9722e-05 - val_my_r2: 0.9929\n",
      "Epoch 986/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7103e-04 - my_r2: 0.9032 - val_loss: 2.3519e-05 - val_my_r2: 0.9917\n",
      "Epoch 987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3380e-04 - my_r2: 0.8943 - val_loss: 2.2480e-05 - val_my_r2: 0.9919\n",
      "Epoch 988/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9235e-04 - my_r2: 0.9330 - val_loss: 1.6110e-05 - val_my_r2: 0.9941\n",
      "Epoch 989/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.3147e-04 - my_r2: 0.9236 - val_loss: 1.5186e-05 - val_my_r2: 0.9949\n",
      "Epoch 990/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7966e-04 - my_r2: 0.9304 - val_loss: 1.5758e-05 - val_my_r2: 0.9946\n",
      "Epoch 991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8579e-04 - my_r2: 0.9091 - val_loss: 3.0872e-05 - val_my_r2: 0.9890\n",
      "Epoch 992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6785e-04 - my_r2: 0.9287 - val_loss: 1.8838e-05 - val_my_r2: 0.9933\n",
      "Epoch 993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1461e-04 - my_r2: 0.9319 - val_loss: 2.2620e-05 - val_my_r2: 0.9917\n",
      "Epoch 994/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5530e-04 - my_r2: 0.9078 - val_loss: 2.3106e-05 - val_my_r2: 0.9919\n",
      "Epoch 995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6924e-04 - my_r2: 0.9218 - val_loss: 2.2884e-05 - val_my_r2: 0.9921\n",
      "Epoch 996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4846e-04 - my_r2: 0.8742 - val_loss: 2.1388e-05 - val_my_r2: 0.9923\n",
      "Epoch 997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1100e-04 - my_r2: 0.9426 - val_loss: 1.8601e-05 - val_my_r2: 0.9932\n",
      "Epoch 998/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7554e-04 - my_r2: 0.9306 - val_loss: 2.0451e-05 - val_my_r2: 0.9928\n",
      "Epoch 999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5877e-04 - my_r2: 0.9391 - val_loss: 1.7590e-05 - val_my_r2: 0.9940\n",
      "Epoch 1000/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7586e-04 - my_r2: 0.9003 - val_loss: 1.5042e-05 - val_my_r2: 0.9949\n",
      "Epoch 1001/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5017e-04 - my_r2: 0.9148 - val_loss: 1.7140e-05 - val_my_r2: 0.9937\n",
      "Epoch 1002/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5218e-04 - my_r2: 0.9036 - val_loss: 1.7323e-05 - val_my_r2: 0.9938\n",
      "Epoch 1003/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9844e-04 - my_r2: 0.9083 - val_loss: 2.2066e-05 - val_my_r2: 0.9924\n",
      "Epoch 1004/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5745e-04 - my_r2: 0.4585 - val_loss: 2.1621e-05 - val_my_r2: 0.9926\n",
      "Epoch 1005/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1070e-04 - my_r2: 0.9259 - val_loss: 2.1573e-05 - val_my_r2: 0.9925\n",
      "Epoch 1006/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2430e-04 - my_r2: 0.9038 - val_loss: 1.7831e-05 - val_my_r2: 0.9937\n",
      "Epoch 1007/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4132e-04 - my_r2: 0.9273 - val_loss: 1.5952e-05 - val_my_r2: 0.9945\n",
      "Epoch 1008/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7295e-04 - my_r2: 0.9189 - val_loss: 1.5138e-05 - val_my_r2: 0.9948\n",
      "Epoch 1009/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6184e-04 - my_r2: 0.9418 - val_loss: 1.4584e-05 - val_my_r2: 0.9948\n",
      "Epoch 1010/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2534e-04 - my_r2: 0.9421 - val_loss: 1.5670e-05 - val_my_r2: 0.9943\n",
      "Epoch 1011/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6498e-04 - my_r2: 0.8975 - val_loss: 1.6662e-05 - val_my_r2: 0.9945\n",
      "Epoch 1012/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5220e-04 - my_r2: 0.9439 - val_loss: 1.6412e-05 - val_my_r2: 0.9945\n",
      "Epoch 1013/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3172e-04 - my_r2: 0.9218 - val_loss: 1.4242e-05 - val_my_r2: 0.9951\n",
      "Epoch 1014/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1479e-04 - my_r2: 0.9295 - val_loss: 1.7051e-05 - val_my_r2: 0.9940\n",
      "Epoch 1015/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8977e-04 - my_r2: 0.8897 - val_loss: 1.8020e-05 - val_my_r2: 0.9937\n",
      "Epoch 1016/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1089e-04 - my_r2: 0.9360 - val_loss: 1.7792e-05 - val_my_r2: 0.9936\n",
      "Epoch 1017/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9188e-04 - my_r2: 0.9234 - val_loss: 1.7868e-05 - val_my_r2: 0.9936\n",
      "Epoch 1018/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1834e-04 - my_r2: 0.8869 - val_loss: 1.6495e-05 - val_my_r2: 0.9941\n",
      "Epoch 1019/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4241e-04 - my_r2: 0.8973 - val_loss: 1.5656e-05 - val_my_r2: 0.9944\n",
      "Epoch 1020/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7349e-04 - my_r2: 0.9375 - val_loss: 2.0888e-05 - val_my_r2: 0.9929\n",
      "Epoch 1021/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2780e-04 - my_r2: 0.9298 - val_loss: 1.7544e-05 - val_my_r2: 0.9942\n",
      "Epoch 1022/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8390e-04 - my_r2: 0.8521 - val_loss: 1.8311e-05 - val_my_r2: 0.9940\n",
      "Epoch 1023/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3620e-04 - my_r2: 0.9486 - val_loss: 1.8888e-05 - val_my_r2: 0.9936\n",
      "Epoch 1024/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9757e-04 - my_r2: 0.9065 - val_loss: 2.2268e-05 - val_my_r2: 0.9922\n",
      "Epoch 1025/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3551e-04 - my_r2: 0.9190 - val_loss: 1.9648e-05 - val_my_r2: 0.9936\n",
      "Epoch 1026/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1146e-04 - my_r2: 0.9252 - val_loss: 1.7803e-05 - val_my_r2: 0.9941\n",
      "Epoch 1027/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5114e-04 - my_r2: 0.9206 - val_loss: 1.7576e-05 - val_my_r2: 0.9943\n",
      "Epoch 1028/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1579e-04 - my_r2: 0.9496 - val_loss: 1.6826e-05 - val_my_r2: 0.9945\n",
      "Epoch 1029/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9103e-04 - my_r2: 0.8757 - val_loss: 1.9267e-05 - val_my_r2: 0.9935\n",
      "Epoch 1030/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9297e-04 - my_r2: 0.9456 - val_loss: 1.7272e-05 - val_my_r2: 0.9942\n",
      "Epoch 1031/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8483e-04 - my_r2: 0.8908 - val_loss: 1.5183e-05 - val_my_r2: 0.9949\n",
      "Epoch 1032/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7726e-04 - my_r2: 0.8961 - val_loss: 1.5311e-05 - val_my_r2: 0.9948\n",
      "Epoch 1033/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4875e-04 - my_r2: 0.9275 - val_loss: 1.7439e-05 - val_my_r2: 0.9940\n",
      "Epoch 1034/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2124e-04 - my_r2: 0.9202 - val_loss: 1.6641e-05 - val_my_r2: 0.9941\n",
      "Epoch 1035/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6961e-04 - my_r2: 0.8606 - val_loss: 1.5748e-05 - val_my_r2: 0.9948\n",
      "Epoch 1036/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8206e-04 - my_r2: 0.9340 - val_loss: 1.4602e-05 - val_my_r2: 0.9951\n",
      "Epoch 1037/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3906e-04 - my_r2: 0.9424 - val_loss: 1.3181e-05 - val_my_r2: 0.9953\n",
      "Epoch 1038/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3506e-04 - my_r2: 0.9459 - val_loss: 1.3111e-05 - val_my_r2: 0.9955\n",
      "Epoch 1039/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0974e-04 - my_r2: 0.9391 - val_loss: 1.4119e-05 - val_my_r2: 0.9952\n",
      "Epoch 1040/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8321e-04 - my_r2: 0.9344 - val_loss: 1.4188e-05 - val_my_r2: 0.9947\n",
      "Epoch 1041/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2842e-04 - my_r2: 0.9288 - val_loss: 1.8885e-05 - val_my_r2: 0.9932\n",
      "Epoch 1042/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7273e-04 - my_r2: 0.9454 - val_loss: 1.6313e-05 - val_my_r2: 0.9941\n",
      "Epoch 1043/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.8015e-04 - my_r2: 0.8408 - val_loss: 2.3616e-05 - val_my_r2: 0.9922\n",
      "Epoch 1044/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1237e-04 - my_r2: 0.9072 - val_loss: 2.0278e-05 - val_my_r2: 0.9930\n",
      "Epoch 1045/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6500e-04 - my_r2: 0.9338 - val_loss: 1.9269e-05 - val_my_r2: 0.9936\n",
      "Epoch 1046/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0174e-04 - my_r2: 0.9097 - val_loss: 1.4469e-05 - val_my_r2: 0.9950\n",
      "Epoch 1047/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5706e-04 - my_r2: 0.9445 - val_loss: 1.5054e-05 - val_my_r2: 0.9948\n",
      "Epoch 1048/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2039e-04 - my_r2: 0.9228 - val_loss: 1.7301e-05 - val_my_r2: 0.9942\n",
      "Epoch 1049/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6836e-04 - my_r2: 0.9375 - val_loss: 1.7067e-05 - val_my_r2: 0.9945\n",
      "Epoch 1050/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6530e-04 - my_r2: 0.9417 - val_loss: 1.4871e-05 - val_my_r2: 0.9950\n",
      "Epoch 1051/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3918e-04 - my_r2: 0.9428 - val_loss: 1.3315e-05 - val_my_r2: 0.9952\n",
      "Epoch 1052/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5832e-04 - my_r2: 0.9446 - val_loss: 1.3558e-05 - val_my_r2: 0.9951\n",
      "Epoch 1053/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0453e-04 - my_r2: 0.9424 - val_loss: 1.2866e-05 - val_my_r2: 0.9950\n",
      "Epoch 1054/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2215e-04 - my_r2: 0.9393 - val_loss: 1.2546e-05 - val_my_r2: 0.9951\n",
      "Epoch 1055/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4679e-04 - my_r2: 0.9379 - val_loss: 1.3494e-05 - val_my_r2: 0.9952\n",
      "Epoch 1056/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4480e-04 - my_r2: 0.9243 - val_loss: 1.4544e-05 - val_my_r2: 0.9945\n",
      "Epoch 1057/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7411e-04 - my_r2: 0.9343 - val_loss: 1.4053e-05 - val_my_r2: 0.9946\n",
      "Epoch 1058/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1501e-04 - my_r2: 0.9223 - val_loss: 1.4410e-05 - val_my_r2: 0.9943\n",
      "Epoch 1059/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5223e-04 - my_r2: 0.9237 - val_loss: 1.3681e-05 - val_my_r2: 0.9948\n",
      "Epoch 1060/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3672e-04 - my_r2: 0.9325 - val_loss: 1.4709e-05 - val_my_r2: 0.9943\n",
      "Epoch 1061/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4228e-04 - my_r2: 0.9358 - val_loss: 1.2914e-05 - val_my_r2: 0.9953\n",
      "Epoch 1062/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5883e-04 - my_r2: 0.9233 - val_loss: 1.3803e-05 - val_my_r2: 0.9951\n",
      "Epoch 1063/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0577e-04 - my_r2: 0.9354 - val_loss: 1.7917e-05 - val_my_r2: 0.9942\n",
      "Epoch 1064/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3215e-04 - my_r2: 0.9326 - val_loss: 1.5345e-05 - val_my_r2: 0.9951\n",
      "Epoch 1065/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0471e-04 - my_r2: 0.9297 - val_loss: 1.1630e-05 - val_my_r2: 0.9959\n",
      "Epoch 1066/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1707e-04 - my_r2: 0.9286 - val_loss: 1.3262e-05 - val_my_r2: 0.9959\n",
      "Epoch 1067/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1113e-04 - my_r2: 0.9194 - val_loss: 1.7966e-05 - val_my_r2: 0.9946\n",
      "Epoch 1068/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4189e-04 - my_r2: 0.9499 - val_loss: 1.3715e-05 - val_my_r2: 0.9956\n",
      "Epoch 1069/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7781e-04 - my_r2: 0.8966 - val_loss: 1.5386e-05 - val_my_r2: 0.9950\n",
      "Epoch 1070/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4479e-04 - my_r2: 0.9235 - val_loss: 1.7409e-05 - val_my_r2: 0.9938\n",
      "Epoch 1071/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8113e-04 - my_r2: 0.8859 - val_loss: 1.8298e-05 - val_my_r2: 0.9931\n",
      "Epoch 1072/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3475e-04 - my_r2: 0.9381 - val_loss: 2.0609e-05 - val_my_r2: 0.9924\n",
      "Epoch 1073/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4732e-04 - my_r2: 0.9348 - val_loss: 1.5577e-05 - val_my_r2: 0.9947\n",
      "Epoch 1074/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4149e-04 - my_r2: 0.9262 - val_loss: 1.7410e-05 - val_my_r2: 0.9940\n",
      "Epoch 1075/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9428e-04 - my_r2: 0.9273 - val_loss: 1.5569e-05 - val_my_r2: 0.9947\n",
      "Epoch 1076/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5824e-04 - my_r2: 0.9222 - val_loss: 1.8105e-05 - val_my_r2: 0.9939\n",
      "Epoch 1077/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3158e-04 - my_r2: 0.8904 - val_loss: 1.7042e-05 - val_my_r2: 0.9940\n",
      "Epoch 1078/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1523e-04 - my_r2: 0.9247 - val_loss: 1.9267e-05 - val_my_r2: 0.9934\n",
      "Epoch 1079/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7259e-04 - my_r2: 0.9047 - val_loss: 2.1414e-05 - val_my_r2: 0.9928\n",
      "Epoch 1080/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3763e-04 - my_r2: 0.9163 - val_loss: 1.9623e-05 - val_my_r2: 0.9934\n",
      "Epoch 1081/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6002e-04 - my_r2: 0.9068 - val_loss: 2.4006e-05 - val_my_r2: 0.9919\n",
      "Epoch 1082/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7635e-04 - my_r2: 0.9135 - val_loss: 2.4423e-05 - val_my_r2: 0.9917\n",
      "Epoch 1083/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6602e-04 - my_r2: 0.9194 - val_loss: 1.8992e-05 - val_my_r2: 0.9938\n",
      "Epoch 1084/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2778e-04 - my_r2: 0.8970 - val_loss: 1.6241e-05 - val_my_r2: 0.9946\n",
      "Epoch 1085/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8776e-04 - my_r2: 0.8597 - val_loss: 1.5188e-05 - val_my_r2: 0.9945\n",
      "Epoch 1086/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7024e-04 - my_r2: 0.9311 - val_loss: 1.5648e-05 - val_my_r2: 0.9942\n",
      "Epoch 1087/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7194e-04 - my_r2: 0.9154 - val_loss: 1.6876e-05 - val_my_r2: 0.9945\n",
      "Epoch 1088/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6059e-04 - my_r2: 0.9315 - val_loss: 1.4554e-05 - val_my_r2: 0.9953\n",
      "Epoch 1089/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3896e-04 - my_r2: 0.9022 - val_loss: 1.4533e-05 - val_my_r2: 0.9951\n",
      "Epoch 1090/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7345e-04 - my_r2: 0.8873 - val_loss: 1.6120e-05 - val_my_r2: 0.9946\n",
      "Epoch 1091/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2433e-04 - my_r2: 0.8958 - val_loss: 1.6722e-05 - val_my_r2: 0.9943\n",
      "Epoch 1092/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4528e-04 - my_r2: 0.9393 - val_loss: 1.4778e-05 - val_my_r2: 0.9949\n",
      "Epoch 1093/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2106e-04 - my_r2: 0.9327 - val_loss: 1.4006e-05 - val_my_r2: 0.9951\n",
      "Epoch 1094/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2349e-04 - my_r2: 0.9342 - val_loss: 1.4250e-05 - val_my_r2: 0.9950\n",
      "Epoch 1095/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0055e-04 - my_r2: 0.9384 - val_loss: 1.2484e-05 - val_my_r2: 0.9954\n",
      "Epoch 1096/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8685e-04 - my_r2: 0.9409 - val_loss: 1.6996e-05 - val_my_r2: 0.9934\n",
      "Epoch 1097/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9412e-04 - my_r2: 0.9474 - val_loss: 1.3962e-05 - val_my_r2: 0.9945\n",
      "Epoch 1098/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9780e-04 - my_r2: 0.9227 - val_loss: 1.4257e-05 - val_my_r2: 0.9948\n",
      "Epoch 1099/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4996e-04 - my_r2: 0.9116 - val_loss: 1.4892e-05 - val_my_r2: 0.9946\n",
      "Epoch 1100/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6327e-04 - my_r2: 0.9212 - val_loss: 1.2103e-05 - val_my_r2: 0.9954\n",
      "Epoch 1101/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0858e-04 - my_r2: 0.9059 - val_loss: 1.3492e-05 - val_my_r2: 0.9950\n",
      "Epoch 1102/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9642e-04 - my_r2: 0.9331 - val_loss: 1.2346e-05 - val_my_r2: 0.9955\n",
      "Epoch 1103/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9167e-04 - my_r2: 0.9252 - val_loss: 1.2681e-05 - val_my_r2: 0.9953\n",
      "Epoch 1104/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6272e-04 - my_r2: 0.9336 - val_loss: 1.3960e-05 - val_my_r2: 0.9948\n",
      "Epoch 1105/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7691e-04 - my_r2: 0.9170 - val_loss: 1.5212e-05 - val_my_r2: 0.9942\n",
      "Epoch 1106/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9244e-04 - my_r2: 0.9091 - val_loss: 1.2409e-05 - val_my_r2: 0.9951\n",
      "Epoch 1107/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3704e-04 - my_r2: 0.9129 - val_loss: 1.8283e-05 - val_my_r2: 0.9937\n",
      "Epoch 1108/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5489e-04 - my_r2: 0.9292 - val_loss: 1.4971e-05 - val_my_r2: 0.9947\n",
      "Epoch 1109/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3623e-04 - my_r2: 0.8765 - val_loss: 1.1818e-05 - val_my_r2: 0.9956\n",
      "Epoch 1110/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7157e-04 - my_r2: 0.9361 - val_loss: 1.3951e-05 - val_my_r2: 0.9950\n",
      "Epoch 1111/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2337e-04 - my_r2: 0.9328 - val_loss: 1.4832e-05 - val_my_r2: 0.9946\n",
      "Epoch 1112/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6148e-04 - my_r2: 0.9368 - val_loss: 1.5748e-05 - val_my_r2: 0.9943\n",
      "Epoch 1113/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3932e-04 - my_r2: 0.9086 - val_loss: 1.4701e-05 - val_my_r2: 0.9946\n",
      "Epoch 1114/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6498e-04 - my_r2: 0.9431 - val_loss: 1.6186e-05 - val_my_r2: 0.9943\n",
      "Epoch 1115/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3275e-04 - my_r2: 0.9354 - val_loss: 1.5831e-05 - val_my_r2: 0.9942\n",
      "Epoch 1116/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1861e-04 - my_r2: 0.9439 - val_loss: 1.7115e-05 - val_my_r2: 0.9937\n",
      "Epoch 1117/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1547e-04 - my_r2: 0.9473 - val_loss: 1.6713e-05 - val_my_r2: 0.9942\n",
      "Epoch 1118/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3987e-04 - my_r2: 0.9523 - val_loss: 1.7939e-05 - val_my_r2: 0.9938\n",
      "Epoch 1119/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4160e-04 - my_r2: 0.9413 - val_loss: 1.5754e-05 - val_my_r2: 0.9945\n",
      "Epoch 1120/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6274e-04 - my_r2: 0.9291 - val_loss: 1.5269e-05 - val_my_r2: 0.9950\n",
      "Epoch 1121/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7484e-04 - my_r2: 0.9081 - val_loss: 1.3597e-05 - val_my_r2: 0.9954\n",
      "Epoch 1122/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8812e-04 - my_r2: 0.9341 - val_loss: 1.4621e-05 - val_my_r2: 0.9950\n",
      "Epoch 1123/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0208e-04 - my_r2: 0.9177 - val_loss: 1.3481e-05 - val_my_r2: 0.9953\n",
      "Epoch 1124/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8701e-04 - my_r2: 0.9283 - val_loss: 1.8806e-05 - val_my_r2: 0.9931\n",
      "Epoch 1125/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4745e-04 - my_r2: 0.9379 - val_loss: 2.9418e-05 - val_my_r2: 0.9894\n",
      "Epoch 1126/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3781e-04 - my_r2: 0.9394 - val_loss: 1.4473e-05 - val_my_r2: 0.9947\n",
      "Epoch 1127/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3951e-04 - my_r2: 0.9103 - val_loss: 1.2294e-05 - val_my_r2: 0.9956\n",
      "Epoch 1128/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6559e-04 - my_r2: 0.9426 - val_loss: 1.3454e-05 - val_my_r2: 0.9953\n",
      "Epoch 1129/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2153e-04 - my_r2: 0.9491 - val_loss: 1.3746e-05 - val_my_r2: 0.9953\n",
      "Epoch 1130/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0929e-04 - my_r2: 0.9428 - val_loss: 1.3557e-05 - val_my_r2: 0.9953\n",
      "Epoch 1131/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7830e-04 - my_r2: 0.9132 - val_loss: 1.2628e-05 - val_my_r2: 0.9956\n",
      "Epoch 1132/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3513e-04 - my_r2: 0.9379 - val_loss: 1.3706e-05 - val_my_r2: 0.9950\n",
      "Epoch 1133/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5765e-04 - my_r2: 0.9478 - val_loss: 1.4833e-05 - val_my_r2: 0.9945\n",
      "Epoch 1134/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1624e-04 - my_r2: 0.9141 - val_loss: 1.5140e-05 - val_my_r2: 0.9946\n",
      "Epoch 1135/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2102e-04 - my_r2: 0.9378 - val_loss: 1.4534e-05 - val_my_r2: 0.9951\n",
      "Epoch 1136/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6062e-04 - my_r2: 0.9466 - val_loss: 1.1687e-05 - val_my_r2: 0.9960\n",
      "Epoch 1137/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3985e-04 - my_r2: 0.9171 - val_loss: 1.0884e-05 - val_my_r2: 0.9960\n",
      "Epoch 1138/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7104e-04 - my_r2: 0.9319 - val_loss: 1.2961e-05 - val_my_r2: 0.9953\n",
      "Epoch 1139/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3725e-04 - my_r2: 0.9439 - val_loss: 1.4465e-05 - val_my_r2: 0.9947\n",
      "Epoch 1140/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4339e-04 - my_r2: 0.9400 - val_loss: 1.3844e-05 - val_my_r2: 0.9948\n",
      "Epoch 1141/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2174e-04 - my_r2: 0.9096 - val_loss: 1.4729e-05 - val_my_r2: 0.9946\n",
      "Epoch 1142/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5713e-04 - my_r2: 0.9479 - val_loss: 1.7152e-05 - val_my_r2: 0.9941\n",
      "Epoch 1143/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4149e-04 - my_r2: 0.9065 - val_loss: 2.0035e-05 - val_my_r2: 0.9933\n",
      "Epoch 1144/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1774e-04 - my_r2: 0.9130 - val_loss: 1.8104e-05 - val_my_r2: 0.9941\n",
      "Epoch 1145/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6037e-04 - my_r2: 0.9150 - val_loss: 1.9370e-05 - val_my_r2: 0.9933\n",
      "Epoch 1146/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4657e-04 - my_r2: 0.9437 - val_loss: 2.4882e-05 - val_my_r2: 0.9915\n",
      "Epoch 1147/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3604e-04 - my_r2: 0.9247 - val_loss: 2.3009e-05 - val_my_r2: 0.9917\n",
      "Epoch 1148/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0723e-04 - my_r2: 0.8881 - val_loss: 2.0618e-05 - val_my_r2: 0.9928\n",
      "Epoch 1149/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7484e-04 - my_r2: 0.9355 - val_loss: 1.5251e-05 - val_my_r2: 0.9947\n",
      "Epoch 1150/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6263e-04 - my_r2: 0.9268 - val_loss: 1.3333e-05 - val_my_r2: 0.9950\n",
      "Epoch 1151/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4364e-04 - my_r2: 0.9341 - val_loss: 1.3491e-05 - val_my_r2: 0.9952\n",
      "Epoch 1152/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0407e-04 - my_r2: 0.9366 - val_loss: 1.6424e-05 - val_my_r2: 0.9942\n",
      "Epoch 1153/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7268e-04 - my_r2: 0.9468 - val_loss: 1.4805e-05 - val_my_r2: 0.9947\n",
      "Epoch 1154/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8503e-04 - my_r2: 0.9240 - val_loss: 1.2867e-05 - val_my_r2: 0.9955\n",
      "Epoch 1155/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3800e-04 - my_r2: 0.8810 - val_loss: 1.3410e-05 - val_my_r2: 0.9951\n",
      "Epoch 1156/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4745e-04 - my_r2: 0.9302 - val_loss: 1.5293e-05 - val_my_r2: 0.9944\n",
      "Epoch 1157/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4310e-04 - my_r2: 0.9049 - val_loss: 1.4426e-05 - val_my_r2: 0.9948\n",
      "Epoch 1158/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1625e-04 - my_r2: 0.8952 - val_loss: 1.5902e-05 - val_my_r2: 0.9944\n",
      "Epoch 1159/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3130e-04 - my_r2: 0.9378 - val_loss: 1.3390e-05 - val_my_r2: 0.9953\n",
      "Epoch 1160/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8410e-04 - my_r2: 0.9224 - val_loss: 1.2172e-05 - val_my_r2: 0.9958\n",
      "Epoch 1161/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1220e-04 - my_r2: 0.8954 - val_loss: 1.3273e-05 - val_my_r2: 0.9949\n",
      "Epoch 1162/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7530e-04 - my_r2: 0.8705 - val_loss: 1.6209e-05 - val_my_r2: 0.9938\n",
      "Epoch 1163/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0144e-04 - my_r2: 0.9188 - val_loss: 1.4789e-05 - val_my_r2: 0.9946\n",
      "Epoch 1164/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0617e-04 - my_r2: 0.9556 - val_loss: 1.3783e-05 - val_my_r2: 0.9951\n",
      "Epoch 1165/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2550e-04 - my_r2: 0.9469 - val_loss: 2.5129e-05 - val_my_r2: 0.9913\n",
      "Epoch 1166/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2827e-04 - my_r2: 0.9517 - val_loss: 1.3662e-05 - val_my_r2: 0.9954\n",
      "Epoch 1167/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2968e-04 - my_r2: 0.9221 - val_loss: 1.2839e-05 - val_my_r2: 0.9956\n",
      "Epoch 1168/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3937e-04 - my_r2: 0.9390 - val_loss: 1.2487e-05 - val_my_r2: 0.9957\n",
      "Epoch 1169/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5728e-04 - my_r2: 0.8987 - val_loss: 1.1868e-05 - val_my_r2: 0.9958\n",
      "Epoch 1170/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3272e-04 - my_r2: 0.9266 - val_loss: 1.7522e-05 - val_my_r2: 0.9938\n",
      "Epoch 1171/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9677e-04 - my_r2: 0.9149 - val_loss: 1.4423e-05 - val_my_r2: 0.9947\n",
      "Epoch 1172/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0136e-04 - my_r2: 0.9567 - val_loss: 1.5796e-05 - val_my_r2: 0.9939\n",
      "Epoch 1173/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0197e-04 - my_r2: 0.9605 - val_loss: 1.5347e-05 - val_my_r2: 0.9940\n",
      "Epoch 1174/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8035e-04 - my_r2: 0.8982 - val_loss: 1.7561e-05 - val_my_r2: 0.9935\n",
      "Epoch 1175/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5565e-04 - my_r2: 0.9115 - val_loss: 1.8468e-05 - val_my_r2: 0.9929\n",
      "Epoch 1176/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4902e-04 - my_r2: 0.9427 - val_loss: 1.5044e-05 - val_my_r2: 0.9939\n",
      "Epoch 1177/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6372e-04 - my_r2: 0.9302 - val_loss: 1.5124e-05 - val_my_r2: 0.9942\n",
      "Epoch 1178/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0293e-04 - my_r2: 0.9337 - val_loss: 1.2434e-05 - val_my_r2: 0.9949\n",
      "Epoch 1179/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1419e-04 - my_r2: 0.9422 - val_loss: 1.3604e-05 - val_my_r2: 0.9942\n",
      "Epoch 1180/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0070e-04 - my_r2: 0.9376 - val_loss: 1.5429e-05 - val_my_r2: 0.9940\n",
      "Epoch 1181/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0119e-04 - my_r2: 0.9011 - val_loss: 1.9120e-05 - val_my_r2: 0.9928\n",
      "Epoch 1182/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4993e-04 - my_r2: 0.9252 - val_loss: 1.5027e-05 - val_my_r2: 0.9945\n",
      "Epoch 1183/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6746e-04 - my_r2: 0.9321 - val_loss: 1.6157e-05 - val_my_r2: 0.9944\n",
      "Epoch 1184/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3238e-04 - my_r2: 0.9323 - val_loss: 2.5595e-05 - val_my_r2: 0.9917\n",
      "Epoch 1185/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1460e-04 - my_r2: 0.9029 - val_loss: 1.3517e-05 - val_my_r2: 0.9956\n",
      "Epoch 1186/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3054e-04 - my_r2: 0.9203 - val_loss: 1.2622e-05 - val_my_r2: 0.9958\n",
      "Epoch 1187/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5789e-04 - my_r2: 0.9332 - val_loss: 1.2152e-05 - val_my_r2: 0.9958\n",
      "Epoch 1188/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4898e-04 - my_r2: 0.8651 - val_loss: 1.3605e-05 - val_my_r2: 0.9954\n",
      "Epoch 1189/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6271e-04 - my_r2: 0.9439 - val_loss: 1.6955e-05 - val_my_r2: 0.9938\n",
      "Epoch 1190/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5516e-04 - my_r2: 0.9073 - val_loss: 1.1554e-05 - val_my_r2: 0.9961\n",
      "Epoch 1191/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4756e-04 - my_r2: 0.8500 - val_loss: 1.0618e-05 - val_my_r2: 0.9964\n",
      "Epoch 1192/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5709e-04 - my_r2: 0.9035 - val_loss: 1.3317e-05 - val_my_r2: 0.9955\n",
      "Epoch 1193/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2426e-04 - my_r2: 0.9460 - val_loss: 1.1993e-05 - val_my_r2: 0.9959\n",
      "Epoch 1194/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3302e-04 - my_r2: 0.9364 - val_loss: 1.2033e-05 - val_my_r2: 0.9960\n",
      "Epoch 1195/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5238e-04 - my_r2: 0.9475 - val_loss: 1.4967e-05 - val_my_r2: 0.9949\n",
      "Epoch 1196/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8916e-04 - my_r2: 0.9139 - val_loss: 1.4223e-05 - val_my_r2: 0.9948\n",
      "Epoch 1197/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.7959e-04 - my_r2: 0.8675 - val_loss: 1.0904e-05 - val_my_r2: 0.9961\n",
      "Epoch 1198/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5885e-04 - my_r2: 0.9264 - val_loss: 1.1368e-05 - val_my_r2: 0.9960\n",
      "Epoch 1199/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7049e-04 - my_r2: 0.9359 - val_loss: 1.2627e-05 - val_my_r2: 0.9957\n",
      "Epoch 1200/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8568e-04 - my_r2: 0.9219 - val_loss: 1.5952e-05 - val_my_r2: 0.9946\n",
      "Epoch 1201/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4246e-04 - my_r2: 0.9270 - val_loss: 1.4049e-05 - val_my_r2: 0.9954\n",
      "Epoch 1202/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7979e-04 - my_r2: 0.9176 - val_loss: 1.2375e-05 - val_my_r2: 0.9958\n",
      "Epoch 1203/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8732e-04 - my_r2: 0.8446 - val_loss: 1.1069e-05 - val_my_r2: 0.9964\n",
      "Epoch 1204/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0896e-04 - my_r2: 0.9347 - val_loss: 1.4705e-05 - val_my_r2: 0.9952\n",
      "Epoch 1205/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.4612e-04 - my_r2: 0.8922 - val_loss: 1.2424e-05 - val_my_r2: 0.9958\n",
      "Epoch 1206/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9661e-04 - my_r2: 0.9348 - val_loss: 1.7167e-05 - val_my_r2: 0.9941\n",
      "Epoch 1207/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7904e-04 - my_r2: 0.9123 - val_loss: 2.4945e-05 - val_my_r2: 0.9911\n",
      "Epoch 1208/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0539e-04 - my_r2: 0.8986 - val_loss: 1.5413e-05 - val_my_r2: 0.9952\n",
      "Epoch 1209/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8687e-04 - my_r2: 0.9224 - val_loss: 1.3004e-05 - val_my_r2: 0.9960\n",
      "Epoch 1210/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7755e-04 - my_r2: 0.8620 - val_loss: 1.3004e-05 - val_my_r2: 0.9957\n",
      "Epoch 1211/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2825e-04 - my_r2: 0.8743 - val_loss: 1.2161e-05 - val_my_r2: 0.9955\n",
      "Epoch 1212/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5237e-04 - my_r2: 0.9248 - val_loss: 1.2437e-05 - val_my_r2: 0.9958\n",
      "Epoch 1213/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0973e-04 - my_r2: 0.9592 - val_loss: 1.3836e-05 - val_my_r2: 0.9955\n",
      "Epoch 1214/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3977e-04 - my_r2: 0.9329 - val_loss: 1.7015e-05 - val_my_r2: 0.9943\n",
      "Epoch 1215/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4149e-04 - my_r2: 0.9292 - val_loss: 1.8855e-05 - val_my_r2: 0.9933\n",
      "Epoch 1216/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2747e-04 - my_r2: 0.9093 - val_loss: 1.5738e-05 - val_my_r2: 0.9945\n",
      "Epoch 1217/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5540e-04 - my_r2: 0.9264 - val_loss: 1.5083e-05 - val_my_r2: 0.9947\n",
      "Epoch 1218/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4597e-04 - my_r2: 0.9413 - val_loss: 1.4022e-05 - val_my_r2: 0.9949\n",
      "Epoch 1219/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7845e-04 - my_r2: 0.9176 - val_loss: 1.1730e-05 - val_my_r2: 0.9959\n",
      "Epoch 1220/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2016e-04 - my_r2: 0.9144 - val_loss: 1.3422e-05 - val_my_r2: 0.9953\n",
      "Epoch 1221/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2351e-04 - my_r2: 0.9456 - val_loss: 1.6249e-05 - val_my_r2: 0.9939\n",
      "Epoch 1222/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3821e-04 - my_r2: 0.9462 - val_loss: 1.1869e-05 - val_my_r2: 0.9957\n",
      "Epoch 1223/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3433e-04 - my_r2: 0.9589 - val_loss: 1.1146e-05 - val_my_r2: 0.9960\n",
      "Epoch 1224/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0542e-04 - my_r2: 0.9057 - val_loss: 1.2739e-05 - val_my_r2: 0.9956\n",
      "Epoch 1225/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4502e-04 - my_r2: 0.9377 - val_loss: 1.1012e-05 - val_my_r2: 0.9964\n",
      "Epoch 1226/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9342e-04 - my_r2: 0.9128 - val_loss: 1.0518e-05 - val_my_r2: 0.9966\n",
      "Epoch 1227/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8494e-04 - my_r2: 0.9383 - val_loss: 1.2138e-05 - val_my_r2: 0.9960\n",
      "Epoch 1228/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3331e-04 - my_r2: 0.8943 - val_loss: 1.3221e-05 - val_my_r2: 0.9953\n",
      "Epoch 1229/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4040e-04 - my_r2: 0.9002 - val_loss: 1.2255e-05 - val_my_r2: 0.9959\n",
      "Epoch 1230/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6431e-04 - my_r2: 0.9353 - val_loss: 1.3238e-05 - val_my_r2: 0.9957\n",
      "Epoch 1231/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1916e-04 - my_r2: 0.9603 - val_loss: 1.2515e-05 - val_my_r2: 0.9959\n",
      "Epoch 1232/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9327e-04 - my_r2: 0.9256 - val_loss: 1.8312e-05 - val_my_r2: 0.9945\n",
      "Epoch 1233/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5163e-04 - my_r2: 0.9087 - val_loss: 1.8611e-05 - val_my_r2: 0.9943\n",
      "Epoch 1234/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6654e-04 - my_r2: 0.9291 - val_loss: 2.4894e-05 - val_my_r2: 0.9918\n",
      "Epoch 1235/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8783e-04 - my_r2: 0.8992 - val_loss: 1.4752e-05 - val_my_r2: 0.9952\n",
      "Epoch 1236/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6618e-04 - my_r2: 0.9231 - val_loss: 1.7766e-05 - val_my_r2: 0.9941\n",
      "Epoch 1237/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8931e-04 - my_r2: 0.9164 - val_loss: 1.6194e-05 - val_my_r2: 0.9945\n",
      "Epoch 1238/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5867e-04 - my_r2: 0.9162 - val_loss: 1.5374e-05 - val_my_r2: 0.9951\n",
      "Epoch 1239/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0787e-04 - my_r2: 0.9579 - val_loss: 2.0589e-05 - val_my_r2: 0.9935\n",
      "Epoch 1240/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5409e-04 - my_r2: 0.8472 - val_loss: 1.2152e-05 - val_my_r2: 0.9959\n",
      "Epoch 1241/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8128e-04 - my_r2: 0.9275 - val_loss: 1.9379e-05 - val_my_r2: 0.9929\n",
      "Epoch 1242/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3990e-04 - my_r2: 0.9460 - val_loss: 1.7876e-05 - val_my_r2: 0.9938\n",
      "Epoch 1243/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5030e-04 - my_r2: 0.9034 - val_loss: 1.3892e-05 - val_my_r2: 0.9957\n",
      "Epoch 1244/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8481e-04 - my_r2: 0.9355 - val_loss: 1.4826e-05 - val_my_r2: 0.9952\n",
      "Epoch 1245/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3597e-04 - my_r2: 0.9236 - val_loss: 1.2441e-05 - val_my_r2: 0.9957\n",
      "Epoch 1246/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3155e-04 - my_r2: 0.9470 - val_loss: 1.4259e-05 - val_my_r2: 0.9950\n",
      "Epoch 1247/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6735e-04 - my_r2: 0.9405 - val_loss: 1.7176e-05 - val_my_r2: 0.9941\n",
      "Epoch 1248/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5656e-04 - my_r2: 0.9392 - val_loss: 1.2150e-05 - val_my_r2: 0.9959\n",
      "Epoch 1249/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5848e-04 - my_r2: 0.9454 - val_loss: 1.5290e-05 - val_my_r2: 0.9948\n",
      "Epoch 1250/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6176e-04 - my_r2: 0.9053 - val_loss: 1.5469e-05 - val_my_r2: 0.9946\n",
      "Epoch 1251/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2269e-04 - my_r2: 0.9130 - val_loss: 1.2942e-05 - val_my_r2: 0.9954\n",
      "Epoch 1252/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3952e-04 - my_r2: 0.9332 - val_loss: 1.5992e-05 - val_my_r2: 0.9944\n",
      "Epoch 1253/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4535e-04 - my_r2: 0.9173 - val_loss: 1.3155e-05 - val_my_r2: 0.9954\n",
      "Epoch 1254/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3600e-04 - my_r2: 0.9364 - val_loss: 1.1331e-05 - val_my_r2: 0.9957\n",
      "Epoch 1255/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4640e-04 - my_r2: 0.9088 - val_loss: 1.1067e-05 - val_my_r2: 0.9957\n",
      "Epoch 1256/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.4951e-04 - my_r2: 0.9275 - val_loss: 1.1542e-05 - val_my_r2: 0.9956\n",
      "Epoch 1257/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4343e-04 - my_r2: 0.8912 - val_loss: 1.1531e-05 - val_my_r2: 0.9959\n",
      "Epoch 1258/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7475e-04 - my_r2: 0.9379 - val_loss: 1.2756e-05 - val_my_r2: 0.9954\n",
      "Epoch 1259/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2377e-04 - my_r2: 0.8437 - val_loss: 1.0205e-05 - val_my_r2: 0.9964\n",
      "Epoch 1260/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6303e-04 - my_r2: 0.9361 - val_loss: 9.4717e-06 - val_my_r2: 0.9968\n",
      "Epoch 1261/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3569e-04 - my_r2: 0.9386 - val_loss: 1.1087e-05 - val_my_r2: 0.9963\n",
      "Epoch 1262/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0052e-04 - my_r2: 0.9134 - val_loss: 1.1727e-05 - val_my_r2: 0.9958\n",
      "Epoch 1263/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3229e-04 - my_r2: 0.9159 - val_loss: 1.4633e-05 - val_my_r2: 0.9946\n",
      "Epoch 1264/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1129e-04 - my_r2: 0.9119 - val_loss: 2.3600e-05 - val_my_r2: 0.9910\n",
      "Epoch 1265/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9617e-04 - my_r2: 0.9259 - val_loss: 1.0686e-05 - val_my_r2: 0.9961\n",
      "Epoch 1266/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9113e-04 - my_r2: 0.9144 - val_loss: 1.5189e-05 - val_my_r2: 0.9949\n",
      "Epoch 1267/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5103e-04 - my_r2: 0.9276 - val_loss: 1.3463e-05 - val_my_r2: 0.9957\n",
      "Epoch 1268/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8968e-04 - my_r2: 0.5269 - val_loss: 1.2828e-05 - val_my_r2: 0.9960\n",
      "Epoch 1269/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4353e-04 - my_r2: 0.9472 - val_loss: 1.3723e-05 - val_my_r2: 0.9956\n",
      "Epoch 1270/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2381e-04 - my_r2: 0.9489 - val_loss: 1.6061e-05 - val_my_r2: 0.9947\n",
      "Epoch 1271/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3503e-04 - my_r2: 0.9426 - val_loss: 1.8513e-05 - val_my_r2: 0.9941\n",
      "Epoch 1272/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0723e-04 - my_r2: 0.7209 - val_loss: 1.9462e-05 - val_my_r2: 0.9941\n",
      "Epoch 1273/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7748e-04 - my_r2: 0.9043 - val_loss: 1.5584e-05 - val_my_r2: 0.9949\n",
      "Epoch 1274/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.2437e-04 - my_r2: 0.9003 - val_loss: 1.7636e-05 - val_my_r2: 0.9938\n",
      "Epoch 1275/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1725e-04 - my_r2: 0.9354 - val_loss: 1.3416e-05 - val_my_r2: 0.9953\n",
      "Epoch 1276/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2800e-04 - my_r2: 0.9411 - val_loss: 1.4116e-05 - val_my_r2: 0.9953\n",
      "Epoch 1277/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6322e-04 - my_r2: 0.9239 - val_loss: 1.4654e-05 - val_my_r2: 0.9948\n",
      "Epoch 1278/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2279e-04 - my_r2: 0.8955 - val_loss: 1.7099e-05 - val_my_r2: 0.9940\n",
      "Epoch 1279/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9158e-04 - my_r2: 0.9285 - val_loss: 2.6055e-05 - val_my_r2: 0.9913\n",
      "Epoch 1280/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8472e-04 - my_r2: 0.9343 - val_loss: 1.0235e-05 - val_my_r2: 0.9966\n",
      "Epoch 1281/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1045e-04 - my_r2: 0.9288 - val_loss: 1.4534e-05 - val_my_r2: 0.9947\n",
      "Epoch 1282/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6101e-04 - my_r2: 0.9378 - val_loss: 1.2708e-05 - val_my_r2: 0.9956\n",
      "Epoch 1283/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6213e-04 - my_r2: 0.9431 - val_loss: 1.2471e-05 - val_my_r2: 0.9956\n",
      "Epoch 1284/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5491e-04 - my_r2: 0.9057 - val_loss: 1.3904e-05 - val_my_r2: 0.9956\n",
      "Epoch 1285/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4472e-04 - my_r2: 0.9335 - val_loss: 1.3734e-05 - val_my_r2: 0.9958\n",
      "Epoch 1286/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0650e-04 - my_r2: 0.9316 - val_loss: 1.4277e-05 - val_my_r2: 0.9950\n",
      "Epoch 1287/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2049e-04 - my_r2: 0.9070 - val_loss: 1.2940e-05 - val_my_r2: 0.9957\n",
      "Epoch 1288/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8270e-04 - my_r2: 0.8961 - val_loss: 1.4663e-05 - val_my_r2: 0.9949\n",
      "Epoch 1289/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3125e-04 - my_r2: 0.9251 - val_loss: 1.1393e-05 - val_my_r2: 0.9966\n",
      "Epoch 1290/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6259e-04 - my_r2: 0.9456 - val_loss: 9.9766e-06 - val_my_r2: 0.9966\n",
      "Epoch 1291/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2693e-04 - my_r2: 0.9308 - val_loss: 1.1401e-05 - val_my_r2: 0.9959\n",
      "Epoch 1292/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1670e-04 - my_r2: 0.9531 - val_loss: 1.2495e-05 - val_my_r2: 0.9954\n",
      "Epoch 1293/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3803e-04 - my_r2: 0.9051 - val_loss: 1.3898e-05 - val_my_r2: 0.9954\n",
      "Epoch 1294/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6113e-04 - my_r2: 0.9457 - val_loss: 1.1741e-05 - val_my_r2: 0.9961\n",
      "Epoch 1295/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4125e-04 - my_r2: 0.9031 - val_loss: 1.3265e-05 - val_my_r2: 0.9954\n",
      "Epoch 1296/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0088e-04 - my_r2: 0.9433 - val_loss: 1.4175e-05 - val_my_r2: 0.9947\n",
      "Epoch 1297/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4919e-04 - my_r2: 0.9277 - val_loss: 1.4315e-05 - val_my_r2: 0.9945\n",
      "Epoch 1298/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4461e-04 - my_r2: 0.9306 - val_loss: 1.3676e-05 - val_my_r2: 0.9947\n",
      "Epoch 1299/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4742e-04 - my_r2: 0.9252 - val_loss: 1.6633e-05 - val_my_r2: 0.9947\n",
      "Epoch 1300/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4400e-04 - my_r2: 0.9241 - val_loss: 1.6615e-05 - val_my_r2: 0.9947\n",
      "Epoch 1301/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9361e-04 - my_r2: 0.9516 - val_loss: 1.5759e-05 - val_my_r2: 0.9948\n",
      "Epoch 1302/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8048e-04 - my_r2: 0.9095 - val_loss: 1.4665e-05 - val_my_r2: 0.9947\n",
      "Epoch 1303/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7671e-04 - my_r2: 0.9370 - val_loss: 1.4546e-05 - val_my_r2: 0.9944\n",
      "Epoch 1304/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6098e-04 - my_r2: 0.9299 - val_loss: 1.1778e-05 - val_my_r2: 0.9956\n",
      "Epoch 1305/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2884e-04 - my_r2: 0.9538 - val_loss: 1.0884e-05 - val_my_r2: 0.9960\n",
      "Epoch 1306/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8818e-04 - my_r2: 0.9060 - val_loss: 1.0229e-05 - val_my_r2: 0.9962\n",
      "Epoch 1307/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4594e-04 - my_r2: 0.9122 - val_loss: 1.3911e-05 - val_my_r2: 0.9950\n",
      "Epoch 1308/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3911e-04 - my_r2: 0.9347 - val_loss: 1.1877e-05 - val_my_r2: 0.9959\n",
      "Epoch 1309/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9873e-04 - my_r2: 0.8752 - val_loss: 1.3919e-05 - val_my_r2: 0.9948\n",
      "Epoch 1310/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2217e-04 - my_r2: 0.9006 - val_loss: 1.0854e-05 - val_my_r2: 0.9964\n",
      "Epoch 1311/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4506e-04 - my_r2: 0.9286 - val_loss: 1.0846e-05 - val_my_r2: 0.9961\n",
      "Epoch 1312/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8396e-04 - my_r2: 0.5747 - val_loss: 1.1086e-05 - val_my_r2: 0.9955\n",
      "Epoch 1313/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5774e-04 - my_r2: 0.9295 - val_loss: 1.7467e-05 - val_my_r2: 0.9932\n",
      "Epoch 1314/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1711e-04 - my_r2: 0.9239 - val_loss: 1.4348e-05 - val_my_r2: 0.9948\n",
      "Epoch 1315/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5347e-04 - my_r2: 0.9239 - val_loss: 1.5458e-05 - val_my_r2: 0.9950\n",
      "Epoch 1316/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7681e-04 - my_r2: 0.9257 - val_loss: 1.2032e-05 - val_my_r2: 0.9959\n",
      "Epoch 1317/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8641e-04 - my_r2: 0.9545 - val_loss: 1.0788e-05 - val_my_r2: 0.9960\n",
      "Epoch 1318/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5675e-04 - my_r2: 0.9374 - val_loss: 1.0611e-05 - val_my_r2: 0.9960\n",
      "Epoch 1319/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1475e-04 - my_r2: 0.9208 - val_loss: 1.7568e-05 - val_my_r2: 0.9936\n",
      "Epoch 1320/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9059e-04 - my_r2: 0.9509 - val_loss: 1.8991e-05 - val_my_r2: 0.9936\n",
      "Epoch 1321/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0123e-04 - my_r2: 0.9167 - val_loss: 1.2433e-05 - val_my_r2: 0.9961\n",
      "Epoch 1322/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7125e-04 - my_r2: 0.9226 - val_loss: 1.1581e-05 - val_my_r2: 0.9962\n",
      "Epoch 1323/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1084e-04 - my_r2: 0.9281 - val_loss: 1.0375e-05 - val_my_r2: 0.9963\n",
      "Epoch 1324/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6594e-04 - my_r2: 0.9311 - val_loss: 1.1045e-05 - val_my_r2: 0.9960\n",
      "Epoch 1325/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3849e-04 - my_r2: 0.9062 - val_loss: 1.0994e-05 - val_my_r2: 0.9961\n",
      "Epoch 1326/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3881e-04 - my_r2: 0.9195 - val_loss: 8.6178e-06 - val_my_r2: 0.9968\n",
      "Epoch 1327/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1139e-04 - my_r2: 0.9394 - val_loss: 1.1570e-05 - val_my_r2: 0.9961\n",
      "Epoch 1328/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1074e-04 - my_r2: 0.9403 - val_loss: 1.2261e-05 - val_my_r2: 0.9960\n",
      "Epoch 1329/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9519e-04 - my_r2: 0.8652 - val_loss: 1.3730e-05 - val_my_r2: 0.9958\n",
      "Epoch 1330/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3503e-04 - my_r2: 0.9351 - val_loss: 2.1010e-05 - val_my_r2: 0.9936\n",
      "Epoch 1331/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0374e-04 - my_r2: 0.9293 - val_loss: 2.2272e-05 - val_my_r2: 0.9923\n",
      "Epoch 1332/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6284e-04 - my_r2: 0.9041 - val_loss: 1.4869e-05 - val_my_r2: 0.9944\n",
      "Epoch 1333/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8604e-04 - my_r2: 0.9278 - val_loss: 1.5218e-05 - val_my_r2: 0.9947\n",
      "Epoch 1334/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4170e-04 - my_r2: 0.9380 - val_loss: 1.4741e-05 - val_my_r2: 0.9950\n",
      "Epoch 1335/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8758e-04 - my_r2: 0.9385 - val_loss: 1.3945e-05 - val_my_r2: 0.9950\n",
      "Epoch 1336/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7751e-04 - my_r2: 0.9430 - val_loss: 1.0352e-05 - val_my_r2: 0.9957\n",
      "Epoch 1337/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5934e-04 - my_r2: 0.9377 - val_loss: 1.3920e-05 - val_my_r2: 0.9948\n",
      "Epoch 1338/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4691e-04 - my_r2: 0.8988 - val_loss: 1.1618e-05 - val_my_r2: 0.9957\n",
      "Epoch 1339/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0998e-04 - my_r2: 0.9570 - val_loss: 1.0939e-05 - val_my_r2: 0.9959\n",
      "Epoch 1340/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2462e-04 - my_r2: 0.9306 - val_loss: 9.6542e-06 - val_my_r2: 0.9961\n",
      "Epoch 1341/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2787e-04 - my_r2: 0.9454 - val_loss: 1.0561e-05 - val_my_r2: 0.9956\n",
      "Epoch 1342/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8789e-04 - my_r2: 0.9588 - val_loss: 1.1222e-05 - val_my_r2: 0.9955\n",
      "Epoch 1343/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7871e-04 - my_r2: 0.9283 - val_loss: 1.3746e-05 - val_my_r2: 0.9949\n",
      "Epoch 1344/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5579e-04 - my_r2: 0.8662 - val_loss: 1.4909e-05 - val_my_r2: 0.9946\n",
      "Epoch 1345/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2515e-04 - my_r2: 0.9421 - val_loss: 1.2164e-05 - val_my_r2: 0.9959\n",
      "Epoch 1346/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3490e-04 - my_r2: 0.9331 - val_loss: 1.2182e-05 - val_my_r2: 0.9962\n",
      "Epoch 1347/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3904e-04 - my_r2: 0.9324 - val_loss: 1.4579e-05 - val_my_r2: 0.9949\n",
      "Epoch 1348/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4696e-04 - my_r2: 0.9428 - val_loss: 1.4787e-05 - val_my_r2: 0.9949\n",
      "Epoch 1349/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7573e-04 - my_r2: 0.9245 - val_loss: 1.3533e-05 - val_my_r2: 0.9950\n",
      "Epoch 1350/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0281e-04 - my_r2: 0.9558 - val_loss: 1.2573e-05 - val_my_r2: 0.9949\n",
      "Epoch 1351/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1608e-04 - my_r2: 0.9300 - val_loss: 1.3197e-05 - val_my_r2: 0.9948\n",
      "Epoch 1352/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6439e-04 - my_r2: 0.9446 - val_loss: 1.8997e-05 - val_my_r2: 0.9928\n",
      "Epoch 1353/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5802e-04 - my_r2: 0.9414 - val_loss: 1.5487e-05 - val_my_r2: 0.9942\n",
      "Epoch 1354/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8756e-04 - my_r2: 0.9151 - val_loss: 1.1771e-05 - val_my_r2: 0.9954\n",
      "Epoch 1355/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7481e-04 - my_r2: 0.9349 - val_loss: 1.1616e-05 - val_my_r2: 0.9955\n",
      "Epoch 1356/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8097e-04 - my_r2: 0.9473 - val_loss: 1.2638e-05 - val_my_r2: 0.9954\n",
      "Epoch 1357/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5753e-04 - my_r2: 0.9296 - val_loss: 1.6849e-05 - val_my_r2: 0.9937\n",
      "Epoch 1358/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7170e-04 - my_r2: 0.9501 - val_loss: 2.5494e-05 - val_my_r2: 0.9905\n",
      "Epoch 1359/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0825e-04 - my_r2: 0.9461 - val_loss: 1.7986e-05 - val_my_r2: 0.9931\n",
      "Epoch 1360/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9744e-04 - my_r2: 0.9179 - val_loss: 1.1971e-05 - val_my_r2: 0.9952\n",
      "Epoch 1361/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7901e-04 - my_r2: 0.9259 - val_loss: 9.2291e-06 - val_my_r2: 0.9967\n",
      "Epoch 1362/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6214e-04 - my_r2: 0.9398 - val_loss: 1.2476e-05 - val_my_r2: 0.9959\n",
      "Epoch 1363/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1575e-04 - my_r2: 0.9606 - val_loss: 8.7305e-06 - val_my_r2: 0.9972\n",
      "Epoch 1364/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7362e-04 - my_r2: 0.8721 - val_loss: 1.0700e-05 - val_my_r2: 0.9965\n",
      "Epoch 1365/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1869e-04 - my_r2: 0.9354 - val_loss: 1.1097e-05 - val_my_r2: 0.9967\n",
      "Epoch 1366/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3523e-04 - my_r2: 0.9294 - val_loss: 1.1736e-05 - val_my_r2: 0.9964\n",
      "Epoch 1367/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7141e-04 - my_r2: 0.9143 - val_loss: 1.0167e-05 - val_my_r2: 0.9967\n",
      "Epoch 1368/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4053e-04 - my_r2: 0.9349 - val_loss: 1.0462e-05 - val_my_r2: 0.9966\n",
      "Epoch 1369/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6683e-04 - my_r2: 0.9263 - val_loss: 1.4579e-05 - val_my_r2: 0.9951\n",
      "Epoch 1370/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6163e-04 - my_r2: 0.9448 - val_loss: 1.1966e-05 - val_my_r2: 0.9959\n",
      "Epoch 1371/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0349e-04 - my_r2: 0.9359 - val_loss: 1.0361e-05 - val_my_r2: 0.9964\n",
      "Epoch 1372/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9464e-04 - my_r2: 0.9321 - val_loss: 1.8741e-05 - val_my_r2: 0.9937\n",
      "Epoch 1373/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4417e-04 - my_r2: 0.9474 - val_loss: 1.2657e-05 - val_my_r2: 0.9960\n",
      "Epoch 1374/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2663e-04 - my_r2: 0.9477 - val_loss: 1.2248e-05 - val_my_r2: 0.9958\n",
      "Epoch 1375/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3624e-04 - my_r2: 0.9300 - val_loss: 1.3519e-05 - val_my_r2: 0.9954\n",
      "Epoch 1376/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3044e-04 - my_r2: 0.9278 - val_loss: 1.4175e-05 - val_my_r2: 0.9950\n",
      "Epoch 1377/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3009e-04 - my_r2: 0.9347 - val_loss: 1.2810e-05 - val_my_r2: 0.9955\n",
      "Epoch 1378/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3316e-04 - my_r2: 0.9393 - val_loss: 1.1474e-05 - val_my_r2: 0.9961\n",
      "Epoch 1379/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0065e-04 - my_r2: 0.9559 - val_loss: 1.1602e-05 - val_my_r2: 0.9963\n",
      "Epoch 1380/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6440e-04 - my_r2: 0.9372 - val_loss: 1.3400e-05 - val_my_r2: 0.9956\n",
      "Epoch 1381/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9369e-04 - my_r2: 0.9296 - val_loss: 1.0977e-05 - val_my_r2: 0.9964\n",
      "Epoch 1382/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7372e-04 - my_r2: 0.9286 - val_loss: 1.3335e-05 - val_my_r2: 0.9958\n",
      "Epoch 1383/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4078e-04 - my_r2: 0.9110 - val_loss: 1.1398e-05 - val_my_r2: 0.9966\n",
      "Epoch 1384/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8985e-04 - my_r2: 0.9181 - val_loss: 1.6434e-05 - val_my_r2: 0.9947\n",
      "Epoch 1385/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3210e-04 - my_r2: 0.9402 - val_loss: 2.1034e-05 - val_my_r2: 0.9930\n",
      "Epoch 1386/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7771e-04 - my_r2: 0.9303 - val_loss: 1.2258e-05 - val_my_r2: 0.9956\n",
      "Epoch 1387/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8022e-04 - my_r2: 0.9442 - val_loss: 1.1045e-05 - val_my_r2: 0.9957\n",
      "Epoch 1388/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3916e-04 - my_r2: 0.9350 - val_loss: 9.3942e-06 - val_my_r2: 0.9967\n",
      "Epoch 1389/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4868e-04 - my_r2: 0.9195 - val_loss: 1.2602e-05 - val_my_r2: 0.9961\n",
      "Epoch 1390/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2378e-04 - my_r2: 0.8841 - val_loss: 1.3662e-05 - val_my_r2: 0.9959\n",
      "Epoch 1391/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4074e-04 - my_r2: 0.9501 - val_loss: 1.1732e-05 - val_my_r2: 0.9964\n",
      "Epoch 1392/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4888e-04 - my_r2: 0.9301 - val_loss: 8.8424e-06 - val_my_r2: 0.9968\n",
      "Epoch 1393/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3910e-04 - my_r2: 0.9152 - val_loss: 8.5604e-06 - val_my_r2: 0.9968\n",
      "Epoch 1394/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4516e-04 - my_r2: 0.9369 - val_loss: 9.0152e-06 - val_my_r2: 0.9968\n",
      "Epoch 1395/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1297e-04 - my_r2: 0.9478 - val_loss: 9.7769e-06 - val_my_r2: 0.9967\n",
      "Epoch 1396/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1397e-04 - my_r2: 0.9389 - val_loss: 1.1702e-05 - val_my_r2: 0.9959\n",
      "Epoch 1397/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2571e-04 - my_r2: 0.9481 - val_loss: 1.1310e-05 - val_my_r2: 0.9962\n",
      "Epoch 1398/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0279e-04 - my_r2: 0.9050 - val_loss: 1.1267e-05 - val_my_r2: 0.9964\n",
      "Epoch 1399/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6918e-04 - my_r2: 0.9365 - val_loss: 1.2536e-05 - val_my_r2: 0.9962\n",
      "Epoch 1400/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6202e-04 - my_r2: 0.9515 - val_loss: 2.2634e-05 - val_my_r2: 0.9933\n",
      "Epoch 1401/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6133e-04 - my_r2: 0.8917 - val_loss: 1.5979e-05 - val_my_r2: 0.9949\n",
      "Epoch 1402/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6997e-04 - my_r2: 0.9472 - val_loss: 1.9355e-05 - val_my_r2: 0.9931\n",
      "Epoch 1403/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2962e-04 - my_r2: 0.9319 - val_loss: 1.5735e-05 - val_my_r2: 0.9943\n",
      "Epoch 1404/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7507e-04 - my_r2: 0.9330 - val_loss: 1.6795e-05 - val_my_r2: 0.9940\n",
      "Epoch 1405/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7467e-04 - my_r2: 0.9649 - val_loss: 1.8124e-05 - val_my_r2: 0.9935\n",
      "Epoch 1406/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5718e-04 - my_r2: 0.9015 - val_loss: 1.6538e-05 - val_my_r2: 0.9943\n",
      "Epoch 1407/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3565e-04 - my_r2: 0.9451 - val_loss: 1.5316e-05 - val_my_r2: 0.9945\n",
      "Epoch 1408/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7958e-04 - my_r2: 0.9263 - val_loss: 1.1921e-05 - val_my_r2: 0.9958\n",
      "Epoch 1409/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4689e-04 - my_r2: 0.9445 - val_loss: 1.1281e-05 - val_my_r2: 0.9962\n",
      "Epoch 1410/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3196e-04 - my_r2: 0.9088 - val_loss: 1.0084e-05 - val_my_r2: 0.9965\n",
      "Epoch 1411/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0719e-04 - my_r2: 0.9518 - val_loss: 1.1283e-05 - val_my_r2: 0.9962\n",
      "Epoch 1412/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1581e-04 - my_r2: 0.8915 - val_loss: 1.1116e-05 - val_my_r2: 0.9962\n",
      "Epoch 1413/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5097e-04 - my_r2: 0.9385 - val_loss: 1.2696e-05 - val_my_r2: 0.9955\n",
      "Epoch 1414/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8605e-04 - my_r2: 0.9110 - val_loss: 1.1031e-05 - val_my_r2: 0.9960\n",
      "Epoch 1415/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5130e-04 - my_r2: 0.9359 - val_loss: 1.1823e-05 - val_my_r2: 0.9959\n",
      "Epoch 1416/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4775e-04 - my_r2: 0.9134 - val_loss: 9.3605e-06 - val_my_r2: 0.9968\n",
      "Epoch 1417/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1806e-04 - my_r2: 0.9268 - val_loss: 9.3406e-06 - val_my_r2: 0.9970\n",
      "Epoch 1418/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1314e-04 - my_r2: 0.9525 - val_loss: 1.0370e-05 - val_my_r2: 0.9967\n",
      "Epoch 1419/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0755e-04 - my_r2: 0.9510 - val_loss: 1.0821e-05 - val_my_r2: 0.9966\n",
      "Epoch 1420/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0033e-04 - my_r2: 0.9535 - val_loss: 1.3403e-05 - val_my_r2: 0.9954\n",
      "Epoch 1421/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4027e-04 - my_r2: 0.9313 - val_loss: 1.1540e-05 - val_my_r2: 0.9962\n",
      "Epoch 1422/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0378e-04 - my_r2: 0.9303 - val_loss: 1.1907e-05 - val_my_r2: 0.9963\n",
      "Epoch 1423/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8811e-04 - my_r2: 0.8235 - val_loss: 1.4042e-05 - val_my_r2: 0.9954\n",
      "Epoch 1424/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9058e-04 - my_r2: 0.9185 - val_loss: 1.4735e-05 - val_my_r2: 0.9953\n",
      "Epoch 1425/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6615e-04 - my_r2: 0.9386 - val_loss: 9.8318e-06 - val_my_r2: 0.9967\n",
      "Epoch 1426/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.3608e-04 - my_r2: 0.9111 - val_loss: 1.7207e-05 - val_my_r2: 0.9940\n",
      "Epoch 1427/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8952e-04 - my_r2: 0.9309 - val_loss: 1.0996e-05 - val_my_r2: 0.9960\n",
      "Epoch 1428/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6887e-04 - my_r2: 0.9371 - val_loss: 1.3926e-05 - val_my_r2: 0.9948\n",
      "Epoch 1429/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8834e-04 - my_r2: 0.8660 - val_loss: 1.4350e-05 - val_my_r2: 0.9940\n",
      "Epoch 1430/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4299e-04 - my_r2: 0.9376 - val_loss: 1.2477e-05 - val_my_r2: 0.9953\n",
      "Epoch 1431/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4513e-04 - my_r2: 0.9378 - val_loss: 1.0106e-05 - val_my_r2: 0.9964\n",
      "Epoch 1432/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5306e-04 - my_r2: 0.9483 - val_loss: 1.5224e-05 - val_my_r2: 0.9949\n",
      "Epoch 1433/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3307e-04 - my_r2: 0.9455 - val_loss: 1.3691e-05 - val_my_r2: 0.9953\n",
      "Epoch 1434/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0342e-04 - my_r2: 0.9220 - val_loss: 1.3252e-05 - val_my_r2: 0.9956\n",
      "Epoch 1435/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2017e-04 - my_r2: 0.9407 - val_loss: 1.1636e-05 - val_my_r2: 0.9963\n",
      "Epoch 1436/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6404e-04 - my_r2: 0.9346 - val_loss: 1.2637e-05 - val_my_r2: 0.9959\n",
      "Epoch 1437/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7476e-04 - my_r2: 0.9307 - val_loss: 1.6720e-05 - val_my_r2: 0.9947\n",
      "Epoch 1438/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3839e-04 - my_r2: 0.9509 - val_loss: 1.2518e-05 - val_my_r2: 0.9954\n",
      "Epoch 1439/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4422e-04 - my_r2: 0.9465 - val_loss: 1.3660e-05 - val_my_r2: 0.9947\n",
      "Epoch 1440/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4270e-04 - my_r2: 0.9171 - val_loss: 1.2551e-05 - val_my_r2: 0.9952\n",
      "Epoch 1441/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5028e-04 - my_r2: 0.9468 - val_loss: 1.1472e-05 - val_my_r2: 0.9956\n",
      "Epoch 1442/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1882e-04 - my_r2: 0.9480 - val_loss: 1.1976e-05 - val_my_r2: 0.9956\n",
      "Epoch 1443/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7379e-04 - my_r2: 0.9372 - val_loss: 1.0392e-05 - val_my_r2: 0.9966\n",
      "Epoch 1444/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8360e-04 - my_r2: 0.9151 - val_loss: 1.5141e-05 - val_my_r2: 0.9950\n",
      "Epoch 1445/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3546e-04 - my_r2: 0.9451 - val_loss: 1.2932e-05 - val_my_r2: 0.9958\n",
      "Epoch 1446/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4958e-04 - my_r2: 0.9253 - val_loss: 9.1789e-06 - val_my_r2: 0.9971\n",
      "Epoch 1447/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2515e-04 - my_r2: 0.9521 - val_loss: 8.6421e-06 - val_my_r2: 0.9973\n",
      "Epoch 1448/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2882e-04 - my_r2: 0.9449 - val_loss: 8.6073e-06 - val_my_r2: 0.9970\n",
      "Epoch 1449/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5068e-04 - my_r2: 0.9324 - val_loss: 1.1078e-05 - val_my_r2: 0.9958\n",
      "Epoch 1450/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7201e-04 - my_r2: 0.9332 - val_loss: 7.6876e-06 - val_my_r2: 0.9972\n",
      "Epoch 1451/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6224e-04 - my_r2: 0.9363 - val_loss: 8.5614e-06 - val_my_r2: 0.9973\n",
      "Epoch 1452/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4479e-04 - my_r2: 0.9305 - val_loss: 1.0728e-05 - val_my_r2: 0.9966\n",
      "Epoch 1453/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6717e-04 - my_r2: 0.8775 - val_loss: 9.9492e-06 - val_my_r2: 0.9968\n",
      "Epoch 1454/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0205e-04 - my_r2: 0.9402 - val_loss: 1.0188e-05 - val_my_r2: 0.9966\n",
      "Epoch 1455/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0711e-04 - my_r2: 0.9523 - val_loss: 1.5160e-05 - val_my_r2: 0.9947\n",
      "Epoch 1456/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5386e-04 - my_r2: 0.9386 - val_loss: 1.1603e-05 - val_my_r2: 0.9959\n",
      "Epoch 1457/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5189e-04 - my_r2: 0.9388 - val_loss: 1.2230e-05 - val_my_r2: 0.9954\n",
      "Epoch 1458/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5902e-04 - my_r2: 0.9173 - val_loss: 1.3200e-05 - val_my_r2: 0.9956\n",
      "Epoch 1459/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4587e-04 - my_r2: 0.9127 - val_loss: 1.4042e-05 - val_my_r2: 0.9955\n",
      "Epoch 1460/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5392e-04 - my_r2: 0.9175 - val_loss: 1.1939e-05 - val_my_r2: 0.9963\n",
      "Epoch 1461/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1959e-04 - my_r2: 0.9461 - val_loss: 1.0969e-05 - val_my_r2: 0.9964\n",
      "Epoch 1462/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2329e-04 - my_r2: 0.9045 - val_loss: 9.8231e-06 - val_my_r2: 0.9967\n",
      "Epoch 1463/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5264e-04 - my_r2: 0.9347 - val_loss: 1.1315e-05 - val_my_r2: 0.9963\n",
      "Epoch 1464/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1142e-04 - my_r2: 0.9556 - val_loss: 1.1686e-05 - val_my_r2: 0.9961\n",
      "Epoch 1465/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1495e-04 - my_r2: 0.9545 - val_loss: 1.1324e-05 - val_my_r2: 0.9961\n",
      "Epoch 1466/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5684e-04 - my_r2: 0.9402 - val_loss: 1.2089e-05 - val_my_r2: 0.9959\n",
      "Epoch 1467/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2125e-04 - my_r2: 0.8691 - val_loss: 1.3028e-05 - val_my_r2: 0.9955\n",
      "Epoch 1468/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4753e-04 - my_r2: 0.9276 - val_loss: 1.1307e-05 - val_my_r2: 0.9957\n",
      "Epoch 1469/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9063e-04 - my_r2: 0.9232 - val_loss: 1.6863e-05 - val_my_r2: 0.9935\n",
      "Epoch 1470/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3930e-04 - my_r2: 0.9263 - val_loss: 1.1698e-05 - val_my_r2: 0.9955\n",
      "Epoch 1471/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8481e-04 - my_r2: 0.8845 - val_loss: 1.1116e-05 - val_my_r2: 0.9959\n",
      "Epoch 1472/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4664e-04 - my_r2: 0.9297 - val_loss: 1.1862e-05 - val_my_r2: 0.9962\n",
      "Epoch 1473/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6612e-04 - my_r2: 0.9174 - val_loss: 1.0711e-05 - val_my_r2: 0.9965\n",
      "Epoch 1474/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0643e-04 - my_r2: 0.9121 - val_loss: 9.3827e-06 - val_my_r2: 0.9970\n",
      "Epoch 1475/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6533e-04 - my_r2: 0.9267 - val_loss: 1.1686e-05 - val_my_r2: 0.9962\n",
      "Epoch 1476/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3822e-04 - my_r2: 0.8831 - val_loss: 1.1175e-05 - val_my_r2: 0.9963\n",
      "Epoch 1477/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6953e-04 - my_r2: 0.9286 - val_loss: 1.1851e-05 - val_my_r2: 0.9963\n",
      "Epoch 1478/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6094e-04 - my_r2: 0.9390 - val_loss: 1.3637e-05 - val_my_r2: 0.9957\n",
      "Epoch 1479/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3505e-04 - my_r2: 0.9203 - val_loss: 1.3263e-05 - val_my_r2: 0.9959\n",
      "Epoch 1480/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8535e-04 - my_r2: 0.8871 - val_loss: 1.3853e-05 - val_my_r2: 0.9960\n",
      "Epoch 1481/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7151e-04 - my_r2: 0.9328 - val_loss: 1.3051e-05 - val_my_r2: 0.9958\n",
      "Epoch 1482/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.6227e-04 - my_r2: 0.9206 - val_loss: 8.9641e-06 - val_my_r2: 0.9971\n",
      "Epoch 1483/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2013e-04 - my_r2: 0.9353 - val_loss: 9.5679e-06 - val_my_r2: 0.9967\n",
      "Epoch 1484/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2423e-04 - my_r2: 0.9453 - val_loss: 9.2554e-06 - val_my_r2: 0.9967\n",
      "Epoch 1485/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.2602e-04 - my_r2: 0.8607 - val_loss: 1.2745e-05 - val_my_r2: 0.9955\n",
      "Epoch 1486/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6017e-04 - my_r2: 0.9485 - val_loss: 1.1078e-05 - val_my_r2: 0.9961\n",
      "Epoch 1487/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7448e-04 - my_r2: 0.9175 - val_loss: 9.7495e-06 - val_my_r2: 0.9964\n",
      "Epoch 1488/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8497e-04 - my_r2: 0.9503 - val_loss: 1.0127e-05 - val_my_r2: 0.9960\n",
      "Epoch 1489/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3224e-04 - my_r2: 0.9138 - val_loss: 1.0406e-05 - val_my_r2: 0.9960\n",
      "Epoch 1490/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9051e-04 - my_r2: 0.8702 - val_loss: 1.3496e-05 - val_my_r2: 0.9950\n",
      "Epoch 1491/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3994e-04 - my_r2: 0.9204 - val_loss: 1.1543e-05 - val_my_r2: 0.9957\n",
      "Epoch 1492/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5773e-04 - my_r2: 0.9356 - val_loss: 1.3226e-05 - val_my_r2: 0.9952\n",
      "Epoch 1493/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1067e-04 - my_r2: 0.9471 - val_loss: 1.3436e-05 - val_my_r2: 0.9950\n",
      "Epoch 1494/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8073e-04 - my_r2: 0.9410 - val_loss: 2.1211e-05 - val_my_r2: 0.9922\n",
      "Epoch 1495/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6081e-04 - my_r2: 0.8599 - val_loss: 1.3531e-05 - val_my_r2: 0.9947\n",
      "Epoch 1496/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2677e-04 - my_r2: 0.9415 - val_loss: 1.2204e-05 - val_my_r2: 0.9957\n",
      "Epoch 1497/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1622e-04 - my_r2: 0.9394 - val_loss: 1.1431e-05 - val_my_r2: 0.9963\n",
      "Epoch 1498/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0204e-04 - my_r2: 0.9316 - val_loss: 1.3411e-05 - val_my_r2: 0.9956\n",
      "Epoch 1499/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3955e-04 - my_r2: 0.9381 - val_loss: 9.3434e-06 - val_my_r2: 0.9969\n",
      "Epoch 1500/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6717e-04 - my_r2: 0.9321 - val_loss: 1.1869e-05 - val_my_r2: 0.9961\n",
      "Epoch 1501/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5271e-04 - my_r2: 0.8952 - val_loss: 1.1118e-05 - val_my_r2: 0.9962\n",
      "Epoch 1502/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3528e-04 - my_r2: 0.9265 - val_loss: 1.0211e-05 - val_my_r2: 0.9966\n",
      "Epoch 1503/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5776e-04 - my_r2: 0.8980 - val_loss: 1.0227e-05 - val_my_r2: 0.9969\n",
      "Epoch 1504/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8062e-04 - my_r2: 0.9107 - val_loss: 1.0077e-05 - val_my_r2: 0.9967\n",
      "Epoch 1505/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4474e-04 - my_r2: 0.8995 - val_loss: 9.2632e-06 - val_my_r2: 0.9966\n",
      "Epoch 1506/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2866e-04 - my_r2: 0.9424 - val_loss: 1.0038e-05 - val_my_r2: 0.9957\n",
      "Epoch 1507/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6296e-04 - my_r2: 0.9324 - val_loss: 1.0282e-05 - val_my_r2: 0.9960\n",
      "Epoch 1508/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0621e-04 - my_r2: 0.9432 - val_loss: 9.7564e-06 - val_my_r2: 0.9966\n",
      "Epoch 1509/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0251e-04 - my_r2: 0.9483 - val_loss: 1.0693e-05 - val_my_r2: 0.9962\n",
      "Epoch 1510/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1210e-04 - my_r2: 0.9347 - val_loss: 1.0138e-05 - val_my_r2: 0.9963\n",
      "Epoch 1511/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6148e-04 - my_r2: 0.9423 - val_loss: 1.0944e-05 - val_my_r2: 0.9959\n",
      "Epoch 1512/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9243e-04 - my_r2: 0.9537 - val_loss: 1.2467e-05 - val_my_r2: 0.9955\n",
      "Epoch 1513/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3942e-04 - my_r2: 0.9288 - val_loss: 1.4598e-05 - val_my_r2: 0.9945\n",
      "Epoch 1514/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1959e-04 - my_r2: 0.9484 - val_loss: 1.0835e-05 - val_my_r2: 0.9958\n",
      "Epoch 1515/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4638e-04 - my_r2: 0.8934 - val_loss: 1.1064e-05 - val_my_r2: 0.9957\n",
      "Epoch 1516/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2636e-04 - my_r2: 0.8973 - val_loss: 1.1484e-05 - val_my_r2: 0.9956\n",
      "Epoch 1517/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5444e-04 - my_r2: 0.8774 - val_loss: 1.1433e-05 - val_my_r2: 0.9954\n",
      "Epoch 1518/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4657e-04 - my_r2: 0.9355 - val_loss: 1.2860e-05 - val_my_r2: 0.9947\n",
      "Epoch 1519/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2208e-04 - my_r2: 0.9433 - val_loss: 1.1634e-05 - val_my_r2: 0.9955\n",
      "Epoch 1520/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0212e-04 - my_r2: 0.9394 - val_loss: 1.2535e-05 - val_my_r2: 0.9950\n",
      "Epoch 1521/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4105e-04 - my_r2: 0.8673 - val_loss: 1.0289e-05 - val_my_r2: 0.9961\n",
      "Epoch 1522/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5047e-04 - my_r2: 0.9532 - val_loss: 1.0840e-05 - val_my_r2: 0.9957\n",
      "Epoch 1523/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7150e-04 - my_r2: 0.9447 - val_loss: 1.2205e-05 - val_my_r2: 0.9949\n",
      "Epoch 1524/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3704e-04 - my_r2: 0.9514 - val_loss: 1.2303e-05 - val_my_r2: 0.9948\n",
      "Epoch 1525/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8928e-04 - my_r2: 0.8655 - val_loss: 1.1012e-05 - val_my_r2: 0.9953\n",
      "Epoch 1526/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2600e-04 - my_r2: 0.9382 - val_loss: 9.9554e-06 - val_my_r2: 0.9958\n",
      "Epoch 1527/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6063e-04 - my_r2: 0.9414 - val_loss: 9.6287e-06 - val_my_r2: 0.9961\n",
      "Epoch 1528/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6056e-04 - my_r2: 0.8395 - val_loss: 1.0995e-05 - val_my_r2: 0.9953\n",
      "Epoch 1529/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4786e-04 - my_r2: 0.9481 - val_loss: 1.1362e-05 - val_my_r2: 0.9951\n",
      "Epoch 1530/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6288e-04 - my_r2: 0.9261 - val_loss: 1.1686e-05 - val_my_r2: 0.9957\n",
      "Epoch 1531/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4494e-04 - my_r2: 0.9166 - val_loss: 1.4399e-05 - val_my_r2: 0.9953\n",
      "Epoch 1532/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6984e-04 - my_r2: 0.9411 - val_loss: 1.3983e-05 - val_my_r2: 0.9953\n",
      "Epoch 1533/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2045e-04 - my_r2: 0.9414 - val_loss: 1.0491e-05 - val_my_r2: 0.9967\n",
      "Epoch 1534/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3877e-04 - my_r2: 0.9406 - val_loss: 1.9104e-05 - val_my_r2: 0.9938\n",
      "Epoch 1535/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3186e-04 - my_r2: 0.9403 - val_loss: 1.0059e-05 - val_my_r2: 0.9967\n",
      "Epoch 1536/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7481e-04 - my_r2: 0.9166 - val_loss: 9.7182e-06 - val_my_r2: 0.9965\n",
      "Epoch 1537/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8620e-04 - my_r2: 0.9272 - val_loss: 1.1641e-05 - val_my_r2: 0.9956\n",
      "Epoch 1538/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0323e-04 - my_r2: 0.8937 - val_loss: 1.0872e-05 - val_my_r2: 0.9960\n",
      "Epoch 1539/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8209e-04 - my_r2: 0.9025 - val_loss: 1.0943e-05 - val_my_r2: 0.9959\n",
      "Epoch 1540/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8946e-04 - my_r2: 0.9016 - val_loss: 1.1838e-05 - val_my_r2: 0.9956\n",
      "Epoch 1541/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4385e-04 - my_r2: 0.9422 - val_loss: 1.1153e-05 - val_my_r2: 0.9959\n",
      "Epoch 1542/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3896e-04 - my_r2: 0.9332 - val_loss: 1.1452e-05 - val_my_r2: 0.9958\n",
      "Epoch 1543/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8785e-04 - my_r2: 0.9189 - val_loss: 1.3877e-05 - val_my_r2: 0.9950\n",
      "Epoch 1544/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4165e-04 - my_r2: 0.9214 - val_loss: 1.1213e-05 - val_my_r2: 0.9961\n",
      "Epoch 1545/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0381e-04 - my_r2: 0.9439 - val_loss: 1.2610e-05 - val_my_r2: 0.9957\n",
      "Epoch 1546/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0590e-04 - my_r2: 0.9481 - val_loss: 9.3408e-06 - val_my_r2: 0.9965\n",
      "Epoch 1547/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5242e-04 - my_r2: 0.8918 - val_loss: 1.0383e-05 - val_my_r2: 0.9963\n",
      "Epoch 1548/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8639e-04 - my_r2: 0.9571 - val_loss: 1.0724e-05 - val_my_r2: 0.9964\n",
      "Epoch 1549/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4802e-04 - my_r2: 0.9346 - val_loss: 1.6453e-05 - val_my_r2: 0.9939\n",
      "Epoch 1550/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4086e-04 - my_r2: 0.9186 - val_loss: 1.6091e-05 - val_my_r2: 0.9940\n",
      "Epoch 1551/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5614e-04 - my_r2: 0.9490 - val_loss: 1.3536e-05 - val_my_r2: 0.9951\n",
      "Epoch 1552/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2248e-04 - my_r2: 0.9263 - val_loss: 1.3694e-05 - val_my_r2: 0.9953\n",
      "Epoch 1553/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3363e-04 - my_r2: 0.9476 - val_loss: 1.9955e-05 - val_my_r2: 0.9932\n",
      "Epoch 1554/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8451e-04 - my_r2: 0.9295 - val_loss: 8.7257e-06 - val_my_r2: 0.9971\n",
      "Epoch 1555/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9183e-04 - my_r2: 0.9095 - val_loss: 1.0128e-05 - val_my_r2: 0.9963\n",
      "Epoch 1556/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4619e-04 - my_r2: 0.9438 - val_loss: 1.8915e-05 - val_my_r2: 0.9928\n",
      "Epoch 1557/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7310e-04 - my_r2: 0.9361 - val_loss: 1.6218e-05 - val_my_r2: 0.9939\n",
      "Epoch 1558/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5885e-04 - my_r2: 0.8858 - val_loss: 1.7532e-05 - val_my_r2: 0.9932\n",
      "Epoch 1559/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.5765e-04 - my_r2: 0.9182 - val_loss: 1.9507e-05 - val_my_r2: 0.9922\n",
      "Epoch 1560/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4206e-04 - my_r2: 0.9420 - val_loss: 2.0332e-05 - val_my_r2: 0.9919\n",
      "Epoch 1561/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0490e-04 - my_r2: 0.8904 - val_loss: 2.0118e-05 - val_my_r2: 0.9920\n",
      "Epoch 1562/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6343e-04 - my_r2: 0.9352 - val_loss: 1.0307e-05 - val_my_r2: 0.9961\n",
      "Epoch 1563/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2091e-04 - my_r2: 0.9584 - val_loss: 9.6719e-06 - val_my_r2: 0.9967\n",
      "Epoch 1564/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4198e-04 - my_r2: 0.9270 - val_loss: 9.5900e-06 - val_my_r2: 0.9967\n",
      "Epoch 1565/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4891e-04 - my_r2: 0.9197 - val_loss: 9.5880e-06 - val_my_r2: 0.9966\n",
      "Epoch 1566/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8794e-04 - my_r2: 0.8713 - val_loss: 1.1897e-05 - val_my_r2: 0.9960\n",
      "Epoch 1567/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1580e-04 - my_r2: 0.9087 - val_loss: 1.5642e-05 - val_my_r2: 0.9943\n",
      "Epoch 1568/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9844e-04 - my_r2: 0.9536 - val_loss: 1.2132e-05 - val_my_r2: 0.9952\n",
      "Epoch 1569/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5120e-04 - my_r2: 0.9429 - val_loss: 9.5200e-06 - val_my_r2: 0.9964\n",
      "Epoch 1570/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5174e-04 - my_r2: 0.9329 - val_loss: 1.8733e-05 - val_my_r2: 0.9935\n",
      "Epoch 1571/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0423e-04 - my_r2: 0.8593 - val_loss: 1.0500e-05 - val_my_r2: 0.9965\n",
      "Epoch 1572/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9826e-04 - my_r2: 0.8744 - val_loss: 8.0830e-06 - val_my_r2: 0.9970\n",
      "Epoch 1573/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8893e-04 - my_r2: 0.9205 - val_loss: 1.0138e-05 - val_my_r2: 0.9965\n",
      "Epoch 1574/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6948e-04 - my_r2: 0.8846 - val_loss: 1.0150e-05 - val_my_r2: 0.9965\n",
      "Epoch 1575/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4492e-04 - my_r2: 0.9178 - val_loss: 1.1480e-05 - val_my_r2: 0.9961\n",
      "Epoch 1576/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2219e-04 - my_r2: 0.9431 - val_loss: 1.0937e-05 - val_my_r2: 0.9964\n",
      "Epoch 1577/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5506e-04 - my_r2: 0.9246 - val_loss: 1.2627e-05 - val_my_r2: 0.9956\n",
      "Epoch 1578/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9735e-04 - my_r2: 0.9200 - val_loss: 1.1405e-05 - val_my_r2: 0.9960\n",
      "Epoch 1579/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1407e-04 - my_r2: 0.9508 - val_loss: 1.0042e-05 - val_my_r2: 0.9964\n",
      "Epoch 1580/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3738e-04 - my_r2: 0.9355 - val_loss: 9.2498e-06 - val_my_r2: 0.9967\n",
      "Epoch 1581/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2803e-04 - my_r2: 0.9554 - val_loss: 1.1134e-05 - val_my_r2: 0.9962\n",
      "Epoch 1582/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8379e-04 - my_r2: 0.9250 - val_loss: 1.0020e-05 - val_my_r2: 0.9967\n",
      "Epoch 1583/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9895e-04 - my_r2: 0.9178 - val_loss: 1.1333e-05 - val_my_r2: 0.9963\n",
      "Epoch 1584/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7566e-04 - my_r2: 0.9340 - val_loss: 1.4766e-05 - val_my_r2: 0.9953\n",
      "Epoch 1585/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7748e-04 - my_r2: 0.9166 - val_loss: 1.1214e-05 - val_my_r2: 0.9965\n",
      "Epoch 1586/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4073e-04 - my_r2: 0.9454 - val_loss: 7.8880e-06 - val_my_r2: 0.9974\n",
      "Epoch 1587/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2522e-04 - my_r2: 0.9514 - val_loss: 9.0240e-06 - val_my_r2: 0.9970\n",
      "Epoch 1588/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5243e-04 - my_r2: 0.9400 - val_loss: 1.1681e-05 - val_my_r2: 0.9960\n",
      "Epoch 1589/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4919e-04 - my_r2: 0.8983 - val_loss: 1.0031e-05 - val_my_r2: 0.9965\n",
      "Epoch 1590/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3336e-04 - my_r2: 0.9442 - val_loss: 1.3856e-05 - val_my_r2: 0.9954\n",
      "Epoch 1591/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4506e-04 - my_r2: 0.9292 - val_loss: 1.2613e-05 - val_my_r2: 0.9955\n",
      "Epoch 1592/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9638e-04 - my_r2: 0.9476 - val_loss: 1.7681e-05 - val_my_r2: 0.9934\n",
      "Epoch 1593/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8035e-04 - my_r2: 0.9368 - val_loss: 1.8078e-05 - val_my_r2: 0.9933\n",
      "Epoch 1594/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8653e-04 - my_r2: 0.9292 - val_loss: 1.2417e-05 - val_my_r2: 0.9955\n",
      "Epoch 1595/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5819e-04 - my_r2: 0.9314 - val_loss: 7.9853e-06 - val_my_r2: 0.9971\n",
      "Epoch 1596/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9753e-04 - my_r2: 0.9494 - val_loss: 7.9310e-06 - val_my_r2: 0.9972\n",
      "Epoch 1597/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6425e-04 - my_r2: 0.9277 - val_loss: 7.4891e-06 - val_my_r2: 0.9973\n",
      "Epoch 1598/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2727e-04 - my_r2: 0.9516 - val_loss: 1.2887e-05 - val_my_r2: 0.9953\n",
      "Epoch 1599/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8959e-04 - my_r2: 0.8931 - val_loss: 1.3161e-05 - val_my_r2: 0.9950\n",
      "Epoch 1600/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6427e-04 - my_r2: 0.9150 - val_loss: 1.4376e-05 - val_my_r2: 0.9947\n",
      "Epoch 1601/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4288e-04 - my_r2: 0.8617 - val_loss: 1.1308e-05 - val_my_r2: 0.9959\n",
      "Epoch 1602/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0640e-04 - my_r2: 0.9475 - val_loss: 1.1260e-05 - val_my_r2: 0.9958\n",
      "Epoch 1603/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5340e-04 - my_r2: 0.9352 - val_loss: 1.1779e-05 - val_my_r2: 0.9954\n",
      "Epoch 1604/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2553e-04 - my_r2: 0.9558 - val_loss: 1.4090e-05 - val_my_r2: 0.9945\n",
      "Epoch 1605/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3454e-04 - my_r2: 0.9435 - val_loss: 6.9763e-06 - val_my_r2: 0.9971\n",
      "Epoch 1606/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1827e-04 - my_r2: 0.9206 - val_loss: 1.0115e-05 - val_my_r2: 0.9963\n",
      "Epoch 1607/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2905e-04 - my_r2: 0.9440 - val_loss: 8.0276e-06 - val_my_r2: 0.9971\n",
      "Epoch 1608/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.5173e-04 - my_r2: 0.8838 - val_loss: 7.5289e-06 - val_my_r2: 0.9975\n",
      "Epoch 1609/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9143e-04 - my_r2: 0.9093 - val_loss: 9.9629e-06 - val_my_r2: 0.9967\n",
      "Epoch 1610/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6305e-04 - my_r2: 0.9434 - val_loss: 8.4460e-06 - val_my_r2: 0.9970\n",
      "Epoch 1611/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3357e-04 - my_r2: 0.9017 - val_loss: 6.9050e-06 - val_my_r2: 0.9973\n",
      "Epoch 1612/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7232e-04 - my_r2: 0.8606 - val_loss: 5.8122e-06 - val_my_r2: 0.9979\n",
      "Epoch 1613/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9908e-04 - my_r2: 0.9250 - val_loss: 7.6868e-06 - val_my_r2: 0.9974\n",
      "Epoch 1614/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6711e-04 - my_r2: 0.9199 - val_loss: 9.0119e-06 - val_my_r2: 0.9972\n",
      "Epoch 1615/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8693e-04 - my_r2: 0.9238 - val_loss: 6.5520e-06 - val_my_r2: 0.9980\n",
      "Epoch 1616/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2201e-04 - my_r2: 0.9578 - val_loss: 7.1758e-06 - val_my_r2: 0.9979\n",
      "Epoch 1617/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7857e-04 - my_r2: 0.9243 - val_loss: 7.6930e-06 - val_my_r2: 0.9975\n",
      "Epoch 1618/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6703e-04 - my_r2: 0.9306 - val_loss: 8.8322e-06 - val_my_r2: 0.9969\n",
      "Epoch 1619/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2481e-04 - my_r2: 0.9275 - val_loss: 9.1311e-06 - val_my_r2: 0.9972\n",
      "Epoch 1620/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1425e-04 - my_r2: 0.7936 - val_loss: 9.4081e-06 - val_my_r2: 0.9972\n",
      "Epoch 1621/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5558e-04 - my_r2: 0.9194 - val_loss: 9.8787e-06 - val_my_r2: 0.9969\n",
      "Epoch 1622/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2203e-04 - my_r2: 0.9068 - val_loss: 1.3559e-05 - val_my_r2: 0.9955\n",
      "Epoch 1623/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8202e-04 - my_r2: 0.9249 - val_loss: 1.0698e-05 - val_my_r2: 0.9964\n",
      "Epoch 1624/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6978e-04 - my_r2: 0.9302 - val_loss: 9.6412e-06 - val_my_r2: 0.9970\n",
      "Epoch 1625/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5639e-04 - my_r2: 0.9361 - val_loss: 7.9532e-06 - val_my_r2: 0.9975\n",
      "Epoch 1626/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.5803e-04 - my_r2: 0.9484 - val_loss: 7.2640e-06 - val_my_r2: 0.9975\n",
      "Epoch 1627/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5724e-04 - my_r2: 0.9279 - val_loss: 8.7868e-06 - val_my_r2: 0.9971\n",
      "Epoch 1628/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1929e-04 - my_r2: 0.9480 - val_loss: 6.9607e-06 - val_my_r2: 0.9977\n",
      "Epoch 1629/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0773e-04 - my_r2: 0.9392 - val_loss: 1.1458e-05 - val_my_r2: 0.9962\n",
      "Epoch 1630/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4818e-04 - my_r2: 0.9404 - val_loss: 1.1782e-05 - val_my_r2: 0.9961\n",
      "Epoch 1631/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0613e-04 - my_r2: 0.9372 - val_loss: 1.1958e-05 - val_my_r2: 0.9959\n",
      "Epoch 1632/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4914e-04 - my_r2: 0.9414 - val_loss: 8.2138e-06 - val_my_r2: 0.9969\n",
      "Epoch 1633/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2659e-04 - my_r2: 0.9111 - val_loss: 7.2288e-06 - val_my_r2: 0.9975\n",
      "Epoch 1634/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3877e-04 - my_r2: 0.9437 - val_loss: 8.1592e-06 - val_my_r2: 0.9971\n",
      "Epoch 1635/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4202e-04 - my_r2: 0.9400 - val_loss: 6.8865e-06 - val_my_r2: 0.9974\n",
      "Epoch 1636/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2131e-04 - my_r2: 0.9383 - val_loss: 7.0956e-06 - val_my_r2: 0.9974\n",
      "Epoch 1637/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2775e-04 - my_r2: 0.9548 - val_loss: 8.7105e-06 - val_my_r2: 0.9969\n",
      "Epoch 1638/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4019e-04 - my_r2: 0.9243 - val_loss: 7.6367e-06 - val_my_r2: 0.9974\n",
      "Epoch 1639/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1700e-04 - my_r2: 0.9310 - val_loss: 8.3589e-06 - val_my_r2: 0.9974\n",
      "Epoch 1640/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2226e-04 - my_r2: 0.9403 - val_loss: 1.0287e-05 - val_my_r2: 0.9964\n",
      "Epoch 1641/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1466e-04 - my_r2: 0.8722 - val_loss: 8.9716e-06 - val_my_r2: 0.9971\n",
      "Epoch 1642/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3705e-04 - my_r2: 0.9314 - val_loss: 1.3962e-05 - val_my_r2: 0.9951\n",
      "Epoch 1643/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4549e-04 - my_r2: 0.9240 - val_loss: 8.8884e-06 - val_my_r2: 0.9969\n",
      "Epoch 1644/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6683e-04 - my_r2: 0.8836 - val_loss: 9.3930e-06 - val_my_r2: 0.9965\n",
      "Epoch 1645/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3314e-04 - my_r2: 0.9391 - val_loss: 1.2949e-05 - val_my_r2: 0.9954\n",
      "Epoch 1646/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0521e-04 - my_r2: 0.9545 - val_loss: 9.3160e-06 - val_my_r2: 0.9969\n",
      "Epoch 1647/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7389e-04 - my_r2: 0.9148 - val_loss: 1.2881e-05 - val_my_r2: 0.9953\n",
      "Epoch 1648/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3251e-04 - my_r2: 0.9180 - val_loss: 1.4916e-05 - val_my_r2: 0.9945\n",
      "Epoch 1649/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9843e-04 - my_r2: 0.9267 - val_loss: 1.8674e-05 - val_my_r2: 0.9938\n",
      "Epoch 1650/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7739e-04 - my_r2: 0.6385 - val_loss: 1.2662e-05 - val_my_r2: 0.9959\n",
      "Epoch 1651/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9967e-04 - my_r2: 0.9425 - val_loss: 1.1776e-05 - val_my_r2: 0.9964\n",
      "Epoch 1652/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5571e-04 - my_r2: 0.9335 - val_loss: 1.4726e-05 - val_my_r2: 0.9951\n",
      "Epoch 1653/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0163e-04 - my_r2: 0.9305 - val_loss: 1.0209e-05 - val_my_r2: 0.9964\n",
      "Epoch 1654/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7546e-04 - my_r2: 0.9319 - val_loss: 1.1173e-05 - val_my_r2: 0.9965\n",
      "Epoch 1655/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8921e-04 - my_r2: 0.8982 - val_loss: 1.0969e-05 - val_my_r2: 0.9967\n",
      "Epoch 1656/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3330e-04 - my_r2: 0.9251 - val_loss: 1.0868e-05 - val_my_r2: 0.9966\n",
      "Epoch 1657/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3674e-04 - my_r2: 0.9250 - val_loss: 1.0023e-05 - val_my_r2: 0.9966\n",
      "Epoch 1658/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4191e-04 - my_r2: 0.9052 - val_loss: 8.7937e-06 - val_my_r2: 0.9970\n",
      "Epoch 1659/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9259e-04 - my_r2: 0.8998 - val_loss: 7.2262e-06 - val_my_r2: 0.9976\n",
      "Epoch 1660/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1969e-04 - my_r2: 0.9226 - val_loss: 9.5504e-06 - val_my_r2: 0.9969\n",
      "Epoch 1661/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3706e-04 - my_r2: 0.9352 - val_loss: 8.5360e-06 - val_my_r2: 0.9975\n",
      "Epoch 1662/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2601e-04 - my_r2: 0.9440 - val_loss: 1.0874e-05 - val_my_r2: 0.9967\n",
      "Epoch 1663/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2660e-04 - my_r2: 0.9484 - val_loss: 1.0943e-05 - val_my_r2: 0.9967\n",
      "Epoch 1664/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6788e-04 - my_r2: 0.9282 - val_loss: 1.0859e-05 - val_my_r2: 0.9962\n",
      "Epoch 1665/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6436e-04 - my_r2: 0.9257 - val_loss: 1.0892e-05 - val_my_r2: 0.9962\n",
      "Epoch 1666/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6599e-04 - my_r2: 0.9199 - val_loss: 1.3040e-05 - val_my_r2: 0.9953\n",
      "Epoch 1667/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5390e-04 - my_r2: 0.9239 - val_loss: 1.1238e-05 - val_my_r2: 0.9958\n",
      "Epoch 1668/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8675e-04 - my_r2: 0.9152 - val_loss: 1.2373e-05 - val_my_r2: 0.9952\n",
      "Epoch 1669/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8790e-04 - my_r2: 0.9229 - val_loss: 1.2124e-05 - val_my_r2: 0.9952\n",
      "Epoch 1670/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7539e-04 - my_r2: 0.9437 - val_loss: 1.0067e-05 - val_my_r2: 0.9962\n",
      "Epoch 1671/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0147e-04 - my_r2: 0.9284 - val_loss: 1.2113e-05 - val_my_r2: 0.9951\n",
      "Epoch 1672/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9355e-04 - my_r2: 0.9548 - val_loss: 1.0480e-05 - val_my_r2: 0.9956\n",
      "Epoch 1673/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9093e-04 - my_r2: 0.9232 - val_loss: 9.6609e-06 - val_my_r2: 0.9959\n",
      "Epoch 1674/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1542e-04 - my_r2: 0.8998 - val_loss: 1.4181e-05 - val_my_r2: 0.9944\n",
      "Epoch 1675/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4645e-04 - my_r2: 0.9344 - val_loss: 1.2288e-05 - val_my_r2: 0.9951\n",
      "Epoch 1676/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2066e-04 - my_r2: 0.9438 - val_loss: 8.5204e-06 - val_my_r2: 0.9971\n",
      "Epoch 1677/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0712e-04 - my_r2: 0.9411 - val_loss: 1.0946e-05 - val_my_r2: 0.9963\n",
      "Epoch 1678/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0593e-04 - my_r2: 0.7793 - val_loss: 1.1955e-05 - val_my_r2: 0.9957\n",
      "Epoch 1679/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0853e-04 - my_r2: 0.9250 - val_loss: 9.8929e-06 - val_my_r2: 0.9967\n",
      "Epoch 1680/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1669e-04 - my_r2: 0.9531 - val_loss: 1.3981e-05 - val_my_r2: 0.9953\n",
      "Epoch 1681/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9125e-04 - my_r2: 0.9586 - val_loss: 1.2948e-05 - val_my_r2: 0.9957\n",
      "Epoch 1682/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4335e-04 - my_r2: 0.7267 - val_loss: 1.0241e-05 - val_my_r2: 0.9964\n",
      "Epoch 1683/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9192e-04 - my_r2: 0.9519 - val_loss: 8.8375e-06 - val_my_r2: 0.9970\n",
      "Epoch 1684/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3774e-04 - my_r2: 0.9378 - val_loss: 7.8537e-06 - val_my_r2: 0.9973\n",
      "Epoch 1685/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2487e-04 - my_r2: 0.9422 - val_loss: 9.9878e-06 - val_my_r2: 0.9967\n",
      "Epoch 1686/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7385e-04 - my_r2: 0.9298 - val_loss: 7.8952e-06 - val_my_r2: 0.9975\n",
      "Epoch 1687/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4783e-04 - my_r2: 0.9406 - val_loss: 6.7542e-06 - val_my_r2: 0.9977\n",
      "Epoch 1688/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1378e-04 - my_r2: 0.9295 - val_loss: 9.2776e-06 - val_my_r2: 0.9967\n",
      "Epoch 1689/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2429e-04 - my_r2: 0.9306 - val_loss: 1.0486e-05 - val_my_r2: 0.9963\n",
      "Epoch 1690/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2105e-04 - my_r2: 0.9434 - val_loss: 1.6117e-05 - val_my_r2: 0.9942\n",
      "Epoch 1691/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4117e-04 - my_r2: 0.9511 - val_loss: 1.3977e-05 - val_my_r2: 0.9952\n",
      "Epoch 1692/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3638e-04 - my_r2: 0.9202 - val_loss: 1.5574e-05 - val_my_r2: 0.9952\n",
      "Epoch 1693/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9741e-04 - my_r2: 0.9186 - val_loss: 6.5811e-06 - val_my_r2: 0.9978\n",
      "Epoch 1694/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5286e-04 - my_r2: 0.9399 - val_loss: 8.2625e-06 - val_my_r2: 0.9970\n",
      "Epoch 1695/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9968e-04 - my_r2: 0.9300 - val_loss: 8.3068e-06 - val_my_r2: 0.9974\n",
      "Epoch 1696/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1579e-04 - my_r2: 0.9529 - val_loss: 1.0036e-05 - val_my_r2: 0.9967\n",
      "Epoch 1697/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0023e-04 - my_r2: 0.9534 - val_loss: 1.1812e-05 - val_my_r2: 0.9962\n",
      "Epoch 1698/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6301e-04 - my_r2: 0.9325 - val_loss: 9.3768e-06 - val_my_r2: 0.9969\n",
      "Epoch 1699/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4099e-04 - my_r2: 0.9493 - val_loss: 7.7551e-06 - val_my_r2: 0.9973\n",
      "Epoch 1700/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3769e-04 - my_r2: 0.9384 - val_loss: 8.3404e-06 - val_my_r2: 0.9970\n",
      "Epoch 1701/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0488e-04 - my_r2: 0.9408 - val_loss: 1.0388e-05 - val_my_r2: 0.9964\n",
      "Epoch 1702/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3788e-04 - my_r2: 0.9375 - val_loss: 8.3100e-06 - val_my_r2: 0.9971\n",
      "Epoch 1703/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5030e-04 - my_r2: 0.9257 - val_loss: 7.6334e-06 - val_my_r2: 0.9976\n",
      "Epoch 1704/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0196e-04 - my_r2: 0.9403 - val_loss: 6.4571e-06 - val_my_r2: 0.9979\n",
      "Epoch 1705/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7553e-04 - my_r2: 0.9136 - val_loss: 1.0462e-05 - val_my_r2: 0.9969\n",
      "Epoch 1706/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6425e-04 - my_r2: 0.9266 - val_loss: 8.8480e-06 - val_my_r2: 0.9974\n",
      "Epoch 1707/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2949e-04 - my_r2: 0.9360 - val_loss: 6.7875e-06 - val_my_r2: 0.9978\n",
      "Epoch 1708/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5529e-04 - my_r2: 0.9434 - val_loss: 6.0281e-06 - val_my_r2: 0.9982\n",
      "Epoch 1709/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8042e-04 - my_r2: 0.9314 - val_loss: 7.7145e-06 - val_my_r2: 0.9976\n",
      "Epoch 1710/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3181e-04 - my_r2: 0.9265 - val_loss: 8.8846e-06 - val_my_r2: 0.9973\n",
      "Epoch 1711/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0949e-04 - my_r2: 0.9371 - val_loss: 7.7693e-06 - val_my_r2: 0.9973\n",
      "Epoch 1712/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0442e-04 - my_r2: 0.8818 - val_loss: 6.5522e-06 - val_my_r2: 0.9978\n",
      "Epoch 1713/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3309e-04 - my_r2: 0.9509 - val_loss: 8.7808e-06 - val_my_r2: 0.9972\n",
      "Epoch 1714/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0302e-04 - my_r2: 0.9505 - val_loss: 1.3048e-05 - val_my_r2: 0.9956\n",
      "Epoch 1715/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6068e-04 - my_r2: 0.9259 - val_loss: 1.0964e-05 - val_my_r2: 0.9961\n",
      "Epoch 1716/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6202e-04 - my_r2: 0.9186 - val_loss: 9.5196e-06 - val_my_r2: 0.9968\n",
      "Epoch 1717/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3641e-04 - my_r2: 0.8723 - val_loss: 1.2598e-05 - val_my_r2: 0.9956\n",
      "Epoch 1718/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1898e-04 - my_r2: 0.9516 - val_loss: 6.5048e-06 - val_my_r2: 0.9981\n",
      "Epoch 1719/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2588e-04 - my_r2: 0.9449 - val_loss: 8.2474e-06 - val_my_r2: 0.9974\n",
      "Epoch 1720/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1219e-04 - my_r2: 0.9555 - val_loss: 7.6541e-06 - val_my_r2: 0.9973\n",
      "Epoch 1721/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1192e-04 - my_r2: 0.9264 - val_loss: 8.8296e-06 - val_my_r2: 0.9972\n",
      "Epoch 1722/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3389e-04 - my_r2: 0.9359 - val_loss: 1.2029e-05 - val_my_r2: 0.9960\n",
      "Epoch 1723/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6977e-04 - my_r2: 0.9021 - val_loss: 7.8279e-06 - val_my_r2: 0.9975\n",
      "Epoch 1724/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1197e-04 - my_r2: 0.9493 - val_loss: 1.3763e-05 - val_my_r2: 0.9952\n",
      "Epoch 1725/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7254e-04 - my_r2: 0.9431 - val_loss: 7.4625e-06 - val_my_r2: 0.9976\n",
      "Epoch 1726/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9814e-04 - my_r2: 0.8141 - val_loss: 6.9983e-06 - val_my_r2: 0.9978\n",
      "Epoch 1727/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6845e-04 - my_r2: 0.9322 - val_loss: 8.1769e-06 - val_my_r2: 0.9973\n",
      "Epoch 1728/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7014e-04 - my_r2: 0.9360 - val_loss: 1.0142e-05 - val_my_r2: 0.9966\n",
      "Epoch 1729/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1777e-04 - my_r2: 0.9358 - val_loss: 1.1593e-05 - val_my_r2: 0.9963\n",
      "Epoch 1730/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9267e-04 - my_r2: 0.9291 - val_loss: 8.7824e-06 - val_my_r2: 0.9970\n",
      "Epoch 1731/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9509e-04 - my_r2: 0.9052 - val_loss: 7.7104e-06 - val_my_r2: 0.9974\n",
      "Epoch 1732/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1438e-04 - my_r2: 0.9055 - val_loss: 9.7147e-06 - val_my_r2: 0.9964\n",
      "Epoch 1733/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5880e-04 - my_r2: 0.9267 - val_loss: 1.6307e-05 - val_my_r2: 0.9941\n",
      "Epoch 1734/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6851e-04 - my_r2: 0.9196 - val_loss: 1.7395e-05 - val_my_r2: 0.9946\n",
      "Epoch 1735/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4406e-04 - my_r2: 0.9404 - val_loss: 1.9505e-05 - val_my_r2: 0.9943\n",
      "Epoch 1736/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1602e-04 - my_r2: 0.8834 - val_loss: 1.0914e-05 - val_my_r2: 0.9966\n",
      "Epoch 1737/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1594e-04 - my_r2: 0.9469 - val_loss: 8.5930e-06 - val_my_r2: 0.9972\n",
      "Epoch 1738/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4406e-04 - my_r2: 0.9376 - val_loss: 1.1155e-05 - val_my_r2: 0.9963\n",
      "Epoch 1739/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2584e-04 - my_r2: 0.9469 - val_loss: 9.3500e-06 - val_my_r2: 0.9973\n",
      "Epoch 1740/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8286e-04 - my_r2: 0.9145 - val_loss: 8.5718e-06 - val_my_r2: 0.9971\n",
      "Epoch 1741/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6506e-04 - my_r2: 0.9001 - val_loss: 1.1467e-05 - val_my_r2: 0.9965\n",
      "Epoch 1742/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9390e-04 - my_r2: 0.9362 - val_loss: 1.0880e-05 - val_my_r2: 0.9968\n",
      "Epoch 1743/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4888e-04 - my_r2: 0.9331 - val_loss: 1.0509e-05 - val_my_r2: 0.9971\n",
      "Epoch 1744/2000\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 2.8393e-04 - my_r2: 0.9320 - val_loss: 9.0625e-06 - val_my_r2: 0.9976\n",
      "Epoch 1745/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3241e-04 - my_r2: 0.9166 - val_loss: 9.0546e-06 - val_my_r2: 0.9974\n",
      "Epoch 1746/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0695e-04 - my_r2: 0.9245 - val_loss: 9.1685e-06 - val_my_r2: 0.9968\n",
      "Epoch 1747/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3526e-04 - my_r2: 0.9338 - val_loss: 7.9306e-06 - val_my_r2: 0.9972\n",
      "Epoch 1748/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3746e-04 - my_r2: 0.9470 - val_loss: 7.6826e-06 - val_my_r2: 0.9977\n",
      "Epoch 1749/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2814e-04 - my_r2: 0.9469 - val_loss: 9.7486e-06 - val_my_r2: 0.9969\n",
      "Epoch 1750/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1991e-04 - my_r2: 0.8516 - val_loss: 9.2778e-06 - val_my_r2: 0.9972\n",
      "Epoch 1751/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5961e-04 - my_r2: 0.9155 - val_loss: 1.0233e-05 - val_my_r2: 0.9969\n",
      "Epoch 1752/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6583e-04 - my_r2: 0.9024 - val_loss: 9.4816e-06 - val_my_r2: 0.9970\n",
      "Epoch 1753/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.1466e-04 - my_r2: 0.9499 - val_loss: 8.1761e-06 - val_my_r2: 0.9975\n",
      "Epoch 1754/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3881e-04 - my_r2: 0.9287 - val_loss: 7.8079e-06 - val_my_r2: 0.9976\n",
      "Epoch 1755/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1560e-04 - my_r2: 0.9113 - val_loss: 9.2165e-06 - val_my_r2: 0.9970\n",
      "Epoch 1756/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8456e-04 - my_r2: 0.9363 - val_loss: 9.5221e-06 - val_my_r2: 0.9968\n",
      "Epoch 1757/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0886e-04 - my_r2: 0.7945 - val_loss: 7.9753e-06 - val_my_r2: 0.9975\n",
      "Epoch 1758/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9882e-04 - my_r2: 0.9371 - val_loss: 9.7773e-06 - val_my_r2: 0.9968\n",
      "Epoch 1759/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4031e-04 - my_r2: 0.9412 - val_loss: 1.5137e-05 - val_my_r2: 0.9952\n",
      "Epoch 1760/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6895e-04 - my_r2: 0.9053 - val_loss: 1.2947e-05 - val_my_r2: 0.9962\n",
      "Epoch 1761/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2748e-04 - my_r2: 0.9432 - val_loss: 1.0208e-05 - val_my_r2: 0.9970\n",
      "Epoch 1762/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9381e-04 - my_r2: 0.9512 - val_loss: 1.0527e-05 - val_my_r2: 0.9969\n",
      "Epoch 1763/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1625e-04 - my_r2: 0.9437 - val_loss: 9.7165e-06 - val_my_r2: 0.9973\n",
      "Epoch 1764/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6461e-04 - my_r2: 0.9164 - val_loss: 9.9487e-06 - val_my_r2: 0.9968\n",
      "Epoch 1765/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5640e-04 - my_r2: 0.9357 - val_loss: 8.2236e-06 - val_my_r2: 0.9972\n",
      "Epoch 1766/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8710e-04 - my_r2: 0.9376 - val_loss: 8.9561e-06 - val_my_r2: 0.9973\n",
      "Epoch 1767/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4688e-04 - my_r2: 0.9277 - val_loss: 8.3414e-06 - val_my_r2: 0.9973\n",
      "Epoch 1768/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5864e-04 - my_r2: 0.9087 - val_loss: 1.1209e-05 - val_my_r2: 0.9961\n",
      "Epoch 1769/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7680e-04 - my_r2: 0.9538 - val_loss: 1.3287e-05 - val_my_r2: 0.9955\n",
      "Epoch 1770/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8216e-04 - my_r2: 0.9031 - val_loss: 7.6969e-06 - val_my_r2: 0.9974\n",
      "Epoch 1771/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3699e-04 - my_r2: 0.9231 - val_loss: 1.0270e-05 - val_my_r2: 0.9964\n",
      "Epoch 1772/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2968e-04 - my_r2: 0.9469 - val_loss: 1.0469e-05 - val_my_r2: 0.9968\n",
      "Epoch 1773/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2395e-04 - my_r2: 0.8980 - val_loss: 1.0420e-05 - val_my_r2: 0.9969\n",
      "Epoch 1774/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8232e-04 - my_r2: 0.8167 - val_loss: 7.3260e-06 - val_my_r2: 0.9977\n",
      "Epoch 1775/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6529e-04 - my_r2: 0.9400 - val_loss: 1.2480e-05 - val_my_r2: 0.9959\n",
      "Epoch 1776/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.4798e-04 - my_r2: 0.8839 - val_loss: 1.5879e-05 - val_my_r2: 0.9948\n",
      "Epoch 1777/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4339e-04 - my_r2: 0.9200 - val_loss: 1.3410e-05 - val_my_r2: 0.9958\n",
      "Epoch 1778/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8290e-04 - my_r2: 0.9123 - val_loss: 1.0962e-05 - val_my_r2: 0.9963\n",
      "Epoch 1779/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7677e-04 - my_r2: 0.9247 - val_loss: 1.4038e-05 - val_my_r2: 0.9949\n",
      "Epoch 1780/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8187e-04 - my_r2: 0.8786 - val_loss: 1.4818e-05 - val_my_r2: 0.9948\n",
      "Epoch 1781/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3584e-04 - my_r2: 0.9275 - val_loss: 1.0687e-05 - val_my_r2: 0.9965\n",
      "Epoch 1782/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3900e-04 - my_r2: 0.9477 - val_loss: 8.6765e-06 - val_my_r2: 0.9972\n",
      "Epoch 1783/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1774e-04 - my_r2: 0.9218 - val_loss: 9.3574e-06 - val_my_r2: 0.9970\n",
      "Epoch 1784/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0385e-04 - my_r2: 0.9444 - val_loss: 8.8623e-06 - val_my_r2: 0.9971\n",
      "Epoch 1785/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.9048e-04 - my_r2: 0.9056 - val_loss: 1.3071e-05 - val_my_r2: 0.9959\n",
      "Epoch 1786/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3940e-04 - my_r2: 0.9268 - val_loss: 1.1464e-05 - val_my_r2: 0.9963\n",
      "Epoch 1787/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1407e-04 - my_r2: 0.9502 - val_loss: 9.6762e-06 - val_my_r2: 0.9969\n",
      "Epoch 1788/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2069e-04 - my_r2: 0.9222 - val_loss: 7.4477e-06 - val_my_r2: 0.9976\n",
      "Epoch 1789/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0881e-04 - my_r2: 0.8990 - val_loss: 8.9484e-06 - val_my_r2: 0.9971\n",
      "Epoch 1790/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7490e-04 - my_r2: 0.9191 - val_loss: 8.5048e-06 - val_my_r2: 0.9974\n",
      "Epoch 1791/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8646e-04 - my_r2: 0.9190 - val_loss: 8.1712e-06 - val_my_r2: 0.9975\n",
      "Epoch 1792/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5680e-04 - my_r2: 0.8940 - val_loss: 8.5415e-06 - val_my_r2: 0.9974\n",
      "Epoch 1793/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3645e-04 - my_r2: 0.9309 - val_loss: 1.0019e-05 - val_my_r2: 0.9968\n",
      "Epoch 1794/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0754e-04 - my_r2: 0.9449 - val_loss: 1.2434e-05 - val_my_r2: 0.9957\n",
      "Epoch 1795/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2021e-04 - my_r2: 0.9333 - val_loss: 1.1639e-05 - val_my_r2: 0.9963\n",
      "Epoch 1796/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4709e-04 - my_r2: 0.9183 - val_loss: 1.1622e-05 - val_my_r2: 0.9963\n",
      "Epoch 1797/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7109e-04 - my_r2: 0.9313 - val_loss: 9.8213e-06 - val_my_r2: 0.9969\n",
      "Epoch 1798/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6707e-04 - my_r2: 0.8990 - val_loss: 1.0059e-05 - val_my_r2: 0.9968\n",
      "Epoch 1799/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2745e-04 - my_r2: 0.9291 - val_loss: 1.2228e-05 - val_my_r2: 0.9962\n",
      "Epoch 1800/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1409e-04 - my_r2: 0.9499 - val_loss: 7.1886e-06 - val_my_r2: 0.9977\n",
      "Epoch 1801/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4439e-04 - my_r2: 0.9488 - val_loss: 8.9388e-06 - val_my_r2: 0.9973\n",
      "Epoch 1802/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4042e-04 - my_r2: 0.9234 - val_loss: 9.2157e-06 - val_my_r2: 0.9971\n",
      "Epoch 1803/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7481e-04 - my_r2: 0.8823 - val_loss: 8.2076e-06 - val_my_r2: 0.9976\n",
      "Epoch 1804/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5653e-04 - my_r2: 0.9296 - val_loss: 7.5254e-06 - val_my_r2: 0.9977\n",
      "Epoch 1805/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2371e-04 - my_r2: 0.9159 - val_loss: 7.9926e-06 - val_my_r2: 0.9974\n",
      "Epoch 1806/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1641e-04 - my_r2: 0.9450 - val_loss: 7.5233e-06 - val_my_r2: 0.9973\n",
      "Epoch 1807/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8468e-04 - my_r2: 0.8612 - val_loss: 8.8669e-06 - val_my_r2: 0.9965\n",
      "Epoch 1808/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4259e-04 - my_r2: 0.9397 - val_loss: 1.5531e-05 - val_my_r2: 0.9937\n",
      "Epoch 1809/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9510e-04 - my_r2: 0.9420 - val_loss: 1.2462e-05 - val_my_r2: 0.9947\n",
      "Epoch 1810/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8496e-04 - my_r2: 0.9527 - val_loss: 1.1868e-05 - val_my_r2: 0.9954\n",
      "Epoch 1811/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6033e-04 - my_r2: 0.9344 - val_loss: 1.0708e-05 - val_my_r2: 0.9956\n",
      "Epoch 1812/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7788e-04 - my_r2: 0.9132 - val_loss: 8.1700e-06 - val_my_r2: 0.9967\n",
      "Epoch 1813/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6750e-04 - my_r2: 0.9434 - val_loss: 8.0102e-06 - val_my_r2: 0.9966\n",
      "Epoch 1814/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6607e-04 - my_r2: 0.9219 - val_loss: 8.7128e-06 - val_my_r2: 0.9962\n",
      "Epoch 1815/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0771e-04 - my_r2: 0.9364 - val_loss: 8.8523e-06 - val_my_r2: 0.9964\n",
      "Epoch 1816/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4028e-04 - my_r2: 0.9137 - val_loss: 9.5132e-06 - val_my_r2: 0.9965\n",
      "Epoch 1817/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2101e-04 - my_r2: 0.9438 - val_loss: 1.0736e-05 - val_my_r2: 0.9958\n",
      "Epoch 1818/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6492e-04 - my_r2: 0.8936 - val_loss: 8.4888e-06 - val_my_r2: 0.9967\n",
      "Epoch 1819/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3123e-04 - my_r2: 0.9034 - val_loss: 8.1678e-06 - val_my_r2: 0.9971\n",
      "Epoch 1820/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9421e-04 - my_r2: 0.9253 - val_loss: 7.7392e-06 - val_my_r2: 0.9972\n",
      "Epoch 1821/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1708e-04 - my_r2: 0.9281 - val_loss: 7.9531e-06 - val_my_r2: 0.9971\n",
      "Epoch 1822/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4010e-04 - my_r2: 0.9370 - val_loss: 1.1045e-05 - val_my_r2: 0.9959\n",
      "Epoch 1823/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6399e-04 - my_r2: 0.9212 - val_loss: 1.0844e-05 - val_my_r2: 0.9956\n",
      "Epoch 1824/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6935e-04 - my_r2: 0.8887 - val_loss: 1.0535e-05 - val_my_r2: 0.9958\n",
      "Epoch 1825/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2123e-04 - my_r2: 0.9488 - val_loss: 7.7747e-06 - val_my_r2: 0.9969\n",
      "Epoch 1826/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7323e-04 - my_r2: 0.9157 - val_loss: 7.4976e-06 - val_my_r2: 0.9971\n",
      "Epoch 1827/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8485e-04 - my_r2: 0.9585 - val_loss: 9.2079e-06 - val_my_r2: 0.9967\n",
      "Epoch 1828/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.8729e-04 - my_r2: 0.9492 - val_loss: 9.4893e-06 - val_my_r2: 0.9969\n",
      "Epoch 1829/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2833e-04 - my_r2: 0.9533 - val_loss: 9.9193e-06 - val_my_r2: 0.9968\n",
      "Epoch 1830/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.5418e-04 - my_r2: 0.9480 - val_loss: 9.0302e-06 - val_my_r2: 0.9973\n",
      "Epoch 1831/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9753e-04 - my_r2: 0.9536 - val_loss: 6.6893e-06 - val_my_r2: 0.9978\n",
      "Epoch 1832/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0058e-04 - my_r2: 0.9561 - val_loss: 6.1801e-06 - val_my_r2: 0.9979\n",
      "Epoch 1833/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3109e-04 - my_r2: 0.9241 - val_loss: 8.6540e-06 - val_my_r2: 0.9970\n",
      "Epoch 1834/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0523e-04 - my_r2: 0.9263 - val_loss: 8.6645e-06 - val_my_r2: 0.9970\n",
      "Epoch 1835/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9414e-04 - my_r2: 0.9315 - val_loss: 8.5823e-06 - val_my_r2: 0.9971\n",
      "Epoch 1836/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8446e-04 - my_r2: 0.9146 - val_loss: 6.4389e-06 - val_my_r2: 0.9978\n",
      "Epoch 1837/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6912e-04 - my_r2: 0.9156 - val_loss: 7.3908e-06 - val_my_r2: 0.9976\n",
      "Epoch 1838/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9285e-04 - my_r2: 0.9362 - val_loss: 1.0028e-05 - val_my_r2: 0.9960\n",
      "Epoch 1839/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6674e-04 - my_r2: 0.9340 - val_loss: 1.0774e-05 - val_my_r2: 0.9953\n",
      "Epoch 1840/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7976e-04 - my_r2: 0.9354 - val_loss: 1.0720e-05 - val_my_r2: 0.9959\n",
      "Epoch 1841/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7084e-04 - my_r2: 0.9376 - val_loss: 1.0948e-05 - val_my_r2: 0.9960\n",
      "Epoch 1842/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7747e-04 - my_r2: 0.9419 - val_loss: 9.6593e-06 - val_my_r2: 0.9967\n",
      "Epoch 1843/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8599e-04 - my_r2: 0.9352 - val_loss: 7.8554e-06 - val_my_r2: 0.9971\n",
      "Epoch 1844/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1895e-04 - my_r2: 0.9531 - val_loss: 8.0051e-06 - val_my_r2: 0.9973\n",
      "Epoch 1845/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.0705e-04 - my_r2: 0.9482 - val_loss: 1.0678e-05 - val_my_r2: 0.9967\n",
      "Epoch 1846/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4517e-04 - my_r2: 0.9400 - val_loss: 1.1018e-05 - val_my_r2: 0.9966\n",
      "Epoch 1847/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.0184e-04 - my_r2: 0.9312 - val_loss: 1.0692e-05 - val_my_r2: 0.9966\n",
      "Epoch 1848/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8230e-04 - my_r2: 0.9340 - val_loss: 1.1999e-05 - val_my_r2: 0.9962\n",
      "Epoch 1849/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0926e-04 - my_r2: 0.9378 - val_loss: 1.1514e-05 - val_my_r2: 0.9965\n",
      "Epoch 1850/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3928e-04 - my_r2: 0.9349 - val_loss: 1.1865e-05 - val_my_r2: 0.9962\n",
      "Epoch 1851/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5815e-04 - my_r2: 0.9192 - val_loss: 7.4731e-06 - val_my_r2: 0.9975\n",
      "Epoch 1852/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3986e-04 - my_r2: 0.9426 - val_loss: 8.0158e-06 - val_my_r2: 0.9973\n",
      "Epoch 1853/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9152e-04 - my_r2: 0.9363 - val_loss: 9.4361e-06 - val_my_r2: 0.9967\n",
      "Epoch 1854/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3264e-04 - my_r2: 0.9199 - val_loss: 1.0473e-05 - val_my_r2: 0.9962\n",
      "Epoch 1855/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1132e-04 - my_r2: 0.9514 - val_loss: 1.3261e-05 - val_my_r2: 0.9951\n",
      "Epoch 1856/2000\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 2.3742e-04 - my_r2: 0.9480 - val_loss: 1.2117e-05 - val_my_r2: 0.9956\n",
      "Epoch 1857/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5467e-04 - my_r2: 0.9267 - val_loss: 1.0618e-05 - val_my_r2: 0.9961\n",
      "Epoch 1858/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7964e-04 - my_r2: 0.9164 - val_loss: 9.0815e-06 - val_my_r2: 0.9965\n",
      "Epoch 1859/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9028e-04 - my_r2: 0.9271 - val_loss: 1.5164e-05 - val_my_r2: 0.9936\n",
      "Epoch 1860/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1135e-04 - my_r2: 0.9596 - val_loss: 1.2428e-05 - val_my_r2: 0.9947\n",
      "Epoch 1861/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9389e-04 - my_r2: 0.9439 - val_loss: 1.0012e-05 - val_my_r2: 0.9962\n",
      "Epoch 1862/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7515e-04 - my_r2: 0.9366 - val_loss: 9.0355e-06 - val_my_r2: 0.9969\n",
      "Epoch 1863/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.3904e-04 - my_r2: 0.9320 - val_loss: 7.2038e-06 - val_my_r2: 0.9976\n",
      "Epoch 1864/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7308e-04 - my_r2: 0.9169 - val_loss: 8.6008e-06 - val_my_r2: 0.9968\n",
      "Epoch 1865/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2612e-04 - my_r2: 0.9305 - val_loss: 1.1872e-05 - val_my_r2: 0.9958\n",
      "Epoch 1866/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4977e-04 - my_r2: 0.9390 - val_loss: 1.3137e-05 - val_my_r2: 0.9953\n",
      "Epoch 1867/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7746e-04 - my_r2: 0.9441 - val_loss: 1.0542e-05 - val_my_r2: 0.9961\n",
      "Epoch 1868/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7712e-04 - my_r2: 0.9392 - val_loss: 9.4579e-06 - val_my_r2: 0.9967\n",
      "Epoch 1869/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5101e-04 - my_r2: 0.9434 - val_loss: 1.0255e-05 - val_my_r2: 0.9964\n",
      "Epoch 1870/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6280e-04 - my_r2: 0.9306 - val_loss: 9.7990e-06 - val_my_r2: 0.9965\n",
      "Epoch 1871/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9422e-04 - my_r2: 0.9587 - val_loss: 1.3410e-05 - val_my_r2: 0.9956\n",
      "Epoch 1872/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9517e-04 - my_r2: 0.9289 - val_loss: 1.2877e-05 - val_my_r2: 0.9959\n",
      "Epoch 1873/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2863e-04 - my_r2: 0.9136 - val_loss: 1.1013e-05 - val_my_r2: 0.9964\n",
      "Epoch 1874/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0502e-04 - my_r2: 0.9388 - val_loss: 1.2104e-05 - val_my_r2: 0.9961\n",
      "Epoch 1875/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9025e-04 - my_r2: 0.9147 - val_loss: 1.0362e-05 - val_my_r2: 0.9967\n",
      "Epoch 1876/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2694e-04 - my_r2: 0.9399 - val_loss: 1.5168e-05 - val_my_r2: 0.9954\n",
      "Epoch 1877/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5924e-04 - my_r2: 0.9262 - val_loss: 1.9406e-05 - val_my_r2: 0.9940\n",
      "Epoch 1878/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3942e-04 - my_r2: 0.8750 - val_loss: 7.7218e-06 - val_my_r2: 0.9976\n",
      "Epoch 1879/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6964e-04 - my_r2: 0.9281 - val_loss: 7.6940e-06 - val_my_r2: 0.9975\n",
      "Epoch 1880/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.8047e-04 - my_r2: 0.9621 - val_loss: 1.0279e-05 - val_my_r2: 0.9962\n",
      "Epoch 1881/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3219e-04 - my_r2: 0.9376 - val_loss: 8.4046e-06 - val_my_r2: 0.9972\n",
      "Epoch 1882/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5386e-04 - my_r2: 0.9317 - val_loss: 8.2769e-06 - val_my_r2: 0.9972\n",
      "Epoch 1883/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1195e-04 - my_r2: 0.9100 - val_loss: 1.0217e-05 - val_my_r2: 0.9967\n",
      "Epoch 1884/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0298e-04 - my_r2: 0.9406 - val_loss: 1.0264e-05 - val_my_r2: 0.9963\n",
      "Epoch 1885/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5443e-04 - my_r2: 0.9312 - val_loss: 8.7748e-06 - val_my_r2: 0.9967\n",
      "Epoch 1886/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0832e-04 - my_r2: 0.9191 - val_loss: 1.1057e-05 - val_my_r2: 0.9964\n",
      "Epoch 1887/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2467e-04 - my_r2: 0.9491 - val_loss: 1.2043e-05 - val_my_r2: 0.9962\n",
      "Epoch 1888/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1285e-04 - my_r2: 0.9354 - val_loss: 1.1125e-05 - val_my_r2: 0.9965\n",
      "Epoch 1889/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8348e-04 - my_r2: 0.9335 - val_loss: 1.0420e-05 - val_my_r2: 0.9967\n",
      "Epoch 1890/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.5359e-04 - my_r2: 0.9571 - val_loss: 9.9902e-06 - val_my_r2: 0.9968\n",
      "Epoch 1891/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7416e-04 - my_r2: 0.9319 - val_loss: 1.0934e-05 - val_my_r2: 0.9962\n",
      "Epoch 1892/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.8287e-04 - my_r2: 0.9357 - val_loss: 9.0894e-06 - val_my_r2: 0.9969\n",
      "Epoch 1893/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3516e-04 - my_r2: 0.9437 - val_loss: 8.6418e-06 - val_my_r2: 0.9974\n",
      "Epoch 1894/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3231e-04 - my_r2: 0.9468 - val_loss: 9.7728e-06 - val_my_r2: 0.9969\n",
      "Epoch 1895/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8305e-04 - my_r2: 0.9359 - val_loss: 1.2791e-05 - val_my_r2: 0.9955\n",
      "Epoch 1896/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0200e-04 - my_r2: 0.9298 - val_loss: 1.2818e-05 - val_my_r2: 0.9959\n",
      "Epoch 1897/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8205e-04 - my_r2: 0.9020 - val_loss: 1.6140e-05 - val_my_r2: 0.9952\n",
      "Epoch 1898/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6822e-04 - my_r2: 0.8978 - val_loss: 9.6099e-06 - val_my_r2: 0.9970\n",
      "Epoch 1899/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6811e-04 - my_r2: 0.9465 - val_loss: 8.4033e-06 - val_my_r2: 0.9970\n",
      "Epoch 1900/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6128e-04 - my_r2: 0.9188 - val_loss: 6.6048e-06 - val_my_r2: 0.9978\n",
      "Epoch 1901/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8921e-04 - my_r2: 0.9402 - val_loss: 7.6055e-06 - val_my_r2: 0.9977\n",
      "Epoch 1902/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4303e-04 - my_r2: 0.9409 - val_loss: 7.8433e-06 - val_my_r2: 0.9976\n",
      "Epoch 1903/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2598e-04 - my_r2: 0.9368 - val_loss: 7.5030e-06 - val_my_r2: 0.9974\n",
      "Epoch 1904/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0987e-04 - my_r2: 0.9461 - val_loss: 8.4264e-06 - val_my_r2: 0.9973\n",
      "Epoch 1905/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3388e-04 - my_r2: 0.9268 - val_loss: 1.0726e-05 - val_my_r2: 0.9965\n",
      "Epoch 1906/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5704e-04 - my_r2: 0.9338 - val_loss: 8.6632e-06 - val_my_r2: 0.9974\n",
      "Epoch 1907/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7555e-04 - my_r2: 0.9648 - val_loss: 8.1359e-06 - val_my_r2: 0.9975\n",
      "Epoch 1908/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2516e-04 - my_r2: 0.9300 - val_loss: 1.4971e-05 - val_my_r2: 0.9948\n",
      "Epoch 1909/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1984e-04 - my_r2: 0.9504 - val_loss: 1.3284e-05 - val_my_r2: 0.9952\n",
      "Epoch 1910/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1994e-04 - my_r2: 0.9529 - val_loss: 7.7555e-06 - val_my_r2: 0.9973\n",
      "Epoch 1911/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0304e-04 - my_r2: 0.9509 - val_loss: 1.6055e-05 - val_my_r2: 0.9944\n",
      "Epoch 1912/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7653e-04 - my_r2: 0.9096 - val_loss: 8.7514e-06 - val_my_r2: 0.9972\n",
      "Epoch 1913/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3546e-04 - my_r2: 0.9219 - val_loss: 8.6091e-06 - val_my_r2: 0.9970\n",
      "Epoch 1914/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1266e-04 - my_r2: 0.9036 - val_loss: 8.1777e-06 - val_my_r2: 0.9972\n",
      "Epoch 1915/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0849e-04 - my_r2: 0.9462 - val_loss: 7.1881e-06 - val_my_r2: 0.9973\n",
      "Epoch 1916/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5983e-04 - my_r2: 0.8994 - val_loss: 7.1849e-06 - val_my_r2: 0.9974\n",
      "Epoch 1917/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1994e-04 - my_r2: 0.9398 - val_loss: 9.4762e-06 - val_my_r2: 0.9964\n",
      "Epoch 1918/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2335e-04 - my_r2: 0.9107 - val_loss: 1.0559e-05 - val_my_r2: 0.9962\n",
      "Epoch 1919/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2456e-04 - my_r2: 0.9553 - val_loss: 8.6432e-06 - val_my_r2: 0.9970\n",
      "Epoch 1920/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9304e-04 - my_r2: 0.9494 - val_loss: 1.0046e-05 - val_my_r2: 0.9969\n",
      "Epoch 1921/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1069e-04 - my_r2: 0.8938 - val_loss: 1.3474e-05 - val_my_r2: 0.9957\n",
      "Epoch 1922/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5603e-04 - my_r2: 0.9369 - val_loss: 1.4231e-05 - val_my_r2: 0.9955\n",
      "Epoch 1923/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 3.1418e-04 - my_r2: 0.9123 - val_loss: 1.5707e-05 - val_my_r2: 0.9949\n",
      "Epoch 1924/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9153e-04 - my_r2: 0.9008 - val_loss: 1.8946e-05 - val_my_r2: 0.9936\n",
      "Epoch 1925/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.6306e-04 - my_r2: 0.9199 - val_loss: 1.4856e-05 - val_my_r2: 0.9951\n",
      "Epoch 1926/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2261e-04 - my_r2: 0.9322 - val_loss: 1.6160e-05 - val_my_r2: 0.9945\n",
      "Epoch 1927/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6583e-04 - my_r2: 0.9361 - val_loss: 1.3682e-05 - val_my_r2: 0.9956\n",
      "Epoch 1928/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6661e-04 - my_r2: 0.9342 - val_loss: 9.1635e-06 - val_my_r2: 0.9965\n",
      "Epoch 1929/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9267e-04 - my_r2: 0.9102 - val_loss: 8.6714e-06 - val_my_r2: 0.9970\n",
      "Epoch 1930/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7700e-04 - my_r2: 0.9402 - val_loss: 1.4306e-05 - val_my_r2: 0.9954\n",
      "Epoch 1931/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6548e-04 - my_r2: 0.9239 - val_loss: 1.2670e-05 - val_my_r2: 0.9962\n",
      "Epoch 1932/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.7328e-04 - my_r2: 0.9147 - val_loss: 1.0459e-05 - val_my_r2: 0.9967\n",
      "Epoch 1933/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3381e-04 - my_r2: 0.9410 - val_loss: 8.4026e-06 - val_my_r2: 0.9971\n",
      "Epoch 1934/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8378e-04 - my_r2: 0.8387 - val_loss: 8.1959e-06 - val_my_r2: 0.9967\n",
      "Epoch 1935/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3420e-04 - my_r2: 0.9421 - val_loss: 7.5208e-06 - val_my_r2: 0.9969\n",
      "Epoch 1936/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2315e-04 - my_r2: 0.9360 - val_loss: 8.9780e-06 - val_my_r2: 0.9966\n",
      "Epoch 1937/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.2112e-04 - my_r2: 0.9443 - val_loss: 7.2272e-06 - val_my_r2: 0.9975\n",
      "Epoch 1938/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1575e-04 - my_r2: 0.9500 - val_loss: 7.4682e-06 - val_my_r2: 0.9973\n",
      "Epoch 1939/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4260e-04 - my_r2: 0.9248 - val_loss: 6.8840e-06 - val_my_r2: 0.9975\n",
      "Epoch 1940/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6085e-04 - my_r2: 0.9239 - val_loss: 5.9071e-06 - val_my_r2: 0.9980\n",
      "Epoch 1941/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5314e-04 - my_r2: 0.9326 - val_loss: 7.8071e-06 - val_my_r2: 0.9974\n",
      "Epoch 1942/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9096e-04 - my_r2: 0.8757 - val_loss: 8.1874e-06 - val_my_r2: 0.9974\n",
      "Epoch 1943/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3328e-04 - my_r2: 0.9468 - val_loss: 6.4396e-06 - val_my_r2: 0.9981\n",
      "Epoch 1944/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4301e-04 - my_r2: 0.9203 - val_loss: 1.4851e-05 - val_my_r2: 0.9953\n",
      "Epoch 1945/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9951e-04 - my_r2: 0.9327 - val_loss: 1.5696e-05 - val_my_r2: 0.9947\n",
      "Epoch 1946/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9404e-04 - my_r2: 0.9400 - val_loss: 7.6458e-06 - val_my_r2: 0.9974\n",
      "Epoch 1947/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5121e-04 - my_r2: 0.9044 - val_loss: 1.1757e-05 - val_my_r2: 0.9958\n",
      "Epoch 1948/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3811e-04 - my_r2: 0.9322 - val_loss: 1.2994e-05 - val_my_r2: 0.9959\n",
      "Epoch 1949/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4199e-04 - my_r2: 0.9397 - val_loss: 1.5200e-05 - val_my_r2: 0.9950\n",
      "Epoch 1950/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3429e-04 - my_r2: 0.9335 - val_loss: 1.2896e-05 - val_my_r2: 0.9955\n",
      "Epoch 1951/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.2342e-04 - my_r2: 0.9457 - val_loss: 9.2754e-06 - val_my_r2: 0.9968\n",
      "Epoch 1952/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1921e-04 - my_r2: 0.9584 - val_loss: 8.2616e-06 - val_my_r2: 0.9969\n",
      "Epoch 1953/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.1727e-04 - my_r2: 0.9233 - val_loss: 6.5958e-06 - val_my_r2: 0.9978\n",
      "Epoch 1954/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4047e-04 - my_r2: 0.8939 - val_loss: 8.5815e-06 - val_my_r2: 0.9969\n",
      "Epoch 1955/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8004e-04 - my_r2: 0.9368 - val_loss: 1.0883e-05 - val_my_r2: 0.9961\n",
      "Epoch 1956/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3476e-04 - my_r2: 0.9411 - val_loss: 9.7864e-06 - val_my_r2: 0.9967\n",
      "Epoch 1957/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.7112e-04 - my_r2: 0.9600 - val_loss: 8.7371e-06 - val_my_r2: 0.9970\n",
      "Epoch 1958/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0886e-04 - my_r2: 0.9336 - val_loss: 7.1676e-06 - val_my_r2: 0.9976\n",
      "Epoch 1959/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4752e-04 - my_r2: 0.8843 - val_loss: 1.1162e-05 - val_my_r2: 0.9961\n",
      "Epoch 1960/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3551e-04 - my_r2: 0.9124 - val_loss: 1.0608e-05 - val_my_r2: 0.9960\n",
      "Epoch 1961/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8644e-04 - my_r2: 0.9118 - val_loss: 8.0455e-06 - val_my_r2: 0.9972\n",
      "Epoch 1962/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.7554e-04 - my_r2: 0.9264 - val_loss: 1.1080e-05 - val_my_r2: 0.9965\n",
      "Epoch 1963/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3130e-04 - my_r2: 0.9387 - val_loss: 9.6179e-06 - val_my_r2: 0.9970\n",
      "Epoch 1964/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4205e-04 - my_r2: 0.9295 - val_loss: 1.1405e-05 - val_my_r2: 0.9955\n",
      "Epoch 1965/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0634e-04 - my_r2: 0.9433 - val_loss: 8.2205e-06 - val_my_r2: 0.9973\n",
      "Epoch 1966/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.5671e-04 - my_r2: 0.9205 - val_loss: 8.1577e-06 - val_my_r2: 0.9976\n",
      "Epoch 1967/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9407e-04 - my_r2: 0.9497 - val_loss: 6.9280e-06 - val_my_r2: 0.9978\n",
      "Epoch 1968/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.9292e-04 - my_r2: 0.9209 - val_loss: 6.1064e-06 - val_my_r2: 0.9979\n",
      "Epoch 1969/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9291e-04 - my_r2: 0.9509 - val_loss: 7.0231e-06 - val_my_r2: 0.9975\n",
      "Epoch 1970/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3907e-04 - my_r2: 0.9320 - val_loss: 8.4771e-06 - val_my_r2: 0.9972\n",
      "Epoch 1971/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4357e-04 - my_r2: 0.9401 - val_loss: 8.1663e-06 - val_my_r2: 0.9974\n",
      "Epoch 1972/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.6522e-04 - my_r2: 0.9376 - val_loss: 1.0356e-05 - val_my_r2: 0.9965\n",
      "Epoch 1973/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4224e-04 - my_r2: 0.9131 - val_loss: 1.2991e-05 - val_my_r2: 0.9955\n",
      "Epoch 1974/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3808e-04 - my_r2: 0.9225 - val_loss: 1.1096e-05 - val_my_r2: 0.9962\n",
      "Epoch 1975/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4155e-04 - my_r2: 0.9484 - val_loss: 1.0947e-05 - val_my_r2: 0.9960\n",
      "Epoch 1976/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9947e-04 - my_r2: 0.9246 - val_loss: 1.5735e-05 - val_my_r2: 0.9939\n",
      "Epoch 1977/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4088e-04 - my_r2: 0.9454 - val_loss: 1.3722e-05 - val_my_r2: 0.9949\n",
      "Epoch 1978/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 1.9514e-04 - my_r2: 0.9448 - val_loss: 1.1202e-05 - val_my_r2: 0.9962\n",
      "Epoch 1979/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4199e-04 - my_r2: 0.9351 - val_loss: 1.3425e-05 - val_my_r2: 0.9954\n",
      "Epoch 1980/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.1707e-04 - my_r2: 0.9461 - val_loss: 1.2655e-05 - val_my_r2: 0.9957\n",
      "Epoch 1981/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5222e-04 - my_r2: 0.9150 - val_loss: 1.1156e-05 - val_my_r2: 0.9958\n",
      "Epoch 1982/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6834e-04 - my_r2: 0.8995 - val_loss: 9.9024e-06 - val_my_r2: 0.9964\n",
      "Epoch 1983/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.5758e-04 - my_r2: 0.9263 - val_loss: 8.7707e-06 - val_my_r2: 0.9966\n",
      "Epoch 1984/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9699e-04 - my_r2: 0.9404 - val_loss: 8.6985e-06 - val_my_r2: 0.9968\n",
      "Epoch 1985/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3120e-04 - my_r2: 0.9506 - val_loss: 9.7208e-06 - val_my_r2: 0.9963\n",
      "Epoch 1986/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0320e-04 - my_r2: 0.9302 - val_loss: 1.0574e-05 - val_my_r2: 0.9959\n",
      "Epoch 1987/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7362e-04 - my_r2: 0.9419 - val_loss: 1.0497e-05 - val_my_r2: 0.9961\n",
      "Epoch 1988/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.3757e-04 - my_r2: 0.9310 - val_loss: 6.4205e-06 - val_my_r2: 0.9975\n",
      "Epoch 1989/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.6345e-04 - my_r2: 0.7368 - val_loss: 1.0027e-05 - val_my_r2: 0.9956\n",
      "Epoch 1990/2000\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 2.4647e-04 - my_r2: 0.9378 - val_loss: 8.4843e-06 - val_my_r2: 0.9963\n",
      "Epoch 1991/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.8873e-04 - my_r2: 0.9277 - val_loss: 9.0409e-06 - val_my_r2: 0.9964\n",
      "Epoch 1992/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.7744e-04 - my_r2: 0.9248 - val_loss: 1.1273e-05 - val_my_r2: 0.9959\n",
      "Epoch 1993/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4855e-04 - my_r2: 0.9382 - val_loss: 1.1303e-05 - val_my_r2: 0.9961\n",
      "Epoch 1994/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.9936e-04 - my_r2: 0.9467 - val_loss: 1.1259e-05 - val_my_r2: 0.9964\n",
      "Epoch 1995/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.3415e-04 - my_r2: 0.9444 - val_loss: 9.8563e-06 - val_my_r2: 0.9970\n",
      "Epoch 1996/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4682e-04 - my_r2: 0.9259 - val_loss: 9.5136e-06 - val_my_r2: 0.9971\n",
      "Epoch 1997/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 1.6637e-04 - my_r2: 0.9617 - val_loss: 9.8568e-06 - val_my_r2: 0.9969\n",
      "Epoch 1998/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 3.0569e-04 - my_r2: 0.9152 - val_loss: 7.7984e-06 - val_my_r2: 0.9973\n",
      "Epoch 1999/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.4149e-04 - my_r2: 0.9427 - val_loss: 1.2445e-05 - val_my_r2: 0.9957\n",
      "Epoch 2000/2000\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 2.0639e-04 - my_r2: 0.9346 - val_loss: 7.3141e-06 - val_my_r2: 0.9973\n",
      "---------- 1\n",
      "---------- 1\n",
      "train = 1.00 test = 1.00 loss-train = -1.000000 loss-test = -1.000000 iter=0\n",
      "Stats for iML1515_ec6_UB_AMN_QP CPU-time 1401.8422\n",
      "R2 = 0.9990 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Q2 = 0.9990 (+/- 0.0000) Constraint = -1.0000 (+/- 0.0000)\n",
      "Iter 4 Collated Q2 0.9989714050257041\n",
      "(5, 110, 1228)\n",
      "Averaged Q2 = 0.9989 (+/- 0.0002)\n",
      "Loss SV average 0.0040 max 0.0095\n",
      "Loss Vin average 0.0000 max 0.0000\n",
      "Loss Vpos average 0.0000 max 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Create, train and evaluate RC E. coli models on experimental training set \n",
    "# Repeat the process with different seeds\n",
    "# This cell takes several hours to execute\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "date_str = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "epoch_input = 2000\n",
    "fold_input = 0\n",
    "seed_input = 5\n",
    "\n",
    "counter = 1\n",
    "seed_q2_dict = {}\n",
    "\n",
    "while counter <= 1:\n",
    "    histories = {}\n",
    "    Maxloop, Q2, PRED = seed_input, [], []  \n",
    "    for Nloop in range(Maxloop):\n",
    "        # What you can change \n",
    "        seed = Nloop+10\n",
    "        np.random.seed(seed=seed)  \n",
    "        random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "        \n",
    "        #trainname = 'iML1515_EXP'\n",
    "        #reservoirname = 'iML1515_UB_AMN_QP'\n",
    "\n",
    "        trainname = 'iML1515_ec_EXP'\n",
    "        reservoirname = 'iML1515_ec6_UB_AMN_QP'\n",
    "\n",
    "        experiment_name = f'{date_str}_{epoch_input}-{fold_input}-{seed_input}_seed{seed}_fixedseed_alr' \n",
    "\n",
    "        # End of What you can change\n",
    "\n",
    "        # Create model \n",
    "        trainingfile = DIRECTORY+'Dataset_input/'+trainname\n",
    "        reservoirfile = DIRECTORY+'Reservoir/'+reservoirname\n",
    "        X, Y = read_XY(trainingfile, 38)\n",
    "        model = RC_Model(reservoirfile = reservoirfile, X=X, Y=Y, \n",
    "                    n_hidden_prior = 1, hidden_dim_prior = 500, activation_prior = 'relu',\n",
    "                    epochs = epoch_input, train_rate = 1.0e-4, xfold = fold_input,\n",
    "                    verbose=True)\n",
    "        model.printout()\n",
    "\n",
    "        # Train and evaluate\n",
    "        start_time = time.time()\n",
    "        reservoir, pred, stats, history = train_evaluate_model(model, verbose=2, seed=seed, alr=False)\n",
    "        delta_time = time.time() - start_time\n",
    "\n",
    "        histories[Nloop] = history\n",
    "\n",
    "        # Printing cross-validation results\n",
    "        printout(reservoirname, stats, model, delta_time)\n",
    "        r2 = r2_score(model.Y, pred[:,0], multioutput='variance_weighted')\n",
    "        print('Iter', Nloop, 'Collated Q2', r2)\n",
    "        Q2.append(r2)\n",
    "        PRED.append(pred)\n",
    "\n",
    "    # Some printings and savings\n",
    "    Q2, PRED = np.asarray(Q2), np.asarray(PRED)\n",
    "    print(PRED.shape)\n",
    "    obj = PRED[:,:,0]\n",
    "    print('Averaged Q2 = %.4f (+/- %.4f)' % (np.mean(Q2), np.std(Q2)))\n",
    "    filename = DIRECTORY+'Result/'+experiment_name+'_'+reservoirname+'_'\\\n",
    "    +str(model.model_type)+'_Q2.csv'\n",
    "    np.savetxt(filename, Q2, delimiter=',')\n",
    "    filename = DIRECTORY+'Result/'+experiment_name+'_'+reservoirname+'_'\\\n",
    "    +str(model.model_type)+'_PRED.csv'\n",
    "    np.savetxt(filename, obj, delimiter=',')\n",
    "\n",
    "    seed_q2_dict[seed] = np.mean(Q2)\n",
    "\n",
    "\n",
    "    # Saving input / output of the first prediction for Cobra runs\n",
    "    pred, obj = PRED[0], pred[:,0]\n",
    "    Loss_SV, Loss_Vin, Loss_Vpos = pred[:,1], pred[:,2], pred[:,3]\n",
    "    print('Loss SV average %.4f max %.4f' % (np.mean(Loss_SV), np.max(Loss_SV)))\n",
    "    print('Loss Vin average %.4f max %.4f' % (np.mean(Loss_Vin), np.max(Loss_Vin)))\n",
    "    print('Loss Vpos average %.4f max %.4f' % (np.mean(Loss_Vpos), np.max(Loss_Vpos)))\n",
    "    V = pred[:, 4:4+model.S.shape[1]]\n",
    "    Vin = pred[:, 4+model.S.shape[1]:]\n",
    "    Vin = model.res.scaler * Vin # rescale back for Cobra\n",
    "    XY = np.concatenate((Vin, Y), axis=1)\n",
    "    filename = DIRECTORY+'Result/'+experiment_name+'_'+reservoirname+'_'\\\n",
    "    +str(model.model_type)+'_solution_for_Cobra.csv'\n",
    "    np.savetxt(filename, XY, delimiter=',')\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49573fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved model components and metadata to: ./Result/20250503232341_1000-0-5_seed14_fixedseed_alr_\n"
     ]
    }
   ],
   "source": [
    "# Save trained model components and metadata (after last training loop)\n",
    "from tensorflow.keras.models import save_model\n",
    "import pickle\n",
    "\n",
    "save_path = DIRECTORY + \"Result/\" + experiment_name + \"_\"\n",
    "\n",
    "# Save the inner keras model of each component if available\n",
    "if model.prior and model.prior.model:\n",
    "    model.prior.model.save(save_path + \"prior_model.h5\")\n",
    "\n",
    "if model.res and model.res.model:\n",
    "    model.res.model.save(save_path + \"reservoir_model.h5\")\n",
    "\n",
    "if model.post and model.post.model:\n",
    "    model.post.model.save(save_path + \"post_model.h5\")\n",
    "\n",
    "# Save model metadata/config for reloading later\n",
    "metadata = {\n",
    "    \"reservoirfile\": model.reservoirfile,\n",
    "    \"scaler\": model.scaler,\n",
    "    \"input_dim\": model.input_dim,\n",
    "    \"output_dim\": model.output_dim,\n",
    "    \"epochs\": model.epochs,\n",
    "    \"regression\": model.regression,\n",
    "    \"train_rate\": model.train_rate,\n",
    "    \"dropout\": model.dropout,\n",
    "    \"batch_size\": model.batch_size,\n",
    "    \"niter\": model.niter,\n",
    "    \"xfold\": model.xfold,\n",
    "    \"es\": model.es,\n",
    "    \"model_type\": model.model_type,\n",
    "    \"mediumbound\": model.mediumbound,\n",
    "}\n",
    "\n",
    "with open(save_path + \"model_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\" Saved model components and metadata to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a796c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c10f1ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'val_loss': [0.018073149025440216,\n",
       "   0.017518315464258194,\n",
       "   0.016885681077837944,\n",
       "   0.016192268580198288,\n",
       "   0.01555590983480215,\n",
       "   0.01482730358839035,\n",
       "   0.014101194217801094,\n",
       "   0.013172313570976257,\n",
       "   0.012306622229516506,\n",
       "   0.011324308812618256,\n",
       "   0.010415182448923588,\n",
       "   0.009596706368029118,\n",
       "   0.008761893957853317,\n",
       "   0.008007559925317764,\n",
       "   0.007247932720929384,\n",
       "   0.006626994349062443,\n",
       "   0.005990899633616209,\n",
       "   0.0055144852958619595,\n",
       "   0.005031522363424301,\n",
       "   0.004650548566132784,\n",
       "   0.004264102317392826,\n",
       "   0.003985981456935406,\n",
       "   0.0037612507585436106,\n",
       "   0.0035155038349330425,\n",
       "   0.0033707383554428816,\n",
       "   0.0032201861031353474,\n",
       "   0.0030814530327916145,\n",
       "   0.002979496493935585,\n",
       "   0.002882398432120681,\n",
       "   0.002783416071906686,\n",
       "   0.002701801946386695,\n",
       "   0.002626619767397642,\n",
       "   0.0025664768181741238,\n",
       "   0.0025051170960068703,\n",
       "   0.0024482908193022013,\n",
       "   0.0023986846208572388,\n",
       "   0.0023535091895610094,\n",
       "   0.0023075605276972055,\n",
       "   0.0022719905246049166,\n",
       "   0.002234571147710085,\n",
       "   0.002192490268498659,\n",
       "   0.002153643174096942,\n",
       "   0.0021131227258592844,\n",
       "   0.0020785559900105,\n",
       "   0.0020435992628335953,\n",
       "   0.0020168463233858347,\n",
       "   0.001986512914299965,\n",
       "   0.001957549713551998,\n",
       "   0.0019301028223708272,\n",
       "   0.00190358969848603,\n",
       "   0.0018797845114022493,\n",
       "   0.0018511525122448802,\n",
       "   0.0018252069130539894,\n",
       "   0.0018006744794547558,\n",
       "   0.0017761000199243426,\n",
       "   0.0017505827127024531,\n",
       "   0.001728129223920405,\n",
       "   0.0017053834162652493,\n",
       "   0.001682810252532363,\n",
       "   0.0016615248750895262,\n",
       "   0.001640562666580081,\n",
       "   0.0016206055879592896,\n",
       "   0.0015994840068742633,\n",
       "   0.0015825232258066535,\n",
       "   0.0015669054118916392,\n",
       "   0.0015483832685276866,\n",
       "   0.0015284050023183227,\n",
       "   0.0015104311751201749,\n",
       "   0.0014924326678737998,\n",
       "   0.0014750061091035604,\n",
       "   0.0014612966915592551,\n",
       "   0.0014437225181609392,\n",
       "   0.0014311649138107896,\n",
       "   0.0014157189289107919,\n",
       "   0.001402106019668281,\n",
       "   0.0013879106845706701,\n",
       "   0.0013741180300712585,\n",
       "   0.0013618167722597718,\n",
       "   0.0013503300724551082,\n",
       "   0.0013378600124269724,\n",
       "   0.001326073193922639,\n",
       "   0.0013161511160433292,\n",
       "   0.0013054972514510155,\n",
       "   0.0012965475907549262,\n",
       "   0.0012847192119807005,\n",
       "   0.0012732951436191797,\n",
       "   0.001260608434677124,\n",
       "   0.0012522770557552576,\n",
       "   0.0012425178429111838,\n",
       "   0.0012342145200818777,\n",
       "   0.0012263996759429574,\n",
       "   0.0012167122913524508,\n",
       "   0.00120815250556916,\n",
       "   0.0012009150814265013,\n",
       "   0.001192792085930705,\n",
       "   0.001183711108751595,\n",
       "   0.0011768888216465712,\n",
       "   0.0011687263613566756,\n",
       "   0.0011605521431192756,\n",
       "   0.0011535820085555315,\n",
       "   0.0011468561133369803,\n",
       "   0.0011397110065445304,\n",
       "   0.0011316959280520678,\n",
       "   0.0011257221922278404,\n",
       "   0.0011182844173163176,\n",
       "   0.001112864469178021,\n",
       "   0.0011076355585828424,\n",
       "   0.0011008811416104436,\n",
       "   0.0010955657344311476,\n",
       "   0.0010899055050686002,\n",
       "   0.00108443817589432,\n",
       "   0.0010793610708788037,\n",
       "   0.0010736160911619663,\n",
       "   0.0010690592462196946,\n",
       "   0.001063751638866961,\n",
       "   0.0010587143478915095,\n",
       "   0.0010535742621868849,\n",
       "   0.0010488383704796433,\n",
       "   0.00104398129042238,\n",
       "   0.0010386259527876973,\n",
       "   0.0010343739995732903,\n",
       "   0.0010303467279300094,\n",
       "   0.0010260536801069975,\n",
       "   0.0010212153429165483,\n",
       "   0.0010169324232265353,\n",
       "   0.00101220712531358,\n",
       "   0.0010080194333568215,\n",
       "   0.0010036361636593938,\n",
       "   0.0009993244893848896,\n",
       "   0.0009951998945325613,\n",
       "   0.0009906921768561006,\n",
       "   0.0009859136771410704,\n",
       "   0.000981810037046671,\n",
       "   0.0009778761304914951,\n",
       "   0.0009749529999680817,\n",
       "   0.0009714211337268353,\n",
       "   0.0009670105646364391,\n",
       "   0.0009623027872294188,\n",
       "   0.0009586354135535657,\n",
       "   0.0009545312495902181,\n",
       "   0.0009503812179900706,\n",
       "   0.0009474385879002512,\n",
       "   0.0009441426955163479,\n",
       "   0.0009416102548129857,\n",
       "   0.0009372519562020898,\n",
       "   0.0009339288808405399,\n",
       "   0.0009301419486291707,\n",
       "   0.0009265082771889865,\n",
       "   0.000922512321267277,\n",
       "   0.0009188444819301367,\n",
       "   0.0009147094679065049,\n",
       "   0.0009111783001571894,\n",
       "   0.0009089618688449264,\n",
       "   0.000907194335013628,\n",
       "   0.0009038228308781981,\n",
       "   0.0008989067864604294,\n",
       "   0.0008968711481429636,\n",
       "   0.0008938585524447262,\n",
       "   0.0008878948865458369,\n",
       "   0.00088473130017519,\n",
       "   0.0008817107300274074,\n",
       "   0.000878245453350246,\n",
       "   0.0008757594623602927,\n",
       "   0.000873078650329262,\n",
       "   0.000869769137352705,\n",
       "   0.000866222195327282,\n",
       "   0.0008626709459349513,\n",
       "   0.0008594281389378011,\n",
       "   0.0008566819014959037,\n",
       "   0.0008539308328181505,\n",
       "   0.0008514114888384938,\n",
       "   0.000849311298225075,\n",
       "   0.0008455151109956205,\n",
       "   0.0008437836659140885,\n",
       "   0.0008403288084082305,\n",
       "   0.0008369264542125165,\n",
       "   0.0008335377206094563,\n",
       "   0.0008305350202135742,\n",
       "   0.0008278586901724339,\n",
       "   0.000824792601633817,\n",
       "   0.0008215777343139052,\n",
       "   0.0008185000042431056,\n",
       "   0.0008174018585123122,\n",
       "   0.0008169173379428685,\n",
       "   0.0008115693344734609,\n",
       "   0.0008085570298135281,\n",
       "   0.0008060272666625679,\n",
       "   0.000802819209638983,\n",
       "   0.0008010629680939019,\n",
       "   0.0007976790075190365,\n",
       "   0.0007951263687573373,\n",
       "   0.000792089500464499,\n",
       "   0.000789869693107903,\n",
       "   0.0007877200259827077,\n",
       "   0.0007849495741538703,\n",
       "   0.0007821653271093965,\n",
       "   0.0007799977320246398,\n",
       "   0.0007773565012030303,\n",
       "   0.0007743192836642265,\n",
       "   0.000772034574765712,\n",
       "   0.0007692545768804848,\n",
       "   0.0007672483916394413,\n",
       "   0.0007649688050150871,\n",
       "   0.0007623489364050329,\n",
       "   0.000759955495595932,\n",
       "   0.0007580075180158019,\n",
       "   0.0007552412571385503,\n",
       "   0.0007535250624641776,\n",
       "   0.0007509593851864338,\n",
       "   0.0007486618123948574,\n",
       "   0.0007466156966984272,\n",
       "   0.0007443460053764284,\n",
       "   0.0007422990165650845,\n",
       "   0.0007402707124128938,\n",
       "   0.000737518013920635,\n",
       "   0.000735051988158375,\n",
       "   0.0007338019204325974,\n",
       "   0.0007311683730222285,\n",
       "   0.0007288531051017344,\n",
       "   0.0007274604868143797,\n",
       "   0.0007255098898895085,\n",
       "   0.0007236271630972624,\n",
       "   0.0007206815644167364,\n",
       "   0.0007182242698036134,\n",
       "   0.0007157112122513354,\n",
       "   0.0007148193544708192,\n",
       "   0.0007113533793017268,\n",
       "   0.0007095530163496733,\n",
       "   0.0007074128952808678,\n",
       "   0.0007050986168906093,\n",
       "   0.0007027096580713987,\n",
       "   0.000701442186255008,\n",
       "   0.0006995325093157589,\n",
       "   0.0006971752736717463,\n",
       "   0.0006951225805096328,\n",
       "   0.0006943306070752442,\n",
       "   0.0006923142937012017,\n",
       "   0.0006899579893797636,\n",
       "   0.0006880223518237472,\n",
       "   0.0006858590641058981,\n",
       "   0.0006845262832939625,\n",
       "   0.0006825248710811138,\n",
       "   0.0006827980978414416,\n",
       "   0.0006797495298087597,\n",
       "   0.0006776158115826547,\n",
       "   0.0006758684176020324,\n",
       "   0.0006742169498465955,\n",
       "   0.0006722447578795254,\n",
       "   0.0006708457949571311,\n",
       "   0.0006704345578327775,\n",
       "   0.0006676661432720721,\n",
       "   0.0006658285274170339,\n",
       "   0.0006632114527747035,\n",
       "   0.0006607452523894608,\n",
       "   0.0006591732962988317,\n",
       "   0.0006573808495886624,\n",
       "   0.000655182811897248,\n",
       "   0.0006547719240188599,\n",
       "   0.0006534386775456369,\n",
       "   0.0006509862141683698,\n",
       "   0.000649084395263344,\n",
       "   0.0006471407832577825,\n",
       "   0.0006453147507272661,\n",
       "   0.0006441622972488403,\n",
       "   0.0006425814353860915,\n",
       "   0.0006437184056267142,\n",
       "   0.000641807506326586,\n",
       "   0.0006378641701303422,\n",
       "   0.0006363074062392116,\n",
       "   0.000634034862741828,\n",
       "   0.0006332063348963857,\n",
       "   0.0006318524247035384,\n",
       "   0.0006321293185465038,\n",
       "   0.0006295298808254302,\n",
       "   0.0006279649096541107,\n",
       "   0.0006257652421481907,\n",
       "   0.0006233350140973926,\n",
       "   0.0006225125398486853,\n",
       "   0.0006227103294804692,\n",
       "   0.0006209313287399709,\n",
       "   0.0006183465011417866,\n",
       "   0.0006165380473248661,\n",
       "   0.0006151273846626282,\n",
       "   0.0006151458946987987,\n",
       "   0.0006131823756732047,\n",
       "   0.0006135892472229898,\n",
       "   0.0006106029031798244,\n",
       "   0.0006083328044041991,\n",
       "   0.0006067033973522484,\n",
       "   0.0006060657324269414,\n",
       "   0.0006050124065950513,\n",
       "   0.0006037325947545469,\n",
       "   0.0006013699457980692,\n",
       "   0.0005995722603984177,\n",
       "   0.0005984855233691633,\n",
       "   0.0005971639766357839,\n",
       "   0.0005956451059319079,\n",
       "   0.000595428457017988,\n",
       "   0.000596619735006243,\n",
       "   0.0005934230284765363,\n",
       "   0.0005912408232688904,\n",
       "   0.0005897092050872743,\n",
       "   0.0005887317238375545,\n",
       "   0.0005874240887351334,\n",
       "   0.0005873771151527762,\n",
       "   0.0005865646526217461,\n",
       "   0.0005855578347109258,\n",
       "   0.0005819867365062237,\n",
       "   0.0005814905161969364,\n",
       "   0.0005794579046778381,\n",
       "   0.0005786269321106374,\n",
       "   0.0005775313475169241,\n",
       "   0.0005775520694442093,\n",
       "   0.0005756685277447104,\n",
       "   0.0005739187472499907,\n",
       "   0.0005729967961087823,\n",
       "   0.0005715069710277021,\n",
       "   0.0005691552651114762,\n",
       "   0.0005679010646417737,\n",
       "   0.0005662819021381438,\n",
       "   0.0005653833504766226,\n",
       "   0.0005638949805870652,\n",
       "   0.0005631057429127395,\n",
       "   0.0005644895136356354,\n",
       "   0.0005643247277475893,\n",
       "   0.0005638190777972341,\n",
       "   0.0005595998954959214,\n",
       "   0.0005584791651926935,\n",
       "   0.0005580519209615886,\n",
       "   0.0005560347926802933,\n",
       "   0.000555668375454843,\n",
       "   0.0005527061293832958,\n",
       "   0.0005512749194167554,\n",
       "   0.0005500180996023118,\n",
       "   0.000548815936781466,\n",
       "   0.0005477816448546946,\n",
       "   0.0005465663853101432,\n",
       "   0.0005452196928672493,\n",
       "   0.0005438497755676508,\n",
       "   0.0005427889991551638,\n",
       "   0.0005414800834842026,\n",
       "   0.0005401719245128334,\n",
       "   0.0005397165659815073,\n",
       "   0.000538312888238579,\n",
       "   0.0005373090971261263,\n",
       "   0.0005398141802288592,\n",
       "   0.0005372741143219173,\n",
       "   0.0005357571644708514,\n",
       "   0.0005344972596503794,\n",
       "   0.0005328868865035474,\n",
       "   0.0005317985196597874,\n",
       "   0.0005315860616974533,\n",
       "   0.0005293340072967112,\n",
       "   0.0005283734644763172,\n",
       "   0.0005283170030452311,\n",
       "   0.0005272003472782671,\n",
       "   0.0005250474787317216,\n",
       "   0.0005248505040071905,\n",
       "   0.0005234237760305405,\n",
       "   0.000524045608472079,\n",
       "   0.0005282019847072661,\n",
       "   0.0005229953094385564,\n",
       "   0.0005216359277255833,\n",
       "   0.0005190594238229096,\n",
       "   0.0005184464389458299,\n",
       "   0.0005173154640942812,\n",
       "   0.0005162935703992844,\n",
       "   0.0005174045800231397,\n",
       "   0.0005143361049704254,\n",
       "   0.000513328704982996,\n",
       "   0.0005146600306034088,\n",
       "   0.0005149017670191824,\n",
       "   0.0005108343320898712,\n",
       "   0.0005101972492411733,\n",
       "   0.0005089546903036535,\n",
       "   0.0005071471678093076,\n",
       "   0.0005058877868577838,\n",
       "   0.0005047115846537054,\n",
       "   0.0005038298550061882,\n",
       "   0.0005029216990806162,\n",
       "   0.0005029026651754975,\n",
       "   0.0005025995196774602,\n",
       "   0.0005006709834560752,\n",
       "   0.000499060086440295,\n",
       "   0.0004983599064871669,\n",
       "   0.0004972452879883349,\n",
       "   0.0004968634457327425,\n",
       "   0.0004958947538398206,\n",
       "   0.0004953272873535752,\n",
       "   0.0004941888037137687,\n",
       "   0.000494300271384418,\n",
       "   0.0004929754068143666,\n",
       "   0.0004924213862977922,\n",
       "   0.0004913848824799061,\n",
       "   0.0004903613007627428,\n",
       "   0.0004908372065983713,\n",
       "   0.0004915031604468822,\n",
       "   0.0004891142598353326,\n",
       "   0.00048617913853377104,\n",
       "   0.00048683249042369425,\n",
       "   0.0004865471855737269,\n",
       "   0.0004851768899243325,\n",
       "   0.00048446448636241257,\n",
       "   0.00048304154188372195,\n",
       "   0.0004822475020773709,\n",
       "   0.00048015423817560077,\n",
       "   0.00048068584874272346,\n",
       "   0.0004789816739503294,\n",
       "   0.0004781059978995472,\n",
       "   0.0004783684271387756,\n",
       "   0.00047707994235679507,\n",
       "   0.000475242268294096,\n",
       "   0.0004745767801068723,\n",
       "   0.00047335372073575854,\n",
       "   0.00047249154886230826,\n",
       "   0.0004716474504675716,\n",
       "   0.0004704874590970576,\n",
       "   0.0004702139412984252,\n",
       "   0.00047048411215655506,\n",
       "   0.00047574687050655484,\n",
       "   0.00047291445662267506,\n",
       "   0.0004702194419223815,\n",
       "   0.0004690051719080657,\n",
       "   0.00046837222180329263,\n",
       "   0.0004683318838942796,\n",
       "   0.00046536148875020444,\n",
       "   0.000463233474874869,\n",
       "   0.0004624706052709371,\n",
       "   0.00046270046732388437,\n",
       "   0.0004609436437021941,\n",
       "   0.00046001782175153494,\n",
       "   0.0004611417243722826,\n",
       "   0.00045923239667899907,\n",
       "   0.0004577854706440121,\n",
       "   0.0004570219316519797,\n",
       "   0.0004566798743326217,\n",
       "   0.0004558672953862697,\n",
       "   0.0004567834548652172,\n",
       "   0.00045677588786929846,\n",
       "   0.0004574891063384712,\n",
       "   0.0004599768726620823,\n",
       "   0.00045374262845143676,\n",
       "   0.00045171595411375165,\n",
       "   0.00045110678183846176,\n",
       "   0.0004513052699621767,\n",
       "   0.00045042013516649604,\n",
       "   0.0004489444545470178,\n",
       "   0.00044797646114602685,\n",
       "   0.0004474326269701123,\n",
       "   0.0004471170832403004,\n",
       "   0.0004450883134268224,\n",
       "   0.0004451892164070159,\n",
       "   0.00044352185796014965,\n",
       "   0.000443715340225026,\n",
       "   0.0004426540981512517,\n",
       "   0.0004413631686475128,\n",
       "   0.0004406105726957321,\n",
       "   0.00043954284046776593,\n",
       "   0.0004381515027489513,\n",
       "   0.00043794672819785774,\n",
       "   0.0004377253644634038,\n",
       "   0.00043701150570996106,\n",
       "   0.0004365946224424988,\n",
       "   0.00043663589167408645,\n",
       "   0.0004350979288574308,\n",
       "   0.0004335087724030018,\n",
       "   0.0004327241040300578,\n",
       "   0.00043192459270358086,\n",
       "   0.00043167834519408643,\n",
       "   0.0004323661560192704,\n",
       "   0.000432485161582008,\n",
       "   0.0004323186876717955,\n",
       "   0.00043124845251441,\n",
       "   0.0004301877925172448,\n",
       "   0.00042925815796479583,\n",
       "   0.0004286397306714207,\n",
       "   0.00042760223732329905,\n",
       "   0.00042681413469836116,\n",
       "   0.0004265799652785063,\n",
       "   0.0004271043580956757,\n",
       "   0.00042575603583827615,\n",
       "   0.0004248647892381996,\n",
       "   0.00042542084702290595,\n",
       "   0.0004245683958288282,\n",
       "   0.00042145929182879627,\n",
       "   0.00042150949593633413,\n",
       "   0.00042111307266168296,\n",
       "   0.0004193527565803379,\n",
       "   0.00041857242467813194,\n",
       "   0.00041788132512010634,\n",
       "   0.0004185106372460723,\n",
       "   0.00041887364932335913,\n",
       "   0.00041771316318772733,\n",
       "   0.00041479989886283875,\n",
       "   0.00041466907714493573,\n",
       "   0.0004138651129323989,\n",
       "   0.00041436898754909635,\n",
       "   0.0004144533595535904,\n",
       "   0.0004133062029723078,\n",
       "   0.00041248544584959745,\n",
       "   0.0004109625006094575,\n",
       "   0.00041011456050910056,\n",
       "   0.0004098645586054772,\n",
       "   0.00041062431409955025,\n",
       "   0.00040849033393897116,\n",
       "   0.00040727859595790505,\n",
       "   0.00040761614218354225,\n",
       "   0.00040902881301008165,\n",
       "   0.0004067731206305325,\n",
       "   0.0004066098772455007,\n",
       "   0.0004050894931424409,\n",
       "   0.00040354151860810816,\n",
       "   0.0004038984188809991,\n",
       "   0.00040330036426894367,\n",
       "   0.0004030610143672675,\n",
       "   0.00040279800305143,\n",
       "   0.00040223487303592265,\n",
       "   0.0004009422264061868,\n",
       "   0.0003998253960162401,\n",
       "   0.0003991023695562035,\n",
       "   0.00039902376011013985,\n",
       "   0.0003988639800809324,\n",
       "   0.00039911194471642375,\n",
       "   0.0003985862713307142,\n",
       "   0.00039735445170663297,\n",
       "   0.0003977544547524303,\n",
       "   0.00039763981476426125,\n",
       "   0.000400113029172644,\n",
       "   0.0003963352064602077,\n",
       "   0.000394795264583081,\n",
       "   0.00039461147389374673,\n",
       "   0.00039555170224048197,\n",
       "   0.00039356964407488704,\n",
       "   0.00039353937609121203,\n",
       "   0.00039277542964555323,\n",
       "   0.00039135877159424126,\n",
       "   0.0003907529462594539,\n",
       "   0.0003903098695445806,\n",
       "   0.0003903158940374851,\n",
       "   0.0003896202251780778,\n",
       "   0.0003902013704646379,\n",
       "   0.00038960546953603625,\n",
       "   0.0003902119933627546,\n",
       "   0.000388606742490083,\n",
       "   0.0003870009386446327,\n",
       "   0.0003868188359774649,\n",
       "   0.000385948340408504,\n",
       "   0.0003852341906167567,\n",
       "   0.00038481419323943555,\n",
       "   0.00038577913073822856,\n",
       "   0.0003838596458081156,\n",
       "   0.0003833414812106639,\n",
       "   0.00038285122718662024,\n",
       "   0.00038334462442435324,\n",
       "   0.0003829666820820421,\n",
       "   0.00038139920798130333,\n",
       "   0.0003806185268331319,\n",
       "   0.0003808167821262032,\n",
       "   0.00037965079536661506,\n",
       "   0.00037868006620556116,\n",
       "   0.0003778760728891939,\n",
       "   0.0003783204301726073,\n",
       "   0.00037742414860986173,\n",
       "   0.0003768863680306822,\n",
       "   0.0003769789473153651,\n",
       "   0.00037792426883243024,\n",
       "   0.0003765593282878399,\n",
       "   0.00037601363146677613,\n",
       "   0.00037598618655465543,\n",
       "   0.00037552244612015784,\n",
       "   0.0003749252064153552,\n",
       "   0.0003746916481759399,\n",
       "   0.0003734194615390152,\n",
       "   0.0003746793372556567,\n",
       "   0.00037363567389547825,\n",
       "   0.00037284239078871906,\n",
       "   0.00037099330802448094,\n",
       "   0.0003712513134814799,\n",
       "   0.0003707672585733235,\n",
       "   0.00037005438935011625,\n",
       "   0.0003711105964612216,\n",
       "   0.00036949969944544137,\n",
       "   0.0003689553705044091,\n",
       "   0.00036793536855839193,\n",
       "   0.00036740931682288647,\n",
       "   0.0003664516261778772,\n",
       "   0.0003675429034046829,\n",
       "   0.0003673158644232899,\n",
       "   0.00036644074134528637,\n",
       "   0.0003664446121547371,\n",
       "   0.00036713617737405,\n",
       "   0.0003671128652058542,\n",
       "   0.00037005668855272233,\n",
       "   0.00036645872751250863,\n",
       "   0.00036883720895275474,\n",
       "   0.0003641701187007129,\n",
       "   0.0003627449623309076,\n",
       "   0.0003642051015049219,\n",
       "   0.0003623679222073406,\n",
       "   0.00036255191662348807,\n",
       "   0.0003616160829551518,\n",
       "   0.00036004220601171255,\n",
       "   0.0003585725207813084,\n",
       "   0.00036130528314970434,\n",
       "   0.0003587585815694183,\n",
       "   0.00035846608807332814,\n",
       "   0.0003580808697734028,\n",
       "   0.00035847764229401946,\n",
       "   0.00035574816865846515,\n",
       "   0.0003565742517821491,\n",
       "   0.00035931463935412467,\n",
       "   0.0003555448492988944,\n",
       "   0.0003549122193362564,\n",
       "   0.0003545604704413563,\n",
       "   0.00035372085403651,\n",
       "   0.00035337175359018147,\n",
       "   0.00035307015059515834,\n",
       "   0.00035328580997884274,\n",
       "   0.0003531859547365457,\n",
       "   0.00035421340726315975,\n",
       "   0.0003531794063746929,\n",
       "   0.00035198108525946736,\n",
       "   0.00035315274726599455,\n",
       "   0.0003538134042173624,\n",
       "   0.00035384573857299984,\n",
       "   0.00035684325848706067,\n",
       "   0.000351818889612332,\n",
       "   0.00035204473533667624,\n",
       "   0.00035051422310061753,\n",
       "   0.00035136975930072367,\n",
       "   0.00034899593447335064,\n",
       "   0.00034903368214145303,\n",
       "   0.00034830693039111793,\n",
       "   0.00034762994619086385,\n",
       "   0.00034796205000020564,\n",
       "   0.0003500411403365433,\n",
       "   0.0003455285041127354,\n",
       "   0.00034505981602706015,\n",
       "   0.00034497276647016406,\n",
       "   0.0003454400284681469,\n",
       "   0.0003446777118369937,\n",
       "   0.0003445891197770834,\n",
       "   0.0003431312507018447,\n",
       "   0.00034381740260869265,\n",
       "   0.00034514578874222934,\n",
       "   0.00034315339871682227,\n",
       "   0.0003428372147027403,\n",
       "   0.0003436575934756547,\n",
       "   0.00034266040893271565,\n",
       "   0.0003425800532568246,\n",
       "   0.0003419792919885367,\n",
       "   0.00034146482357755303,\n",
       "   0.0003412399091757834,\n",
       "   0.00034434074768796563,\n",
       "   0.0003409346390981227,\n",
       "   0.00033906669705174863,\n",
       "   0.0003395176609046757,\n",
       "   0.0003389105841051787,\n",
       "   0.00033881861600093544,\n",
       "   0.0003393316001165658,\n",
       "   0.0003377888642717153,\n",
       "   0.000338355457643047,\n",
       "   0.0003391612262930721,\n",
       "   0.00033664648071862757,\n",
       "   0.0003382177965249866,\n",
       "   0.0003417629050090909,\n",
       "   0.0003394719969946891,\n",
       "   0.000335618678946048,\n",
       "   0.0003358595713507384,\n",
       "   0.0003355860826559365,\n",
       "   0.0003370028571225703,\n",
       "   0.00033358632936142385,\n",
       "   0.0003344160213600844,\n",
       "   0.00033551963861100376,\n",
       "   0.0003339456452522427,\n",
       "   0.00033394660567864776,\n",
       "   0.00033398589584976435,\n",
       "   0.00033307247213087976,\n",
       "   0.0003324410936329514,\n",
       "   0.00033275672467425466,\n",
       "   0.0003323022974655032,\n",
       "   0.00033199519384652376,\n",
       "   0.0003326818405184895,\n",
       "   0.0003334043431095779,\n",
       "   0.00033119157887995243,\n",
       "   0.0003313701308798045,\n",
       "   0.00033143957261927426,\n",
       "   0.00033201262704096735,\n",
       "   0.0003307316219434142,\n",
       "   0.00032916147029027343,\n",
       "   0.0003283402184024453,\n",
       "   0.0003281701938249171,\n",
       "   0.00032783689675852656,\n",
       "   0.00032837819890119135,\n",
       "   0.0003282636753283441,\n",
       "   0.0003284604172222316,\n",
       "   0.00032930090674199164,\n",
       "   0.00032875026226975024,\n",
       "   0.00032910911249928176,\n",
       "   0.0003302736149635166,\n",
       "   0.00032959444797597826,\n",
       "   0.0003278158837929368,\n",
       "   0.0003280627715867013,\n",
       "   0.0003293845511507243,\n",
       "   0.0003305991704110056,\n",
       "   0.0003280509263277054,\n",
       "   0.00032866906258277595,\n",
       "   0.0003278733347542584,\n",
       "   0.00033003732096403837,\n",
       "   0.0003281422541476786,\n",
       "   0.00032562014530412853,\n",
       "   0.0003243621322326362,\n",
       "   0.00032413945882581174,\n",
       "   0.0003235447802580893,\n",
       "   0.00032260542502626777,\n",
       "   0.00032168850884772837,\n",
       "   0.0003215612086933106,\n",
       "   0.00032197506516240537,\n",
       "   0.0003220275684725493,\n",
       "   0.00031991611467674375,\n",
       "   0.00032053596805781126,\n",
       "   0.0003213369636796415,\n",
       "   0.00032045404077507555,\n",
       "   0.00031962303910404444,\n",
       "   0.0003204585809726268,\n",
       "   0.00031983182998374104,\n",
       "   0.0003202920197509229,\n",
       "   0.000320030958391726,\n",
       "   0.00031931765261106193,\n",
       "   0.0003199455968569964,\n",
       "   0.0003191929718013853,\n",
       "   0.0003218551864847541,\n",
       "   0.00031869355007074773,\n",
       "   0.0003180975036229938,\n",
       "   0.00031978878541849554,\n",
       "   0.00031786010367795825,\n",
       "   0.00031688547460362315,\n",
       "   0.0003167291870340705,\n",
       "   0.0003154633450321853,\n",
       "   0.0003158555191475898,\n",
       "   0.00031527530518360436,\n",
       "   0.0003175673191435635,\n",
       "   0.00031568564008921385,\n",
       "   0.0003142215427942574,\n",
       "   0.00031551189022138715,\n",
       "   0.00031481581390835345,\n",
       "   0.00031364994356408715,\n",
       "   0.0003143350186292082,\n",
       "   0.00031626797863282263,\n",
       "   0.00031514058355242014,\n",
       "   0.0003169918491039425,\n",
       "   0.00031539160409010947,\n",
       "   0.00031873260741122067,\n",
       "   0.0003181378997396678,\n",
       "   0.0003158058098051697,\n",
       "   0.0003127613163087517,\n",
       "   0.00031207353458739817,\n",
       "   0.00031368303461931646,\n",
       "   0.0003117004525847733,\n",
       "   0.0003108617675025016,\n",
       "   0.00031205808045342565,\n",
       "   0.00031342176953330636,\n",
       "   0.0003123359347227961,\n",
       "   0.00031025937641970813,\n",
       "   0.00030930241337046027,\n",
       "   0.0003083985939156264,\n",
       "   0.0003078214358538389,\n",
       "   0.0003082530165556818,\n",
       "   0.0003082434122916311,\n",
       "   0.0003080047608818859,\n",
       "   0.0003073830157518387,\n",
       "   0.00030858052195981145,\n",
       "   0.00030898285331204534,\n",
       "   0.0003085400094278157,\n",
       "   0.00030833054916001856,\n",
       "   0.00030735202017240226,\n",
       "   0.0003076667198911309,\n",
       "   0.0003077186702284962,\n",
       "   0.00030815592617727816,\n",
       "   0.000305674911942333,\n",
       "   0.00030611129477620125,\n",
       "   0.0003049472870770842,\n",
       "   0.0003050429804716259,\n",
       "   0.00030945948674343526,\n",
       "   0.0003093140840064734,\n",
       "   0.0003080508904531598,\n",
       "   0.0003042146854568273,\n",
       "   0.000304188288282603,\n",
       "   0.0003036552225239575,\n",
       "   0.00030378473456948996,\n",
       "   0.0003035736153833568,\n",
       "   0.00030321269878186285,\n",
       "   0.0003029917133972049,\n",
       "   0.00030342891113832593,\n",
       "   0.0003090949612669647,\n",
       "   0.0003048494691029191,\n",
       "   0.00030350140877999365,\n",
       "   0.0003040347364731133,\n",
       "   0.00030206976225599647,\n",
       "   0.00030055269598960876,\n",
       "   0.0003027564089279622,\n",
       "   0.00030355717171914876,\n",
       "   0.00030196813168004155,\n",
       "   0.000303085456835106,\n",
       "   0.00030040700221434236,\n",
       "   0.000304171146126464,\n",
       "   0.0002999792923219502,\n",
       "   0.000299059902317822,\n",
       "   0.0002994947717525065,\n",
       "   0.0002994414826389402,\n",
       "   0.0002979347773361951,\n",
       "   0.0002977853000629693,\n",
       "   0.0002969565975945443,\n",
       "   0.0003019623691216111,\n",
       "   0.0002977572148665786,\n",
       "   0.0002975843381136656,\n",
       "   0.0002975173993036151,\n",
       "   0.00029780552722513676,\n",
       "   0.0002978802367579192,\n",
       "   0.0002965846215374768,\n",
       "   0.0002955694799311459,\n",
       "   0.0002958896802738309,\n",
       "   0.00029694323893636465,\n",
       "   0.00029633200028911233,\n",
       "   0.0002973887021653354,\n",
       "   0.000298363680485636,\n",
       "   0.00029998281388543546,\n",
       "   0.00029684993205592036,\n",
       "   0.0002958188997581601,\n",
       "   0.0002966485917568207,\n",
       "   0.0002951830392703414,\n",
       "   0.00029544340213760734,\n",
       "   0.0002942762221209705,\n",
       "   0.00029437299235723913,\n",
       "   0.00029342545894905925,\n",
       "   0.00029342458583414555,\n",
       "   0.0002937048557214439,\n",
       "   0.00029399548657238483,\n",
       "   0.0002930892806034535,\n",
       "   0.0002944284933619201,\n",
       "   0.00029250167426653206,\n",
       "   0.0002921878476627171,\n",
       "   0.00029314536368474364,\n",
       "   0.00029231628286652267,\n",
       "   0.0002936437667813152,\n",
       "   0.000294208264676854,\n",
       "   0.0002926200977526605,\n",
       "   0.00029231130611151457,\n",
       "   0.00029182288562878966,\n",
       "   0.00029092669137753546,\n",
       "   0.0002915180812124163,\n",
       "   0.0002925335429608822,\n",
       "   0.0002908380120061338,\n",
       "   0.00029021481168456376,\n",
       "   0.00029043559334240854,\n",
       "   0.0002914727374445647,\n",
       "   0.00029008896672166884,\n",
       "   0.00028959737392142415,\n",
       "   0.00028960834606550634,\n",
       "   0.00029080762760713696,\n",
       "   0.00029007752891629934,\n",
       "   0.00028959597693756223,\n",
       "   0.0002902509877458215,\n",
       "   0.00028876110445708036,\n",
       "   0.0002884932910092175,\n",
       "   0.00028844078769907355,\n",
       "   0.00028972685686312616,\n",
       "   0.0002903883869294077,\n",
       "   0.0002876361249946058,\n",
       "   0.00028875053976662457,\n",
       "   0.0002881809778045863,\n",
       "   0.0002872419427148998,\n",
       "   0.00028844183543697,\n",
       "   0.0002861395478248596,\n",
       "   0.0002847386058419943,\n",
       "   0.00028797078994102776,\n",
       "   0.0002856308128684759,\n",
       "   0.00028494143043644726,\n",
       "   0.00028641612152568996,\n",
       "   0.00028521238709799945,\n",
       "   0.0002851743483915925,\n",
       "   0.0002857219078578055,\n",
       "   0.00028506378293968737,\n",
       "   0.0002848836884368211,\n",
       "   0.0002841513487510383,\n",
       "   0.0002841265231836587,\n",
       "   0.0002841317909769714,\n",
       "   0.00028267817106097937,\n",
       "   0.0002828919386956841,\n",
       "   0.00028601870872080326,\n",
       "   0.00028624144033528864,\n",
       "   0.0002846851130016148,\n",
       "   0.0002829082077369094,\n",
       "   0.0002834671176970005,\n",
       "   0.0002831713645718992,\n",
       "   0.0002828161232173443,\n",
       "   0.0002825163537636399,\n",
       "   0.0002819187648128718,\n",
       "   0.00028126235702075064,\n",
       "   0.0002808982681017369,\n",
       "   0.00028250616742298007,\n",
       "   0.00028051613480784,\n",
       "   0.0002803073439281434,\n",
       "   0.0002801684895530343,\n",
       "   0.0002801661903504282,\n",
       "   0.00028332415968179703,\n",
       "   0.00028030620887875557,\n",
       "   0.0002820395748130977,\n",
       "   0.00028260413091629744,\n",
       "   0.0002791034057736397,\n",
       "   0.00027885689632967114,\n",
       "   0.0002790824219118804,\n",
       "   0.0002800458169076592,\n",
       "   0.0002792890300042927,\n",
       "   0.0002799459907691926,\n",
       "   0.0002789894351735711,\n",
       "   0.0002816414344124496,\n",
       "   0.00028311298228800297,\n",
       "   0.00027974590193480253,\n",
       "   0.00027851545019075274,\n",
       "   0.0002785648393910378,\n",
       "   0.0002790146099869162,\n",
       "   0.00027800106909126043,\n",
       "   0.0002794145839288831,\n",
       "   0.0002784570388030261,\n",
       "   0.0002771795552689582,\n",
       "   0.000277591374469921,\n",
       "   0.000279139174381271,\n",
       "   0.0002791066945064813,\n",
       "   0.00027845357544720173,\n",
       "   0.0002790437429212034,\n",
       "   0.0002809921861626208,\n",
       "   0.00027877214597538114,\n",
       "   0.00027878605760633945,\n",
       "   0.00028111241408623755,\n",
       "   0.0002801359514705837,\n",
       "   0.00027588469674810767,\n",
       "   0.0002763538504950702,\n",
       "   0.0002753121661953628,\n",
       "   0.00027549013611860573,\n",
       "   0.0002769300772342831,\n",
       "   0.00027811157633550465,\n",
       "   0.00028125973767600954,\n",
       "   0.0002751696156337857,\n",
       "   0.00027512555243447423,\n",
       "   0.0002737816539593041,\n",
       "   0.00027367888833396137,\n",
       "   0.00027325734845362604,\n",
       "   0.0002742169890552759,\n",
       "   0.0002735565649345517,\n",
       "   0.00027324375696480274,\n",
       "   0.0002728499239310622,\n",
       "   0.00027502593002282083,\n",
       "   0.0002752130094449967,\n",
       "   0.00027336401399224997,\n",
       "   0.0002733759465627372,\n",
       "   0.00027359608793631196,\n",
       "   0.0002731400018092245,\n",
       "   0.00027269366546534,\n",
       "   0.00027325499104335904,\n",
       "   0.0002750778803601861,\n",
       "   0.00027433643117547035,\n",
       "   0.0002723165671341121,\n",
       "   0.00027272733859717846,\n",
       "   0.0002737089234869927,\n",
       "   0.00027198740281164646,\n",
       "   0.0002718072209972888,\n",
       "   0.0002716589951887727,\n",
       "   0.00027189162210561335,\n",
       "   0.0002733104338403791,\n",
       "   0.00027217293973080814,\n",
       "   0.0002720114716794342,\n",
       "   0.000270793738309294,\n",
       "   0.00027033916558139026,\n",
       "   0.00027174962451681495,\n",
       "   0.00027191260596737266,\n",
       "   0.0002722616191022098,\n",
       "   0.0002709922264330089,\n",
       "   0.0002712614950723946,\n",
       "   0.0002735019370447844,\n",
       "   0.000274434860330075,\n",
       "   0.00027204136131331325,\n",
       "   0.00027378686354495585,\n",
       "   0.0002695606090128422,\n",
       "   0.00026806682581081986,\n",
       "   0.0002685420331545174,\n",
       "   0.0002686861844267696,\n",
       "   0.00026883717509917915,\n",
       "   0.00027035182574763894,\n",
       "   0.00026943598641082644,\n",
       "   0.00027100445004180074,\n",
       "   0.00026995805092155933,\n",
       "   0.0002712890272960067,\n",
       "   0.0002679578319657594,\n",
       "   0.00026759342290461063,\n",
       "   0.00026870527653954923,\n",
       "   0.00026937914662994444,\n",
       "   0.0002677831507753581,\n",
       "   0.00026693393010646105,\n",
       "   0.0002676050062291324,\n",
       "   ...],\n",
       "  'train_loss': [0.018427792936563492,\n",
       "   0.019566526636481285,\n",
       "   0.017122628167271614,\n",
       "   0.01773962751030922,\n",
       "   0.01613554172217846,\n",
       "   0.015003912150859833,\n",
       "   0.014074219390749931,\n",
       "   0.013474535197019577,\n",
       "   0.013580884784460068,\n",
       "   0.012615016661584377,\n",
       "   0.011269470676779747,\n",
       "   0.010308249853551388,\n",
       "   0.009533758275210857,\n",
       "   0.008825421333312988,\n",
       "   0.0077184769324958324,\n",
       "   0.006899751257151365,\n",
       "   0.006343175657093525,\n",
       "   0.005689625162631273,\n",
       "   0.005230313632637262,\n",
       "   0.00517137860879302,\n",
       "   0.004793260712176561,\n",
       "   0.004600682761520147,\n",
       "   0.004121426958590746,\n",
       "   0.004131231922656298,\n",
       "   0.0037959767505526543,\n",
       "   0.003404465038329363,\n",
       "   0.00312197208404541,\n",
       "   0.0030148110818117857,\n",
       "   0.0031789534259587526,\n",
       "   0.003002848709002137,\n",
       "   0.002799034584313631,\n",
       "   0.002861544257029891,\n",
       "   0.0029007422272115946,\n",
       "   0.0027346997521817684,\n",
       "   0.0028967796824872494,\n",
       "   0.0023353281430900097,\n",
       "   0.0024446924217045307,\n",
       "   0.002442721975967288,\n",
       "   0.002314405981451273,\n",
       "   0.002481976058334112,\n",
       "   0.0025538746267557144,\n",
       "   0.002460646443068981,\n",
       "   0.0024030550848692656,\n",
       "   0.002588939620181918,\n",
       "   0.002343402709811926,\n",
       "   0.002213858300819993,\n",
       "   0.0019325787434354424,\n",
       "   0.002084125066176057,\n",
       "   0.0019343742169439793,\n",
       "   0.0021349508315324783,\n",
       "   0.002004032488912344,\n",
       "   0.002120443619787693,\n",
       "   0.002072381554171443,\n",
       "   0.0021922725718468428,\n",
       "   0.0022533333394676447,\n",
       "   0.002189547987654805,\n",
       "   0.0019425292266532779,\n",
       "   0.0019116324838250875,\n",
       "   0.0019575112964957952,\n",
       "   0.0017059509409591556,\n",
       "   0.002020488493144512,\n",
       "   0.001731107127852738,\n",
       "   0.0018609411781653762,\n",
       "   0.0018391476478427649,\n",
       "   0.001675992039963603,\n",
       "   0.001820762176066637,\n",
       "   0.001705460250377655,\n",
       "   0.0017536106752231717,\n",
       "   0.001917996909469366,\n",
       "   0.0016964906826615334,\n",
       "   0.0016396038699895144,\n",
       "   0.0017447348218411207,\n",
       "   0.0017198794521391392,\n",
       "   0.0018128801602870226,\n",
       "   0.0015518410364165902,\n",
       "   0.0015813783975318074,\n",
       "   0.001695943996310234,\n",
       "   0.0013649259926751256,\n",
       "   0.0014140309067443013,\n",
       "   0.0015650112181901932,\n",
       "   0.0016452620038762689,\n",
       "   0.001373612554743886,\n",
       "   0.0016141139203682542,\n",
       "   0.001481665181927383,\n",
       "   0.0015365362633019686,\n",
       "   0.0016144474502652884,\n",
       "   0.0013552244054153562,\n",
       "   0.0014418504433706403,\n",
       "   0.001520726247690618,\n",
       "   0.0015963194891810417,\n",
       "   0.0014086830196902156,\n",
       "   0.0013800596352666616,\n",
       "   0.001444056979380548,\n",
       "   0.0015229659620672464,\n",
       "   0.0013691422063857317,\n",
       "   0.0013987767742946744,\n",
       "   0.0014892738545313478,\n",
       "   0.0014141459250822663,\n",
       "   0.0015621037455275655,\n",
       "   0.0014621344162151217,\n",
       "   0.0013598620425909758,\n",
       "   0.0013944068923592567,\n",
       "   0.0013709579361602664,\n",
       "   0.0013215746730566025,\n",
       "   0.0013314352836459875,\n",
       "   0.0015110541135072708,\n",
       "   0.001164145884104073,\n",
       "   0.0014420211082324386,\n",
       "   0.0014291505794972181,\n",
       "   0.0012773416237905622,\n",
       "   0.0013803039910271764,\n",
       "   0.0014101724373176694,\n",
       "   0.0013961427612230182,\n",
       "   0.001136637176387012,\n",
       "   0.0013749946374446154,\n",
       "   0.0012696072226390243,\n",
       "   0.0012952216202393174,\n",
       "   0.0013239352265372872,\n",
       "   0.0013740487629547715,\n",
       "   0.001425315742380917,\n",
       "   0.001193519914522767,\n",
       "   0.0010846687946468592,\n",
       "   0.0012372208293527365,\n",
       "   0.0011320952326059341,\n",
       "   0.001172830699943006,\n",
       "   0.0013023815117776394,\n",
       "   0.0011538051767274737,\n",
       "   0.0013481698697432876,\n",
       "   0.0011284936917945743,\n",
       "   0.0012176797026768327,\n",
       "   0.0012897697743028402,\n",
       "   0.0011557479156181216,\n",
       "   0.0011471029138192534,\n",
       "   0.0011796413455158472,\n",
       "   0.0011493685888126493,\n",
       "   0.001213881652802229,\n",
       "   0.0011905141873285174,\n",
       "   0.0011103093856945634,\n",
       "   0.0010801871540024877,\n",
       "   0.0011869705049321055,\n",
       "   0.001225862535648048,\n",
       "   0.0011450698366388679,\n",
       "   0.0010379455052316189,\n",
       "   0.001133463461883366,\n",
       "   0.0012776893563568592,\n",
       "   0.0011692600091919303,\n",
       "   0.001132369041442871,\n",
       "   0.0011051829205825925,\n",
       "   0.0010600438108667731,\n",
       "   0.0013881678460165858,\n",
       "   0.0011468474986031651,\n",
       "   0.0011626874329522252,\n",
       "   0.001093757338821888,\n",
       "   0.0010467178653925657,\n",
       "   0.001096898689866066,\n",
       "   0.0012273015454411507,\n",
       "   0.0010589007288217545,\n",
       "   0.0012100909370929003,\n",
       "   0.0012900106376037002,\n",
       "   0.001201626961119473,\n",
       "   0.00108730245847255,\n",
       "   0.0012490139342844486,\n",
       "   0.001107068732380867,\n",
       "   0.0010899824555963278,\n",
       "   0.0010687429457902908,\n",
       "   0.001073341933079064,\n",
       "   0.001076328451745212,\n",
       "   0.0011299772886559367,\n",
       "   0.0010399349266663194,\n",
       "   0.00106725562363863,\n",
       "   0.0010764674516394734,\n",
       "   0.001099103013984859,\n",
       "   0.0012091494863852859,\n",
       "   0.001298402901738882,\n",
       "   0.001004082732833922,\n",
       "   0.000991098233498633,\n",
       "   0.0010852142004296184,\n",
       "   0.0010460781631991267,\n",
       "   0.0009886383777484298,\n",
       "   0.0010797906434163451,\n",
       "   0.001096303341910243,\n",
       "   0.0011053252965211868,\n",
       "   0.0011340879136696458,\n",
       "   0.0009936810238286853,\n",
       "   0.0010186027502641082,\n",
       "   0.0009723890107125044,\n",
       "   0.0010348118375986814,\n",
       "   0.0010446093510836363,\n",
       "   0.0011319463374093175,\n",
       "   0.001122252899222076,\n",
       "   0.0008986704633571208,\n",
       "   0.0010278362315148115,\n",
       "   0.00113889470230788,\n",
       "   0.0008688097004778683,\n",
       "   0.0009760427055880427,\n",
       "   0.0009270102018490434,\n",
       "   0.0009208711562678218,\n",
       "   0.0009762189583852887,\n",
       "   0.0010134608019143343,\n",
       "   0.000995744252577424,\n",
       "   0.0009597359457984567,\n",
       "   0.0008880594978109002,\n",
       "   0.0010422109626233578,\n",
       "   0.001073086983524263,\n",
       "   0.0009485739283263683,\n",
       "   0.0009720775415189564,\n",
       "   0.0011100200936198235,\n",
       "   0.0009356607333756983,\n",
       "   0.0010898669715970755,\n",
       "   0.0010114359902217984,\n",
       "   0.0009941394673660398,\n",
       "   0.0009656145703047514,\n",
       "   0.0009462620946578681,\n",
       "   0.001028623664751649,\n",
       "   0.0010353976394981146,\n",
       "   0.0009899339638650417,\n",
       "   0.0011104451259598136,\n",
       "   0.0009315190254710615,\n",
       "   0.0008998296689242125,\n",
       "   0.0009792221244424582,\n",
       "   0.0010596879292279482,\n",
       "   0.0008906310540623963,\n",
       "   0.000856748316437006,\n",
       "   0.0007927092374302447,\n",
       "   0.0008879632223397493,\n",
       "   0.001053457148373127,\n",
       "   0.0009360213880427182,\n",
       "   0.0008337146136909723,\n",
       "   0.0009714826010167599,\n",
       "   0.0010444186627864838,\n",
       "   0.0008643275359645486,\n",
       "   0.0009290605084970593,\n",
       "   0.0009346242877654731,\n",
       "   0.0008694553980603814,\n",
       "   0.0007825195789337158,\n",
       "   0.0008906678995117545,\n",
       "   0.0009155420702882111,\n",
       "   0.0007909268024377525,\n",
       "   0.000888572889380157,\n",
       "   0.0008656448335386813,\n",
       "   0.0008501738193444908,\n",
       "   0.0008368649869225919,\n",
       "   0.0008929576142691076,\n",
       "   0.0008668397786095738,\n",
       "   0.0007297362899407744,\n",
       "   0.0008619790314696729,\n",
       "   0.0008234306005761027,\n",
       "   0.0008506390149705112,\n",
       "   0.0008730475092306733,\n",
       "   0.0008493629866279662,\n",
       "   0.0008682060870341957,\n",
       "   0.0009712190367281437,\n",
       "   0.0009864186868071556,\n",
       "   0.0009123820927925408,\n",
       "   0.001014919951558113,\n",
       "   0.0009558948804624379,\n",
       "   0.0008385257096961141,\n",
       "   0.0008886052528396249,\n",
       "   0.0009739005472511053,\n",
       "   0.0007801245083101094,\n",
       "   0.0008675157441757619,\n",
       "   0.0008038720698095858,\n",
       "   0.0008520614937879145,\n",
       "   0.0006908125360496342,\n",
       "   0.0007270419737324119,\n",
       "   0.0009837972465902567,\n",
       "   0.0008805801044218242,\n",
       "   0.0009250281727872789,\n",
       "   0.0009490204392932355,\n",
       "   0.0008117079269140959,\n",
       "   0.0008416390628553927,\n",
       "   0.0010241121053695679,\n",
       "   0.0007927943370305002,\n",
       "   0.0008278577006421983,\n",
       "   0.0008791800355538726,\n",
       "   0.000827789306640625,\n",
       "   0.0007962720701470971,\n",
       "   0.0009039892465807498,\n",
       "   0.0007798291044309735,\n",
       "   0.0008264989592134953,\n",
       "   0.0008377529447898269,\n",
       "   0.0008270721300505102,\n",
       "   0.0006673382595181465,\n",
       "   0.0008378511411137879,\n",
       "   0.0008684660424478352,\n",
       "   0.000835311773698777,\n",
       "   0.0009278703946620226,\n",
       "   0.0007252033683471382,\n",
       "   0.0007674553780816495,\n",
       "   0.0008493158384226263,\n",
       "   0.0009039252763614058,\n",
       "   0.0009323254344053566,\n",
       "   0.0008418809156864882,\n",
       "   0.0008081214036792517,\n",
       "   0.0008950959309004247,\n",
       "   0.0007820517057552934,\n",
       "   0.0007279145647771657,\n",
       "   0.000936537457164377,\n",
       "   0.0008112197392620146,\n",
       "   0.0006601132918149233,\n",
       "   0.0008863189723342657,\n",
       "   0.0008040252723731101,\n",
       "   0.000829172320663929,\n",
       "   0.0007877313764765859,\n",
       "   0.0006820517010055482,\n",
       "   0.0007823818596079946,\n",
       "   0.0007836906588636339,\n",
       "   0.00082896783715114,\n",
       "   0.0008789295679889619,\n",
       "   0.0007367341313511133,\n",
       "   0.0008283695206046104,\n",
       "   0.0008085694280453026,\n",
       "   0.0008040655520744622,\n",
       "   0.0007222865824587643,\n",
       "   0.0006422264850698411,\n",
       "   0.0007841055630706251,\n",
       "   0.00077486177906394,\n",
       "   0.0007459524204023182,\n",
       "   0.0007669449551030993,\n",
       "   0.0007288794149644673,\n",
       "   0.0007344293990172446,\n",
       "   0.0008076159283518791,\n",
       "   0.0006842170841991901,\n",
       "   0.0008846683194860816,\n",
       "   0.0007891402347013354,\n",
       "   0.0008679103339090943,\n",
       "   0.0007715813117101789,\n",
       "   0.0007909498526714742,\n",
       "   0.0009347533923573792,\n",
       "   0.0007634853245690465,\n",
       "   0.0007299408898688853,\n",
       "   0.0007896485039964318,\n",
       "   0.0007704466697759926,\n",
       "   0.0007924129604361951,\n",
       "   0.000904108805116266,\n",
       "   0.0008477137889713049,\n",
       "   0.0007277061231434345,\n",
       "   0.0007231826893985271,\n",
       "   0.0008466150029562414,\n",
       "   0.0007708181510679424,\n",
       "   0.0007865180377848446,\n",
       "   0.0006606155075132847,\n",
       "   0.0006770194740965962,\n",
       "   0.0008246786892414093,\n",
       "   0.0007321157027035952,\n",
       "   0.0007392098195850849,\n",
       "   0.0008023058762773871,\n",
       "   0.0007285603787750006,\n",
       "   0.000889881222974509,\n",
       "   0.0008312843274325132,\n",
       "   0.0008989735506474972,\n",
       "   0.0007021364872343838,\n",
       "   0.000825768627692014,\n",
       "   0.0008521138224750757,\n",
       "   0.0006869320059195161,\n",
       "   0.0007620723336003721,\n",
       "   0.0008350972202606499,\n",
       "   0.0008054234785959125,\n",
       "   0.0007207977469079196,\n",
       "   0.0007375084096565843,\n",
       "   0.0007548303692601621,\n",
       "   0.0008158981218002737,\n",
       "   0.00059990503359586,\n",
       "   0.0006725249113515019,\n",
       "   0.00071614800253883,\n",
       "   0.0007294341921806335,\n",
       "   0.0007344170589931309,\n",
       "   0.000713831337634474,\n",
       "   0.0007374136475846171,\n",
       "   0.0007051036227494478,\n",
       "   0.0006936409045010805,\n",
       "   0.0006533570121973753,\n",
       "   0.0008650332456454635,\n",
       "   0.0008322192006744444,\n",
       "   0.0006677435012534261,\n",
       "   0.0007628061575815082,\n",
       "   0.0006969517562538385,\n",
       "   0.0007109166472218931,\n",
       "   0.0007156028877943754,\n",
       "   0.0006977877928875387,\n",
       "   0.0007104048854671419,\n",
       "   0.0005797938210889697,\n",
       "   0.0006788847385905683,\n",
       "   0.0008848235011100769,\n",
       "   0.0007414869032800198,\n",
       "   0.0006557307206094265,\n",
       "   0.0008250151295214891,\n",
       "   0.0007109395228326321,\n",
       "   0.0007288935012184083,\n",
       "   0.0007468547555617988,\n",
       "   0.0007308324566110969,\n",
       "   0.0007061344222165644,\n",
       "   0.0006828678888268769,\n",
       "   0.0008015893981792033,\n",
       "   0.0007014710572548211,\n",
       "   0.000751082319766283,\n",
       "   0.0006933037075214088,\n",
       "   0.0006560535402968526,\n",
       "   0.0007563683320768178,\n",
       "   0.000778724264819175,\n",
       "   0.0006538344896398485,\n",
       "   0.0007604919373989105,\n",
       "   0.0007885073428042233,\n",
       "   0.0010704142041504383,\n",
       "   0.0006140442565083504,\n",
       "   0.0007755967089906335,\n",
       "   0.0007607495062984526,\n",
       "   0.0008477807277813554,\n",
       "   0.0007170175085775554,\n",
       "   0.0005707535310648382,\n",
       "   0.0007431978592649102,\n",
       "   0.0007581773679703474,\n",
       "   0.0006589433760382235,\n",
       "   0.0007279961719177663,\n",
       "   0.0006507251528091729,\n",
       "   0.0006563192000612617,\n",
       "   0.0006044253241270781,\n",
       "   0.0007229914772324264,\n",
       "   0.0006838105618953705,\n",
       "   0.0007017600000835955,\n",
       "   0.000585066678468138,\n",
       "   0.0008182793972082436,\n",
       "   0.0006908549112267792,\n",
       "   0.0006130937836132944,\n",
       "   0.0006196793401613832,\n",
       "   0.0006974111893214285,\n",
       "   0.0006881761364638805,\n",
       "   0.0006668490241281688,\n",
       "   0.0008010524907149374,\n",
       "   0.0006114082643762231,\n",
       "   0.0007314118556678295,\n",
       "   0.0005673955893144011,\n",
       "   0.0005994632374495268,\n",
       "   0.0007142344838939607,\n",
       "   0.000529168639332056,\n",
       "   0.0008035518694669008,\n",
       "   0.000589584291446954,\n",
       "   0.0007127507124096155,\n",
       "   0.0007235474186018109,\n",
       "   0.0007554241456091404,\n",
       "   0.0008445322746410966,\n",
       "   0.0006351559422910213,\n",
       "   0.0006416053511202335,\n",
       "   0.0006814018124714494,\n",
       "   0.0006340296240523458,\n",
       "   0.0007160942768678069,\n",
       "   0.0005965688033029437,\n",
       "   0.0007044374942779541,\n",
       "   0.0006887142662890255,\n",
       "   0.0006671883165836334,\n",
       "   0.0006229145801626146,\n",
       "   0.00068739551352337,\n",
       "   0.0007100130314938724,\n",
       "   0.0006348798633553088,\n",
       "   0.0006398314726538956,\n",
       "   0.0005993027007207274,\n",
       "   0.0007533291936852038,\n",
       "   0.0006052101962268353,\n",
       "   0.0007268031476996839,\n",
       "   0.0006482215248979628,\n",
       "   0.0006047800998203456,\n",
       "   0.0006330172182060778,\n",
       "   0.0006429117056541145,\n",
       "   0.0005949031910859048,\n",
       "   0.0005965403979644179,\n",
       "   0.0005971456994302571,\n",
       "   0.0006131075788289309,\n",
       "   0.0005748499534092844,\n",
       "   0.0006412226939573884,\n",
       "   0.0007035712478682399,\n",
       "   0.0005545124877244234,\n",
       "   0.000625821587163955,\n",
       "   0.0007243009167723358,\n",
       "   0.0006845375755801797,\n",
       "   0.0006111497059464455,\n",
       "   0.0006673436146229506,\n",
       "   0.0006304814596660435,\n",
       "   0.000696077651809901,\n",
       "   0.0006947775254957378,\n",
       "   0.000575643964111805,\n",
       "   0.0006387461326085031,\n",
       "   0.000629856891464442,\n",
       "   0.0006103056948632002,\n",
       "   0.0006739025702700019,\n",
       "   0.0006943442858755589,\n",
       "   0.0006182246142998338,\n",
       "   0.000619908154476434,\n",
       "   0.0006395676173269749,\n",
       "   0.000644730927888304,\n",
       "   0.0006036466220393777,\n",
       "   0.0006219392525963485,\n",
       "   0.0005723928334191442,\n",
       "   0.000840846449136734,\n",
       "   0.0006674206233583391,\n",
       "   0.0006803728174418211,\n",
       "   0.0007514482131227851,\n",
       "   0.0006834210362285376,\n",
       "   0.0007454546284861863,\n",
       "   0.0006629801355302334,\n",
       "   0.0007076030597090721,\n",
       "   0.0006228031124919653,\n",
       "   0.0005874698399566114,\n",
       "   0.000611661234870553,\n",
       "   0.0007121210219338536,\n",
       "   0.000649572117254138,\n",
       "   0.00077291444176808,\n",
       "   0.0006157515454106033,\n",
       "   0.0005212375544942915,\n",
       "   0.0006780163384974003,\n",
       "   0.0005456411163322628,\n",
       "   0.0005711696576327085,\n",
       "   0.0005502095445990562,\n",
       "   0.0005855883937329054,\n",
       "   0.0006901454762555659,\n",
       "   0.0005802690866403282,\n",
       "   0.0007879912736825645,\n",
       "   0.0006600646302103996,\n",
       "   0.0006704418337903917,\n",
       "   0.0007782785105518997,\n",
       "   0.0007729937206022441,\n",
       "   0.0006617435137741268,\n",
       "   0.0006186243263073266,\n",
       "   0.0007125272531993687,\n",
       "   0.0005915150977671146,\n",
       "   0.0006165528320707381,\n",
       "   0.0007261637365445495,\n",
       "   0.0006749790627509356,\n",
       "   0.0006773727363906801,\n",
       "   0.0005603523459285498,\n",
       "   0.0006653853342868388,\n",
       "   0.0004915877361781895,\n",
       "   0.0006448604981414974,\n",
       "   0.0006440318538807333,\n",
       "   0.0005341083742678165,\n",
       "   0.0006438811542466283,\n",
       "   0.0005890431930311024,\n",
       "   0.0005872666370123625,\n",
       "   0.0007053602603264153,\n",
       "   0.0005823629326187074,\n",
       "   0.0005564440507441759,\n",
       "   0.0006514323758892715,\n",
       "   0.0006547929369844496,\n",
       "   0.0006741339457221329,\n",
       "   0.0006364142172969878,\n",
       "   0.0006279280059970915,\n",
       "   0.0005807819543406367,\n",
       "   0.0006047413335181773,\n",
       "   0.0006968126399442554,\n",
       "   0.000692700850777328,\n",
       "   0.0005405101110227406,\n",
       "   0.0006513888947665691,\n",
       "   0.0006793001084588468,\n",
       "   0.000525353942066431,\n",
       "   0.0005519039114005864,\n",
       "   0.0006111026741564274,\n",
       "   0.0005281510530039668,\n",
       "   0.0006584263755939901,\n",
       "   0.0005895434296689928,\n",
       "   0.000539300381205976,\n",
       "   0.0005909503670409322,\n",
       "   0.0007021091296337545,\n",
       "   0.0005761971115134656,\n",
       "   0.0006508657825179398,\n",
       "   0.0006286698044277728,\n",
       "   0.000602408719714731,\n",
       "   0.0007189504103735089,\n",
       "   0.0006189250852912664,\n",
       "   0.0006548600504174829,\n",
       "   0.000589816365391016,\n",
       "   0.0007280064164660871,\n",
       "   0.0005666179931722581,\n",
       "   0.0004949727444909513,\n",
       "   0.0006299628876149654,\n",
       "   0.0005467677838169038,\n",
       "   0.0006935253040865064,\n",
       "   0.0005634280969388783,\n",
       "   0.0007016624440439045,\n",
       "   0.0006184238009154797,\n",
       "   0.0006268351571634412,\n",
       "   0.0006597890751436353,\n",
       "   0.000625237007625401,\n",
       "   0.000659314391668886,\n",
       "   0.0005778176127932966,\n",
       "   0.0005935064982622862,\n",
       "   0.0005939782131463289,\n",
       "   0.0005176966078579426,\n",
       "   0.0007086503901518881,\n",
       "   0.0006446181214414537,\n",
       "   0.0006571784615516663,\n",
       "   0.0005759337218478322,\n",
       "   0.0004772246757056564,\n",
       "   0.0005776454927399755,\n",
       "   0.00045805948320776224,\n",
       "   0.0005797281046397984,\n",
       "   0.0005935886292718351,\n",
       "   0.0005559434066526592,\n",
       "   0.0007048081024549901,\n",
       "   0.0005583694437518716,\n",
       "   0.000562570639885962,\n",
       "   0.0006249976577237248,\n",
       "   0.0005541993887163699,\n",
       "   0.0005956147215329111,\n",
       "   0.0005207673530094326,\n",
       "   0.0005218048463575542,\n",
       "   0.0007017722236923873,\n",
       "   0.0005866923602297902,\n",
       "   0.0006541219772771001,\n",
       "   0.000624379375949502,\n",
       "   0.0005127740441821516,\n",
       "   0.0005831060698255897,\n",
       "   0.0005798671045340598,\n",
       "   0.0005613663815893233,\n",
       "   0.0004934438038617373,\n",
       "   0.0005597130511887372,\n",
       "   0.0005744089721702039,\n",
       "   0.0004865191294811666,\n",
       "   0.0005889390595257282,\n",
       "   0.0005664448253810406,\n",
       "   0.0006194293382577598,\n",
       "   0.0006519675953313708,\n",
       "   0.0005702800117433071,\n",
       "   0.0005464334972202778,\n",
       "   0.0005098687834106386,\n",
       "   0.0004654268268495798,\n",
       "   0.0005903712590225041,\n",
       "   0.0006912461831234396,\n",
       "   0.0005549520719796419,\n",
       "   0.0006269736331887543,\n",
       "   0.000521276262588799,\n",
       "   0.000615743629168719,\n",
       "   0.0006198345217853785,\n",
       "   0.00045269003021530807,\n",
       "   0.000594656856264919,\n",
       "   0.0006242742529138923,\n",
       "   0.0005298611940816045,\n",
       "   0.000579190265852958,\n",
       "   0.0005624378100037575,\n",
       "   0.0005693380371667445,\n",
       "   0.0006337375962175429,\n",
       "   0.0007498986087739468,\n",
       "   0.0005937368259765208,\n",
       "   0.000504662748426199,\n",
       "   0.0006186614045873284,\n",
       "   0.0005164845497347414,\n",
       "   0.00046534297871403396,\n",
       "   0.0005199103034101427,\n",
       "   0.000572740042116493,\n",
       "   0.0005797938792966306,\n",
       "   0.0005838361685164273,\n",
       "   0.0005578638520091772,\n",
       "   0.00048346526455134153,\n",
       "   0.0006032547680661082,\n",
       "   0.0006054688710719347,\n",
       "   0.0005437625222839415,\n",
       "   0.0005873867776244879,\n",
       "   0.0005367404082790017,\n",
       "   0.0006554602296091616,\n",
       "   0.000621694081928581,\n",
       "   0.0005945388693362474,\n",
       "   0.0005385837284848094,\n",
       "   0.0006433777743950486,\n",
       "   0.0005554438102990389,\n",
       "   0.000590671319514513,\n",
       "   0.0004776214191224426,\n",
       "   0.0006186337559483945,\n",
       "   0.0005729220574721694,\n",
       "   0.00042849514284171164,\n",
       "   0.0004532960883807391,\n",
       "   0.0006924023618921638,\n",
       "   0.000610368384514004,\n",
       "   0.0006546092336066067,\n",
       "   0.0006177809555083513,\n",
       "   0.0005598796415142715,\n",
       "   0.000531448342371732,\n",
       "   0.0006457019480876625,\n",
       "   0.0006773273344151676,\n",
       "   0.0005821268423460424,\n",
       "   0.0005129821365699172,\n",
       "   0.000582628941629082,\n",
       "   0.0007192075136117637,\n",
       "   0.0006528397207148373,\n",
       "   0.0004869461408816278,\n",
       "   0.0005527583998627961,\n",
       "   0.0005275949370115995,\n",
       "   0.0005898656090721488,\n",
       "   0.00040908390656113625,\n",
       "   0.0005898157833144069,\n",
       "   0.000583429413381964,\n",
       "   0.00043485217611305416,\n",
       "   0.0005568674532696605,\n",
       "   0.00045038407552056015,\n",
       "   0.0006305825081653893,\n",
       "   0.0005946981837041676,\n",
       "   0.000610587652772665,\n",
       "   0.0005912143969908357,\n",
       "   0.0006172431749291718,\n",
       "   0.000538056599907577,\n",
       "   0.00061336177168414,\n",
       "   0.0005122113507241011,\n",
       "   0.0005593984387814999,\n",
       "   0.0006045012851245701,\n",
       "   0.0006587425014004111,\n",
       "   0.0005932516069151461,\n",
       "   0.0005127109470777214,\n",
       "   0.0005859984667040408,\n",
       "   0.0005682313349097967,\n",
       "   0.0005244036437943578,\n",
       "   0.0005056374939158559,\n",
       "   0.0005342490621842444,\n",
       "   0.0005544971209019423,\n",
       "   0.0005857782671228051,\n",
       "   0.000628944777417928,\n",
       "   0.000510751677211374,\n",
       "   0.0005446488503366709,\n",
       "   0.0004944820539094508,\n",
       "   0.0005251116817817092,\n",
       "   0.0005052098422311246,\n",
       "   0.0004650965565815568,\n",
       "   0.0006513391272164881,\n",
       "   0.0004719936696346849,\n",
       "   0.0005369932623580098,\n",
       "   0.0005074532818980515,\n",
       "   0.00044209917541593313,\n",
       "   0.000635096977930516,\n",
       "   0.0005775910103693604,\n",
       "   0.0005805360269732773,\n",
       "   0.0005982553702779114,\n",
       "   0.0005516651435755193,\n",
       "   0.0005282003548927605,\n",
       "   0.0005170507938601077,\n",
       "   0.00047333750990219414,\n",
       "   0.0005181088927201927,\n",
       "   0.0006407125620171428,\n",
       "   0.0006152841378934681,\n",
       "   0.00045193295227363706,\n",
       "   0.000602979154791683,\n",
       "   0.0005086022429168224,\n",
       "   0.0005151802324689925,\n",
       "   0.0006072722608223557,\n",
       "   0.0005917693488299847,\n",
       "   0.0005993719096295536,\n",
       "   0.00048756005708128214,\n",
       "   0.00047200723201967776,\n",
       "   0.0006953589618206024,\n",
       "   0.0005357794580049813,\n",
       "   0.0005963886505924165,\n",
       "   0.00043696959619410336,\n",
       "   0.00043907680083066225,\n",
       "   0.0006168524851091206,\n",
       "   0.0005605029291473329,\n",
       "   0.0006531997350975871,\n",
       "   0.0005405080155469477,\n",
       "   0.0004962743259966373,\n",
       "   0.0005306840175762773,\n",
       "   0.0006753457710146904,\n",
       "   0.0006137706222943962,\n",
       "   0.0005405729170888662,\n",
       "   0.0005824421532452106,\n",
       "   0.0005634615663439035,\n",
       "   0.0005862619145773351,\n",
       "   0.000596262514591217,\n",
       "   0.0005457725492306054,\n",
       "   0.0005798969650641084,\n",
       "   0.00045925448648631573,\n",
       "   0.0005283918580971658,\n",
       "   0.0005108843324705958,\n",
       "   0.0005214117700234056,\n",
       "   0.00048082141438499093,\n",
       "   0.0005845954292453825,\n",
       "   0.0004169144667685032,\n",
       "   0.000621140468865633,\n",
       "   0.0005588812637142837,\n",
       "   0.0005731305573135614,\n",
       "   0.0005583624006249011,\n",
       "   0.0005305249360390007,\n",
       "   0.0006625154055655003,\n",
       "   0.0005329750711098313,\n",
       "   0.00037859746953472495,\n",
       "   0.0005501586711034179,\n",
       "   0.0005859112716279924,\n",
       "   0.00048422071267850697,\n",
       "   0.0005298866308294237,\n",
       "   0.0005380226066336036,\n",
       "   0.0005151164950802922,\n",
       "   0.0005206760833971202,\n",
       "   0.0005508612375706434,\n",
       "   0.0005930609186179936,\n",
       "   0.0006144866347312927,\n",
       "   0.0005317755858413875,\n",
       "   0.0004938940401189029,\n",
       "   0.0004586012801155448,\n",
       "   0.000590872426982969,\n",
       "   0.0006417264812625945,\n",
       "   0.0005989604978822172,\n",
       "   0.0005892962217330933,\n",
       "   0.0005575129762291908,\n",
       "   0.0005048296297900379,\n",
       "   0.0005463826237246394,\n",
       "   0.0005427252617664635,\n",
       "   0.000558970496058464,\n",
       "   0.0005400705267675221,\n",
       "   0.0005616553244180977,\n",
       "   0.0005686260992661119,\n",
       "   0.0005182906170375645,\n",
       "   0.0006095052231103182,\n",
       "   0.0005873728659935296,\n",
       "   0.0005734829464927316,\n",
       "   0.0004475285822991282,\n",
       "   0.0005186671623960137,\n",
       "   0.0005722148925997317,\n",
       "   0.0004985476261936128,\n",
       "   0.0006859354907646775,\n",
       "   0.0005075170192867517,\n",
       "   0.0005681776092387736,\n",
       "   0.0005869684391655028,\n",
       "   0.000533745507709682,\n",
       "   0.0005676356377080083,\n",
       "   0.0005549904890358448,\n",
       "   0.00044480786891654134,\n",
       "   0.0006087035872042179,\n",
       "   0.0005152160301804543,\n",
       "   0.0004933623713441193,\n",
       "   0.000611661234870553,\n",
       "   0.0005281665944494307,\n",
       "   0.0004320080333855003,\n",
       "   0.0005236188299022615,\n",
       "   0.0005480757099576294,\n",
       "   0.0004406369407661259,\n",
       "   0.0005405796109698713,\n",
       "   0.0005102833965793252,\n",
       "   0.0004589700256474316,\n",
       "   0.0004971053567714989,\n",
       "   0.0005920942639932036,\n",
       "   0.0004636600206140429,\n",
       "   0.0004910083371214569,\n",
       "   0.0005078667891211808,\n",
       "   0.0005798241472803056,\n",
       "   0.0006242753588594496,\n",
       "   0.0005125263705849648,\n",
       "   0.0005549220368266106,\n",
       "   0.0005216336576268077,\n",
       "   0.000567002862226218,\n",
       "   0.0005646492354571819,\n",
       "   0.0006216607289388776,\n",
       "   0.000571444455999881,\n",
       "   0.00048567831981927156,\n",
       "   0.0004675146483350545,\n",
       "   0.0005745668313466012,\n",
       "   0.0005338986520655453,\n",
       "   0.0005757928011007607,\n",
       "   0.0004681394493672997,\n",
       "   0.0006022439920343459,\n",
       "   0.000419337215134874,\n",
       "   0.0005449071759358048,\n",
       "   0.0005758826155215502,\n",
       "   0.00042187428334727883,\n",
       "   0.0005141113069839776,\n",
       "   0.0006010746001265943,\n",
       "   0.0004248024779371917,\n",
       "   0.0005594393005594611,\n",
       "   0.0005430065211839974,\n",
       "   0.00048499490367248654,\n",
       "   0.0005436840583570302,\n",
       "   0.0006297811632975936,\n",
       "   0.0005584073369391263,\n",
       "   0.0005251534166745842,\n",
       "   0.0005695662111975253,\n",
       "   0.0004784516931977123,\n",
       "   0.0005249542882665992,\n",
       "   0.0004511929291766137,\n",
       "   0.0005166126065887511,\n",
       "   0.0004991997266188264,\n",
       "   0.0005314612062647939,\n",
       "   0.0004993156180717051,\n",
       "   0.0004933038144372404,\n",
       "   0.0004650297632906586,\n",
       "   0.0005705385119654238,\n",
       "   0.0005237663863226771,\n",
       "   0.0005073235952295363,\n",
       "   0.0004717183473985642,\n",
       "   0.00047399746836163104,\n",
       "   0.0005067765596322715,\n",
       "   0.0006126585067249835,\n",
       "   0.0005924862925894558,\n",
       "   0.000526427524164319,\n",
       "   0.0005315172602422535,\n",
       "   0.00044864421943202615,\n",
       "   0.00046648093848489225,\n",
       "   0.0005279085016809404,\n",
       "   0.0004260648274794221,\n",
       "   0.0005478340899571776,\n",
       "   0.0005229311063885689,\n",
       "   0.0004085495602339506,\n",
       "   0.0005418441141955554,\n",
       "   0.0005029761814512312,\n",
       "   0.00045996939297765493,\n",
       "   0.0005658874870277941,\n",
       "   0.0005579272983595729,\n",
       "   0.0004975979099981487,\n",
       "   0.0005144535098224878,\n",
       "   0.0005793520831502974,\n",
       "   0.0005204770714044571,\n",
       "   0.0005067041493020952,\n",
       "   0.00047746929340064526,\n",
       "   0.0005129222990944982,\n",
       "   0.0005673343548551202,\n",
       "   0.0005706159863620996,\n",
       "   0.0006271885358728468,\n",
       "   0.0005427952273748815,\n",
       "   0.0005226372159086168,\n",
       "   0.0006209674174897373,\n",
       "   0.0005084587610326707,\n",
       "   0.0004881223139818758,\n",
       "   0.0004907366819679737,\n",
       "   0.00047745188931003213,\n",
       "   0.000520923815201968,\n",
       "   0.0004893428995274007,\n",
       "   0.0006081887404434383,\n",
       "   0.0004880354681517929,\n",
       "   0.0005676671280525625,\n",
       "   0.0004957691417075694,\n",
       "   0.0004943860112689435,\n",
       "   0.0005635337438434362,\n",
       "   0.0006080021848902106,\n",
       "   0.0004596620565280318,\n",
       "   0.0005492158816196024,\n",
       "   0.0005139100831001997,\n",
       "   0.0005878080264665186,\n",
       "   0.0005243683117441833,\n",
       "   0.0005938721005804837,\n",
       "   0.0004125750856474042,\n",
       "   0.00047496537445113063,\n",
       "   0.00047199343680404127,\n",
       "   0.000550841330550611,\n",
       "   0.0005737307947129011,\n",
       "   0.0004921770305372775,\n",
       "   0.0005036463844589889,\n",
       "   0.0004525214317254722,\n",
       "   0.000484753429191187,\n",
       "   0.0004369211383163929,\n",
       "   0.0005310741835273802,\n",
       "   0.0004925118410028517,\n",
       "   0.0006044969195500016,\n",
       "   0.0005540260463021696,\n",
       "   0.0005756343598477542,\n",
       "   0.0004949384601786733,\n",
       "   0.0004969739238731563,\n",
       "   0.00046291580656543374,\n",
       "   0.000538057298399508,\n",
       "   0.0005684993229806423,\n",
       "   0.0004905300447717309,\n",
       "   0.0005103523726575077,\n",
       "   0.0005142492591403425,\n",
       "   0.000491684942971915,\n",
       "   0.0005213079857639968,\n",
       "   0.0005219179438427091,\n",
       "   0.0005385238910093904,\n",
       "   0.00044005815288983285,\n",
       "   0.0005345991812646389,\n",
       "   0.0005736699677072465,\n",
       "   0.0004909401177428663,\n",
       "   0.0004280896973796189,\n",
       "   0.0005335297901183367,\n",
       "   0.0004757814749609679,\n",
       "   0.000622643914539367,\n",
       "   0.0004975266638211906,\n",
       "   0.00038837105967104435,\n",
       "   0.00044860379421152174,\n",
       "   0.0006574262515641749,\n",
       "   0.00046081558684818447,\n",
       "   0.0004480550123844296,\n",
       "   0.0005591828376054764,\n",
       "   0.000516607949975878,\n",
       "   0.0003933207190129906,\n",
       "   0.0005476698861457407,\n",
       "   0.00048193521797657013,\n",
       "   0.00039799572550691664,\n",
       "   0.0005653806147165596,\n",
       "   0.0006003967137075961,\n",
       "   0.0005754487356171012,\n",
       "   0.00043996513704769313,\n",
       "   0.0005270122783258557,\n",
       "   0.0004817774461116642,\n",
       "   0.0006452943198382854,\n",
       "   0.0006051685195416212,\n",
       "   0.0005095430533401668,\n",
       "   0.0004920173087157309,\n",
       "   0.0005030661122873425,\n",
       "   0.0004711846122518182,\n",
       "   0.0005151400691829622,\n",
       "   0.0005714588332921267,\n",
       "   0.000452944019343704,\n",
       "   0.0005069379694759846,\n",
       "   0.0004578686784952879,\n",
       "   0.0004710999201051891,\n",
       "   0.0006544824573211372,\n",
       "   0.0004966361448168755,\n",
       "   0.000494728097692132,\n",
       "   0.00042287673568353057,\n",
       "   0.0004552388854790479,\n",
       "   ...],\n",
       "  'train_acc': [-4.082061290740967,\n",
       "   -5.059171676635742,\n",
       "   -3.9434030055999756,\n",
       "   -4.069056987762451,\n",
       "   -3.1978719234466553,\n",
       "   -2.329340934753418,\n",
       "   -3.019986867904663,\n",
       "   -1.8647868633270264,\n",
       "   -2.217751979827881,\n",
       "   -6.205995559692383,\n",
       "   -1.7853684425354004,\n",
       "   -0.9808599352836609,\n",
       "   -0.8540927767753601,\n",
       "   -1.0983484983444214,\n",
       "   -0.4747908413410187,\n",
       "   -0.5117552280426025,\n",
       "   -0.49000081419944763,\n",
       "   -0.45892801880836487,\n",
       "   -0.40069082379341125,\n",
       "   -0.23940448462963104,\n",
       "   -0.7683194875717163,\n",
       "   -0.031326208263635635,\n",
       "   0.11911794543266296,\n",
       "   -0.17629863321781158,\n",
       "   0.19859005510807037,\n",
       "   -0.10490617156028748,\n",
       "   0.09414929896593094,\n",
       "   0.32948315143585205,\n",
       "   0.10641377419233322,\n",
       "   0.016928071156144142,\n",
       "   0.33620890974998474,\n",
       "   0.08183599263429642,\n",
       "   0.44854310154914856,\n",
       "   0.31393036246299744,\n",
       "   0.2446645349264145,\n",
       "   0.33262503147125244,\n",
       "   0.5407775640487671,\n",
       "   0.4272316098213196,\n",
       "   0.5163583159446716,\n",
       "   0.3768117129802704,\n",
       "   0.4162842035293579,\n",
       "   0.3991091549396515,\n",
       "   0.18462441861629486,\n",
       "   0.4374065399169922,\n",
       "   0.3912179172039032,\n",
       "   0.22012782096862793,\n",
       "   0.4609144330024719,\n",
       "   -0.04787710681557655,\n",
       "   0.4810377061367035,\n",
       "   0.3324555456638336,\n",
       "   0.25464555621147156,\n",
       "   0.4127011001110077,\n",
       "   0.26721516251564026,\n",
       "   0.3676946759223938,\n",
       "   -0.053851064294576645,\n",
       "   0.5407500267028809,\n",
       "   0.49420347809791565,\n",
       "   0.5451053380966187,\n",
       "   0.5373430848121643,\n",
       "   0.5501995086669922,\n",
       "   0.5177856087684631,\n",
       "   0.6023509502410889,\n",
       "   0.4962732493877411,\n",
       "   0.10715842247009277,\n",
       "   0.6394568085670471,\n",
       "   0.567063570022583,\n",
       "   0.21398621797561646,\n",
       "   0.639624297618866,\n",
       "   0.6110405921936035,\n",
       "   0.5545622110366821,\n",
       "   0.6517803072929382,\n",
       "   0.5546215772628784,\n",
       "   0.551555871963501,\n",
       "   0.5086769461631775,\n",
       "   0.5822035074234009,\n",
       "   0.5794370174407959,\n",
       "   0.6423195600509644,\n",
       "   0.6217488646507263,\n",
       "   0.6720346808433533,\n",
       "   0.4866251051425934,\n",
       "   0.6134254336357117,\n",
       "   0.43063727021217346,\n",
       "   0.6278913021087646,\n",
       "   0.6586136817932129,\n",
       "   0.5556915998458862,\n",
       "   0.6422560214996338,\n",
       "   0.5954759120941162,\n",
       "   0.6389116644859314,\n",
       "   0.5446597337722778,\n",
       "   0.6164202094078064,\n",
       "   0.568980872631073,\n",
       "   0.5939037203788757,\n",
       "   0.6800096035003662,\n",
       "   0.5792455673217773,\n",
       "   0.5187529921531677,\n",
       "   0.719948410987854,\n",
       "   0.6678957939147949,\n",
       "   0.551021158695221,\n",
       "   0.2960797846317291,\n",
       "   0.669115424156189,\n",
       "   0.6568257212638855,\n",
       "   0.663881242275238,\n",
       "   0.5072526335716248,\n",
       "   0.6612134575843811,\n",
       "   0.6838641166687012,\n",
       "   0.6488363742828369,\n",
       "   0.5858890414237976,\n",
       "   0.627479612827301,\n",
       "   0.6108835935592651,\n",
       "   0.6648644804954529,\n",
       "   0.6547949314117432,\n",
       "   0.524038553237915,\n",
       "   0.5115875601768494,\n",
       "   0.6439296007156372,\n",
       "   0.7112737894058228,\n",
       "   0.7270451188087463,\n",
       "   0.744633138179779,\n",
       "   0.6700644493103027,\n",
       "   0.6562615633010864,\n",
       "   0.7249962091445923,\n",
       "   0.6622768640518188,\n",
       "   0.7083130478858948,\n",
       "   0.6532804369926453,\n",
       "   0.6769091486930847,\n",
       "   0.7771432995796204,\n",
       "   0.7297735810279846,\n",
       "   0.6618338227272034,\n",
       "   0.6059530377388,\n",
       "   0.7310689687728882,\n",
       "   0.7331767678260803,\n",
       "   0.6968963146209717,\n",
       "   0.707112729549408,\n",
       "   0.7636306285858154,\n",
       "   0.6782395839691162,\n",
       "   0.7425840497016907,\n",
       "   0.7661037445068359,\n",
       "   0.7045417428016663,\n",
       "   0.7672273516654968,\n",
       "   0.7680883407592773,\n",
       "   0.6884773373603821,\n",
       "   0.6517447829246521,\n",
       "   0.7207812666893005,\n",
       "   0.588297426700592,\n",
       "   0.715553343296051,\n",
       "   0.7388217449188232,\n",
       "   0.5416433811187744,\n",
       "   0.7112361788749695,\n",
       "   0.6867064833641052,\n",
       "   0.4580373764038086,\n",
       "   0.5350229740142822,\n",
       "   0.7921255230903625,\n",
       "   0.7366933226585388,\n",
       "   0.754223108291626,\n",
       "   0.6278051733970642,\n",
       "   0.7596160769462585,\n",
       "   0.7036054134368896,\n",
       "   0.7368282675743103,\n",
       "   0.6758931875228882,\n",
       "   0.6735532283782959,\n",
       "   0.703370988368988,\n",
       "   0.5776029229164124,\n",
       "   0.6905571818351746,\n",
       "   0.748173713684082,\n",
       "   0.6632381081581116,\n",
       "   0.7576839327812195,\n",
       "   0.7359735369682312,\n",
       "   0.5380876660346985,\n",
       "   0.65705806016922,\n",
       "   0.6892080307006836,\n",
       "   0.6105601787567139,\n",
       "   0.7321094274520874,\n",
       "   0.7148985266685486,\n",
       "   0.6686148643493652,\n",
       "   0.5027872323989868,\n",
       "   0.7821713089942932,\n",
       "   0.7712617516517639,\n",
       "   0.5031377673149109,\n",
       "   0.537818968296051,\n",
       "   0.7186761498451233,\n",
       "   0.7830734848976135,\n",
       "   0.7800226807594299,\n",
       "   0.6280515789985657,\n",
       "   0.6664378643035889,\n",
       "   0.7246002554893494,\n",
       "   0.7314170598983765,\n",
       "   0.7522728443145752,\n",
       "   0.7750598788261414,\n",
       "   0.7771141529083252,\n",
       "   0.7850240468978882,\n",
       "   0.7686337828636169,\n",
       "   0.7366609573364258,\n",
       "   0.7766194939613342,\n",
       "   0.6769877672195435,\n",
       "   0.7893418669700623,\n",
       "   0.8070631623268127,\n",
       "   0.7850010395050049,\n",
       "   0.8147692084312439,\n",
       "   0.6986216902732849,\n",
       "   0.7299530506134033,\n",
       "   0.7549071907997131,\n",
       "   0.8105846643447876,\n",
       "   0.7503291964530945,\n",
       "   0.7791818976402283,\n",
       "   0.6506155133247375,\n",
       "   0.797353982925415,\n",
       "   0.8096511960029602,\n",
       "   0.7317962646484375,\n",
       "   0.605127215385437,\n",
       "   0.7414790987968445,\n",
       "   0.514793872833252,\n",
       "   0.6346362829208374,\n",
       "   0.5062089562416077,\n",
       "   0.7703912854194641,\n",
       "   0.7081356644630432,\n",
       "   0.7611698508262634,\n",
       "   0.7581945061683655,\n",
       "   0.7245648503303528,\n",
       "   0.8217286467552185,\n",
       "   0.8140861988067627,\n",
       "   0.6405613422393799,\n",
       "   0.7542579174041748,\n",
       "   0.8083686232566833,\n",
       "   0.8043007254600525,\n",
       "   0.8202474117279053,\n",
       "   0.8328970074653625,\n",
       "   0.7707153558731079,\n",
       "   0.7818397283554077,\n",
       "   0.8224348425865173,\n",
       "   0.6499089002609253,\n",
       "   0.7102618217468262,\n",
       "   0.6844679117202759,\n",
       "   0.7851009368896484,\n",
       "   0.7352905869483948,\n",
       "   0.7328414916992188,\n",
       "   0.789649248123169,\n",
       "   0.8380017876625061,\n",
       "   0.7017252445220947,\n",
       "   0.788400411605835,\n",
       "   0.48630353808403015,\n",
       "   0.763236939907074,\n",
       "   0.7893314361572266,\n",
       "   0.8296626806259155,\n",
       "   0.7755138874053955,\n",
       "   0.7957708239555359,\n",
       "   0.6830453276634216,\n",
       "   0.7151980996131897,\n",
       "   0.5497038960456848,\n",
       "   0.7921876907348633,\n",
       "   0.7578655481338501,\n",
       "   -0.19286231696605682,\n",
       "   0.8030788898468018,\n",
       "   0.8119983673095703,\n",
       "   0.6974253058433533,\n",
       "   0.7704489231109619,\n",
       "   0.7925998568534851,\n",
       "   0.73514324426651,\n",
       "   0.7481001019477844,\n",
       "   0.6387868523597717,\n",
       "   0.7734901309013367,\n",
       "   0.8413382172584534,\n",
       "   0.8065307140350342,\n",
       "   0.7696269750595093,\n",
       "   0.76712965965271,\n",
       "   0.7741164565086365,\n",
       "   0.8010444045066833,\n",
       "   0.788109540939331,\n",
       "   0.6904825568199158,\n",
       "   0.7902906537055969,\n",
       "   0.7973861694335938,\n",
       "   0.833777666091919,\n",
       "   0.8311523795127869,\n",
       "   0.7626762986183167,\n",
       "   0.7024548053741455,\n",
       "   0.8113752007484436,\n",
       "   0.7835578918457031,\n",
       "   0.8397180438041687,\n",
       "   0.8003997802734375,\n",
       "   0.8200035095214844,\n",
       "   0.7575475573539734,\n",
       "   0.8176741003990173,\n",
       "   0.7067562341690063,\n",
       "   0.8204065561294556,\n",
       "   0.8443042635917664,\n",
       "   0.8251697421073914,\n",
       "   0.8127073049545288,\n",
       "   0.8405323028564453,\n",
       "   0.758965253829956,\n",
       "   0.8047751784324646,\n",
       "   0.8295126557350159,\n",
       "   0.8189031481742859,\n",
       "   0.7465233206748962,\n",
       "   0.7907333970069885,\n",
       "   0.8257038593292236,\n",
       "   0.8440917730331421,\n",
       "   0.7773986458778381,\n",
       "   0.6898441910743713,\n",
       "   0.8552914261817932,\n",
       "   0.7992982268333435,\n",
       "   0.7327278256416321,\n",
       "   0.7932798862457275,\n",
       "   0.7354085445404053,\n",
       "   0.8112812042236328,\n",
       "   0.6753495931625366,\n",
       "   0.8277859091758728,\n",
       "   0.8132320046424866,\n",
       "   0.7928338646888733,\n",
       "   0.7564753890037537,\n",
       "   0.5468845963478088,\n",
       "   0.6718838810920715,\n",
       "   0.8495094180107117,\n",
       "   0.8187883496284485,\n",
       "   0.7957227230072021,\n",
       "   0.8120460510253906,\n",
       "   0.8059700131416321,\n",
       "   0.8800612092018127,\n",
       "   0.7943357229232788,\n",
       "   0.8288511037826538,\n",
       "   0.8452300429344177,\n",
       "   0.8015455603599548,\n",
       "   0.8070379495620728,\n",
       "   0.8488684296607971,\n",
       "   0.7990518808364868,\n",
       "   0.8538913130760193,\n",
       "   0.8373937606811523,\n",
       "   0.8361263871192932,\n",
       "   0.7809422016143799,\n",
       "   0.7576658725738525,\n",
       "   0.8340010046958923,\n",
       "   0.7691040635108948,\n",
       "   0.7911760807037354,\n",
       "   0.7967230081558228,\n",
       "   0.8068493008613586,\n",
       "   0.7336569428443909,\n",
       "   0.847686767578125,\n",
       "   0.7789174914360046,\n",
       "   0.8245894908905029,\n",
       "   0.7518177032470703,\n",
       "   0.8273699283599854,\n",
       "   0.8088838458061218,\n",
       "   0.7279694080352783,\n",
       "   0.8498668074607849,\n",
       "   0.8571718335151672,\n",
       "   0.8110899329185486,\n",
       "   0.8079907894134521,\n",
       "   0.8502516150474548,\n",
       "   0.8294187188148499,\n",
       "   0.7916905879974365,\n",
       "   0.7779695391654968,\n",
       "   0.5995817184448242,\n",
       "   0.82015061378479,\n",
       "   0.807705283164978,\n",
       "   0.8560500741004944,\n",
       "   0.8519302606582642,\n",
       "   0.7596442699432373,\n",
       "   0.8428921103477478,\n",
       "   0.6885297298431396,\n",
       "   0.796251118183136,\n",
       "   0.8229283690452576,\n",
       "   0.7768262624740601,\n",
       "   0.8295512795448303,\n",
       "   0.8043760061264038,\n",
       "   0.7972226738929749,\n",
       "   0.8370602130889893,\n",
       "   0.8249803185462952,\n",
       "   0.8222173452377319,\n",
       "   0.8242934942245483,\n",
       "   0.7782636880874634,\n",
       "   0.8043212890625,\n",
       "   0.8483967185020447,\n",
       "   0.5789818167686462,\n",
       "   0.8152546286582947,\n",
       "   0.8381904363632202,\n",
       "   0.7479938864707947,\n",
       "   0.8070430755615234,\n",
       "   0.8235617280006409,\n",
       "   0.7523557543754578,\n",
       "   0.8440391421318054,\n",
       "   0.8613560199737549,\n",
       "   0.8429401516914368,\n",
       "   0.7695171236991882,\n",
       "   0.8168845772743225,\n",
       "   0.8410511612892151,\n",
       "   0.8347633481025696,\n",
       "   0.7938329577445984,\n",
       "   0.833308219909668,\n",
       "   0.8619688153266907,\n",
       "   0.7435060739517212,\n",
       "   0.8321568965911865,\n",
       "   0.8108168244361877,\n",
       "   0.7531751394271851,\n",
       "   0.7603044509887695,\n",
       "   0.7835471630096436,\n",
       "   0.8369068503379822,\n",
       "   0.8126197457313538,\n",
       "   0.875457227230072,\n",
       "   0.8183087706565857,\n",
       "   0.8429247140884399,\n",
       "   0.8479213118553162,\n",
       "   0.8256088495254517,\n",
       "   0.8440372943878174,\n",
       "   0.8243756890296936,\n",
       "   0.8176392912864685,\n",
       "   0.8346989154815674,\n",
       "   0.6143560409545898,\n",
       "   0.8759958148002625,\n",
       "   0.8614356517791748,\n",
       "   0.6974387764930725,\n",
       "   0.8117656707763672,\n",
       "   0.8534128665924072,\n",
       "   0.8454687595367432,\n",
       "   0.8265870809555054,\n",
       "   0.8005882501602173,\n",
       "   0.8758867383003235,\n",
       "   0.8047326803207397,\n",
       "   0.8846892714500427,\n",
       "   0.8399860858917236,\n",
       "   0.8523649573326111,\n",
       "   0.8039976358413696,\n",
       "   0.7600811123847961,\n",
       "   0.8285821676254272,\n",
       "   0.8841719031333923,\n",
       "   0.822923481464386,\n",
       "   0.8359861969947815,\n",
       "   0.8620939254760742,\n",
       "   0.8062595129013062,\n",
       "   0.8593612313270569,\n",
       "   0.8543636798858643,\n",
       "   0.8393645882606506,\n",
       "   0.6691468358039856,\n",
       "   0.820573627948761,\n",
       "   0.7977092266082764,\n",
       "   0.832737922668457,\n",
       "   0.8806951642036438,\n",
       "   0.8356689810752869,\n",
       "   0.8996505737304688,\n",
       "   0.8414364457130432,\n",
       "   0.8888485431671143,\n",
       "   0.854421079158783,\n",
       "   0.8528613448143005,\n",
       "   0.8124868273735046,\n",
       "   0.7490766048431396,\n",
       "   0.8541982769966125,\n",
       "   0.8681191802024841,\n",
       "   0.7022321224212646,\n",
       "   0.8398540019989014,\n",
       "   0.8076112270355225,\n",
       "   0.8575185537338257,\n",
       "   0.8371622562408447,\n",
       "   0.8009424805641174,\n",
       "   0.827928364276886,\n",
       "   0.8555012345314026,\n",
       "   0.8655187487602234,\n",
       "   0.8647539615631104,\n",
       "   0.8539491891860962,\n",
       "   0.8509578108787537,\n",
       "   0.8804733157157898,\n",
       "   0.8503490090370178,\n",
       "   0.8894829154014587,\n",
       "   0.8398154973983765,\n",
       "   0.8128532767295837,\n",
       "   0.876718282699585,\n",
       "   0.8645564913749695,\n",
       "   0.85190349817276,\n",
       "   0.8365812301635742,\n",
       "   0.8353531360626221,\n",
       "   0.8597762584686279,\n",
       "   0.8638810515403748,\n",
       "   0.8660064339637756,\n",
       "   0.8596935868263245,\n",
       "   0.8057113289833069,\n",
       "   0.8553895354270935,\n",
       "   0.8718101382255554,\n",
       "   0.8351473212242126,\n",
       "   0.8043201565742493,\n",
       "   0.8809372186660767,\n",
       "   0.840528666973114,\n",
       "   0.8250938653945923,\n",
       "   0.8452647924423218,\n",
       "   0.7208872437477112,\n",
       "   0.8671132922172546,\n",
       "   0.7491400241851807,\n",
       "   0.8675908446311951,\n",
       "   0.8326683044433594,\n",
       "   0.8500908017158508,\n",
       "   0.8094493746757507,\n",
       "   0.82454913854599,\n",
       "   0.8650006651878357,\n",
       "   0.8543243408203125,\n",
       "   0.801953911781311,\n",
       "   0.8532114624977112,\n",
       "   0.8802915215492249,\n",
       "   0.8338381052017212,\n",
       "   0.8071898818016052,\n",
       "   0.8085305690765381,\n",
       "   0.8782493472099304,\n",
       "   0.8545011281967163,\n",
       "   0.8528276085853577,\n",
       "   0.8205971121788025,\n",
       "   0.8525127172470093,\n",
       "   0.6728861331939697,\n",
       "   0.8752347826957703,\n",
       "   0.8324403762817383,\n",
       "   0.8275502920150757,\n",
       "   0.8478096127510071,\n",
       "   0.8694489002227783,\n",
       "   0.7832050323486328,\n",
       "   0.8352916836738586,\n",
       "   0.8841855525970459,\n",
       "   0.8706309795379639,\n",
       "   0.8858989477157593,\n",
       "   0.7775638103485107,\n",
       "   0.8342189192771912,\n",
       "   0.8376861214637756,\n",
       "   0.8341009020805359,\n",
       "   0.8539478778839111,\n",
       "   0.8739233016967773,\n",
       "   0.8231277465820312,\n",
       "   0.8175196051597595,\n",
       "   0.7238611578941345,\n",
       "   0.7744746804237366,\n",
       "   0.7437625527381897,\n",
       "   0.8546407222747803,\n",
       "   0.834852397441864,\n",
       "   0.8533561825752258,\n",
       "   0.8845996260643005,\n",
       "   0.8296201825141907,\n",
       "   0.8638743758201599,\n",
       "   0.8655881881713867,\n",
       "   0.880267858505249,\n",
       "   0.8275962471961975,\n",
       "   0.8521192669868469,\n",
       "   0.8488739132881165,\n",
       "   0.8331854343414307,\n",
       "   0.8827167749404907,\n",
       "   0.8726457357406616,\n",
       "   0.8716073632240295,\n",
       "   0.8025331497192383,\n",
       "   0.8244502544403076,\n",
       "   0.853478193283081,\n",
       "   0.8900731801986694,\n",
       "   0.8629223108291626,\n",
       "   0.8372297286987305,\n",
       "   0.8549787998199463,\n",
       "   0.8503338098526001,\n",
       "   0.8641573190689087,\n",
       "   0.8293559551239014,\n",
       "   0.8115024566650391,\n",
       "   0.7955225110054016,\n",
       "   0.7944343090057373,\n",
       "   0.8798239231109619,\n",
       "   0.835008442401886,\n",
       "   0.864531934261322,\n",
       "   0.8635244965553284,\n",
       "   0.874513566493988,\n",
       "   0.8630543351173401,\n",
       "   0.8757388591766357,\n",
       "   0.8682146072387695,\n",
       "   0.850860595703125,\n",
       "   0.8472309708595276,\n",
       "   0.8780232071876526,\n",
       "   0.8051410913467407,\n",
       "   0.855251133441925,\n",
       "   0.8820372223854065,\n",
       "   0.8637201189994812,\n",
       "   0.8750846982002258,\n",
       "   0.8544349074363708,\n",
       "   0.83292156457901,\n",
       "   0.8612783551216125,\n",
       "   0.890768826007843,\n",
       "   0.8415315747261047,\n",
       "   0.8277290463447571,\n",
       "   0.8883690237998962,\n",
       "   0.8780385851860046,\n",
       "   0.8665544390678406,\n",
       "   0.8526524901390076,\n",
       "   0.7881421446800232,\n",
       "   0.7358867526054382,\n",
       "   0.8404457569122314,\n",
       "   0.8835089206695557,\n",
       "   0.8145434856414795,\n",
       "   0.8390260338783264,\n",
       "   0.8620321154594421,\n",
       "   0.8864057064056396,\n",
       "   0.8521679043769836,\n",
       "   0.8845530152320862,\n",
       "   0.8961188793182373,\n",
       "   0.8288344740867615,\n",
       "   0.8501806855201721,\n",
       "   0.8287933468818665,\n",
       "   0.8339433073997498,\n",
       "   0.8526611924171448,\n",
       "   0.8703615069389343,\n",
       "   0.9115406274795532,\n",
       "   0.759350061416626,\n",
       "   0.847553014755249,\n",
       "   0.8519276976585388,\n",
       "   0.8160222768783569,\n",
       "   0.8176101446151733,\n",
       "   0.8539113998413086,\n",
       "   0.8718492388725281,\n",
       "   0.8316840529441833,\n",
       "   0.8439209461212158,\n",
       "   0.8827203512191772,\n",
       "   0.8225901126861572,\n",
       "   0.8439049124717712,\n",
       "   0.8392512798309326,\n",
       "   0.774334192276001,\n",
       "   0.8039817810058594,\n",
       "   0.8327512741088867,\n",
       "   0.8632013201713562,\n",
       "   0.8820936679840088,\n",
       "   0.8495360016822815,\n",
       "   0.8442531228065491,\n",
       "   0.8530920147895813,\n",
       "   0.842864453792572,\n",
       "   0.9006592035293579,\n",
       "   0.8578040599822998,\n",
       "   0.8768048882484436,\n",
       "   0.8306718468666077,\n",
       "   0.8025075793266296,\n",
       "   0.8527833819389343,\n",
       "   0.7661024332046509,\n",
       "   0.8721367120742798,\n",
       "   0.8442847728729248,\n",
       "   0.8738691806793213,\n",
       "   0.8208749890327454,\n",
       "   0.827002763748169,\n",
       "   0.49204331636428833,\n",
       "   0.8871164321899414,\n",
       "   0.8771809935569763,\n",
       "   0.8462924361228943,\n",
       "   0.8588362336158752,\n",
       "   0.8510695099830627,\n",
       "   0.8418712615966797,\n",
       "   0.8800680041313171,\n",
       "   0.7841300964355469,\n",
       "   0.8747172951698303,\n",
       "   0.8399742245674133,\n",
       "   0.8239738941192627,\n",
       "   0.8032071590423584,\n",
       "   0.7705228924751282,\n",
       "   0.8780415058135986,\n",
       "   0.8795313239097595,\n",
       "   0.8455325961112976,\n",
       "   0.8921838402748108,\n",
       "   0.8499608635902405,\n",
       "   0.8609389662742615,\n",
       "   0.8200713992118835,\n",
       "   0.8496946096420288,\n",
       "   0.813647985458374,\n",
       "   0.885810911655426,\n",
       "   0.8359307646751404,\n",
       "   0.8741983771324158,\n",
       "   0.8807294368743896,\n",
       "   0.8484378457069397,\n",
       "   0.872747004032135,\n",
       "   0.7232118248939514,\n",
       "   0.8665750622749329,\n",
       "   0.889375627040863,\n",
       "   0.8595507144927979,\n",
       "   0.7641019225120544,\n",
       "   0.8492612242698669,\n",
       "   0.847494900226593,\n",
       "   0.868960440158844,\n",
       "   0.8679112792015076,\n",
       "   0.7904642224311829,\n",
       "   0.907622754573822,\n",
       "   0.8905975818634033,\n",
       "   0.7728039026260376,\n",
       "   0.8677045702934265,\n",
       "   0.6515213251113892,\n",
       "   0.8481113910675049,\n",
       "   0.7637798190116882,\n",
       "   0.8645656704902649,\n",
       "   0.7974762320518494,\n",
       "   0.8502932190895081,\n",
       "   0.882175087928772,\n",
       "   0.8614283204078674,\n",
       "   0.8487144708633423,\n",
       "   0.8013770580291748,\n",
       "   0.8016700744628906,\n",
       "   0.9086655974388123,\n",
       "   0.8835041522979736,\n",
       "   0.8399987816810608,\n",
       "   0.9000238180160522,\n",
       "   0.8783748149871826,\n",
       "   0.7052392363548279,\n",
       "   0.8895034193992615,\n",
       "   0.8831809759140015,\n",
       "   0.8443307280540466,\n",
       "   0.8778102993965149,\n",
       "   0.890068769454956,\n",
       "   0.8474707007408142,\n",
       "   0.8448259234428406,\n",
       "   0.8289565443992615,\n",
       "   0.8755538463592529,\n",
       "   0.8488752841949463,\n",
       "   0.7879152894020081,\n",
       "   0.8442460894584656,\n",
       "   0.8709671497344971,\n",
       "   0.8397777676582336,\n",
       "   0.8355242013931274,\n",
       "   0.8878681063652039,\n",
       "   0.902836263179779,\n",
       "   0.6811361312866211,\n",
       "   0.769656240940094,\n",
       "   0.899722695350647,\n",
       "   0.874174177646637,\n",
       "   0.8332263827323914,\n",
       "   0.8442615270614624,\n",
       "   0.8813630938529968,\n",
       "   0.8550195097923279,\n",
       "   0.8575062155723572,\n",
       "   0.8153495788574219,\n",
       "   0.825612485408783,\n",
       "   0.8773068785667419,\n",
       "   0.8277646899223328,\n",
       "   0.9204285144805908,\n",
       "   0.719336211681366,\n",
       "   0.9045448899269104,\n",
       "   0.8518548607826233,\n",
       "   0.8403245806694031,\n",
       "   0.8789039850234985,\n",
       "   0.8092076778411865,\n",
       "   0.8667289614677429,\n",
       "   0.8445688486099243,\n",
       "   0.7690383791923523,\n",
       "   0.8796449303627014,\n",
       "   0.8895406126976013,\n",
       "   0.8721857666969299,\n",
       "   0.8836214542388916,\n",
       "   0.888333261013031,\n",
       "   0.8532091379165649,\n",
       "   0.823263943195343,\n",
       "   0.8644178509712219,\n",
       "   0.7654958367347717,\n",
       "   0.8925157785415649,\n",
       "   0.8447028994560242,\n",
       "   0.866320788860321,\n",
       "   0.8450976014137268,\n",
       "   0.8862924575805664,\n",
       "   0.8666832447052002,\n",
       "   0.8835650086402893,\n",
       "   0.8646702170372009,\n",
       "   0.8982439041137695,\n",
       "   0.8431568741798401,\n",
       "   0.8845372796058655,\n",
       "   0.8721469640731812,\n",
       "   0.8055351376533508,\n",
       "   0.8314611911773682,\n",
       "   0.8515186905860901,\n",
       "   0.7396318316459656,\n",
       "   0.8567245006561279,\n",
       "   0.870278537273407,\n",
       "   0.840629518032074,\n",
       "   0.8831079006195068,\n",
       "   0.8649091124534607,\n",
       "   0.8242189288139343,\n",
       "   0.8962852358818054,\n",
       "   0.8391500115394592,\n",
       "   0.8700578808784485,\n",
       "   0.8823347091674805,\n",
       "   0.8222616314888,\n",
       "   0.8972160816192627,\n",
       "   0.891930341720581,\n",
       "   0.8806049823760986,\n",
       "   0.8416547775268555,\n",
       "   0.830833375453949,\n",
       "   0.8890255093574524,\n",
       "   0.9125071167945862,\n",
       "   0.7135396599769592,\n",
       "   0.7327255010604858,\n",
       "   0.8834332823753357,\n",
       "   0.9041364789009094,\n",
       "   0.8910856246948242,\n",
       "   0.8701978921890259,\n",
       "   0.9022502899169922,\n",
       "   0.9230896234512329,\n",
       "   0.8938131332397461,\n",
       "   0.861703097820282,\n",
       "   0.9000866413116455,\n",
       "   0.7401204705238342,\n",
       "   0.8494395613670349,\n",
       "   0.8936090469360352,\n",
       "   0.842994749546051,\n",
       "   0.8819730877876282,\n",
       "   0.8185405135154724,\n",
       "   0.8523275256156921,\n",
       "   0.8687272667884827,\n",
       "   0.8879749178886414,\n",
       "   0.885630190372467,\n",
       "   0.7577210068702698,\n",
       "   0.8588840961456299,\n",
       "   0.8389613628387451,\n",
       "   0.8728704452514648,\n",
       "   0.8950439095497131,\n",
       "   0.8817690014839172,\n",
       "   0.8566734194755554,\n",
       "   0.8329194784164429,\n",
       "   0.8592588305473328,\n",
       "   0.8243408203125,\n",
       "   0.8835964798927307,\n",
       "   0.7131578922271729,\n",
       "   0.843332827091217,\n",
       "   0.846228837966919,\n",
       "   0.8431461453437805,\n",
       "   0.8768025040626526,\n",
       "   0.9055951833724976,\n",
       "   0.8724260926246643,\n",
       "   0.835332453250885,\n",
       "   0.8907585144042969,\n",
       "   0.8641499876976013,\n",
       "   0.8857263922691345,\n",
       "   0.8888063430786133,\n",
       "   0.8626505136489868,\n",
       "   0.8859851360321045,\n",
       "   0.8655720353126526,\n",
       "   0.868263840675354,\n",
       "   0.9030386209487915,\n",
       "   0.8431273102760315,\n",
       "   0.8677722811698914,\n",
       "   0.9130491614341736,\n",
       "   0.8588452935218811,\n",
       "   0.882709264755249,\n",
       "   0.9203583002090454,\n",
       "   0.8915395140647888,\n",
       "   0.8871698379516602,\n",
       "   0.9119622707366943,\n",
       "   0.8655858635902405,\n",
       "   0.8668517470359802,\n",
       "   0.8718785643577576,\n",
       "   0.8999404907226562,\n",
       "   0.8626798391342163,\n",
       "   0.8429878354072571,\n",
       "   0.8964613080024719,\n",
       "   0.8762897253036499,\n",
       "   0.8792385458946228,\n",
       "   0.8581966161727905,\n",
       "   0.8999562859535217,\n",
       "   0.845957338809967,\n",
       "   0.898205578327179,\n",
       "   0.837044894695282,\n",
       "   0.839577853679657,\n",
       "   0.8683263659477234,\n",
       "   0.8530699014663696,\n",
       "   0.9114429950714111,\n",
       "   0.9048898220062256,\n",
       "   0.8025385141372681,\n",
       "   0.8870506286621094,\n",
       "   0.8667860627174377,\n",
       "   0.9047020077705383,\n",
       "   0.8548097014427185,\n",
       "   0.8908746838569641,\n",
       "   0.9029586911201477,\n",
       "   0.8982037901878357,\n",
       "   0.8768461346626282,\n",
       "   0.8857637643814087,\n",
       "   0.8585174679756165,\n",
       "   0.8599315881729126,\n",
       "   0.8824976086616516,\n",
       "   0.8370590806007385,\n",
       "   0.8384847640991211,\n",
       "   0.8843857049942017,\n",
       "   0.8765594363212585,\n",
       "   0.8608493804931641,\n",
       "   0.8367677330970764,\n",
       "   0.8301516175270081,\n",
       "   0.8902002573013306,\n",
       "   0.8997508883476257,\n",
       "   0.8913043737411499,\n",
       "   0.7562566995620728,\n",
       "   0.9037353992462158,\n",
       "   0.8593178391456604,\n",
       "   0.8363426327705383,\n",
       "   0.8799692392349243,\n",
       "   0.8986696004867554,\n",
       "   0.8419283032417297,\n",
       "   0.8324683904647827,\n",
       "   0.8554931879043579,\n",
       "   0.8998324871063232,\n",
       "   0.8986107110977173,\n",
       "   0.8842630982398987,\n",
       "   0.8358312249183655,\n",
       "   0.8807677626609802,\n",
       "   0.8938825726509094,\n",
       "   0.866338312625885,\n",
       "   0.8831186890602112,\n",
       "   0.9001659750938416,\n",
       "   0.89894700050354,\n",
       "   0.8972904086112976,\n",
       "   0.8589101433753967,\n",
       "   0.8124583959579468,\n",
       "   0.871637761592865,\n",
       "   0.8725244402885437,\n",
       "   0.8879596590995789,\n",
       "   0.8771928548812866,\n",
       "   0.8796160221099854,\n",
       "   0.8945951461791992,\n",
       "   0.9011518955230713,\n",
       "   0.8872408270835876,\n",
       "   0.8804271817207336,\n",
       "   0.8617501854896545,\n",
       "   0.8972629308700562,\n",
       "   0.8962118625640869,\n",
       "   0.8832489252090454,\n",
       "   0.8389976620674133,\n",
       "   0.8828094005584717,\n",
       "   0.8595911264419556,\n",
       "   0.8889579772949219,\n",
       "   0.834423303604126,\n",
       "   0.8598670363426208,\n",
       "   0.855165421962738,\n",
       "   0.8652797937393188,\n",
       "   0.8426392078399658,\n",
       "   0.8497833609580994,\n",
       "   0.8706114292144775,\n",
       "   0.8988893032073975,\n",
       "   0.8218279480934143,\n",
       "   0.8893264532089233,\n",
       "   0.8406527042388916,\n",
       "   0.8854737877845764,\n",
       "   0.9031291604042053,\n",
       "   0.8753000497817993,\n",
       "   0.8891038298606873,\n",
       "   0.8912866711616516,\n",
       "   0.850253701210022,\n",
       "   0.8740077614784241,\n",
       "   0.864986777305603,\n",
       "   0.8554107546806335,\n",
       "   0.8726691603660583,\n",
       "   0.9075950384140015,\n",
       "   0.868058443069458,\n",
       "   0.8991379141807556,\n",
       "   0.900126039981842,\n",
       "   0.8691890835762024,\n",
       "   0.8873790502548218,\n",
       "   0.879347562789917,\n",
       "   0.8912258744239807,\n",
       "   0.9076547026634216,\n",
       "   0.8824896216392517,\n",
       "   0.8880004286766052,\n",
       "   0.871939480304718,\n",
       "   0.8005542755126953,\n",
       "   0.886745274066925,\n",
       "   0.9076281189918518,\n",
       "   0.8786316514015198,\n",
       "   0.8984456658363342,\n",
       "   0.917758047580719,\n",
       "   0.863402783870697,\n",
       "   0.8887651562690735,\n",
       "   0.9199418425559998,\n",
       "   0.8714873790740967,\n",
       "   0.737619161605835,\n",
       "   0.8249334692955017,\n",
       "   0.8770708441734314,\n",
       "   0.8797544240951538,\n",
       "   0.7979428768157959,\n",
       "   0.896036684513092,\n",
       "   0.8362668752670288,\n",
       "   0.7847452759742737,\n",
       "   0.8099777698516846,\n",
       "   0.85049968957901,\n",
       "   0.8777312636375427,\n",
       "   0.8224311470985413,\n",
       "   0.8169559240341187,\n",
       "   0.8909622430801392,\n",
       "   0.8678886890411377,\n",
       "   0.8881170749664307,\n",
       "   0.8880611062049866,\n",
       "   0.8822411894798279,\n",
       "   0.8866393566131592,\n",
       "   0.8267160058021545,\n",
       "   0.9010021686553955,\n",
       "   0.881966769695282,\n",
       "   0.8201520442962646,\n",
       "   0.8972511291503906,\n",
       "   0.8620086908340454,\n",
       "   0.8725244402885437,\n",
       "   0.8347305059432983,\n",
       "   0.8487908840179443,\n",
       "   0.896580696105957,\n",
       "   0.8535206317901611,\n",
       "   0.8798801898956299,\n",
       "   0.8644682168960571,\n",
       "   0.8587312698364258,\n",
       "   0.8789370059967041,\n",
       "   0.8697618842124939,\n",
       "   0.918356716632843,\n",
       "   0.9211498498916626,\n",
       "   0.8834743499755859,\n",
       "   0.8531356453895569,\n",
       "   0.8536781072616577,\n",
       "   0.8940942883491516,\n",
       "   0.8966861367225647,\n",
       "   0.8724374175071716,\n",
       "   0.8817291259765625,\n",
       "   0.8537209033966064,\n",
       "   0.8797593712806702,\n",
       "   0.9161056280136108,\n",
       "   0.8809384703636169,\n",
       "   ...],\n",
       "  'val_acc': [-4.250182151794434,\n",
       "   -4.080313682556152,\n",
       "   -3.886972188949585,\n",
       "   -3.674290418624878,\n",
       "   -3.481989860534668,\n",
       "   -3.2635231018066406,\n",
       "   -3.0468063354492188,\n",
       "   -2.7730562686920166,\n",
       "   -2.5178024768829346,\n",
       "   -2.229861259460449,\n",
       "   -1.9643371105194092,\n",
       "   -1.7245814800262451,\n",
       "   -1.4815243482589722,\n",
       "   -1.2626925706863403,\n",
       "   -1.0422961711883545,\n",
       "   -0.86404949426651,\n",
       "   -0.6820858120918274,\n",
       "   -0.5455045700073242,\n",
       "   -0.40827780961990356,\n",
       "   -0.30113768577575684,\n",
       "   -0.1930587738752365,\n",
       "   -0.11721944808959961,\n",
       "   -0.05564093589782715,\n",
       "   0.01042723935097456,\n",
       "   0.04769005626440048,\n",
       "   0.0873866155743599,\n",
       "   0.12396716326475143,\n",
       "   0.1508682519197464,\n",
       "   0.1758115589618683,\n",
       "   0.20141644775867462,\n",
       "   0.22194881737232208,\n",
       "   0.24072685837745667,\n",
       "   0.25542593002319336,\n",
       "   0.27068427205085754,\n",
       "   0.28501203656196594,\n",
       "   0.297944575548172,\n",
       "   0.30956074595451355,\n",
       "   0.3210720419883728,\n",
       "   0.33038416504859924,\n",
       "   0.34017106890678406,\n",
       "   0.3508044481277466,\n",
       "   0.36122384667396545,\n",
       "   0.37126076221466064,\n",
       "   0.3807555139064789,\n",
       "   0.389644056558609,\n",
       "   0.39745914936065674,\n",
       "   0.4050785303115845,\n",
       "   0.4138214886188507,\n",
       "   0.421040803194046,\n",
       "   0.4287126660346985,\n",
       "   0.4363042116165161,\n",
       "   0.4446057975292206,\n",
       "   0.4518170654773712,\n",
       "   0.4588920474052429,\n",
       "   0.46552184224128723,\n",
       "   0.4726249575614929,\n",
       "   0.4790970981121063,\n",
       "   0.4856695234775543,\n",
       "   0.49259623885154724,\n",
       "   0.49835139513015747,\n",
       "   0.5041206479072571,\n",
       "   0.5092871189117432,\n",
       "   0.5150133967399597,\n",
       "   0.5203930139541626,\n",
       "   0.5246223211288452,\n",
       "   0.5295925736427307,\n",
       "   0.5356428623199463,\n",
       "   0.5413272976875305,\n",
       "   0.5460265278816223,\n",
       "   0.5513960123062134,\n",
       "   0.5549520254135132,\n",
       "   0.5602843165397644,\n",
       "   0.5642036199569702,\n",
       "   0.568062961101532,\n",
       "   0.5726096034049988,\n",
       "   0.577409029006958,\n",
       "   0.581291139125824,\n",
       "   0.5847732424736023,\n",
       "   0.588466227054596,\n",
       "   0.5911675095558167,\n",
       "   0.5952277779579163,\n",
       "   0.5979134440422058,\n",
       "   0.6011570692062378,\n",
       "   0.6031122803688049,\n",
       "   0.6068357825279236,\n",
       "   0.6101958751678467,\n",
       "   0.6137370467185974,\n",
       "   0.6160315275192261,\n",
       "   0.6186941266059875,\n",
       "   0.6212852001190186,\n",
       "   0.6231583952903748,\n",
       "   0.625607967376709,\n",
       "   0.627630889415741,\n",
       "   0.6297589540481567,\n",
       "   0.6316248178482056,\n",
       "   0.6343002915382385,\n",
       "   0.6361178755760193,\n",
       "   0.6378335952758789,\n",
       "   0.6407384872436523,\n",
       "   0.6425207853317261,\n",
       "   0.6443288922309875,\n",
       "   0.6464991569519043,\n",
       "   0.6478427052497864,\n",
       "   0.6497138142585754,\n",
       "   0.6518175601959229,\n",
       "   0.6537562012672424,\n",
       "   0.6553679704666138,\n",
       "   0.6574416160583496,\n",
       "   0.658844530582428,\n",
       "   0.6606200933456421,\n",
       "   0.6620662808418274,\n",
       "   0.663228452205658,\n",
       "   0.6652656197547913,\n",
       "   0.6668409705162048,\n",
       "   0.6677834987640381,\n",
       "   0.6693465709686279,\n",
       "   0.6704287528991699,\n",
       "   0.6717825531959534,\n",
       "   0.6724267601966858,\n",
       "   0.67362380027771,\n",
       "   0.6748067140579224,\n",
       "   0.6758199334144592,\n",
       "   0.6771474480628967,\n",
       "   0.6784521341323853,\n",
       "   0.6793121695518494,\n",
       "   0.6805430054664612,\n",
       "   0.6831859946250916,\n",
       "   0.6841034293174744,\n",
       "   0.6846315860748291,\n",
       "   0.6859824657440186,\n",
       "   0.6870925426483154,\n",
       "   0.6890197992324829,\n",
       "   0.6899634003639221,\n",
       "   0.6911014914512634,\n",
       "   0.6925493478775024,\n",
       "   0.6935217976570129,\n",
       "   0.6944363117218018,\n",
       "   0.6955869197845459,\n",
       "   0.6961637735366821,\n",
       "   0.6968193650245667,\n",
       "   0.6976911425590515,\n",
       "   0.699100911617279,\n",
       "   0.699996292591095,\n",
       "   0.7010424733161926,\n",
       "   0.702195942401886,\n",
       "   0.7030913829803467,\n",
       "   0.7043340802192688,\n",
       "   0.7050294280052185,\n",
       "   0.7057374715805054,\n",
       "   0.7067725658416748,\n",
       "   0.708409309387207,\n",
       "   0.7095131874084473,\n",
       "   0.7107310891151428,\n",
       "   0.7114118933677673,\n",
       "   0.7116748094558716,\n",
       "   0.7127960920333862,\n",
       "   0.7130981087684631,\n",
       "   0.7145901918411255,\n",
       "   0.715084969997406,\n",
       "   0.7165321707725525,\n",
       "   0.7176476120948792,\n",
       "   0.7187998294830322,\n",
       "   0.7196910977363586,\n",
       "   0.7206211686134338,\n",
       "   0.7220893502235413,\n",
       "   0.7229190468788147,\n",
       "   0.724258542060852,\n",
       "   0.7253725528717041,\n",
       "   0.7265130877494812,\n",
       "   0.7273386120796204,\n",
       "   0.7285299301147461,\n",
       "   0.7294684052467346,\n",
       "   0.7303966879844666,\n",
       "   0.7305746674537659,\n",
       "   0.7317964434623718,\n",
       "   0.7327919602394104,\n",
       "   0.7337779998779297,\n",
       "   0.7343273162841797,\n",
       "   0.7345353960990906,\n",
       "   0.735504686832428,\n",
       "   0.7363941073417664,\n",
       "   0.7375093102455139,\n",
       "   0.7378343343734741,\n",
       "   0.7370347380638123,\n",
       "   0.7393078207969666,\n",
       "   0.7404300570487976,\n",
       "   0.7414722442626953,\n",
       "   0.7417352199554443,\n",
       "   0.7412347793579102,\n",
       "   0.7423076629638672,\n",
       "   0.742917001247406,\n",
       "   0.7440857291221619,\n",
       "   0.7451513409614563,\n",
       "   0.7460098266601562,\n",
       "   0.746715784072876,\n",
       "   0.7474849224090576,\n",
       "   0.7473798990249634,\n",
       "   0.7486904263496399,\n",
       "   0.7495470643043518,\n",
       "   0.7498307824134827,\n",
       "   0.751211404800415,\n",
       "   0.7516046166419983,\n",
       "   0.7519727945327759,\n",
       "   0.7529458999633789,\n",
       "   0.7537336349487305,\n",
       "   0.7539517879486084,\n",
       "   0.7542473077774048,\n",
       "   0.7541024684906006,\n",
       "   0.7549852132797241,\n",
       "   0.7563943266868591,\n",
       "   0.7575206756591797,\n",
       "   0.7585801482200623,\n",
       "   0.7588878273963928,\n",
       "   0.7598691582679749,\n",
       "   0.7595852017402649,\n",
       "   0.7602613568305969,\n",
       "   0.7605159282684326,\n",
       "   0.762326180934906,\n",
       "   0.7628740072250366,\n",
       "   0.7629127502441406,\n",
       "   0.7634686827659607,\n",
       "   0.7646024823188782,\n",
       "   0.7653576135635376,\n",
       "   0.7662925720214844,\n",
       "   0.7670875787734985,\n",
       "   0.7668805718421936,\n",
       "   0.7681140303611755,\n",
       "   0.7686643600463867,\n",
       "   0.7695596814155579,\n",
       "   0.7695915102958679,\n",
       "   0.7706817388534546,\n",
       "   0.7708232402801514,\n",
       "   0.7716322541236877,\n",
       "   0.773024320602417,\n",
       "   0.7740404605865479,\n",
       "   0.7739961743354797,\n",
       "   0.7748932242393494,\n",
       "   0.7753466963768005,\n",
       "   0.7751551270484924,\n",
       "   0.775757372379303,\n",
       "   0.7764024138450623,\n",
       "   0.7771838903427124,\n",
       "   0.7777886986732483,\n",
       "   0.7784642577171326,\n",
       "   0.778892457485199,\n",
       "   0.7797502279281616,\n",
       "   0.7804933190345764,\n",
       "   0.7812321186065674,\n",
       "   0.7818136811256409,\n",
       "   0.7823458313941956,\n",
       "   0.7826173305511475,\n",
       "   0.7832568883895874,\n",
       "   0.7840572595596313,\n",
       "   0.7841623425483704,\n",
       "   0.784492552280426,\n",
       "   0.7854600548744202,\n",
       "   0.7860798239707947,\n",
       "   0.787295401096344,\n",
       "   0.787214994430542,\n",
       "   0.7886660099029541,\n",
       "   0.7890457510948181,\n",
       "   0.7891202569007874,\n",
       "   0.7892343997955322,\n",
       "   0.7896282076835632,\n",
       "   0.7900669574737549,\n",
       "   0.7905325293540955,\n",
       "   0.7908360958099365,\n",
       "   0.7916088700294495,\n",
       "   0.7914848923683167,\n",
       "   0.7922672033309937,\n",
       "   0.7933521270751953,\n",
       "   0.7936641573905945,\n",
       "   0.7931323647499084,\n",
       "   0.7947849631309509,\n",
       "   0.7946006655693054,\n",
       "   0.7949937582015991,\n",
       "   0.7955028414726257,\n",
       "   0.7962930202484131,\n",
       "   0.7966280579566956,\n",
       "   0.796714723110199,\n",
       "   0.7973031401634216,\n",
       "   0.7983521819114685,\n",
       "   0.799285888671875,\n",
       "   0.7997159957885742,\n",
       "   0.800510048866272,\n",
       "   0.8009099960327148,\n",
       "   0.801303505897522,\n",
       "   0.8016647100448608,\n",
       "   0.8015415072441101,\n",
       "   0.8030674457550049,\n",
       "   0.8031501173973083,\n",
       "   0.8037281036376953,\n",
       "   0.8045456409454346,\n",
       "   0.8050105571746826,\n",
       "   0.8049793243408203,\n",
       "   0.805415689945221,\n",
       "   0.8052992224693298,\n",
       "   0.8042904138565063,\n",
       "   0.8036689758300781,\n",
       "   0.805392861366272,\n",
       "   0.8054266571998596,\n",
       "   0.8057258725166321,\n",
       "   0.8060874938964844,\n",
       "   0.8070362210273743,\n",
       "   0.8068580031394958,\n",
       "   0.8074874877929688,\n",
       "   0.8071303963661194,\n",
       "   0.8092502355575562,\n",
       "   0.8087828159332275,\n",
       "   0.8103904128074646,\n",
       "   0.8106264472007751,\n",
       "   0.8111900687217712,\n",
       "   0.8116065859794617,\n",
       "   0.8123353719711304,\n",
       "   0.8132207989692688,\n",
       "   0.8136268258094788,\n",
       "   0.8142064809799194,\n",
       "   0.8150842785835266,\n",
       "   0.8148736953735352,\n",
       "   0.81581711769104,\n",
       "   0.8164252638816833,\n",
       "   0.8164325952529907,\n",
       "   0.8170852661132812,\n",
       "   0.8176001906394958,\n",
       "   0.8178859353065491,\n",
       "   0.8181211948394775,\n",
       "   0.8192571997642517,\n",
       "   0.8194273114204407,\n",
       "   0.8199164867401123,\n",
       "   0.8206687569618225,\n",
       "   0.8209004998207092,\n",
       "   0.8218435645103455,\n",
       "   0.8219093680381775,\n",
       "   0.8225940465927124,\n",
       "   0.8225290179252625,\n",
       "   0.8231340050697327,\n",
       "   0.8240132927894592,\n",
       "   0.8242935538291931,\n",
       "   0.8244343400001526,\n",
       "   0.8245469927787781,\n",
       "   0.8248496651649475,\n",
       "   0.8251619338989258,\n",
       "   0.8253028392791748,\n",
       "   0.825003445148468,\n",
       "   0.8254619836807251,\n",
       "   0.8259477019309998,\n",
       "   0.8263326287269592,\n",
       "   0.8261517882347107,\n",
       "   0.8266468644142151,\n",
       "   0.826439619064331,\n",
       "   0.8266512155532837,\n",
       "   0.826747715473175,\n",
       "   0.827204167842865,\n",
       "   0.8281193375587463,\n",
       "   0.8286903500556946,\n",
       "   0.829464852809906,\n",
       "   0.8292667865753174,\n",
       "   0.8300685882568359,\n",
       "   0.8306702971458435,\n",
       "   0.8311288356781006,\n",
       "   0.8304889798164368,\n",
       "   0.8319272398948669,\n",
       "   0.8321200013160706,\n",
       "   0.832114577293396,\n",
       "   0.8325191140174866,\n",
       "   0.8321483135223389,\n",
       "   0.8318625092506409,\n",
       "   0.832740068435669,\n",
       "   0.8327047228813171,\n",
       "   0.832319974899292,\n",
       "   0.8324391841888428,\n",
       "   0.8328519463539124,\n",
       "   0.8342033624649048,\n",
       "   0.8337644338607788,\n",
       "   0.8330726623535156,\n",
       "   0.8340648412704468,\n",
       "   0.8345503211021423,\n",
       "   0.8348071575164795,\n",
       "   0.8359369039535522,\n",
       "   0.8368621468544006,\n",
       "   0.8369897603988647,\n",
       "   0.8369334936141968,\n",
       "   0.8367776870727539,\n",
       "   0.8381744623184204,\n",
       "   0.8378803730010986,\n",
       "   0.8387428522109985,\n",
       "   0.8393678069114685,\n",
       "   0.8398447632789612,\n",
       "   0.8395206332206726,\n",
       "   0.8402974605560303,\n",
       "   0.8399219512939453,\n",
       "   0.8401334285736084,\n",
       "   0.8405613303184509,\n",
       "   0.8418813347816467,\n",
       "   0.8422724008560181,\n",
       "   0.8426700830459595,\n",
       "   0.8426143527030945,\n",
       "   0.8429948687553406,\n",
       "   0.8424990177154541,\n",
       "   0.8422634601593018,\n",
       "   0.8422229886054993,\n",
       "   0.841687798500061,\n",
       "   0.8434629440307617,\n",
       "   0.8440874218940735,\n",
       "   0.8435583114624023,\n",
       "   0.8445138931274414,\n",
       "   0.8448166251182556,\n",
       "   0.8452619314193726,\n",
       "   0.845329999923706,\n",
       "   0.84543776512146,\n",
       "   0.8454376459121704,\n",
       "   0.846028745174408,\n",
       "   0.8457899689674377,\n",
       "   0.846626341342926,\n",
       "   0.8463883996009827,\n",
       "   0.846522867679596,\n",
       "   0.8470920324325562,\n",
       "   0.8463022708892822,\n",
       "   0.8473770618438721,\n",
       "   0.8457494974136353,\n",
       "   0.8465014100074768,\n",
       "   0.8475202918052673,\n",
       "   0.8475425243377686,\n",
       "   0.8484010696411133,\n",
       "   0.8486908078193665,\n",
       "   0.8502227663993835,\n",
       "   0.8508214950561523,\n",
       "   0.8515441417694092,\n",
       "   0.8520864248275757,\n",
       "   0.852665364742279,\n",
       "   0.8528981804847717,\n",
       "   0.852473258972168,\n",
       "   0.8533201217651367,\n",
       "   0.853302001953125,\n",
       "   0.8544263243675232,\n",
       "   0.8535665273666382,\n",
       "   0.8530948758125305,\n",
       "   0.8524461984634399,\n",
       "   0.852665901184082,\n",
       "   0.8514804244041443,\n",
       "   0.8511175513267517,\n",
       "   0.8532319664955139,\n",
       "   0.8534253835678101,\n",
       "   0.8529701232910156,\n",
       "   0.8524671792984009,\n",
       "   0.8525871634483337,\n",
       "   0.8542697429656982,\n",
       "   0.8556230068206787,\n",
       "   0.855616569519043,\n",
       "   0.8557639718055725,\n",
       "   0.8574367761611938,\n",
       "   0.8578989505767822,\n",
       "   0.8586820960044861,\n",
       "   0.8592574596405029,\n",
       "   0.8590486645698547,\n",
       "   0.8583942651748657,\n",
       "   0.8588500618934631,\n",
       "   0.85971999168396,\n",
       "   0.8596385717391968,\n",
       "   0.8598167300224304,\n",
       "   0.859688937664032,\n",
       "   0.8602659106254578,\n",
       "   0.8606383800506592,\n",
       "   0.8602480292320251,\n",
       "   0.8605679869651794,\n",
       "   0.8611552715301514,\n",
       "   0.8615930676460266,\n",
       "   0.8622583150863647,\n",
       "   0.862851083278656,\n",
       "   0.8629083633422852,\n",
       "   0.8636629581451416,\n",
       "   0.8642314672470093,\n",
       "   0.8639402985572815,\n",
       "   0.8640525937080383,\n",
       "   0.8641297817230225,\n",
       "   0.8644434213638306,\n",
       "   0.8640633225440979,\n",
       "   0.8640130758285522,\n",
       "   0.8642593026161194,\n",
       "   0.8621758818626404,\n",
       "   0.863269031047821,\n",
       "   0.8637731671333313,\n",
       "   0.8637285828590393,\n",
       "   0.8644355535507202,\n",
       "   0.8658133149147034,\n",
       "   0.865168035030365,\n",
       "   0.8649652004241943,\n",
       "   0.866414487361908,\n",
       "   0.8671659827232361,\n",
       "   0.8679518103599548,\n",
       "   0.8683663010597229,\n",
       "   0.8686649203300476,\n",
       "   0.8689237833023071,\n",
       "   0.8687766790390015,\n",
       "   0.8685134649276733,\n",
       "   0.8696077466011047,\n",
       "   0.8685497045516968,\n",
       "   0.868910014629364,\n",
       "   0.8700214624404907,\n",
       "   0.8704932928085327,\n",
       "   0.8715205788612366,\n",
       "   0.8719974160194397,\n",
       "   0.8721722960472107,\n",
       "   0.8723129630088806,\n",
       "   0.8723968863487244,\n",
       "   0.8726035952568054,\n",
       "   0.8736496567726135,\n",
       "   0.87355637550354,\n",
       "   0.8742386102676392,\n",
       "   0.874812662601471,\n",
       "   0.8745569586753845,\n",
       "   0.8738648891448975,\n",
       "   0.8729120492935181,\n",
       "   0.8732326626777649,\n",
       "   0.8737061023712158,\n",
       "   0.8733730912208557,\n",
       "   0.8740740418434143,\n",
       "   0.8740736246109009,\n",
       "   0.8745402693748474,\n",
       "   0.8750756978988647,\n",
       "   0.8751658797264099,\n",
       "   0.875308632850647,\n",
       "   0.8753511905670166,\n",
       "   0.8760073184967041,\n",
       "   0.8760557770729065,\n",
       "   0.8762621283531189,\n",
       "   0.8765845894813538,\n",
       "   0.8758894205093384,\n",
       "   0.8767964839935303,\n",
       "   0.8763691186904907,\n",
       "   0.8762930035591125,\n",
       "   0.8760108947753906,\n",
       "   0.8760656118392944,\n",
       "   0.8750175833702087,\n",
       "   0.8756123185157776,\n",
       "   0.8761900067329407,\n",
       "   0.8768041729927063,\n",
       "   0.8770905137062073,\n",
       "   0.8767255544662476,\n",
       "   0.8763552308082581,\n",
       "   0.8759964108467102,\n",
       "   0.8768603801727295,\n",
       "   0.8775035738945007,\n",
       "   0.8785663843154907,\n",
       "   0.879080057144165,\n",
       "   0.8791627883911133,\n",
       "   0.8787866234779358,\n",
       "   0.8792012929916382,\n",
       "   0.8793916702270508,\n",
       "   0.8798551559448242,\n",
       "   0.879325807094574,\n",
       "   0.8788080811500549,\n",
       "   0.8783983588218689,\n",
       "   0.878116250038147,\n",
       "   0.8780748844146729,\n",
       "   0.8788841366767883,\n",
       "   0.880099892616272,\n",
       "   0.8797922730445862,\n",
       "   0.8806769847869873,\n",
       "   0.8810976147651672,\n",
       "   0.8814060688018799,\n",
       "   0.8820958733558655,\n",
       "   0.8822624683380127,\n",
       "   0.8824694156646729,\n",
       "   0.8820661306381226,\n",
       "   0.8823398351669312,\n",
       "   0.8823370337486267,\n",
       "   0.8830839395523071,\n",
       "   0.8827342987060547,\n",
       "   0.8828956484794617,\n",
       "   0.8828413486480713,\n",
       "   0.8827903270721436,\n",
       "   0.8834558725357056,\n",
       "   0.8829355239868164,\n",
       "   0.8824455142021179,\n",
       "   0.8821357488632202,\n",
       "   0.8824634552001953,\n",
       "   0.8824948668479919,\n",
       "   0.8829857110977173,\n",
       "   0.8829596042633057,\n",
       "   0.8825314044952393,\n",
       "   0.8837801218032837,\n",
       "   0.8852763772010803,\n",
       "   0.885525107383728,\n",
       "   0.8857395648956299,\n",
       "   0.8855466842651367,\n",
       "   0.8855542540550232,\n",
       "   0.8858858346939087,\n",
       "   0.8866979479789734,\n",
       "   0.8868213891983032,\n",
       "   0.8860122561454773,\n",
       "   0.8861371874809265,\n",
       "   0.8853540420532227,\n",
       "   0.8869078159332275,\n",
       "   0.8863670229911804,\n",
       "   0.8875137567520142,\n",
       "   0.8878747820854187,\n",
       "   0.887372612953186,\n",
       "   0.887913167476654,\n",
       "   0.8871856927871704,\n",
       "   0.88797926902771,\n",
       "   0.888938844203949,\n",
       "   0.8899176120758057,\n",
       "   0.8891582489013672,\n",
       "   0.8901948928833008,\n",
       "   0.889701247215271,\n",
       "   0.8894891142845154,\n",
       "   0.888638436794281,\n",
       "   0.8905680179595947,\n",
       "   0.8910194635391235,\n",
       "   0.8905782699584961,\n",
       "   0.8913379907608032,\n",
       "   0.890862226486206,\n",
       "   0.8902187943458557,\n",
       "   0.8907776474952698,\n",
       "   0.8910834789276123,\n",
       "   0.8912749886512756,\n",
       "   0.8909909725189209,\n",
       "   0.8913133144378662,\n",
       "   0.8917388916015625,\n",
       "   0.8917126059532166,\n",
       "   0.8911159038543701,\n",
       "   0.8905292749404907,\n",
       "   0.8902463316917419,\n",
       "   0.8899530172348022,\n",
       "   0.8887109756469727,\n",
       "   0.8916401267051697,\n",
       "   0.8913240432739258,\n",
       "   0.8918750286102295,\n",
       "   0.89096599817276,\n",
       "   0.8922534584999084,\n",
       "   0.8918443322181702,\n",
       "   0.892194390296936,\n",
       "   0.8926958441734314,\n",
       "   0.8927599191665649,\n",
       "   0.8926149606704712,\n",
       "   0.8939521312713623,\n",
       "   0.8940669894218445,\n",
       "   0.8939530849456787,\n",
       "   0.8932573795318604,\n",
       "   0.8937761187553406,\n",
       "   0.893686056137085,\n",
       "   0.8948890566825867,\n",
       "   0.8951011896133423,\n",
       "   0.8950891494750977,\n",
       "   0.8957391381263733,\n",
       "   0.8960050344467163,\n",
       "   0.895960807800293,\n",
       "   0.8955830931663513,\n",
       "   0.895711362361908,\n",
       "   0.8959750533103943,\n",
       "   0.8961912393569946,\n",
       "   0.8963012099266052,\n",
       "   0.8941906094551086,\n",
       "   0.8954796195030212,\n",
       "   0.8957164287567139,\n",
       "   0.895694375038147,\n",
       "   0.8956384658813477,\n",
       "   0.8955727219581604,\n",
       "   0.8953983187675476,\n",
       "   0.895987868309021,\n",
       "   0.8968298435211182,\n",
       "   0.897064745426178,\n",
       "   0.897494912147522,\n",
       "   0.8974979519844055,\n",
       "   0.8967152237892151,\n",
       "   0.8971115350723267,\n",
       "   0.897282063961029,\n",
       "   0.8972671627998352,\n",
       "   0.8966273665428162,\n",
       "   0.8956563472747803,\n",
       "   0.8979959487915039,\n",
       "   0.8978139758110046,\n",
       "   0.8977100253105164,\n",
       "   0.8982965350151062,\n",
       "   0.8983287215232849,\n",
       "   0.8983608484268188,\n",
       "   0.8983350992202759,\n",
       "   0.8979992270469666,\n",
       "   0.8974802494049072,\n",
       "   0.8976148366928101,\n",
       "   0.8980541229248047,\n",
       "   0.8975396752357483,\n",
       "   0.8977106213569641,\n",
       "   0.8986492156982422,\n",
       "   0.8980985879898071,\n",
       "   0.8984003067016602,\n",
       "   0.8981201648712158,\n",
       "   0.8982309103012085,\n",
       "   0.8986622095108032,\n",
       "   0.8993738293647766,\n",
       "   0.8997447490692139,\n",
       "   0.9002376198768616,\n",
       "   0.9002031087875366,\n",
       "   0.9003986716270447,\n",
       "   0.8998963832855225,\n",
       "   0.899431049823761,\n",
       "   0.8994768261909485,\n",
       "   0.8996455669403076,\n",
       "   0.899380624294281,\n",
       "   0.899441659450531,\n",
       "   0.899936854839325,\n",
       "   0.8994730114936829,\n",
       "   0.8988339900970459,\n",
       "   0.898786187171936,\n",
       "   0.8996406197547913,\n",
       "   0.8995500206947327,\n",
       "   0.8998369574546814,\n",
       "   0.8993701338768005,\n",
       "   0.8999794125556946,\n",
       "   0.9007758498191833,\n",
       "   0.9009700417518616,\n",
       "   0.9009612798690796,\n",
       "   0.901294469833374,\n",
       "   0.901487410068512,\n",
       "   0.9017366170883179,\n",
       "   0.9017967581748962,\n",
       "   0.9019464254379272,\n",
       "   0.9018639922142029,\n",
       "   0.9022725820541382,\n",
       "   0.9020991921424866,\n",
       "   0.9019418954849243,\n",
       "   0.9023544788360596,\n",
       "   0.9025953412055969,\n",
       "   0.9020547270774841,\n",
       "   0.9028360247612,\n",
       "   0.9023681282997131,\n",
       "   0.9023451209068298,\n",
       "   0.9033910632133484,\n",
       "   0.9031767249107361,\n",
       "   0.9038495421409607,\n",
       "   0.9033496975898743,\n",
       "   0.9044795632362366,\n",
       "   0.9046599864959717,\n",
       "   0.9043793082237244,\n",
       "   0.9048405289649963,\n",
       "   0.9048433899879456,\n",
       "   0.9042957425117493,\n",
       "   0.9050965905189514,\n",
       "   0.9049292206764221,\n",
       "   0.9045858383178711,\n",
       "   0.9032948017120361,\n",
       "   0.9044317007064819,\n",
       "   0.9054266214370728,\n",
       "   0.9048076272010803,\n",
       "   0.9052417874336243,\n",
       "   0.9056549668312073,\n",
       "   0.905266523361206,\n",
       "   0.9043266773223877,\n",
       "   0.9046370983123779,\n",
       "   0.9041715264320374,\n",
       "   0.9051336050033569,\n",
       "   0.9043497443199158,\n",
       "   0.9045020937919617,\n",
       "   0.9050891399383545,\n",
       "   0.9058234691619873,\n",
       "   0.9062029123306274,\n",
       "   0.9060103893280029,\n",
       "   0.9062039852142334,\n",
       "   0.9060625433921814,\n",
       "   0.9058342576026917,\n",
       "   0.9051708579063416,\n",
       "   0.9055967926979065,\n",
       "   0.9058908224105835,\n",
       "   0.905879020690918,\n",
       "   0.9060554504394531,\n",
       "   0.9062787890434265,\n",
       "   0.9062626361846924,\n",
       "   0.9064556360244751,\n",
       "   0.9062356948852539,\n",
       "   0.906196653842926,\n",
       "   0.9059345722198486,\n",
       "   0.9059286713600159,\n",
       "   0.906174898147583,\n",
       "   0.9063799381256104,\n",
       "   0.9065327048301697,\n",
       "   0.9063785076141357,\n",
       "   0.9063866138458252,\n",
       "   0.9061989188194275,\n",
       "   0.9065455794334412,\n",
       "   0.9055445194244385,\n",
       "   0.9061412215232849,\n",
       "   0.9067239761352539,\n",
       "   0.9064341187477112,\n",
       "   0.9068973660469055,\n",
       "   0.9074623584747314,\n",
       "   0.9082351922988892,\n",
       "   0.9082993865013123,\n",
       "   0.9080533981323242,\n",
       "   0.9080362915992737,\n",
       "   0.9079793691635132,\n",
       "   0.9086523056030273,\n",
       "   0.9088355898857117,\n",
       "   0.9082367420196533,\n",
       "   0.9065759778022766,\n",
       "   0.9075812697410583,\n",
       "   0.9084687829017639,\n",
       "   0.9074936509132385,\n",
       "   0.9081210494041443,\n",
       "   0.9085260629653931,\n",
       "   0.9079613089561462,\n",
       "   0.9073984026908875,\n",
       "   0.9079639911651611,\n",
       "   0.9079399108886719,\n",
       "   0.9089103937149048,\n",
       "   0.9075004458427429,\n",
       "   0.9085490703582764,\n",
       "   0.9090347290039062,\n",
       "   0.9087492227554321,\n",
       "   0.9083808660507202,\n",
       "   0.9088796973228455,\n",
       "   0.9087873101234436,\n",
       "   0.9091184139251709,\n",
       "   0.906818151473999,\n",
       "   0.9085870385169983,\n",
       "   0.9093673229217529,\n",
       "   0.9094380736351013,\n",
       "   0.9085584282875061,\n",
       "   0.9086340069770813,\n",
       "   0.9093990921974182,\n",
       "   0.9098837375640869,\n",
       "   0.9099080562591553,\n",
       "   0.9097487926483154,\n",
       "   0.9098869562149048,\n",
       "   0.9087173938751221,\n",
       "   0.9082634449005127,\n",
       "   0.9081092476844788,\n",
       "   0.9094991683959961,\n",
       "   0.9100432991981506,\n",
       "   0.9102296233177185,\n",
       "   0.9106199145317078,\n",
       "   0.9102334976196289,\n",
       "   0.9107750058174133,\n",
       "   0.9107614755630493,\n",
       "   0.9108688831329346,\n",
       "   0.9111217260360718,\n",
       "   0.9116085171699524,\n",
       "   0.9116849899291992,\n",
       "   0.9117397665977478,\n",
       "   0.911207377910614,\n",
       "   0.9116235375404358,\n",
       "   0.9113152027130127,\n",
       "   0.9111831784248352,\n",
       "   0.9110661149024963,\n",
       "   0.9105302691459656,\n",
       "   0.9101325869560242,\n",
       "   0.910723865032196,\n",
       "   0.9109676480293274,\n",
       "   0.9107993841171265,\n",
       "   0.9113444685935974,\n",
       "   0.9108771681785583,\n",
       "   0.9108903408050537,\n",
       "   0.9117116332054138,\n",
       "   0.9120389819145203,\n",
       "   0.9122154712677002,\n",
       "   0.9120872020721436,\n",
       "   0.9123712182044983,\n",
       "   0.9123647212982178,\n",
       "   0.9122480750083923,\n",
       "   0.9119266867637634,\n",
       "   0.9121031165122986,\n",
       "   0.9119386672973633,\n",
       "   0.9118309617042542,\n",
       "   0.9123392701148987,\n",
       "   0.9130275845527649,\n",
       "   0.9124433994293213,\n",
       "   0.9124599695205688,\n",
       "   0.9121948480606079,\n",
       "   0.9130056500434875,\n",
       "   0.9130123257637024,\n",
       "   0.9130837917327881,\n",
       "   0.9133779406547546,\n",
       "   0.9129322171211243,\n",
       "   0.9135259985923767,\n",
       "   0.9136514663696289,\n",
       "   0.9120700359344482,\n",
       "   0.9131785035133362,\n",
       "   0.9135599136352539,\n",
       "   0.9130761623382568,\n",
       "   0.9137134552001953,\n",
       "   0.9139312505722046,\n",
       "   0.9137634038925171,\n",
       "   0.914198637008667,\n",
       "   0.9140613675117493,\n",
       "   0.914546549320221,\n",
       "   0.9146962761878967,\n",
       "   0.9145917892456055,\n",
       "   0.9148464202880859,\n",
       "   0.9146440625190735,\n",
       "   0.9131838083267212,\n",
       "   0.9129922389984131,\n",
       "   0.9140405654907227,\n",
       "   0.9148942828178406,\n",
       "   0.9150779843330383,\n",
       "   0.9151614904403687,\n",
       "   0.9152280688285828,\n",
       "   0.9149417877197266,\n",
       "   0.9151624441146851,\n",
       "   0.9156602621078491,\n",
       "   0.9157636165618896,\n",
       "   0.91522616147995,\n",
       "   0.9157313704490662,\n",
       "   0.9153060913085938,\n",
       "   0.9153156280517578,\n",
       "   0.915314257144928,\n",
       "   0.9146787524223328,\n",
       "   0.9155121445655823,\n",
       "   0.9142005443572998,\n",
       "   0.9138264060020447,\n",
       "   0.9155906438827515,\n",
       "   0.9157165288925171,\n",
       "   0.9159202575683594,\n",
       "   0.9156782627105713,\n",
       "   0.9160568118095398,\n",
       "   0.9158823490142822,\n",
       "   0.9161626696586609,\n",
       "   0.915402352809906,\n",
       "   0.9150013327598572,\n",
       "   0.9159993529319763,\n",
       "   0.9164627194404602,\n",
       "   0.916141152381897,\n",
       "   0.915722668170929,\n",
       "   0.9162368774414062,\n",
       "   0.9153260588645935,\n",
       "   0.9156169295310974,\n",
       "   0.9160969257354736,\n",
       "   0.9156827330589294,\n",
       "   0.9151447415351868,\n",
       "   0.9148523807525635,\n",
       "   0.9151651263237,\n",
       "   0.9149379134178162,\n",
       "   0.9141851663589478,\n",
       "   0.915165364742279,\n",
       "   0.9150920510292053,\n",
       "   0.9144745469093323,\n",
       "   0.9147605299949646,\n",
       "   0.9161179065704346,\n",
       "   0.9161654114723206,\n",
       "   0.9166937470436096,\n",
       "   0.9169763922691345,\n",
       "   0.9168623089790344,\n",
       "   0.9166750311851501,\n",
       "   0.9157435894012451,\n",
       "   0.9170124530792236,\n",
       "   0.917179524898529,\n",
       "   0.9172926545143127,\n",
       "   0.9172431826591492,\n",
       "   0.9175881743431091,\n",
       "   0.9175268411636353,\n",
       "   0.9176210761070251,\n",
       "   0.9178562760353088,\n",
       "   0.9178484082221985,\n",
       "   0.917424738407135,\n",
       "   0.9173625707626343,\n",
       "   0.9177168011665344,\n",
       "   0.9170781970024109,\n",
       "   0.917441189289093,\n",
       "   0.9175258874893188,\n",
       "   0.9178227782249451,\n",
       "   0.9176061749458313,\n",
       "   0.9168190360069275,\n",
       "   0.9166243076324463,\n",
       "   0.9170728325843811,\n",
       "   0.9168934226036072,\n",
       "   0.9166259765625,\n",
       "   0.9174333214759827,\n",
       "   0.9181534051895142,\n",
       "   0.918247640132904,\n",
       "   0.9179267883300781,\n",
       "   0.9172676205635071,\n",
       "   0.9175418615341187,\n",
       "   0.9175989031791687,\n",
       "   0.9183498620986938,\n",
       "   0.9187948107719421,\n",
       "   0.9185222387313843,\n",
       "   0.9184905886650085,\n",
       "   0.9181582927703857,\n",
       "   0.9183962345123291,\n",
       "   0.9182553887367249,\n",
       "   0.9167817831039429,\n",
       "   0.9159248471260071,\n",
       "   0.9178114533424377,\n",
       "   0.9177727103233337,\n",
       "   0.9189993143081665,\n",
       "   0.9191635251045227,\n",
       "   0.9190530180931091,\n",
       "   0.9187775254249573,\n",
       "   0.9180002808570862,\n",
       "   0.9173177480697632,\n",
       "   0.9172820448875427,\n",
       "   0.9163618087768555,\n",
       "   0.9171216487884521,\n",
       "   0.9165794849395752,\n",
       "   0.9182172417640686,\n",
       "   0.9188215136528015,\n",
       "   0.9187315106391907,\n",
       "   0.9188192486763,\n",
       "   0.919223964214325,\n",
       "   0.9193554520606995,\n",
       "   0.9193108081817627,\n",
       "   ...],\n",
       "  'epochs': [1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   208,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   212,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   224,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   252,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   272,\n",
       "   273,\n",
       "   274,\n",
       "   275,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   280,\n",
       "   281,\n",
       "   282,\n",
       "   283,\n",
       "   284,\n",
       "   285,\n",
       "   286,\n",
       "   287,\n",
       "   288,\n",
       "   289,\n",
       "   290,\n",
       "   291,\n",
       "   292,\n",
       "   293,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   297,\n",
       "   298,\n",
       "   299,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   303,\n",
       "   304,\n",
       "   305,\n",
       "   306,\n",
       "   307,\n",
       "   308,\n",
       "   309,\n",
       "   310,\n",
       "   311,\n",
       "   312,\n",
       "   313,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   331,\n",
       "   332,\n",
       "   333,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   337,\n",
       "   338,\n",
       "   339,\n",
       "   340,\n",
       "   341,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   345,\n",
       "   346,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   350,\n",
       "   351,\n",
       "   352,\n",
       "   353,\n",
       "   354,\n",
       "   355,\n",
       "   356,\n",
       "   357,\n",
       "   358,\n",
       "   359,\n",
       "   360,\n",
       "   361,\n",
       "   362,\n",
       "   363,\n",
       "   364,\n",
       "   365,\n",
       "   366,\n",
       "   367,\n",
       "   368,\n",
       "   369,\n",
       "   370,\n",
       "   371,\n",
       "   372,\n",
       "   373,\n",
       "   374,\n",
       "   375,\n",
       "   376,\n",
       "   377,\n",
       "   378,\n",
       "   379,\n",
       "   380,\n",
       "   381,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   387,\n",
       "   388,\n",
       "   389,\n",
       "   390,\n",
       "   391,\n",
       "   392,\n",
       "   393,\n",
       "   394,\n",
       "   395,\n",
       "   396,\n",
       "   397,\n",
       "   398,\n",
       "   399,\n",
       "   400,\n",
       "   401,\n",
       "   402,\n",
       "   403,\n",
       "   404,\n",
       "   405,\n",
       "   406,\n",
       "   407,\n",
       "   408,\n",
       "   409,\n",
       "   410,\n",
       "   411,\n",
       "   412,\n",
       "   413,\n",
       "   414,\n",
       "   415,\n",
       "   416,\n",
       "   417,\n",
       "   418,\n",
       "   419,\n",
       "   420,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   424,\n",
       "   425,\n",
       "   426,\n",
       "   427,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   431,\n",
       "   432,\n",
       "   433,\n",
       "   434,\n",
       "   435,\n",
       "   436,\n",
       "   437,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   441,\n",
       "   442,\n",
       "   443,\n",
       "   444,\n",
       "   445,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   449,\n",
       "   450,\n",
       "   451,\n",
       "   452,\n",
       "   453,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   457,\n",
       "   458,\n",
       "   459,\n",
       "   460,\n",
       "   461,\n",
       "   462,\n",
       "   463,\n",
       "   464,\n",
       "   465,\n",
       "   466,\n",
       "   467,\n",
       "   468,\n",
       "   469,\n",
       "   470,\n",
       "   471,\n",
       "   472,\n",
       "   473,\n",
       "   474,\n",
       "   475,\n",
       "   476,\n",
       "   477,\n",
       "   478,\n",
       "   479,\n",
       "   480,\n",
       "   481,\n",
       "   482,\n",
       "   483,\n",
       "   484,\n",
       "   485,\n",
       "   486,\n",
       "   487,\n",
       "   488,\n",
       "   489,\n",
       "   490,\n",
       "   491,\n",
       "   492,\n",
       "   493,\n",
       "   494,\n",
       "   495,\n",
       "   496,\n",
       "   497,\n",
       "   498,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   502,\n",
       "   503,\n",
       "   504,\n",
       "   505,\n",
       "   506,\n",
       "   507,\n",
       "   508,\n",
       "   509,\n",
       "   510,\n",
       "   511,\n",
       "   512,\n",
       "   513,\n",
       "   514,\n",
       "   515,\n",
       "   516,\n",
       "   517,\n",
       "   518,\n",
       "   519,\n",
       "   520,\n",
       "   521,\n",
       "   522,\n",
       "   523,\n",
       "   524,\n",
       "   525,\n",
       "   526,\n",
       "   527,\n",
       "   528,\n",
       "   529,\n",
       "   530,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   535,\n",
       "   536,\n",
       "   537,\n",
       "   538,\n",
       "   539,\n",
       "   540,\n",
       "   541,\n",
       "   542,\n",
       "   543,\n",
       "   544,\n",
       "   545,\n",
       "   546,\n",
       "   547,\n",
       "   548,\n",
       "   549,\n",
       "   550,\n",
       "   551,\n",
       "   552,\n",
       "   553,\n",
       "   554,\n",
       "   555,\n",
       "   556,\n",
       "   557,\n",
       "   558,\n",
       "   559,\n",
       "   560,\n",
       "   561,\n",
       "   562,\n",
       "   563,\n",
       "   564,\n",
       "   565,\n",
       "   566,\n",
       "   567,\n",
       "   568,\n",
       "   569,\n",
       "   570,\n",
       "   571,\n",
       "   572,\n",
       "   573,\n",
       "   574,\n",
       "   575,\n",
       "   576,\n",
       "   577,\n",
       "   578,\n",
       "   579,\n",
       "   580,\n",
       "   581,\n",
       "   582,\n",
       "   583,\n",
       "   584,\n",
       "   585,\n",
       "   586,\n",
       "   587,\n",
       "   588,\n",
       "   589,\n",
       "   590,\n",
       "   591,\n",
       "   592,\n",
       "   593,\n",
       "   594,\n",
       "   595,\n",
       "   596,\n",
       "   597,\n",
       "   598,\n",
       "   599,\n",
       "   600,\n",
       "   601,\n",
       "   602,\n",
       "   603,\n",
       "   604,\n",
       "   605,\n",
       "   606,\n",
       "   607,\n",
       "   608,\n",
       "   609,\n",
       "   610,\n",
       "   611,\n",
       "   612,\n",
       "   613,\n",
       "   614,\n",
       "   615,\n",
       "   616,\n",
       "   617,\n",
       "   618,\n",
       "   619,\n",
       "   620,\n",
       "   621,\n",
       "   622,\n",
       "   623,\n",
       "   624,\n",
       "   625,\n",
       "   626,\n",
       "   627,\n",
       "   628,\n",
       "   629,\n",
       "   630,\n",
       "   631,\n",
       "   632,\n",
       "   633,\n",
       "   634,\n",
       "   635,\n",
       "   636,\n",
       "   637,\n",
       "   638,\n",
       "   639,\n",
       "   640,\n",
       "   641,\n",
       "   642,\n",
       "   643,\n",
       "   644,\n",
       "   645,\n",
       "   646,\n",
       "   647,\n",
       "   648,\n",
       "   649,\n",
       "   650,\n",
       "   651,\n",
       "   652,\n",
       "   653,\n",
       "   654,\n",
       "   655,\n",
       "   656,\n",
       "   657,\n",
       "   658,\n",
       "   659,\n",
       "   660,\n",
       "   661,\n",
       "   662,\n",
       "   663,\n",
       "   664,\n",
       "   665,\n",
       "   666,\n",
       "   667,\n",
       "   668,\n",
       "   669,\n",
       "   670,\n",
       "   671,\n",
       "   672,\n",
       "   673,\n",
       "   674,\n",
       "   675,\n",
       "   676,\n",
       "   677,\n",
       "   678,\n",
       "   679,\n",
       "   680,\n",
       "   681,\n",
       "   682,\n",
       "   683,\n",
       "   684,\n",
       "   685,\n",
       "   686,\n",
       "   687,\n",
       "   688,\n",
       "   689,\n",
       "   690,\n",
       "   691,\n",
       "   692,\n",
       "   693,\n",
       "   694,\n",
       "   695,\n",
       "   696,\n",
       "   697,\n",
       "   698,\n",
       "   699,\n",
       "   700,\n",
       "   701,\n",
       "   702,\n",
       "   703,\n",
       "   704,\n",
       "   705,\n",
       "   706,\n",
       "   707,\n",
       "   708,\n",
       "   709,\n",
       "   710,\n",
       "   711,\n",
       "   712,\n",
       "   713,\n",
       "   714,\n",
       "   715,\n",
       "   716,\n",
       "   717,\n",
       "   718,\n",
       "   719,\n",
       "   720,\n",
       "   721,\n",
       "   722,\n",
       "   723,\n",
       "   724,\n",
       "   725,\n",
       "   726,\n",
       "   727,\n",
       "   728,\n",
       "   729,\n",
       "   730,\n",
       "   731,\n",
       "   732,\n",
       "   733,\n",
       "   734,\n",
       "   735,\n",
       "   736,\n",
       "   737,\n",
       "   738,\n",
       "   739,\n",
       "   740,\n",
       "   741,\n",
       "   742,\n",
       "   743,\n",
       "   744,\n",
       "   745,\n",
       "   746,\n",
       "   747,\n",
       "   748,\n",
       "   749,\n",
       "   750,\n",
       "   751,\n",
       "   752,\n",
       "   753,\n",
       "   754,\n",
       "   755,\n",
       "   756,\n",
       "   757,\n",
       "   758,\n",
       "   759,\n",
       "   760,\n",
       "   761,\n",
       "   762,\n",
       "   763,\n",
       "   764,\n",
       "   765,\n",
       "   766,\n",
       "   767,\n",
       "   768,\n",
       "   769,\n",
       "   770,\n",
       "   771,\n",
       "   772,\n",
       "   773,\n",
       "   774,\n",
       "   775,\n",
       "   776,\n",
       "   777,\n",
       "   778,\n",
       "   779,\n",
       "   780,\n",
       "   781,\n",
       "   782,\n",
       "   783,\n",
       "   784,\n",
       "   785,\n",
       "   786,\n",
       "   787,\n",
       "   788,\n",
       "   789,\n",
       "   790,\n",
       "   791,\n",
       "   792,\n",
       "   793,\n",
       "   794,\n",
       "   795,\n",
       "   796,\n",
       "   797,\n",
       "   798,\n",
       "   799,\n",
       "   800,\n",
       "   801,\n",
       "   802,\n",
       "   803,\n",
       "   804,\n",
       "   805,\n",
       "   806,\n",
       "   807,\n",
       "   808,\n",
       "   809,\n",
       "   810,\n",
       "   811,\n",
       "   812,\n",
       "   813,\n",
       "   814,\n",
       "   815,\n",
       "   816,\n",
       "   817,\n",
       "   818,\n",
       "   819,\n",
       "   820,\n",
       "   821,\n",
       "   822,\n",
       "   823,\n",
       "   824,\n",
       "   825,\n",
       "   826,\n",
       "   827,\n",
       "   828,\n",
       "   829,\n",
       "   830,\n",
       "   831,\n",
       "   832,\n",
       "   833,\n",
       "   834,\n",
       "   835,\n",
       "   836,\n",
       "   837,\n",
       "   838,\n",
       "   839,\n",
       "   840,\n",
       "   841,\n",
       "   842,\n",
       "   843,\n",
       "   844,\n",
       "   845,\n",
       "   846,\n",
       "   847,\n",
       "   848,\n",
       "   849,\n",
       "   850,\n",
       "   851,\n",
       "   852,\n",
       "   853,\n",
       "   854,\n",
       "   855,\n",
       "   856,\n",
       "   857,\n",
       "   858,\n",
       "   859,\n",
       "   860,\n",
       "   861,\n",
       "   862,\n",
       "   863,\n",
       "   864,\n",
       "   865,\n",
       "   866,\n",
       "   867,\n",
       "   868,\n",
       "   869,\n",
       "   870,\n",
       "   871,\n",
       "   872,\n",
       "   873,\n",
       "   874,\n",
       "   875,\n",
       "   876,\n",
       "   877,\n",
       "   878,\n",
       "   879,\n",
       "   880,\n",
       "   881,\n",
       "   882,\n",
       "   883,\n",
       "   884,\n",
       "   885,\n",
       "   886,\n",
       "   887,\n",
       "   888,\n",
       "   889,\n",
       "   890,\n",
       "   891,\n",
       "   892,\n",
       "   893,\n",
       "   894,\n",
       "   895,\n",
       "   896,\n",
       "   897,\n",
       "   898,\n",
       "   899,\n",
       "   900,\n",
       "   901,\n",
       "   902,\n",
       "   903,\n",
       "   904,\n",
       "   905,\n",
       "   906,\n",
       "   907,\n",
       "   908,\n",
       "   909,\n",
       "   910,\n",
       "   911,\n",
       "   912,\n",
       "   913,\n",
       "   914,\n",
       "   915,\n",
       "   916,\n",
       "   917,\n",
       "   918,\n",
       "   919,\n",
       "   920,\n",
       "   921,\n",
       "   922,\n",
       "   923,\n",
       "   924,\n",
       "   925,\n",
       "   926,\n",
       "   927,\n",
       "   928,\n",
       "   929,\n",
       "   930,\n",
       "   931,\n",
       "   932,\n",
       "   933,\n",
       "   934,\n",
       "   935,\n",
       "   936,\n",
       "   937,\n",
       "   938,\n",
       "   939,\n",
       "   940,\n",
       "   941,\n",
       "   942,\n",
       "   943,\n",
       "   944,\n",
       "   945,\n",
       "   946,\n",
       "   947,\n",
       "   948,\n",
       "   949,\n",
       "   950,\n",
       "   951,\n",
       "   952,\n",
       "   953,\n",
       "   954,\n",
       "   955,\n",
       "   956,\n",
       "   957,\n",
       "   958,\n",
       "   959,\n",
       "   960,\n",
       "   961,\n",
       "   962,\n",
       "   963,\n",
       "   964,\n",
       "   965,\n",
       "   966,\n",
       "   967,\n",
       "   968,\n",
       "   969,\n",
       "   970,\n",
       "   971,\n",
       "   972,\n",
       "   973,\n",
       "   974,\n",
       "   975,\n",
       "   976,\n",
       "   977,\n",
       "   978,\n",
       "   979,\n",
       "   980,\n",
       "   981,\n",
       "   982,\n",
       "   983,\n",
       "   984,\n",
       "   985,\n",
       "   986,\n",
       "   987,\n",
       "   988,\n",
       "   989,\n",
       "   990,\n",
       "   991,\n",
       "   992,\n",
       "   993,\n",
       "   994,\n",
       "   995,\n",
       "   996,\n",
       "   997,\n",
       "   998,\n",
       "   999,\n",
       "   1000,\n",
       "   ...]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = histories_to_metrics(histories,kfold=False)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c55a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_to_excel(metrics, f\"Result/history/{reservoirname}_{experiment_name}_history.xlsx\", kfold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ef1a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABcwAAAJICAYAAABcy6dXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzddXgUVxcH4N+sxz0kBJcE4kEiBA3ukEJxKUWKlKIFStG2UAotLkVaWkpLocCHFShOcSnu7kkgQjwrc78/lt3sZiWbEIXzPg9tMjM7c/eu5M6ZM+dyjDEGQgghhBBCCCGEEEIIIeQ9JyjuBhBCCCGEEEIIIYQQQgghJQEFzAkhhBBCCCGEEEIIIYQQUMCcEEIIIYQQQgghhBBCCAFAAXNCCCGEEEIIIYQQQgghBAAFzAkhhBBCCCGEEEIIIYQQABQwJ4QQQgghhBBCCCGEEEIAUMCcEEIIIYQQQgghhBBCCAFAAXNCCCGEEEIIIYQQQgghBAAFzAkhhBBCCCGEEEIIIYQQABQwJ6RE+Pzzz+Hj44OVK1cWd1OK1OnTp+Hj44PTp08Xd1PMunXrFj7//HM0bNgQ/v7+aNy4McaMGYNLly4Vd9PQp08f+Pj4mPwXHR1d5G16+vQpfHx8sGXLlkI7xldffYX58+cbLD958iR8fHzQpk0bo4/bsmULfHx88PTpUwDAxIkTERUVlevxtmzZgu7du6NWrVoICgpC27ZtMX/+fKSmpr7dE8mDoujXt5Wzf42ZOHGi2ffstm3bLD7e4sWL4ePjY3abvH7PyOVytGzZEhcvXrS4HYQQQorf+zqe1khLS0NISAh8fX0RGxtb3M0pcXiex//+9z/06dMHderUQXBwMNq1a4fFixcjMTGxWNumGeOZ+/fbb78VebssGWe9jZSUFDRt2hT37t0zWJfb57lPnz7o06eP9ncfHx8sXrzY7PESExMxe/ZsNGvWDP7+/ggNDUW/fv2wd+/et3sieVTY/VoQcvavMeber76+vnk6XlRUFCZOnGh2G0vP2zSOHz+OTp06QaFQ5KkthGiIirsBhLzvUlNT8c8//8Db2xsbN27EoEGDwHFccTeLvLFt2zZMnjwZNWvWxKhRo1CuXDnExMTgr7/+Qo8ePTB+/Hh89NFHxdpGX19fTJs2zeg6a2vrIm5N4Tt16hT++ecfo4PbzZs3w9vbG7dv38aZM2cQGhr61sdbsmQJVqxYgf79+2Po0KEQi8W4evUqVq9ejWPHjmHDhg0Qi8VvfZz3iZubG5YsWWJ0XYUKFYq4NfokEgnGjh2LiRMnYtu2bZBKpcXaHkIIIbmj8TTw999/QyaTwcbGBps2bcKIESOKu0klhkKhwGeffYYjR44gOjoaAwYMgEwmw5UrV/Drr79iy5YtWLFiRbEHMYcOHYrGjRsbXVeuXLmibUwR+Oabb9CkSRNUrVpVb3lhfJ4zMzPRq1cvKJVKDBo0CJUqVUJKSgp2796NkSNHYtKkSejfv/9bHeN91KVLF3Tt2tVgeUn4/o2MjMS6deuwfPlyjBw5sribQ0ohCpgTUsx27doFlUqFL7/8En379sWxY8fQoEGD4m4WAXD9+nVMnjwZ7du3x9dffw2hUKhd16FDB3zzzTeYM2cOfHx8UK9evWJrp62tLYKDg4vt+EVt9uzZ6Nu3r8HFgJSUFOzbtw9ffvkl1q5diw0bNrx1wFwul2PVqlUYMGAAxowZo11er149VKlSBcOHD8f+/fvRunXrtzrO+0YikZTo92yLFi2wcOFC/PHHH3TyRAghpQCNp9VJA/Xr14etrS02bdqEoUOH6o1d32c//PADjhw5ghUrVui9LyIiItC5c2f07t0bI0eOxLZt2yCTyYqtnRUqVCjR46OCdO3aNWzfvh2HDh0yWFcYn+c9e/bg3r172LNnDypXrqxd3qxZM2RmZmLx4sXo06cPfWbyyMPDo0S/Z4cNG4ZevXqhe/fucHd3L+7mkFKGSrIQUsw2b96MsLAwhIWFoXLlytiwYYN23YABA9CpUyeDx4waNQpt27bV/n7u3Dn07t0bQUFBCA0NxYQJE5CQkKBdv2XLFvj6+mLTpk2oX78+GjZsiDt37kClUmHlypVo164dAgMDERwcjO7du+PkyZN6xzt8+DCio6MRGBiIli1bYufOnWjevLnebW9JSUmYOnUq6tWrh4CAAHz44YcG+8mvlJQU7e1zAQEBaNeuHf766y+9ba5du4Z+/fqhdu3aCAkJQf/+/fVKpiQkJGDcuHGIjIxEQEAAOnbsiP/9739mj7tixQpYW1tjypQpRgdP48ePh6enJ5YuXQoAmDJlCsLDw6FUKvW2mzt3LkJDQyGXywEAt2/fxpAhQ1CrVi3UqlULw4cPx5MnT7Tba0pIbNiwAU2aNEG9evVw7NixPPWZMVFRUZg/fz5mz56N0NBQhIaGYvz48Qa3oR4/fhw9e/ZE7dq1ERYWhrFjx+LFixd62zx+/BgjR45EaGgo6tati0GDBuHOnTt627x8+RIjR45ESEgIQkNDMWXKFKSnp2vX5/aaGXP48GHcunUL7dq1M1i3c+dOyOVyNGzYEB06dMA///yj9znIj9TUVGRmZoIxZrCuUaNGGD16NMqXL69dlpWVhe+++w6NGjWCv78/2rdvj7///tvgsZs2bULbtm21JX4WL15s8L75559/0KFDBwQGBqJz5864efOmwX7WrVuHVq1aISAgAA0aNMD06dNzLRNz9uxZfPzxx6hbty78/f0RFRWFxYsXg+d5ANm3BWsybkJCQlC3bl1MnjwZaWlp2v3wPI9ly5ahcePGCAoKwrBhw/D69WvzHZpHf//9N6KjoxESEoLIyEhMnTo112Ns2LABLVu2RGBgIHr37o3nz5/rred5HgsXLkRUVJT2+f/www8Gt2u2b98eP/30k/ZzSwghpOR638fT9+/fx4ULF9CkSRN06NABMTExRgORaWlpmD17Nho2bIjg4GBER0fj4MGD2vWMMaxfvx5t27ZFYGAgmjdvjlWrVmnHQcZKIuQsGWduHLtp0yZER0cjODgYgYGB6Nixo8E4ydwY84MPPkD37t0NntfHH39ssnxEYmIi1q9fj+joaKNBVzc3N0yePBkPHz7Ezp07kZWVhTp16mDWrFl62/E8j/r162PGjBnaZbmN5yZOnIh+/fph2rRpqFOnDjp37mww3ssrTf8eO3YMvXr10r5OOUu2ZGVlYenSpdpxYosWLbBy5UrteE9j165diI6ORlBQEBo3boy5c+cajH0OHz6MDh06ICAgAC1btjQ4h8rPePTHH39EWFgYypQpY7DO3Oc5v169egUARsf0Q4YMwbBhw/Sed27na4Bln9esrCzMnj0bkZGRCAkJwaRJk5CVlaW3TX7OUy353lm8eDGaN2+Ow4cPo3379vD390fLli2xdetWvX09f/4cI0aMQO3atREZGYmff/7Z7LHzypJz+Zxev36NSZMmISwsDHXr1sXcuXMN3rtPnjzB0KFDERYWhqCgIHTr1g1HjhzR2yYwMBBly5bF2rVrC/Q5kfcDBcwJKUb37t3DpUuX0LlzZwBAdHQ0Dh06pK072LFjR9y4cQP379/XPiYtLQ2HDh1Cx44dAaiDX/3794dMJsOCBQvwxRdf4MyZM+jbty8yMzO1j1OpVFixYgW+/vprjBo1CtWqVcO8efOwdOlSdOvWDatXr8bMmTORmJiIzz77TBvYPHXqFIYNGwZPT08sXrwYvXr1wrRp0/QCqFlZWejXrx8OHDiA0aNHY8mSJfDw8MDAgQPfOmiemZmJnj17Yvv27RgwYACWLVuG2rVrY/LkyVixYgUAdVBz4MCBcHJywqJFizB//nxkZGTg448/RkpKCgB1cPvu3buYMWMGVq5cCV9fX0yYMMFkXWOe53H8+HGEh4ebLGsikUjQrFkznD9/HomJiejYsSMSExP1njNjDH///TdatWoFiUSCBw8eoHv37oiPj8e3336Lb775Bk+ePEGPHj0QHx+vt//58+djwoQJmDBhgtkr94wxKJVKo/9yDgp///13nD9/HrNmzcK4ceNw9OhRDBw4UDsA2bZtGwYMGIAyZcrghx9+wKRJk3DhwgV069ZN2764uDh07doV9+/fx7Rp0zBv3jy8fv0a/fv31zuxXLhwITw9PbFs2TL07dsXGzdu1J4UWvKaGbN9+3YEBwfD09PTYN3mzZtRr149lClTBp06dQLP87kOxnLj7OyMoKAgrFmzBhMmTMD+/fu1z1EsFuOTTz6Bv78/APXrMHz4cGzYsAEfffQRli9fjpCQEIwePVpv0Pvjjz9iypQpiIiIwIoVK9CrVy+sWrUKU6dO1W5z8OBBjBw5EtWrV8eSJUvQunVrjB8/Xq9tu3btwpw5c9CrVy+sWbMGw4cPx7Zt2/D111+bfD43b95E//794ejoiPnz52P58uWoVasWlixZgl27dultO23aNHh5eWHZsmUYOHAgNm/erP3MAeoLQUuXLsUHH3yAJUuWwMnJCd9//73FfZvb+3XZsmUYPXo0goKCsGjRIgwfPhx79+5Fnz599L7bdP3222+YNm0aGjRogGXLliEoKAhTpkzR22bVqlVYv349hg8fjp9++gk9evTA6tWr9Z4bALRu3RqxsbE4c+aMxc+JEEJI0aPxNPDXX3/Bzs4OTZs2RUhICKpUqWIQZOR5HgMHDsTWrVsxePBgLF++HN7e3hgxYoR2PPzDDz/gm2++QaNGjbB8+XJ07doV8+fPx7Jly/L8uuQcx65fvx5Tp05F06ZN8eOPP2Lu3LkQi8UYP3689uJ2bmPMLl264MKFC3j06JH2OLGxsTh58iQ++OADo+04c+YMsrKy0LRpU5NtrV+/PhwdHXHgwAFIpVK0bNkSu3fv1gvQnT59Gi9fvtS+ZywZzwHqCzGPHj3C4sWLMXz4cIhEpm/y53ne6PhIpVIZbDt69Gj4+vpi6dKliIyMxFdffYV169YBUI9JP/nkE6xevRpdunTBihUr0KpVKyxYsECvjOOGDRswZswY1KxZE0uWLMGQIUPw+++/Y/r06XrHmjp1Kvr374/ly5fD3d0dEydO1CZy5Gc8mpaWhoMHD6JVq1YG63L7POdXgwYNIBKJ0K9fPyxZsgQXL17UJksEBgbi448/hpWVFQBYdL5m6ed1/Pjx+PPPPzFo0CAsWLAAr1+/Ngje5vU8FYBF3zuAOoFp5syZ6Nu3L1auXIly5cph4sSJ2rrx6enp6N27N27evImZM2di6tSp2LRpEy5cuGBRv5p6z2pYci5vbJ8DBw7E4cOHMW7cOMyZMwcXLlzQu7jG8zyGDBmC9PR0fPfdd1i2bBkcHR0xbNgwve8HAGjVqhW2b99u0fMhRA8jhBSbb7/9ltWpU4dlZmYyxhiLjY1lNWvWZIsXL2aMMZaWlsaCg4O1vzPG2NatW5mPjw97/vw5Y4yxbt26sXbt2jGlUqnd5v79+6xmzZrst99+Y4wxtnnzZubt7c02btyod/wxY8awn3/+WW/Z3r17mbe3N/vvv/8YY4z17NmTtW/fnvE8r91m586dzNvbmy1atIgxxtiff/7JvL292cWLF7Xb8DzPevXqxaKjo00+/1OnTjFvb2926tQpk9usX7+eeXt7s3Pnzukt/+KLL1hAQABLTExkFy5cMNjm0aNHbM6cOdp+8vf3Z8uWLdOuV6lU7Ntvv2Vnz541etz4+Hjm7e3N5syZY7JtjDG2bt065u3tza5du8Z4nmdRUVFs4sSJ2vVnz57Va9uYMWNYREQES0lJ0W6TmJjIateuzb799lu9fvnhhx/MHpsxxnr37s28vb1N/vvf//6n3bZJkyasbt26LDk5Wbts3759zNvbmx06dIipVCoWGRnJ+vfvr3eMR48eMT8/P/bdd98xxtTv28DAQBYXF6fdJjY2ljVu3JgdOHCAPXnyhHl7e7NRo0bp7ad79+6sU6dOjDFm0WtmTEREBPv6668Nlt++fZt5e3uzXbt2aZcNHDiQNW3aVO+9q/ksPHnyhDHG2IQJE1iTJk1MHo8xxl68eKHXzz4+Pqxdu3ZswYIFLCkpSbvdsWPHDNrAGGPjxo1jkZGRTKFQsOTkZBYUFMSmTp2qt83GjRuZt7c3u337NmOMsejoaIPPzo8//si8vb3Z5s2bGWOMTZkyhbVo0YKpVCrtNtu2bWNr1641+Vy2bt3KBg4cqPcYlUrFateuzaZMmcIYY9rXb9y4cXqP7dOnD2vXrh1jjLHXr18zPz8/7XtW4+OPP9brX2MmTJhg8v26dOlSxhhjSUlJzN/fn02ePFnvsZrP0/r16xljjC1atIh5e3szxtTfOREREezTTz/Ve8zUqVP1vmcGDBhg8B5ft24d27p1q0Fb69atq33fE0IIKZne9/G0QqFgkZGRemOLlStXsho1arDHjx9rlx06dIh5e3uz/fv36+2/e/fubMGCBdq/7bNmzdLb/+zZs9lHH33EGDM+btKMGzTjE1Pj2NmzZxv8Tb169Srz9vZmO3bsYIzlPsZMTk5mgYGBbOHChXrPNSQkhKWlpRntn9WrVzNvb292584dEz2o1rlzZ9a+fXvGGGOnT59m3t7e7PTp09r1kyZNYs2bN2eMMYvHc5oxz8OHD80eW9OHpv7VrFlTu62mf3XPNxhjbOjQoSwiIoKpVCp2+PBh5u3tzbZt26a3zdKlS7V9oVKpWL169djw4cP1tvn5559Zhw4dWFZWlnacdeTIEe36hw8fMm9vb/bLL78wxvI3HtW07/r16wbrcvs8a/Tu3Zv17t1b+7vuZ8mUvXv3snr16mn7NTAwkA0YMMBg7G7J+Zoln1fN+YnmO4Qx9bi7TZs22vErY3k/T9W0MbfvHc3rd+LECe02z549Y97e3mzNmjWMMcZ+++035uPjw27evKnd5vnz58zPz0+vf40x95598eIFY8yyc3nG1OepEyZMYIxlf1cdOnRIu31aWhoLCwvTfv/ExcUZvMeTk5PZrFmz2K1bt/SOpTnfvXv3rtnnQ0hOVMOckGKiVCqxfft2NGvWDFlZWcjKyoJMJkNYWJi27qC1tTWaN2+Ov//+Wztxz65duxAaGgpPT09kZGTg0qVL+Pjjj7VZxgBQvnx5VK1aFcePH0evXr20x/T29tZrgyYbNCEhAY8ePcKDBw+0t2UqFArI5XJcuHABw4cP15u4o2XLlnrZESdPnoSbmxv8/Pz0rig3adIE3333HV6/fg0HB4d89dOZM2fg5eWF2rVr6y3v0KED/vrrL1y6dAl16tSBs7Mzhg4ditatW6NRo0aIiIjA559/rt0+LCwMixcvxs2bN9GoUSM0bNgQEyZMyPX4uU3mqCnVwhgDx3Ho0KED1q1bhxkzZkAikWDnzp0oX768tv2nTp1CWFgYZDKZtq9sbW1Rp04dnDhxQm/flk485Ofnp3d7qC7dciGA+jWxs7PT/h4VFQWxWIxz586hfPnyePnypV6tbkBdTzEkJESb5XD+/HkEBwfDzc1Nu427u7v21t+nT58CAOrUqWPQlvPnzwMAqlevnutrllNGRgbi4+ONTnr0119/wcbGBqGhoUhOTgagzib44osv3rruoYeHB9atW4e7d+/i6NGjOH36NM6ePYtly5Zh48aNWL9+PSpVqoSTJ0+C4zg0atRI73MQFRWF7du3486dO3j58iUyMjIQFRVlsA2gLodTvnx5XLt2zWBymtatW+tlcIeHh+PPP/9EdHQ0WrRogcaNG6N9+/ZmJ9np1KkTOnXqhKysLDx+/BiPHj3CtWvXoFKpDEqS5LyrwcPDA8+ePQMAbVZOzmyt1q1b499//821T93c3LB8+XKD5Zrbci9evAi5XI727dvrra9Tpw68vLxw+vRp9OzZU2/d/fv3ER8fb7RNull2YWFh+P7779GzZ080b94cDRs2RO/evY22s2zZstr3MyGEkJKHxtPAkSNH8PLlS7Ro0UI7BmratCl++OEHbNy4EWPHjgWgznQWi8Vo0qSJ9rEcx+GPP/4AABw9ehQKhQLNmzfX2//EiRMtei1yyjmO1ewnJSUFDx8+xMOHD7WZuJoxSG5jTEA9z8j27du146T//e9/aNWqlck7Qtmbu9fMZXZr1mvaUbduXXh5eWnfJ3K5HPv27UO/fv0AABcuXMh1PFe9enUAgEwms3hC8xEjRhid9NPY2E6T6a7RokULHDhwAA8ePMCZM2cgFArRpk0bvW06dOiAhQsX4vTp0+A4Dq9evUKzZs30tunfv7/B/C26Y3rNuYXmvZaf8ahmbJVzTG/J5/ltaoy3aNECTZo0walTp3DixAmcPn0aJ06cwLFjx7Bnzx4sXLgQHMdZdL5myef13LlzAKA3NhUIBGjZsiXu3r2rXZaf89Tcvnd06Y7pPTw8AECbha45B9T9vHp6elpcl/zDDz/Ehx9+aLDcxcUFgGXn8o0aNdJbp/muatiwoXaZtbU1GjVqhLNnzwIAXF1dUa1aNUyZMgUnTpxAw4YNUb9+fUyaNMmgLZr32dOnTw0mmCXEHAqYE1JMDh8+jFevXmHLli3amn+6Dh06hGbNmqFTp07Ytm0bbt68CXd3d5w4cQIzZ84EoB6o8DyPVatWYdWqVQb7kEqler9r/nBpXLlyBTNmzMCVK1cgk8lQrVo1eHl5AVAPLpOSkqBSqQweJxKJ4OTkpP09KSkJL1++hJ+fn9Hn+vLly3wHzF+/fg1XV1eD5ZplycnJsLGxwfr167F8+XL8/fff2LBhA6ysrNChQwdMnjwZUqkU8+fPx4oVK7B7927s2bMHAoEA9erVw/Tp0w2CygDg5OQEa2vrXINlmlp2mhIhnTp1wrJly3D06FE0btwYe/bs0QvsJSUl4e+//zZa19rZ2Vnv95z9boqNjQ0CAgIs2jbnZCcCgQCOjo5ITk5GUlISAJjs7+vXrwNQPwdjQeucNLc16h5Lc9JiyWuWk2ZgnvOESKFQYPv27UhLS0NkZKTB4zZs2FAgE39Vq1YN1apVw4ABA6BQKLBlyxbMnDkTP/zwAxYtWoSkpCQwxlCrVi2jj4+Li9PW3x48eLDZbRhjBu+HnK9dmzZtwPM8fv/9dyxZsgQLFy6El5cXxo4dq1eTVVdmZia++uorbNu2DUqlEuXKlUNISAhEIpFB+R5zr5/meeRso+4JrjkSicTse1azf1PvRWNleyxt08CBA2FjY4PNmzdjzpw5+Pbbb+Ht7Y0vvvgCERERettaWVnlWoOTEEJI8aHxtLokHaCu1W5s3aeffgqJRIKkpCQ4OjpCIDBelVUzDsz5dzS/cj7fx48fY+rUqTh16hREIhGqVKmiDdJpxheWjDG7dOmC7du349y5c5BIJNpSFqZoXotnz56hUqVKJrd78uQJgoKCAKgD1O3bt8fGjRsxZcoUHD16FMnJydogtaavzI3nNFxcXMwGj3O2Nb9jek1/Jycn4/Xr13BycjK4SKAZE6WkpGifgyXnG7pjb837R/Oa5Wc8qhnH5RxrWvp5fhtisRgNGjTQnhvExcXh66+/xt69e3H48GE0adLEovM1Sz6vlo5N83qeCuT+vaNLt59zvn6vX782+pl3c3PT1n03x93dPdcxfW7n8sYeY+y7SrffOI7DTz/9hOXLl2Pfvn3YunUrxGIxmjVrhunTp8PR0VG7reb5myv7SYgxFDAnpJj89ddf8PLywuzZsw3WjRw5Ehs2bECzZs0QHh6OMmXKYPfu3ShTpgxEIhFatmwJQB105DgO/fv3NzogyTkI0aWpIe3j44OdO3eiatWqEAgEOHLkCPbu3QtAPYASi8UGtbV5ntebKNLOzg6VKlXCvHnzjB7LkuCqKQ4ODgZ1yAD1IASA9kSjSpUqmDt3LlQqFS5fvoxt27bhjz/+QLly5TB48GDY2dlh/PjxGD9+PO7fv48DBw5g2bJlmDFjBlavXm2wf47j0KRJExw7dgzp6elGs1ZUKhX279+PWrVqaQcaFStWRHBwMHbv3g2xWIzExER06NBB+xg7OzvUq1cPH330kcH+cst8KQiawbGGSqVCYmIinJ2dtQMLY4Ojly9favvazs7O6GSaJ0+eRLly5Sw+KcjtNctJc/ycA6tDhw4hISEB06dPR5UqVfTWbdy4Ebt370ZsbKzRSYVy88svv2D58uU4dOiQ3udJLBZrJ5bRZIjY2dnB2toav/76q9F9VaxYEf/99x8Add1BYydtrq6u2gFiztch52sHAO3atUO7du2QkpKCY8eOYdWqVRg/fjzq1Klj9Pl+88032Lt3LxYsWIB69epp39c5A8W50bwW8fHxen1urI35oQkIvHr1yiAT5OXLlyYvcmnapCtnmwQCAXr16oVevXohPj4eR44cwYoVK/Dpp5/ixIkTkEgk2m2Tk5NRtmzZgnhKhBBCCsH7Pp6Oj4/H0aNH0a1bN4O2X758GfPmzcP+/fvRpk0b2NnZISkpCTzP6wWibty4AaVSCXt7ewDqjFXdv+0vXrzAo0ePULt2bXAcZ1BPW7desik8z2Pw4MEQi8XYuHEjfH19IRKJcPfuXb3awrmNMcuXL4/Q0FBUqFABe/bsgVgsRsWKFQ3uatRVv359SCQS7N2712hiBaDOgk1ISNDLBO7YsSNWrFiB06dPY+fOnahVq5Z2/KHpK3PjucKWc3yjeX+5uLjAwcEBiYmJUCqVeucXmkC+k5OT3uudc7/Xrl2zOMMYyPt4VHdMrxustfTznB/du3dH5cqVDfbt7u6uDZjfvXtXezdubudrlnxeNc/z1atXeuPJnK9dXs9TLfnesZSTk5PRc+2CHNNbci6fs02JiYlQqVR6dxTkbFOZMmUwffp0TJs2DTdv3sSePXuwatUqODg46F1E01y4MHYsQsyhST8JKQavXr3Cv//+i7Zt22pnANf916ZNGxw/fhxPnjyBQCBAu3btcODAAezZswdNmzaFra0tAPWtYb6+vrh//z4CAgK0/zQTBZqbKOT+/ftISkpC3759Ub16de3A+ejRowDUA1uhUIhatWph//79eo89ePCg3q1noaGhePHiBVxcXPTacfLkSaxevfqtbp2rW7cunj17pi3lobF9+3aIxWIEBgZiz549CA8Px8uXLyEUChESEoLp06fD3t4eMTExePbsGRo1aoQ9e/YAUAdqBw0ahHr16iEmJsbksTUTiUyfPt1gVm5APTHSo0eP8Mknn+gt79ChA44ePYqdO3ciODhYbyAdGhqKu3fvombNmtp+8vf3x9q1a7Fv375895Ol/v33X70Z4A8cOAClUomIiAhUrlwZbm5u2LFjh95jnjx5gosXL2ozp+vUqYOLFy/qnfglJCRg0KBBOHDggEXtyO01M0YikcDNzU1vgixAnT3l7u6Obt26GXyW+vXrB5VKhU2bNlnUrpyqVauGxMRE7SRKulQqFZ48eaK9NTs0NBTp6elgjOl9Du7cuYOlS5dCqVQiKCgIYrEYsbGxetuIxWJ8//33ePr0KaRSKUJCQvDPP//oZYhobrPUGDVqlPbWcjs7O7Ru3RrDhg2DSqXSy2zSdf78eYSFhaFZs2baYPnVq1eRkJBg9D1uSkhICGQymfYzpaF7y/TbCAoKgkQiMXgvnjt3Ds+fPzeaxV+pUiV4enrm2qbu3btrJ6JycXFBdHQ0evXqhZSUFL1scsYYYmNjtdk6hBBCShYaT6vLkSgUCvTv39/oGMjBwUFbcqVOnTpQKBQ4cuSI9vGMMUyePBnLly9HYGAgxGKxwVjul19+wWeffQaO42BjY4PExERkZWVp12uSAcxJTEzEgwcP0KVLFwQGBmqDjrr9pGljbmNMjuMQHR2N/fv3Y//+/drJIU2xs7PDRx99hL/++kt7vJxtmzFjBipUqKB30aFKlSoICAjArl27cPjwYb0SKJaM5wpbznHhnj174OXlhQoVKiA0NBQqlcogQ1pzcaJ27dqoUqUKnJycDF7vHTt2YNCgQXqvsTn5GY9qgse6Y/68fJ7zw8vLC3v27DH6+AcPHgCA3pg+t/M1Sz6v4eHhAGB2bJqf81RLvncsFR4ejqdPn+LKlSvaZQkJCbh48aLF+zDHknP5nCIiIqBUKvW+M+VyOY4fP679/cKFC6hXrx4uX74MjuNQs2ZNjB49Gt7e3gb9pvmdkmBIXlGGOSHFYOvWrVAqlSZvU+vcuTN+//13bd3BTp06Yc2aNRAKhQZ1f8eMGYPBgwdj7Nix6NChA1QqFX766SdcunQJQ4cONdmGypUrw9bWFitWrIBIJIJIJMLevXvx119/AVDXiwbUV/P79OmDkSNHokuXLnj+/DkWLlwIILueXnR0NH777Td89NFH+OSTT+Dp6YkTJ05g1apV6N27d651wPfu3YsbN24YLO/SpQuio6Px+++/Y8SIERg5ciTKly+PgwcPYvPmzRgxYgTs7e1Rq1Yt8DyP4cOHY/DgwbCxscHu3buRkpKCFi1awMvLCx4eHvj666+RmpqKChUq4OrVqzhy5AiGDBlisl0+Pj749ttvMWnSJDx+/Bjdu3dHuXLlEBcXhy1btuD48eMYN26cQd21tm3bYvbs2di1axcmT56st27YsGHo3r07hgwZgh49ekAqleLPP//E/v37sWjRIrP9ZEpqaqrZQY2/v7/2xCQmJgZDhw5F37598eLFC/zwww+oX78+wsLCAKjfT5MmTcLo0aPRqVMnJCYmYsmSJXBwcNBmWfTv3x//+9//8PHHH+OTTz6BVCrFjz/+CHd3d3Tq1MmiEha5vWamREZG6p2YxcXF4d9//0WfPn2M3mIcGBiIqlWrause5lVkZCTatWuHH374Abdu3ULLli3h7OyMmJgYbNiwATExMViwYAEAoFGjRqhbty6GDRuGYcOGoWrVqrh8+TIWL16M+vXrazNoBg4ciIULFyI1NRVhYWGIjY3V1kysUaMGAPXr0K9fP4wYMQLdunXDw4cPDT774eHhmDZtGubMmYOGDRsiOTkZS5YsQaVKlbT7MdYfu3fvxh9//IGqVavi5s2bWL58OTiO037mLWFjY4Nhw4ZhwYIFsLKyQnh4OI4cOVJgAXNHR0cMHjwYS5YsgVgsRtOmTfH06VMsXLgQ1apVQ3R0tMFjOI7DuHHjMHbsWHz55Zdo1aoVLl68qA0UaNStWxc//fQTXF1dERISgtjYWPz8888IDQ3Vy3K6desWUlJSCqScDyGEkIJH42lgy5Yt8PX1NbjDDlAnGrRp0wZ//PEH7t27h8aNGyMkJASTJk3CZ599hooVK2LHjh24ffs2pkyZAmdnZ/Tt2xe//PILJBIJwsPDceXKFfz2228YM2YMRCIRmjRpgnXr1uGLL75A165dcefOHfz000+5Jse4uLjAy8sL69evh4eHB+zt7XHs2DH88ssvev2U2xhTIzo6GosXLwZjTG+5KZ9++ikePXqEoUOHokuXLoiKioKVlRWuX7+OtWvXgjGGFStWGNxR2qlTJ8yaNQsCgQCtW7fWLndycrJoPJdXjx8/Njmmt7e313ud165dC5lMhuDgYPzzzz84dOiQtq51w4YNERYWhmnTpiEuLg6+vr44c+YMVq1ahc6dO6NatWrafpk5cyamT5+O5s2b4+HDh1iwYAF69OhhcWme/IxH69SpA5lMhv/++w++vr4A8v55zqvRo0fj9OnT6NKlC/r27YuQkBAIBAJcuXIFP/30Exo2bKitmW3J+Zoln9eKFSuiW7dumD9/PpRKJWrWrIlt27bh1q1b2nbl5zzV0u8dS3Ts2BG//vorRowYgdGjR8PW1hbLly/PU9DdHEvO5XOKiIhA/fr18eWXXyI+Ph5eXl749ddfkZCQoC0h5OvrC5lMhs8//xyffvopXF1dceLECdy4cQN9+/bV29/58+dRrlw5VK5cuUCeE3mPFOkUo4QQxhhjrVu3Zm3btjW7TatWrVhERATLyspijDHWsWNHFh4ezhQKhcG2J06cYD179mSBgYGsdu3arG/fvnqzam/evJl5e3uzJ0+e6D3u1KlTLDo6mgUGBrKIiAg2YMAAdu7cORYSEsLmzJmj3W7fvn2sXbt2zM/Pj7Vo0YLt2rWLeXt7s59++km7zatXr9ikSZNYREQE8/f3Zy1btmSrVq3SmzE9J80s76b+adobHx/PvvjiCxYeHs78/f1Zhw4d2KZNm/T2denSJTZgwAAWGhrKAgICWHR0NPvnn3+06+Pi4tjEiRNZ/fr1mZ+fH2vWrBlbvny52fZp3Llzh02ePJk1adKE+fv7s4YNG7IxY8awCxcumHzMsGHDmJ+fH0tISDBYd/XqVfbxxx+zkJAQFhwczD788EO2f/9+g345depUrm3r3bu32T709vZm8fHxjDH17ONjxoxh06dPZ8HBwaxevXps1qxZLCMjQ2+fe/bsYZ07d2Z+fn4sLCyMjRs3jj1//lxvm7t377IhQ4aw4OBgFhoaykaMGMEeP37MGGPsyZMnzNvbm23evFnvMRMmTNDObM5Y7q+ZMQcOHGA1a9ZksbGxjDHGfvzxR+bt7c0uX75s8jGabfbt22fwWcjZJmNUKhX7888/Wc+ePVndunWZn58fa9CgAZs4caL2OWukpaWxWbNmsYYNGzI/Pz8WFRXFvv/+e5aZmam33W+//cbatGnD/Pz8WL169djYsWPZs2fP9LY5fvw4++CDD1hAQABr3bo1O3jwoEG//vrrr6xNmzYsMDCQhYaGss8++4w9ffrU5HNJTExkY8aMYaGhoSw4OJi1a9eO/fLLL2zKlCksMjKSKZVKi18/zfGbNm3K/P39WZ8+fdjvv/9u9Lsmt/2Y8vvvv2v7KTIykk2fPp0lJSVp1y9atIh5e3vrPWbXrl2sbdu2zN/fn0VHR7OdO3fqfZ4UCgVbtGgRa9asGfP392cRERFs8uTJBp/VH3/8UdsnhBBCSp73fTx98eJF5u3tzVavXm3y+Wu2+eabbxhjjCUnJ7Np06axiIgIFhQUxLp168ZOnjyp3Z7nebZmzRrt38hWrVqx9evX6+1zzZo1rHHjxszf359169aNXb16lfn7+2vHDabGsTdu3GC9e/fWjh179uzJjh49ylq1asVGjhyp3c7cGFNX586d2YABA0w+d2N27drF+vbty8LCwlhQUBBr27YtW7hwodHxOmPqcxA/Pz82fPhwo+tzG89ZOubRjL3M/Rs8eDBjLLt/169fz7p06aI9N9qzZ4/ePtPT09m3337LGjRowPz8/LTvpZzjmi1btrC2bdtqx61LlixhcrmcMWZ8nMUYY97e3mzRokXa3/M6HmWMsREjRrCBAwdqf8/r57l3796sd+/eJttkTFxcHPvqq69Yy5YtWVBQEAsMDGTt27dnq1at0n5HaOR2vsaYZZ9XpVLJFi5cyBo0aMACAwPZ8OHD2bJly/T6NT/nqZZ871j6+sXHx7OxY8eyOnXqsLp167K5c+eyzz77TK9/jbGkzzX7z+1cvkmTJmzChAna39PT09nMmTNZWFgYCw4OZl988QX7+uuv9T5PDx48YCNGjGARERHMz8+PtW3blm3YsMHg+K1bt2bfffddru0kJCeOsRwzAhBCiI4DBw7Aw8NDb0KTO3fuoF27dli2bJlerT9SskVFRSE0NBTffvttcTcl3xhj6NixI1q2bInhw4cXd3PIO4oxhhYtWqBXr17o379/cTeHEEJIKUfj6YIVGxuLqKgo/PDDD9pa9O+L06dPo2/fvvj111+1d4iWRleuXEG3bt2wb98+Kn9HCs2ZM2cwcOBA7N+/32CiXEJyQzXMCSFmHTt2DAMGDMCmTZtw7tw57Ny5E6NGjUKVKlVQv3794m4eec9oym788ccfFpV+ISQ/du/eDZ7n0b179+JuCiGEkHcAjacLxo0bN7BkyRIMHDgQ5cqVy/cEkKT4BQQEoFWrVkYntSSkoKxatQr9+vWjYDnJF6phTggxa8KECZDJZFi+fDni4uLg6OiIBg0aYOzYsZBKpcXdPPIeatiwIZo2bYoff/wxXzUMCTFHLpdj/vz5mDNnDmQyWXE3hxBCyDuAxtMFIysrCz///DPKlCmDBQsW5Fo7nZRsU6dORXR0NO7evautq05IQfn3338RExODpUuXFndTSClFJVkIIYQQQgghhBBCCCGEEFBJFkIIIYQQQgghhBBCCCEEAAXMCSGEEEIIIYQQQgghhBAAFDAnhBBCCCGEEEIIIYQQQgDQpJ8Wu3DhAhhjEIvFxd0UQgghhBBSiigUCnAch5CQkOJuSqlHY3JCCCGEEJIfeRmTU4a5hRhjKOr5URljkMvlRX7c0oj6ynLUV5ajvrIc9ZXlqK8sQ/1kOeoryxVXXxXHOPJdRWPyko36ynLUV5ajvrIc9ZXlqK8sQ/1kOeory5WGMTllmFtIk8USEBBQZMdMT0/HjRs3UK1aNVhbWxfZcUsj6ivLUV9ZjvrKctRXlqO+sgz1k+WoryxXXH115cqVIjvWu47G5CUb9ZXlqK8sR31lOeory1FfWYb6yXLUV5YrDWNyyjAnhBBCCCGEEEIIIYQQQkABc0IIIYQQQgghhBBCCCEEAAXMCSGEEEIIIYQQQgghhBAAFDAnhBBCCCGEEEIIIYQQQgBQwJwQQgghhBBCCCGEEEIIAQCIirsBhBBCCCHvC5VKBYVCUaD7zMrK0v5fIKBcCHMKo6/EYjGEQmGB7OtdtnXrVqxatQpKpRKjRo1CmzZtirtJhBBCCCGEGEUBc0IIIYSQQsYYQ0xMDJKSkgp83zzPQyQS4fnz5xQwz0Vh9ZWjoyM8PDzAcVyB7fNdEhsbi+XLl2Pz5s3geR7dunVDeHg4nJ2di7tphBBCCCGEGKCAOSGEEEJIIdMEy93d3WFtbV2ggVWVSoWsrCxIpVLKdM5FQfcVYwzp6emIi4sDAHh6er71Pt9FJ06cQGRkJOzs7AAA9erVw6FDh/DBBx8Uc8sIIYQQQggxRAFzQgghhJBCpFKptMFyFxeXQtk/AMhkMgqY56Iw+srKygoAEBcXB3d3d3oNjIiLi4Obm5v2d1dXV7x8+bIYW0QIIYQQQohpdN8uIYQQQkgh0tQst7a2LuaWkMKieW0Luj79u4LneYO7Kqh8ECGEEEIIKalopEoIIYQQUgSovvW7i15b8zw8PPDq1Svt7/Hx8XB3dy/GFhFCCCGEEGIaBcwJIYQQQkixYYwVdxNIIYuIiMCxY8eQnJyM5ORkHDt2DOHh4cXdLEIIIYQQQoyigDkhhBBCCLHY7du3MXr0aERGRsLf3x/169fHqFGjcP369TztJyYmBkOGDMGzZ88KqaWkpPDw8MDQoUPRs2dPdO3aFYMGDYKHh0dxN4sQQgghhBCjKGBOCCGEEEIscufOHXTr1g0JCQmYPHkyfvrpJ3z++ed4/vw5unXrhosXL1q8rxMnTuDw4cOF1lZSOJYtW4Y+ffroLeN5HosWLUKDBg0QFBSEAQMG4NGjR3rbdOrUCTt37sTevXvRpUuXomwyIYQQQggheSIq7gYQQgghhJDS4eeff4ajoyNWr14NsVisXd6sWTO0bt0ay5Ytw8qVK4uxhaQwrV27FosWLULdunX1li9btgwbNmzA7NmzUaZMGcydOxeDBg3Czp07IZFICrwdjDGkp6cX+H5NycjI0Ps/MY36ynLUV5ajvrIc9ZXlqK8sQ/1kOeoryxVXXzHGLJ57iALmhBBCCCHEIpqJG3PWHbe2tsakSZP0Br379+/HsmXLcOfOHdjb26N169YYM2YMrK2tsWXLFkyaNAkA0LRpU3Tu3Bnffvtt0T0RkiexsbGYPHkyzp8/j8qVK+utk8vl+OmnnzB+/Hg0atQIADB//nw0aNAA+/btQ9u2bQu8PQqFAjdu3Cjw/ebm4cOHRX7M0or6ynLUV5ajvrIc9ZXlqK8sQ/1kOeoryxVHX1mazEEBc0IIIYQQYpHGjRvjyJEj6N69Oz744AOEh4ejSpUq4DgOrVq10m63Y8cOjBs3Du3bt8eoUaPw7NkzzJ8/H3fv3sXPP/+Mxo0bY+jQoVi+fDmWLFkCHx+fYnxWJDfXrl2Dg4MDtm/fjqVLl+rVnb958ybS0tL0JvG0t7eHr68vzp49WygBc7FYjGrVqhX4fk3JyMjAw4cPUalSJVhZWRXZcUsj6ivLUV9ZjvrKctRXlqO+sgz1k+WoryxXXH119+5di7elgDkhhBBCCLFIz5498fLlS6xZswYzZ84EADg5OaF+/fro06cPgoKCwBjDvHnz0KBBA8ybN0/72EqVKqF///44cuQIGjdujAoVKgAAatasiXLlyhXL8yGWiYqKQlRUlNF1MTExAABPT0+95e7u7njx4kWhtIfjOFhbWxfKvs2xsrIqluOWRtRXlqO+shz1leWoryxHfWUZ6ifLUV9Zrqj7ytJyLABN+kkIIYQQQvLgs88+w7///ovvv/8eXbp0ga2tLXbs2IFu3brhl19+wf379xETE4OoqCgolUrtv7p168LW1hbHjx8v7qdACpCmDE/O21ulUimysrKKo0mEEEIIIYS8FcowL+GyFDwUSr64m0EIIYQQouXg4IB27dqhXbt2AIDr16/j888/x7x58+Dv7w8AmDFjBmbMmGHw2Li4uCJtKylcMpkMgLqWueZnAMjKyqLbkQkhhBBC3jMp6XLYWWcnUvA8Q1qmQm9ZaUAB8xIsS67C7E3P4WT3Cr9Ob13czSGEEELIeyw2NhYffPABPvvsM3Tt2lVvna+vL0aNGoXhw4dDpVIBAD7//HOEhoYa7MfBwaFI2kuKhqYUS1xcnLbMjub3GjVqFFezCCGEkCLDeBXAceA4y4s4aCZQN1YigjEGMB5gPBjPA29+Z4wB/JvljIdALAPjlVClvVY/kBOAEwjAiSRQpSe/WSQEBAJ12wRCcAIBmEoJPisDTCkHU8oBjkP860xcvpeAesHlIJOIwBRZAGOQK1UQvXqArCcMEHLghGKIHN2hSnsNPisdnEAIXp4Bxnh12ziBuu0cp+4TcGB4M1k8e/MfzfMDB04o0j6WqRQAY2BKBRivBDghspgQYo6HSCIFJ5ZC7OAGxeuXYPIM7TEADnxGCjihGBCKwAnUz5XJM9XHyu7ZHP0Mk+vAGHgGHLnwBO5O1vCr7AJengmBRApVRirA8+DEEgjEUnVf8iooFHLIXr5ESsJVZHBQP8c3/c2Uyjevh0Ddxjf9xHgeAokMjPFgWRmAQAiBlS0EIjH4rHQwngefkaruG4EAgEC9TpEFVXoyhDIb8PJMKFMSYFU5AEypgDIxBgJrBzBlVvZ7gBMAvAoMDHIFj9iEDHi62YDnAZFQAKFIAEDdl2AqcAIROJEYfGYaIBC8ea1FAMep3+8qpbo93Jt1nAAQCPDytRz3nyejiocN3J1tAI7DvaeJePU6CwE+ZWEtYur3rFIJmdgRqFnT4s9MUaOAeQn2ODYVAJCYIgdjLE+1dgghhBBCCpKrqytEIhF+//13dOjQAVKpVG/9/fv3IZVKUb16dbi4uODp06f4+OOPtetfvnyJ8ePHo3v37qhQoQIEAqoM+C6oUaMGbG1tcfr0aW3APDk5GdevX0fv3r2LuXWEkJKsoLIOn71MxYPnrxEZWDZP58yMMaRlKGBbwrIelSoejAFikfm/k4wxqHh14Kusqw3AePCKTIBXgs9IgYopwBgDk6e/CcAybXCRyTPAK94E00QScG8CYnxGmnrnb/qRMRVOX34GK5kYAZUdwatU4IRiCKUyMJVCHSgTisFnZaiDf4pMMAYIJFJwAlF24PdNkJQxTfA3OwCsDtoqAF4FCEXa9UwpVwctVUooOAlkMjF4eaZewBhvgsnagCvjIRBLkZqajsfxClTxsIaYz1IHCpVyMJUSAqkV+KwM8LwKdulpiL9ogySxNPvYvAqqzFTw8iwolDwkVlbg3gSveaVCHXDmGRRZmRCq1H3IlHJ1vwlE4EQicCKJuu2ZaW+isky7HowHJ5KoA9KcOmCs3zfFV2GgBoCE64bL7QAknivq1pQMgQDwCoi/Y9n2VgDS7hdig8yQx9yzeFtnAFmv1D8rCrANtgACJQASgPQE9TJPAJ4SAA+eIl1nWyknBFP2B1Ay671TwLwEU6iyvyiVKgaxiALmhBBCCCkeQqEQ06dPx/Dhw/HBBx+gV69eqFq1KjIyMnD8+HGsX78en332GZycnDB69GhMnToVQqEQTZo0QXJyMpYtW4bY2Fj4+fkBAOzt7QEA+/btQ8OGDVG1atXifHoknyQSCXr37o158+bB2dkZXl5emDt3Ljw8PNC8efPibh4h5I3XqVmwlolzDcIWpRlrTuG/m3FYPK4JKnna5/nxj14kw9lBhk++PQAAmD4oHLVrlLHosemZCnw67wDiEjPxRZ9aqFvNHkyRqQ7IMh5pShFsbaRgmalQZaYCSiUYmDrICYDPTAMvzwQnFIGXZ4IpMqHOzmTqzF6WHSDmM1PfZHfyYLxKnQEslkIgswEnEEGVlgRVRgqYIgvx8a+RkZYOoVAAd2cbaCIATKVQB7h5FSAQQmhtD1V6MpTpKYhX2SLdVgxhhjo65QSgIIufVX7z/5dXCnCn+ZDXWTEqAFCmAEoz24gAKJJNBwwFAJRpptexnAt5JZhcqc5sNoZXt0b9fkH2hYG3JJDZAOCyLxwo5VAJpVBBAImQg0KhhFAACKF+D4JXIZmXwc7eFpI35dSexKZACB4C8HBztIJQKgM4AeSZmYhPyoCVtRQ2tjbgFXKoXseCgQPsPaBUyJEsF6GCpz0EQiHikzKQnC6HlVQMO2sRHr9IhquTNdwcraBUMdx+kgglz4FB/YkRQYXyZexgayOFQCwDOIATiqHgOVy9+xKcIh1ZTISa5e3w/FkcXAQpeMXbIZWXwdZKhBoVHQHGIJDZajOfGa8Cn5mG+DQV7r1UQijgUL6MHR6+SIaVVIRq5R1hayXG07hUWMvEcHGwAgNDllwFqUSIlHQFVDyDQsnj1qNEAEDdmu6Ifa2AjVCJ2GQVHifxEEMFKadEJhNBBSFEQgHcHMVQMRFq+3nh9tNkJCWnIyywHIQiMX7fewsCjmn7uX5IeZRxtgGflQ4IhIhPYzh0/gkcBOmoUcEBVx+lwIqTQ+RaHpXKuaKSpx3O33iBhPhkvEiSQ8jxyGIS9G9eEen3LkDs7IlUOXD5+hO8UDnC37cSrtx9CY6p0C6yMuztZFiz7Zq6j3XevZqfOQBNapfDwfNPIQIPEadCOpNCBBX6d64DgOH8jRicvfkSSiZA3RquCKvpBoGVHcAYrtyJw+FzDyGGCnImgoswFZUqlcW5eykQcSrUru6MsKAKYAIRVm67iofpNviaF8LmrT8BhYMC5iWYUqkbMOdL1OCGEEIIIe+fxo0bY+PGjVizZg1WrFiBhIQESCQS+Pr6Yv78+WjRogUAoGvXrrCxscHq1avx559/wtraGrVq1cK8efNQvnx5AEBYWBjq1auH77//HidPnsTKlSuL86mRtzBy5EgolUp8+eWXyMzMRN26dbFmzRqDiUAJIcUjJj4Ng2btRxUvBywc07hA9/3r39chFgnRo4UPAODqvVf4acc1fBIdCO8KTnrbpqbLoVDxcLKTgfEq3L71CO4CObZsOYjhHwRClfYaqowUqFIS3tzmnwZOIIJCqYTs5SukJF5HuiIDnFiKpKRknLr0FBwH9LRRB6AUh6/ixUVAlZqoDjDLMyEQiQGhGPLkBKh4HmKOB5NnQsU4TOYY0h3FsN61Do+NPLekAu0py9gCsBWqf1YmvDa5Hf+m3IYAgJswBcgwvU8VJ4JYKgXPOPAqJThehVSlEOlMAg9na2RmZkLAq5CmEiMx683BOXUArYKHA+6/UN/5LhCJkKUARJwKUk6hLt5g5YC09AwwADWqlYVAag2pWIjzVx9DpVAgoLo7JGIREpKz4O5iA6FQCHAceAYolAycQIBbT5NhLZPgyqMUCDkebo7W8K3sjH3/xYCBg5IJYCvIAmMcHJzs4O5ihwt34sGDAwMHxjjwUP8DgHZhXth1+ilsODkUEEJgbY/yno5oFloRPOMQ/yoRbmXcoFCp8OTZc5T3KguJWIit+67hwfNkyJkIkz9pgs+Xn4YYSog5Faq7cgiq6oTnvCv2nX4EBg5WnByOgnTcV7ojnUnAgUHE8RBBBRHHQwCG5g18seWIOt04qm4F3H0Yh8Gdg2AtAcYuOQUAEHE8GACevXk+b56L5ud1M9pg6qqTuP0kGQxA+wZV0apeFSSlylHWRoURC08goLwX6geVhZVUhKv34rH54G2dEGi2oR8E4vz1WJy9oe7bumXLoH5QWfxz+jGuvY7P3vA10CqiEoZGB+KH38/iyKMXgM7bUQB1rIpPzI5RdQ2qDhcHK6zYctnwTZgM4JHp9yhSAGuZCMO7BEEoEGD1tit49ToTgE/2NjeMPC4VmNi6LmrXcIdcyWPb0Xt4EpuCsb1q49r9ePy+5yZupSZmt+HNYxAP+Fd1wdV76udsayVGaobhZZPmoRWwL0397fCLpRn2KW/+/xRQX74CfnyivmNEoQzS23T7CWD7vA648TABFcrYYdyU3QBcDZ9vGoBH6tdv+TUOQFm9/bSt3ACPrSLRLLQCZsw9hCep6iSU3ecBwBEA8OyOK74ZGonDGw0u8+g5cDz7MbrEDz1x9kYsXiU5Q52fDpy6BBxJc8TLxAzUruGOnceTAHjrP/AGAKgvZB6/CgRm2iCwmiuOJqdAIABEwpKbGMwxxsz3FgEAXLmivpwaEBBQZMf898IjfPfbRQDA71+1LnUF8otSeno6bty4gZo1a8LaumTezlFSUF9ZjvrKctRXlqO+ssy71E+ZmZl48OABKleurDcpYkFRqVTIzMyETCZTn4gSkwqrr3J7jYtjHPmuKo6+fJe+jwrbu9xXtx8nYsWWy/i4gz/8qri89f6Koq94nuHguce4/iAB+86ogz47vu+ot01cQjomLD2GdpGV8UFUdQB4U8YjA8rkV1Clp4Ap5ZDHvwDLSn2TncpDILVGRlI8jpy6BSF4hFW1hlgswsU7r6DiOUg4BaqXd4IwMxGq9BQIrWwRk5ABJYRwFssh4TNhJD+3WMkhRgavzimUcQpwYMhgEqTxUlQo6wShUIB0OcOjmBSkMylknAJZTAQbJxdkMRFYZjKqlHdBqtARJ67GwsXRCsGV7eHhJIXA1hUH/3uKB08SkQUR2tT1xKnzdyCBCplMjFe8HTKZCAomghwicGDo0cIHEpEQNSs7gxMI8PClHPP/OA87axE6BNlCZmuPpf/EwE2QDDGnxFOVC9J5CYQcj0wm0QY1VRBiUr+6mP3LWQBA/aCyOHbpOQDAy80Gz16aSKEG8OVHofj65zN56kdPFxu8iM/eZ1B1V1y6o679sGl2W6zcegX7zjyGQMChdg13nL0em6f959emWW3x045r2H3yIT7pHIAmtTy0n8F7z9PxxfLj2m0Xj2uCz344DJ4vnPfotIHhmLH61FvvJzKoLI6/eS3zI7i6Gy7eeWly/czBEZi68mS+919cypexxZM3JY5Lutb1KmH3iYcFsq+IAE+cvPLC5Hrfys64/iChQI5VEFztRVg6vkmRjhfyMo6kDPMSTK7QyTBXFl8dK0IIIYQQQgh5X7xKyoCLgwzpmUrYWIkBAFN/PIG0TCUmLj2G7fM66NXKvv04EduP3ke/tr5wc7LK07EYY7j9OBG21mKUdbXVLktMyYKzvUz7+/Itl2FnLUGf1sYnSIuJT8OkZcchFTBMG1wPbk7WWLZ2P17euwlbLhMNpAwyToFXe19CkfAcypQE8FkZyExNwShwkJ5Q4MF5sXpyOpVK73Z9c0LfTGehfKoufeGjE2FgMXHachhKeQZcNdcpdU5t03kxlBDCwc4KiRkMcqENnmVYQSRgECozkcak8HCSwd1RiMeJwL0EDjJOgUwmhpIJtFm4ACDhlEjirVG3dk08iUvGk8ex8Cjrih7NvPH1L+fxmreCmyAFGUyCGJUDeHBwEaQimVkhibfR7seYwVEBaFq3PEZM3QOlKse5uW5c7pXmhzJADLAhRp2l6s5bY/1dJQAPAMCpYwAQYrZvv/k7CQAwvEs5LP3r0pulzkAKMPeY5sC2SOBt9R/45qVTIfvCsCZYDkAbLAdgNlgOAC+TzKSum6AbLAegDZYDQL8Ze5GeqX5X8DwrsmA5AHT9Ypf25xVbr+DK3ZcIqcjQ/+tDyMjSL9zy6bxDhdqWggiWA3irYDkAs8FyAKUyWA6g1ATLARRYsByA2WA5gBIVLAcAG1nJrqJBAfMSTK5QaX9WqkrW1XdCCCGEEEIIKSyJKZn4c99ttAyviMplHQrlGKkZCti+CYjHJaTD1dEKmw7exm+7b2q3mTOiPmpWckZaZnZA7dr9ePhXVd82r1LxGLvwKADgv1tx+P2r1gCAhy+SsX7PDfRqVVNbn/vS7ZfYe/oRfCs7I9zXBXIlj3V7bmPPiQcAgC1zOwMqObbv+Q+Hjl2BNSeHnasbYhIyYMunQMIpESc4hYcvXkMsFsPTzRZJD28jLfYJrLgsTOAYJFAh+cfVSAVDO46pa3zoSD53Ue93MQDxm5gFk6vPPzVhY54TQmDtgJhkFeJU9kjhrWAvSIcIPJKZFTKYBMm8Fay5LIT7uuJqki3uPEmCEDxUEEIFDq95a9g5O6NL/XJYs/0KJFAimVnDyc0Vt2Lk4PHm4ElmXqg0YLC/O1Zetawy98kTKgA2AKoAD4Gdq2MAeAEAnquc9bZNVVl2gWPl/65g5f/yV8Rbk92fX9nB8qK3brexOhj5l55prqJ40Tp+JQZ3n4gNguUllUQs1IsREVLaWUkoYE7yybeyE8oIkhDP2xlexSaEEEIIIYSQd9TijRdx9nosdh1/YFBGpCAcPv8E3//+Hz5q5wdHOwnm/3EBbepVwt85sv1+230TH3fw01t2+e4reLrawMXBCkcuPNMuT0mX4/il56hewVGboXr+Zhy++7QBKnnaYcnqv1FOGI87NzPwfE8WKopeIUSQjiinNHBguDlrI2ScAoEAAjVzYGYBujOipZ7TVrhF+kNAAiBnzEHMvakvzDg8UrkgmbeGFSeHkgmRwOyQpLJCIm8DFTg8VzpBwDFkMRFknBJewgQk8DZ4rnJGBhMD8ZbVl5U5Vsem83cAGClXEwdc2hILwF27KCZGCcDyYMnKPQU5jSWxVEkKcBeG2CRT032WPEM6B+DGgwTsP/t2F2As8XnvOvjuN0uLdpOCNKFvHfx78RlOXDafLV6Qereqgd/23Mx9wwImo4A5yS/713fxheN2HMmsAaWqeXE3hxBCCCGEEEKKxN0nSQW2r8TkTBy9+AxN65THwXNPUMHDDj/88R8A4Oed1+Bop64rkjNYDgBX7r1CTEK63rI//rmFTQdu49thkfj5z39RRZQKGaeAoyAdVzZdwDNBKj6zS4WNIAsi8Ej9eQMeCzIxKZdEeSHUwTs5EyKZt4IKAsg4BYTgkciro+b3le6QMxHEnApiqOtWP1c5Io2XQQUBFEwIMacEDwEymRiZLG/zYD1XOeW+kRGbDtzJ1+NI6SSVCJElL1nZzjUqOuHmo8TibkahcbCR4LPuIThy4SkUhVSy95POAWhbvwoAwK+qC/rN2FsoxzFlQt86kCt4nL8Ri6MXnxndZu7IBhi/6N+3Ok7/tr5Yu+v6W+2jsEQElEWdGmVQxeue3t1OtWu44/zNwrlwKJWIsGRcE4wooFJEPVv44J8zj/Eql5JOlGFO8o3PVA/MvIQJeJ2ahfJl7Iq5RYQQQgghhBBSsPaeeoRzN2IwvncdSMRCqFQ8MnWCcTv+vQ+JWICW4ZW0yx69SMaUH08gMqgsohtXx6xfzqBVeEW9bRhj4DgOc9adw7X78fjr4B0kpWQBAARc9pST5dxttctzchGk4N8Nv6GjVQpknAIZTAwhx2DFyZH66ybMcEw3+jhjlEyAB0o3pPIyKCDEfaU7YlUOSOJtwIFBBBVeM2tkMjFgpp52rqiaZ7Hyr+qCB89e65XxMadVRCWcuvICSanG34MlQeWy9njwPFn7+7SB4ZCKhbj5MAEPXyS/ddkZjS5R1fHXwfxdfPnqk3roOmlX7hsWollDI/UmDy1IMok6fCcSCgotYC6VZNe9l0n0J0cPqOqMK/csq4H905ct8MMf53H1Xrx2mZ21BH3b1NQrMfTph8H4bfcNJL75/q0fpC6d1KR2OUQEekIoEODIf09x/HJ2rXYB9xbfjW842RtO0F7UBBxgbE5ZoYCDUCpCt2Y++OfUI8QlqoPO/lVdYSUV6c1B0DDEC/5VXbHsLcs28Tyvna/DHBuZyKLvteio6vjn9KNct5OI3v61LEwUMC/BBNbq+/DsBJmY+9t5/DKtZTG3iBBCCCGEEEJyp1TxyJKrjJ6En7sRC6WKR5ifB2Li07Fk00UAwO6TD9GxYVVMWHpMr66wpnZ0REBZHLv0DHtPPsKjmGSoeIadxx5g5zF1DfAlT5LQMKQcrKQiJCZnYszCo2hapzyu3VcHbTRBcQ4MUk4FOy4NzsJU2D69gzZWqXAUpMOay4KjIB32ggyIoYS1wHzJBp4Br3lrpDEpEnkb7b8E3hYpvBWcBKlQQYAYlSPiVbaQI/egBMmbOjXL4NwNyyePDKjqiiv31BNRft67DpzspZi0zHSQUyDgwBuLbJlgIxNjwZjGGDRrf67bTuxXF5GBZXHmWox2mYOtBK9T5RYfLzceLtaIibf8wo4xw7oEabN67azFCHhTw9+7glOeJ7B0dbQymXkqEBgPoM0f3QjJaXJMMzMJpUwiQuPa5XD4/FODdc72UiQk5/+ChKerDVZOaoYshQpdJu40uo1IyCGgmiv++raddpuPO/gh/nUm/nfkXr6PrSGVqgPYFT3sTGbSh/l54LTOeynPxxBnhwjFIv2A+ee9gtFn5kEA6sCpUChAcprx96mbkxV8K7toA+bNQyugY8OqqOhpD5+KThj5/WEAQGA1V4jFQoPHcxynDZ7fe5qUY53lz6eihx0exaQYLNfcVWSJnBeLAKB8GTv0b+sLjgNmrjkNAGhWt0KeyuWIRPo16dvUq4RWEZX0tpnQt652jgwBx2FC37o4Nnabdv2o7iEQCQXYevguXrzSn2xXIOBQs5Kz9u+fOUoVg9DEZ0/XkOhA/PD7f7luJxUL0atVDSz886LBup4ta+D3verMea4ALn4UJgqYl2BCTcCcy0TC68xibg0hhBBCCCHkfZeSLoetlRgcx2H1tqtwtJOiS1R1g+3GLDiCB8+T8duMVhAKBUhNl2Pe+vNwdbDSZgtWL++IOzqlV16/ybC9ZSIY9L8jd3Mt/TFr7RnUqeGOrXsvQClX4tihp/ATZ6CcMB5eokS4CFJQVpgEAWdZAFTFONxXuuOp0gWZTAwnobr8ymveGo+VLrih8EIaM5etWMai4+QHxwGMsskhEuYt6NIstII2YN4gxAtxiaaDyZGBZTGqR0ieMpdVPIOHi43ZbWpWckariIqoF+AJQJ1tqlG9vJPFFwC2ze2AS3deIv51JqxkIrjZizB2sX5QeeGYxohLzNDW1TdlxcSm+OTbA0bXabKbAaBqOUe9daYmovRwsUbbyMpwc7TGt7+e1S5fM7k5bjxMwMSlxwweYyp+JhYJIDUSWM1pVLcQowHzX6a1wvHLz/Hdr2cxrEswlm2+ZHARRCQUYNnnUfjn9COUL2OH+X9kBwbZmw+aSKhfQiLqTZkn3XU525nXoGD9oLJ6WcQa4jf7bxle0WTAvFOjqujYqCq+MHMByByJOPv55fxcScRC/PlNG9x9mgS/Kq6YtPQYktPUGec1KzljTM9aGDRrP5zeBKM7N6qKq/deoVGtcmhTr7J2PxU87LU/W8vEKOdmi7gE059BPseXHMdxmNC3Dub8ar7GukwixJLxUbh89yUmLz+ht84zl8+nLmtZ9kXOTz8MRvzrTLSNrAx7Gwku3MoukdIgxMtkwNzOWoKUdP2LC2KRQO+zE+LjbjDBtXeF7BJZzg6Gf2c0FzWqejkYBMzBGAKqupoMmOt+3lUqHkKh+fIoKyc1g6erjUUBcwBoWreCQcB87dQWsJaJswPmFu2p+FDAvAQT2Kg/LNYCOYQoWfXBCCGEEEIIIe+XC7fiMHXlSXRtWh2NapXDtqPqrMkHz14jMqgsgqs5arfVZOT9dysO247ew72nrwEAt5Ad6LmTo075/WevzWZiZgfLGey5DDgK0mEnyIAdlwkpp4S78DXKvkiEZ1wSptoo9CbLNCaTiZCgsgUAxKgc8FzlhHQmRQovg7VAjte8Ne4r3JGVS1b4gPZ++GnHNfMHKwQD2vthzfaiP25RmTUs0qLAn1CQtzq4DUO8cO9pEoKquwEA3J2s0bdNTYhFAuw/81gvI9WnohOkYiHsrMVISTd9t4FQwEH1JgCrVKnLZYzoGoQlmwxLJVTytMd3nzbQX6gTVFVaWG6jdg13CAQcQnyyJ1NNT9cPPA7q5A9rmRiVPMX4/rOG2mxVY7zcbE2uk4hM97Gp8iAzBkegrKvhPgUCDg62eautb28tMVsz3epN9rVQKNB7LQD1+whQX/z485u2kElFKOtmY/De+nFiU7g7W6NfW1/ceqRfekSzO90s3A+beaNP65oGAfOcPF2sLXyWQN82NdG+fhWjAXNN9r2xQKSGRCzUC7Lmle573Fig31omRmA1N4PlXw4Ig72NBD9PaaG9q8jWWoI5IxoYbCsUcPhmaD3IFTzsbSQY3jUIy/66hA4Nqxptk1KVI2AOdemWOTAMmI/oGgSpRITFf17AB28u5AZUddW7sAGoM+AtZaMTMPd0sUGLsIra33VfcxuZ8fCqq4MME/rVRXqGEtNWZV/MEud4v5i6u2Ji37q4/iAeDYK9TLYxZzBeQ6HM/sxEBHjCu4ITfnlTu91ap70q3jDDfETXIKzYckX7feaRh/cxoH7/5PzedHGw0mtTSY+YU8C8BOOkNlAxDkKOwU5AGeaEEEIIIYSQovc4Jhmrt13FhdsvAagD17on+0cvPsPRi8/w51fNDR4rV6i0wfLcnL8ZZzCpmQA8PIVJ8BQmwUuYAC9RAryEibAV5F5eQcU4qCBAOpPgodINj5SuSOat8EjphmRe9iYQ/vZn7JXLZmdMli9jiyexqW+9T2NyBgLbRlZ564B589AKCKruhnnrz79t8wpcQFVXlHO3xdM48/1pSSkBQD0RnW8VF4iEAgzqFKC3rmtTbwBAVJ0KuHb/FWatVWdEcxwHjuPw1ZB6uP4gQVseSKOKlwPC/TzQIMQLQ+eoy1VoMpdbhldCUHU3/LnvNiICPfHVm9INxu7I0H0KcqVlyXKfdQ8xu97WSowODbKDkGVzBMRbhleEs70Mf/xzS7vsi/6hmLX2jMG+ZFLToSOFynjA3FiwvE5N9R0XuhnrukzdMeFkLzNa471HCx/4VnbWy1quVs4Rtx5nX5jTlI8Bsp9HQFVX/PlVc+w5chE/71d/r4l1sqtzBmmZkYblfNcZC5j7V3FFpbL2WLb5ssE6nwpOeu0Est+HHRtW1V6Q1LB603aO4wwCwBpiIxc2/vdde/Sf+Y/ZGvma75aq5XKZmViHbjzd3kZ9AcTV0bJAtG7Q3d3JGtMHRZjcVpHj82AqsNyzZQ3tHBaNa5XTaSeH0T1qYXSPWoiJT4NQIDB5cUNXuL8HElOy4F3REWeuxxg9tu7vZd1s8VFbH6S9fonzD5S49ywZ43vXRsOQcjAmZwa/s53xO5Uig8oiMqis2bbeeGh4xwGD/mfzi/6h2jtr1MfXfb/zEOZoT4NgLzSrWwETlx6Dp6uNwQUU3Xrmro5WKONsbZDNLpWIDC406l7gLOEVWShgXpJxHIcUJoMjlwEvW8owJ4QQQgghhBSeLYfuYtvRe4huUg3+VVxgay1BGWdr/L73ljZYrvG7TpBNQ6nicejya3DW2Sfv5rJyjRGAR03xc1QQvUI1UQwqi15CaKR8iopxSOatkMpkSOGtkMVEiOdt8VzlhOcqJ8Sp7MHAgUfeso/NCfF2M+gHAHrlN0b3qIUxC0xn8Vqqdg13hPl7QiYRam+B7926pjY7sFo5B73gWMvwimhTrzJmrT2DWJ3yBrOHReLk1RfYfvS+0ePY20j0JvoraDMGRehlVeZVqon3TyVPezx8ob6LQWCiJIutlRipGdmP79GyRq7Hs7eRICIgOzilCehULef45p8DJixRlxL5dnh9+FVxMdiHUidI5eFiow1syyRCZMpVRh+jGzlSmaiX3r25DzbsU3/uvhoSAScTATZj7QDU/aFRu4Y7RnQNxuW7+u/niABP7Pi+I9rr1EkGAGfdSRJzNE83Iz5nn+ekeZq6E0rqTn5Y1Ss7YKsp2aQJsucMclbytEfXptUNa233qYOPv9lnsg3G2gPo1+zOmU3LjL0mOd52ugHQX6a1xMvEdFQr72jwsPpBZdG+QRVU8rRHt8l/G21XVJ3yegHzXq1q5FrmB1BnmOckFAow5eMwjF14FFKJEFlyFUb3CIGniy1Wb7+CQR0DUMbZGi+TMgxKgpQEOe9gMFXixpLrZrp9mFtJq8kfhYExhh3/Zn935gwq6xIKOLQKr4AbN9LQtok34lOUqJajfJEu3fdbv7a+Rt8rlurTuobRi6c5S8/oTpiqe6HRSirSC2R3alRVW4pm7siGRo8pynFxxlhprJwTxwL6FxlKeLycAuYlncjaFsjMQDl7Kk5HCCGEkOI1ceJEbN261ew2t24ZBtFy06dPH3h5eeHbb7+1aPuoqCh07twZn376aZ6PRQgx7klsCn7eqT7hXr3tqnb59nkdkJZpWdB744F7OHI1BUeuZt8qf+XuK4PtODB4CJNQVpgID+FreAiTUEb4GgIwuAkNJ2hL58WIUTniqcoZz5TOeKZyQozKEYoCOJ2tUdHJZD3gnEydkekGEx1sLZ9MzhzvCk5oHVEJCiWvDZhX8szOon2dY7I9gYBDFS8HfNCkml42q39VV/hUdNILmM8YHKGdPNHBVmpRtmXbyMrYdfxBnp+HsdIbHKfO8L1s5L1hJRUiIys7WUy31ICNTIQN37RFQnImOAB9Z+wFoB8EynkcU5NAWirnnt0cswOpOYNnXm42ePYyzWQ26C/TWiI1Q2E0C1c32GdsglH/qi74sFl1NAzxwuPYFAR7uxtsk5O5eUo1r3lAVVd83MEfFTzs9NbrBr5tjUzcq8unohMevkiGQMBhbK/amLH6FPq2qWn2MVKdDHOpRKSd5DciwBOfdQtGtfJOcLCV4Mh/TxFVp4LRfbQIq2gQLAcAd2frfL1fdcvOuDhYYf6oRhi94AgAdXmRnDTvO4lYPXmjv04mu7O9TO97YfJHofjm5zPax/lWVl80MZUpnlP35j4WPQdTnwXvCk74bUYr2FlLoFTx2sD695810m7jZG/6AoxEZLjfopqw0SBgbmrDPDZHKBAYXFQy2CXH6V2EMHc3i1AogGZ3MokQ1cvbmdxWve/sn43ddZIXHRtWNQiYu9jL0DK8EuJfZ6LWm7JNegFzoQDDugTh2MVnaN+git5zq1DGfNsNMAbOyAtgLGCuizLMyVtRCdVfWkKl8VmkCSGEEEKKyuTJkzF27Fjt7/Xr18cXX3yBNm3avNV+Fy9eDKHQ8gzHv/76C1JpwQSlCHlXZWQptbfwW2JVjlITGkoVbzIIk9PfJw0nPPvvVhxsuQyUFyWgvDAeFUSvUFH0Cva5lJz8L6si7io9cEfhgVe8XYFmiuuaOjAcPafstmhbY4FMQB0wW/Z5FBhj+pm4b0ETjNXN2tPNeo1/re4/d2drxCWko0GQur6tbrb4lI/D1PvSyRwc3iUIrg66AX6JRW32r+qCD5pUx6OYZMxYfcri52GsfILgTYmT6Ak7DLKpxSL9gHnfNr7aCzkazvYyvUC67ttTE7x8s8b0VQ4LuTvrZxrrBslFOWqnzxnRALceJaJ2DePBbGuZWG8CQV0OtlLExKvvDDBW/mPW0EhwHIfyZexQ3sJglrH364iuQdi4/zb6tFYHtDmOQ6dGhrWjpw0Kx/hF/6r3k8vMsh+184OzvQwNgr1QvowdNnzdRlvHOidNUE337giZRKgNmHMch2ah2TWiOzWqpv05Z7mREB/DWtp5pfvUcl44qlbeEVM+DsMfe29iVI9aBo/VvBMWjG6EQ+efILpxNYNtNML9PQ0fCPUdKYfOP8nz5L2mvpLNfVVrLuZJBJaPt74dXh+XbsfA29UwHpWzBndhyfl3TPMcP+9TB9+ty744a+nfKY0GwWVxyIKLafoBc/3nrHtIkYDL08yDuX2u8iLnxQv/qi4Y3CkAYpEA/dr6apfrNl8k4NA6ohJaR1TKdX9Gj5kjQG7sIVITpZd091KSUcC8hOPF6sGLiALmhBBCCClmdnZ2sLOzM1jm5vZ2J62Ojo552t7Z2fmtjkfIu+6f04+weONFjOgapK3pak5SShZiEtKNrlMoeZOBYmPbcuBRXpiAKqI4VBK9REXRKzgL0wy2zWIiPFU6I0bliFiVAwCGRN4GT1QuSOatoELhlQnR8KvikmuQJczPA6evqevXdmxU1SAremyv2gCgF8T8bUYrxCWm65VmCfF2g7uzNRoEeUHFmDbD2xRNMIrj1Fm7KWlylHO3g6erDV68SkOIt/p7d9GYxoiJT0PVN7f+6wYoNNl9ukFrgYDT28beRooqXrmXYeA4Dm5OVkhMMX+hQyTk4OZkjRev0gyOrbsvgYAzGmDJGRTt3LiqNmCu+y7Ufd10Aze6h8ut5II50weF486TJIT5eegt183CzNl+B1spQnNsb6nRPWph/u//oWvT6kbLHeUnm9dYQK5leCWLvhNqVMz+O2ssgK/LxkqMnjrlbkwFywH9Pqvl446nL1PhW9nZorsAPFxs0DayMnjG0KFBFZRzN33hILc2a4hF+p+NnEJ9PRDqa/w15d5sX76MHfq28TW6jdHH5QgS1q3pgTPXY+Dpml0+w85IRruuD5t648DZ7Mz0OjXLICNLCXcn9QWekR8GY9HGi/ikc4CpXVjEr4oLKntY4caNGwbrBncOwJcrTqBr07fLjs5N9+Y+encLaD4LDYK9IBJy2vkG8uqT6ECLAuZSMxnmet89Fs6lAABVyznAzlqivUiWHzlLoAzs6I/1e27im6H1UL288Ylfdb9HzLXXonmUczzc2P4ow5wUKiZ6EzDnadJPQggh5F3DGEOW/O3mKVHxKmTKVYBACaHA8siAVCIs8Ntpt2zZgiVLlqBp06bYunUr6tSpgxUrVuDgwYNYuXIlbt26BaVSCR8fH4wZMwb16tUDoF+SRbOPTz/9FMuWLcOLFy/g4+ODL7/8EiEh6jqwuiVZFi9ejDNnzqBhw4ZYt24dEhMTERISgunTp6NKlSoAgISEBHz11Vf4999/IRQK8cEHH+Dq1auoW7culXUhpdarpAw42UkhNJLlt3jjRQDAkk2XDIJj8a8zMPe384iJT4OtlRhDPwjCjNWntBmeOckVvNlMOE15leqiGFQXx6CaKBbWAv1yITwD4ngHPFG64InSBY9VLnisdMk1KN4wxAthfh7wcLHB2IVvXxc8J5lEaHCSP6iTP67dj8eJyy8AZE9mBwB1apTR27ZTo6p6k8tpONhK4WArRS0fd/x3Sz2JqbVMjBFdgwEAl4zUQc9JN9tV9xjTB4Zj+7/3EVWnPAB1cLKqTp1c3eCOsVIrAo6DRGdyQ2MlU4zJfPP+yG2CzS1z2uPmw0R8vuRfk9tr+tzY2ypnwNxkvWIT7dDdnuMAls8U89o1yqB2jtcbgN7nrSCLpnq52WLeZ+pawev33iyQfVoaNDZFU++6Wjn94Ft++zSn6YPCwfPMYCJVcz6JDrRoO0tb6OkkRlRtL5R1y2MJCqhrrOdHzrf0Z91DsPvkAzSpXV67zM3J/OSZZd1ssWVOO7x4lQaZVAR3J2swxrTv/+ZhFREZVNbkHQ0FoXwZO6yd2qLQS7M42kkxvndtzP1NPTGx7uF0M77zmmFuqm8a1yqHFmHZdzno3rWT83vHyT77bse89IOLvRWGRAdgzfarehPz5kXO59uxYVW0q1/F7Hd0RU97iEUCONlJzbbXsgzz3B9janJfU/soaShgXsJpMswlKsowJ4QQQt4ljDFMWHIMNx4mFMvxa1ZyxpwR9Qv8ROfZs2eIjY3F1q1bkZmZiatXr2L48OEYP3485s6di7S0NMyfPx/jxo3D4cOHIZEYBmvi4uKwYcMGzJ07F2KxGNOnT8eECROwd+9eo+29cOECrKyssHLlSqSlpWHChAmYMWMGfvnlF/A8jyFDhkClUuHHH38EYwwLFizAuXPnULdu3QJ97oQUlav3XmHSsuMI9nbDV0PUF540wbHcPtM7/r2Pa/fjAajLekxceszs9k/jUvAoJllvmbvgNWqKnyNE+gDlhQkQcfp1YNN5Me4py+Ch0g0PlW54onRGFiwLzOoa1b0WxCIBnr9KNbmNs70UCclZed43ANhaSQyCVwKOM7jtXrsuZ3Zhbt+fJlZbkolo7EIIoA6UmQsa6mb0GQ2YCzi9oLqpoJFPRSekpsvx7KU6U1xzQcVUuzQ4jtMrWyIQcPhxUlPcfJiI+X+oa7Gb20XOgLkpAhOZ3noZ5gVQkiUnUR6ySPNL946OmpWc9YJ3efG2FR9++Kwhdh5/gG7NvN9uR1BfmHmdKtfL2Ne8V3JOHlggLHzuHMdhSKeasLa2zn3jN5Z9HoWHL5K1daHzKufXhr2NBN2aGdYo150M1RixSIgKHtnzGuT8PirMYLmpYxbFcQQmsqTz0xY3Jyu8TMyAo50USSlZEL6pw69Lt4Z/znkLPFxsMKZnrVzvCDDG3ckak/qF5vlxGpyR76LcLmhKxUL88XWbXLcz15cD2vvhpx3X8Fn3EExfdUrzANhZG77fcptQmjLMydsRq68sShhlmBNCCCGkdBg2bBjKl1dnSt24cQNffvklevXqpV3ft29fDBgwAPHx8fD09DR4vEKhwPTp01GzprrG6pAhQzB8+HC8fPkS7u6GJ6hKpRLfffedtrRLnz59MHfuXADAmTNncPnyZezevRsVK1ZEZmYmfvjhBzRr1qygnzYhRebvEw8BABdvv8S9p0kYs+AIeKa+LX/awHCzj829pqi+ScuOQwAeAeKn8Jc8RQ3xczgK9G8jlzMh7indcUfhiTsKDzxVORdIzXHNSb2Tneka2/Y2hgHzNvUqQSwSYtvRe2b372QvNXKLvX5AxlzQMbdzfVPrdYM8s4eGISlNhcPnn0LFM5y7EQsAUOUyGZ0pugEKY8FnAad+D2iCcW5GJqAE1MvnjWyI9mO3AcgOGuUWaFEfQz+45eFig7KuttqAuWa9sa41dbHC3DF06QV6Cj5enusFg4KgGyT97tMGhX48Uyp42GPYB0EGy/MTiF88rgnuPEkyuEsDKJxa2AX9uuvKSx15Y4oqyPwu0SvBpDtxpZkSSZZYPqEpUtPlUPEMG/65hfYNqhhso1tiyNj3ju6dAUUprxn1GroXTE1vY/oz2blxNbSNrKxX2x2MoX87Pzx/lYY29SppF+dWksVaWjR18POLAuYlnUQ9OJSy/GVNEEIIIaRk4jgOc0bUL5iSLJlZkMmkEOZhIqfCKMmiUalSJe3PNWvWhIODA1atWoUHDx7g4cOH2lqYKpXp5161avYtqpq66QqFwui2rq6uenXQ7ezstNtev34dDg4OqFKlivZ4Li4uqFy5cr6eGyElgW4gdNT8I9qfNcFWoYDTTqa45+RDNAzxwrrdN9Ag2MviiUDtuXRUEcUhUnYbFUWvIOWyS7YomQD3lGVwU+GJq/LyhTYppyawbK7NOTP+ACC6SXWkpMkNAuZRdcrj4Lnsur921hKD70FOwFlWvxW5B2hMfcfqBnnEQgHqB3mgfpAXvl9/XrvcVImc3Jirtwuo+1Qo4LD+qzYAY9qgR+fG1bD18F2D7Sf2q4srd1+hQbCX0X2O7VVbr90ADDLMjbUhpzA/Dzjby1DWzRZrtl/Nta660ERmaY54+VuXJTF33MKKyhZ0m0sCJzuZyVrg9QLLYvOhu3C2L7jJvN/FPnyf6X2udUuyCI1/D1hKKhZC6qC+aDiyW4jRbWxzCZjnR0GUNSqMm126NffGncdJJj+rGhIjQXdnexnmjWyot8zU3+7RPWrh0u1Y+Ofv5pkiQwHzEk4gVX94ZciCSsUXyRVtQgghhBQNjuMgszB4ZYpKxQG8EjKJCEJh4U+SZwmZLDsb9OzZsxgwYAAaNWqEOnXqoG3btsjIyMDw4cPN7sNYqRZTJ8DGttUQCoXg+fxlahJSUuVWtkIiFmoDrkv/uoT/bsXh5JUX2HnsAQZ29Df6GA4M5YXx8JM8hZ/4KcqL9MtFpfNinJVXxVNxJVxJdkAGyz245WQnRWJK4Sb+GAtgiITGJ5Qc3aOWXsC8jLO1weM5jkOPFjVw6uoLtAyvhNR04xfq3qrNAuMBZaVOVnl+A+a6NWN1S11oJgsNqq6eLNQ2x8SM/dr6omGIF0brXIABgMjAsogMLGu0vR8280ZkYFltwDwiQH3HkMBEBqixfWh8OSAMAKDiGSp52hmdtE73T4Cpkiwcx6FmJWfceJiA5mEV8eyl6XI++aF7XDubvJdhsISlk+yaIhFxkCsZPF1sct+4BPCu4IQl45uYvNshPzxK8HM3ViqJmJfzM66he0dKYSXu62aYK/J5509hyMsko5bq3apmge6vbWQVbD5keCE2qk55hPu6GJ1MtiShgHkJx0nUfzSsOTmyFCpY05crIYQQQkqRNWvWICwsDEuWLNEuW7duHYCiyQCrUaMGUlJScO/ePW3me1JSEh49elToxyakoO0++RA7j91HGWfT9XZT3txeruvklRfan3N+7CqJXiJY/Ai+kqcoI8yuVc4zIFbliBuKsjgrr4pYlT1UEMLTxQYZLM2i9poLltvIREjLtDwovHpyc/x3Kw7L/rqkt9xUWY+cwYQQbzeD7eoFehoEWTiog8u/f9UGIqEACzb8Z3EbzalRKTsArF+DN3sb3dctPb8Z5roT1OkcZ+n4KGQpVAaBcg2hgEM1nclDTX076/Zru/qV9X6vUdHZYBtjQR1twMvI3wChgEOwd/5qQ6v3DUwbGI7rD+IR4uOOeb+dz/1BeTT5o1CkpivMfg7fxtv+aRzQ3A2XnnDo29avYBpUBCrq1OIuCB0aVEFCciZCfQ1LwBSXfm19sfvkQ/RsaViv3BjKkc9m6i6Sty3JYgmJWIhaPu54nZZVoi7EFEbAvKC5OVkhsJorLt99VdxNyRcKmJdwAqk6Q8taIIdcwcPadPk+QgghhJASx9PTE/v378e5c+fg4eGB06dPY+HChQAAuVxe6McPCwtDcHAwPv/8c0yePBkcx2HJkiXIyMigOqKk1NEEix/HpJjcpueU3Wb38fxVKtwEyagrvYdQyT04CbPrkWcyEW4ovHBNXg43FGWRygwzPkWigvnciEVCTBtYBzNWn7Jo+zLO1kaD3sYuvIlEAr3Pd0BVV8wYHAEgu1zNj5OaQiwyvCtH8zhNFmjO3S+fEIWhcw7qbWuJdvWza+OamqhON8O8UUg5i/etSzdgzus0XiwSWDyhpjmcTmV2kVCgP8kmp/9/wPgdAPkta+DmaPxkWHd/5dztYGMlRt03JQUKovRBTuH+hnNvFCT+LSPmHk4SNKmXt4ks3zUSsRCDOwUUdzP0dImqji5R1Yu7GaWSXg1zGL8gx+U6q0T+TR+knhukJI0bS1JbzCmUSX2LCAXMS7o3GeZWnBxyxdvVOCWEEEIIKWojR47Eq1ev8MknnwAAqlWrhlmzZmH8+PG4fPmyXq3ywrJo0SLMnDkTAwYMgFQqRY8ePXD//n2IxcYzLQl5FwnAo67kHnyu7EYrx5d6667Iy+GGwgvnsqogC+Y/F5ZOymiJOjXLYHzv2phrYRawsXqozMgd8iKBfkmWDg2raIMLv81ohddpcpR1tTV6jNxiEOXcLZ/sT3dfumUYdLMidQNBKlV2oNSviovFx9El0bkI4GhbcDWhjeE4Ti9oYyzOa6ykqDbIxXEWpVPPGFgH63ZdwfAPA000BPj+s4bYfvQ++rX11VtVGktZj+oegsnLj6N/u9KTIU5IYdK7CCfQ/Vn3u7Qwj1/ygtOlIMFcrRR+B2tQwLyEY+I3k35ySmRmZgJ4f68SE0IIIaRkuXXrlt7v0dHRiI6O1lvm5OSExYsXGzy2RYsW2p81JVpM7SMsLEzvWAcPHtT+/Omnn+LTTz812Y6EhARcv34dCxYsgEAgQGZmJgQCAX755ReUKVNybtUmRNfpqy+gUPGoH+SlXZaemb9a2hx4hEruo5nVFbgL1ZnpPONwU1EW5+WV8UDphso+1bQThuZGZGSSzbeRl0CEg60Un34YDJGQw/w/LgAA3J2tcP/5a73thEKBftkTnZ9trSWwtTZdezovQYhcJ/00kfFoqmRJQdQ2Fgg4rJncHEoVD2tZ4V4UNPX0Oc58EEv7nC2MZteo6ITeTdxQ1tV4OQR7awm8KzhhXO/aFu2vpPOv6oq/vm1fIHcEFIbSeBGiNKJ+zqZfksXEHAklMKhdmApqAlJiGgXMSzgmkoEx9Wc/KzUZgHNxN4kQQgghpNQQiUQYPXo0unfvjg8//BBpaWn47bffIJFI0LBhw+JuHiEGVCoeX/98BgDgP90VjnbqLOHYhHRzDzPgKEhDXcl91JHeh4dQHVBO5aU4mOmHs1lVkMyyE3Gal3O0OGBekBnmQN7rsLYIqwgAqFbOEWkZShy98NRgm5yTfuYtrlBwQYiIAE+cuR4DZ3v9UiKmsiI/7uiHx7HJiG5c7a2O614QtbVNBOtcHGSoUtYBIhEHa5l+OEFbkkVnmbHXV7MosLobLt5+Cde3nOzRyd503dKimCujMJTUYDkhxUHv+1xnubCIMsxLotJQwxxAQf5JLXIUMC/pOA5ZnAQyyJGZmpz79oQQQgghRMve3h4rVqzAggUL8Oeff4LjONSqVQu//vornJ0pEYGUPLr1i1+nZWkD5nEWBsztuAw0lN1EY9l1SDh1Scd0XoJ9mf44lukDuZGSKzZWlp8WFvRJen6z5Cq8mSTw6EXDgDnHcfo1b/NwjLw0J7f9RtUpD2cHGap6Oegt15/0M/vnsq62WPVFc8sbUAwEAg7zRzcCYFm/Co0GzNXLxvWqjV3HH6Bp3Qpv1SZ/M+Vrgqu74dTVmNITXCoFcl4oIaSw6d6to5dhrnNXTkksm1KY3rfnWxzom64UyOJkkDE5slJMT+5DCCGEEEKMCw8Px4YNG6BSqZCZmQmZTAah0HCyP0JKAl4nIVahyC7QnSE3P5+RkyAVjWU3ECm9BTGnftx9hRtOy6vhkrwCMpjpetZ5Kd3xtpmvXm62ePYyFS0j1JnixgKqBYEzUZIlL4/LfVvz6wUCDrV83M0+rjTeVp9b8Fn3/WR00s83j3ewlaJnyxr5bseyz6OQlJqFip72JrdpVa8ybKwl8K1MF0jf1tietbDt6L0SN5nmu2rG4AjMXnsGw7sGF3dTip+JO4b0L4wWYXvekrX07ctllca/HaUNBcxLAYVABqiSoUinDHNCCCGEEELeZbxOxFyuzA6SKxTGA+YughS0tLqMOpL7EHLqxz5QuOFgpi8uKyrAkvuhbfIQMH+bAHdwdTd88VEobj5MQEA1VwBvn7FuKsBtapK43Ggy+s35oEk1HL34DO3rV7F8xzo4ExnmpZ3maTnaSTGmZy1IxELzk36+pfJl7FC+jPlJWIUCDo1rlSuQ473vGtcuj8a1yxd3M94btXzcseGbtoV2UbG0MvX9WRoyrkf3CMHfxx/io/a+uW+ciwKujpYv1co74u6TJDQy8x1bycMe/92MK8JWFRwKmJcCSqEMUAGKtNTibgohhBBCCCGkEOnWXJYrVDhzPQZVvRwgV/J621lxckTJrumVXrmt8MD+DH/cUnoiL4VDdUssNAophyNG6oJr5DfY+Ul0INpGVgYAhOhkXVtJC+eUNL8lWUK83fR+N1YDu387P/Rr61sgAZp3NUuwiZnA6rv6nAkpaBQsN2Rqns/S0FNRdSogqs7blaDSKIhJot/WV4MjcOXeK9Sp6WFymx4tfMBxQL3AskXYsoJBAfNSQCWyBuSAKpMC5oQQQgghhLzLNu6/rf15/5knOHLhKRztpPigSXXt8qqiWHSzOYkyQvUdqLcVHtiRXguPVa75OqaVTsC8fYPKZgPmeTXl4zBcuBmHluEVja73reyMluEVYSMTY8vhu3nev26QpHJZe/hUVJfeyE9Jlk6NqhoEwU1NGVlQ2YzvUoa5pShgTgjJL70a5qUsw7wgfPphMNbvuYExPWsXd1Ngay1BRID5QLhMKkL/dn5F1KKCRQHzUoCJ1bOG85mWTfRDCCGEEEIIKR3+d+Qu7G0k2qyzzYeyg8aawHVSShYUShXcBMnoYH0egZInAIBUXoo/08JxRVEB7C3y66Ti7Jr+udUzV6myQ8jdmnnjT50AvzGhvh4I9TWdfcZxHEZ0DUZymjxfAXNdi8Y20f6se7v6exJHKVDM5KWC/BvQ3g+/7bmJ4V2DCnzfhJD3w7tQw/xttAiriOahFd6bCwTFiQLmpQAnVtfR4xWZxdwSQgghhBBCSEF5GpeCNduvAUCut2nfPrQbnzsch4RTQcU4nMyqjj0ZQUhhVhjRNQhLNl3KdzukYiE+6xaMxJSsXGtC8zolSlwcZPk+Zk72NhI0DPbC0YvP8vZAEzEDLp8lWXKys5bk+7Gm6NZJFwvfn6BH58bV0KFBFaN1zQkhxBLv6hwQeUHB8qJBAfNSQCBRD6iYIquYW0IIIYQQQggpKIkp2eP7n3ZcQ+fGVQ22EYBHJ+tzaCS7CQC4pfDA5rRQxPKO2m1ahld6u4C5RIhmocZLpgDqzD1NnFylMylpQQcrxvepk/eAuQm68YS3qQPco4UPnsSkIKpuwU12KBULsXRcA9y7e/e9Cx6/b8+XEFKw9Cd0zv7FyHQThLwV+mtVCggl6syNV69eG510hhBCCCGksPXp0wcdO3Y0uX7q1KmIiooyO1bZsmULfHx8tL9HRUVh8eLFJrdfvHgxoqKiLG4jYwxbt25FfHy80eMRUlIkpWTh97038fxl9hxFWw/fxZKN+kFvCRQYZrdPGyzfkxGI5SnN9ILlBSG3iTd1s9lUKt3JR0tulpuggGaDs7OW4KtP6pmdxDI/XB1ksLcW5r4hIYS853S/wk3PT0GxMlKwKMO8FFBy6jqCYiiRmJIFZ/uCu/WREEIIIcQSXbp0weeff447d+6gevXqeuvkcjn27NmDvn375uk20b/++gtSqTT3DS109uxZTJw4EQcOHAAAtGnTBg0aNCiw/RNSUOatP4dLd14ZLL/2IF77swgqDLQ7hOriWGQyMdal1sdVhemgbZWyDrj//HW+2iOTWH5aaG1lvsZ5UeNMRMPplvW3k588rQoe9gXfEELIe0/360hAGeakiFCGeSlQqZx6pncJp0SWXFXMrSGEEELI+6hly5aws7PDjh07DNYdOHAAKSkp+OCDD/K0T2dnZ9jY2BRUEw2y22UyGdzc3Aps/4S8LblChe9/P280WA4AaRkKAOoyLP1tj8BHHINMJsKy5GbaYHm/tr7Y8HUbg8dO6Fsn3+3KWVpl/uhGJrcN9/dE89AKGNE1GCU5oy8/VVgo4JI/80c1wqjuIQjxpu9bQkjhMlXDnKfvb1LAKGBeCljZqk8kJZwSmXJlMbeGEEIIIe8jmUyGdu3aYefOnQaB6W3btiEyMhIcx2HcuHGoV68e/Pz80KhRI8yfPx88zxvdZ86SLH/++SeaN2+OwMBADBs2DK9f62fL3rlzB8OGDUNYWBj8/f3RvHlz/PLLLwCA06dPo2/fvgCApk2bYsuWLQYlWZKSkjB79mxERUUhMDAQPXr0wLlz57TrFy9ejD59+mDVqlVo2LAhAgIC0LdvX9y/f//tOo+QN3Yee4DD55+a3YYDjz42xxAgeQo5E2JVShQeqbIDkSIhBxsrMaQS/XIe9jYFNzlltXKOer/zPNPe5Vq7hjtGdgtBy3DTNc81alZyLrA2mWIqkVxv0s9Cb8X7rVp5RzStW4Gy+gkhhYIz8bP+hVGKmJOCRQHzUoATqQe/Ek6JLAVlmBNCCCHvCsYYeHnmW/9jiqy8PyYfqZRdunTBs2fPcP78ee2y+Ph4/Pvvv+jatSuGDBmChIQErFmzBnv27MHAgQOxYsUKHDx4MNd979q1CzNnzkT//v2xbds2BAcHY/369dr1GRkZ+Oijj2BtbY3ff/8du3btQuvWrTFr1izcuHEDISEh2uD7pk2b0KaNfgauSqXCoEGDcOHCBcyePRtbt25FjRo10L9/f1y5ckW73YULF3D27FmsXLkSa9euxfPnzzFjxow89xUhxrxMSs9lC4Zu1qdQS/oQSibAz6mNcFfpobeFpmZrzoksC3oCzpxWftEMv05vCXcn61y37VC/EoZ1CcLkj0ILtU3m5Cd2yyjgQgghJY7uN7OpGuZ0hxApaFTDvBTgxOranlIokZVFAXNCCCHkXcAYw/NfJyPr6a1iOb60XA2U7ft1njIC/f39UaNGDezYsQN16qjLP+zYsQP29vaIjIzEs2fP0LJlS3h5eQFQTxS6cuVK3Lp1C82aNTO7719//RVt2rRBr169AACDBw/GxYsXcfOmerLDjIwM9O3bFz179oStrS0AYMSIEfjxxx9x69Yt1KxZEw4ODgDUpV5kMv05X44dO4Zr165h48aN8Pf3h1AoxNSpU3Hp0iWsWbMGCxYsAAAolUp89913cHR01D6HuXPnWtxHhJgjEprPV2oiu44I2V3wjMOvqQ1wXVHOYBtNYDxnwFyYy75N8anoZNF2UrEQUrFlk1Ray0RoHVEpX+0pKALKdiaEkHcOZ7KGOUXMScGiDPNSQBMwF3MqyjAnhBBC3imlL6DTpUsX7NmzBwqFutby//73P3Tq1Am2trbo3bs3zp8/j1mzZmHIkCFo1KgR4uLiTJZk0XX79m0EBAToLQsJCdH+7OzsjJ49e+Lvv//GzJkz8fHHH6Nx48YAYPH+7ezsUK1aNe0yjuNQp04d3LqVfdHC1dVVGywHADs7O+1zJeRtiUWmT78qiV6ivdV/AIDN6XVxSWG85IkmUJ4zQJ4zgA4AvVvXyLVNc4bXz3WbkszURT+ukDPuCSGEFA29kiy65bZ0M8yLsD3k/UAZ5qUAJ1IHzCWcEilUw5wQQgh5J3Ach7J9vwZTZL3VflQqFbKysiCVSiEUWpb9CagvyOen3mz79u3x3Xff4ejRoyhfvjxu3LiB77//HhkZGejVqxcyMjLQunVrdOzYEVOmTNFmjFsiZ3aQWCzW/vzq1St8+OGHcHJyQtOmTREREYGAgAA0amR6csKc+zb2fHmeh0iUPSSWSAquDjQhOZnKMBeAR1frUxByDOezKuFYlo/R7YDsjLqcGdTGMqrLutiabY+NlTjfmeklXX7i5UHVaNJKQggpjSjBnBQ0CpiXApoMcwmUyJJThjkhhBDyruA4DpxElvuGZjCVChwPCCQyCPIQMM8vR0dHNG/eHHv27IGHhwdq1aqFqlWr4p9//sG1a9dw/PhxuLq6AlBPshkfH2/RbbI1a9bE+fPn0a9fP+0y3driO3bsQFJSEvbu3asNpGsywzX7N3cBwMfHB8nJybh79y78/f21y8+fP6+XdU5IYWGM4Y9/jJdgqi+9hXKiRKTxEmxOD4W5u080gfGybjZISM7MXm4kQszlEgs3F1O2t5EgOU1ufgclgKnnYCoL0Zi1U1vg4Ytk1PJxL8CWEUIIKQgWxcIpYk4K2LuZTvCO4cTZk37KqSQLIYQQQopZly5dcPjwYezZswddunQBAHh4qCcm3L59O549e4Zz585h2LBhUCgUkMtzD7oNHjwY+/btw+rVq/Hw4UOsW7cOe/fu1a738PBARkYGdu/ejefPn+PYsWMYM2YMAGj3b22tnozw5s2bSEtL09t/ZGQkfHx88MUXX+DMmTO4d+8eZsyYgdu3b+sF6QkpLGdvxBpd7sClo631RQDAzoxaSGPmL6IJherg7+getRDu74HZwyJNbptboNjc6mkDw80+FigZ8QkbK7HR5Xm5g8bFwQq1a5TJ1103hBBCih9fAv4ekXcLZZiXApqSLEKOQUk1NAkhhBBSzCIiImBnZ4f4+Hi0bt0aABAYGIhJkyZh7dq1WLBgAcqUKYM2bdrA09MTly5dynWfjRs3xvfff4/Fixdj4cKFCA4OxoABA7Bz504AQKtWrXDt2jXMmTMHqamp8PLyQteuXXHgwAFcvnwZPXr0gLe3Nxo1aoRRo0ZhzJgxerXIRSIRVq9ejW+//RYjR46EQqGAn58f1q5di+Dg4MLoJkL0JOpkg+tqYXUZMk6BB0pXnMyqnut+NBnm7k7WmPxRWC7b5r2dGtXLO+b/wUWoQ4MquPYgHvUCPPWWUwlzQgh5N1jydc6oijkpYBQwLwU0JVkAQCU3PtAmhBBCCCkqHMfh4MGDBsv79++P/v37m3xcdHQ0oqOjtb/n3EebNm3Qpk0bvWWaLHKO4zBu3DiMGzdOb/1HH32k/VkikWDlypUGx9RwcXHBV199BZlMZrTe+6effopPP/3UbJsJyS9j2dg2XCZCpfcAADvTa4FZEBYwVnrFlNwzpk2vtyTbuiQkZMukIswYFGGwnLLFCSHk/VES7ngi7xYqyVIKcEIR+DcvlSqLAuaEEEIIIYSUNsbO5SOltyHhVHiidMZdZRmL9iMUmD6F6xKln6FubCJQXW8bU6YABSGEEELeRRQwLyV4gbo2H6+ggDkhhBBCCCGlTo7oshAqNJDdBAAcyvSFuWxv/6ou2p/NxMvRNrKy3u8cB1Qua29y+/clCbuMs3VxN4EQQkg+eVdwAgA425ue44Mu4JKCRiVZSgmVQAIRnwVenlXcTSGEEEIIIYTkUc5z+dqSB7AXZCKJt8YFeSWzj7W3kWh/Npc1nrNcC8dxmDYwHPvOPMb6PTcNtucsqgxbeq36ohmyFCo42Epz35joYRR9IoSUEDZWYvz5TRuIRYbl9DToO4sUNMowLyWYUD1IZgoKmBNCCCGEEFLa6J/LMzSS3QAAHMmsoS2/aIpukNxcDfOcsXSOA1wcrNC9uY+JB5g9bKnn4WKDih6mM+wJIYSUDtYyMcQi038rKV5OChoFzEsJXqguycKUFDAnhBBCCCGktNHNfvMSJqKcKBFKJsCprOpmHqWmW7fcXA3znBnjNPElIYSQ9wFlmJOCRgHz0kL45jZChbx420EIIYSQfKGB/LuLXltiiWv347U/R0jvAACuKsohneVeLkQo1M0wt/yYZpLRAbzzCeaEEELeEzQSIwWNAualBBO9qVuoogxzQgghpDQRi9V3iaWnpxdzS0hh0by2mteakJxS0uU4duk5AEAAHrUkDwAAJzK9LXq8UGBZSRaWI2SQW4Y5JaATQgh5F1DyAiloNOlnafGmhjmUlGFOCCGElCZCoRCOjo6Ii4sDAFhbWxdomQSVSoWsrCztsYhpBd1XjDGkp6cjLi4Ojo6O1P8Exy8/x+aDdzC+dx14utpol/ecslv7cyXRS9gI5EjjJbij9LBov7pBcnMlWQwel+t3TcF9F43oGoQlmy699X6+6F8X89b/h3G9ahVAqwghhLwPqpd3LO4mkHcMBcxLCU6syTBXFG9DCCGEEJJnHh7qoJgmaF6QeJ6HUqmESCSCIC+1Gt5DhdVXjo6O2teYvN++/eUsAGDRxguYPay+0W38xU8BADcUXgaTfU7sWxertl1B/OtMveW6F9nMBcEdbfXLu+QWLy/IDPPKZR0KZD8RAWWxcZanXlY9KXqUrEkIKQ2WT4jCk9gUBHu7F3dTyDuGAualBCdS3+LLUcCcEEIIKXU4joOnpyfc3d2hUBTs3/KMjAzcv38fFSpUgJWVVYHu+11TGH0lFosps5wYSE03/Tn3k6gD5lcV5QzW1arhjhoXnXH88nO95folWUwfl+M4fD2kHr788YT2d41ZQyNx8uoL7Pj3vnaZWFQyL7JRsJwQQoglyrnboZy7XXE3g7yD3vmA+XfffYejR4+CMYauXbuif//+xd2kfOE0Ncx5KslCCCGElFZCobDAg6s8zwMApFIpZDJZge77XUN9RYoKbyI910WQAg/ha6gYhxsKL4P1HGc8WJyXkiyczmqRzmShAdVcEVDNVS9gLhXTxR5CCCGEkJze6YD5wYMHcfv2bWzbtg1ZWVno0qUL6tWrB29vyybXKUkEb0qycCplMbeEEEIIIYQQYk5yqvEkF3/xEwDAPWUZZDKJwXoBxxmd1FM3sG1u0k9AP6tcKDQfXJdKKGBOCCGEEJJTybwHr4CULVsWo0ePhlAohLW1NSpUqICYmJjibla+CETqeoQCnkqyEEIIIYQQUpIlpWaBGcky99eUY5EblmMB1MHueoFlDZa7OWWXEOJ588WldWuci3ILmIvfLn+KylwTQggh5F30TgfMa9SoAT8/PwDApUuXcP36ddSqVTpnWxdI3mSY85RhTgghhBBCSEmXM64t4+SoKooFAFxVlDf6GAEHhPt7oHloBb3lDjbZk3mq3pQWMkV3Is/caoFLxO/06SAhhBBCSL68EyOk7du3IyoqSu/frFmztOsvXryIESNGYPbs2bC1tS3GluafUKweJAsZBcwJIYQQQggp6XJmmNcQP4eQY4hROSCeNz5BGcdx4DgOvpWd9Za7OsowvEsQ2tWvDO8KTmaPm6cM81xKsvRo4WN2PSGEEELIu+idqGHeoUMHdOjQwei6Y8eOYcKECZg3bx4iIiKKuGUFRySRQAFAyKgkCyGEEEIIISVdzoC5t+gFAOCGXD3ZZ81KzhjQwQ/jF/2r3SY71p0d9G4bWRk+FZ3hU1E/iG6KXoa50DDD3NVBhlevMwEAYX6eZvcV3bgaHr5INlomhhBCCCHkXfVOBMxNefToET7//HP8+OOPCAgIKO7mvBWRRAoFAAFlmBNCCCGEEFLiqXgGsc7vVURxAIC7yjIAALFIgBo5guCaCTsFOonhbSMr5+m4XC4Z5t+PaoQDZx+jrKst6gWaD5jLpCJ80T80T8cnhBBCCCnt3umA+Zo1a6BQKPDll19ql40bNw4NGjQoxlblj1gqQwYAMVRQqvhcb68khBBCCCGEFB/dBHNrLhOeotcAgAdKNwsenR305syXITfLWA1zZ3sZujb1zv9OyXuhfBnjZYMIIYSQ90GJC5gvW7YMJ0+exLp167TLeJ7HkiVLsGnTJiQnJ6N27dqYNm0aKlasaHZfM2fOxMyZMwu7yUVCLFPXMBdDCblCRQFzQgghhBBCSjCeZ8jMUkIg4FBF9BIAEKNyQBqT5fpY3SA5l8eIOa8TqS/scwZraYk7nSRvad7IBjh55QW6NaeLKoQQQt5fJWqEs3btWixatAh169bVW75s2TJs2LABs2fPRpkyZTB37lwMGjQIO3fuhEQiKbL2McaQnp5eZMfLyMjQ/p9j6oGyiOOR9DoV4KVF1o7SQLeviHnUV5ajvrIc9ZXlqK8sQ/1kOeoryxVXXzHG8hz0JKVflkKFfjP2wkoqQnOxuhzLfYW7wXYBVV1x5d4rvWVv827h+eyAubEa5gWpflBZnLz6An6VXQr1OKTo5KVePiGEEPKuKhEB89jYWEyePBnnz59H5cr6Nfrkcjl++uknjB8/Ho0aNQIAzJ8/Hw0aNMC+ffvQtm3bImunQqHAjRs3iux4Gg8fPoQo/gXsoM4wv3bzNpxtS8RLV+I8fPiwuJtQalBfWY76ynLUV5ajvrIM9ZPlqK8sVxx9VZRJHqRkePYyFQCQkaVERTt1hrmxciz1Aj0NAua6KeZ5DXmr+KLLMBcKBZjYV53sdPtxYqEeixBCCCGkqJSIqOu1a9fg4OCA7du3Y+nSpXj27Jl23c2bN5GWlobw8HDtMnt7e/j6+uLs2bNFGjAXi8WoVq1akR0vIyMDDx8+RKVKlSB0ECHhLCDmVChfoTIqlLEtsnaUBrp9ZWVlVdzNKdGoryxHfWU56ivLUV9ZhvrJctRXliuuvrp7926RHYuUPBx4lBclAACeqCzLxNYrPZ7HiLlehrmRGuaEEEIIIcS8EhEwj4qKQlRUlNF1MTExAABPT/0Z3N3d3fHixYtCb5sujuNgbW1dpMcEACsrKwgV9gDUAXOBUFws7SgNrKysqG8sRH1lOeory1FfWY76yjLUT5ajvrJcUfcVlWN5P2ledXdBMqScEllMhFiVg4WP5Yz+bAndgDm99wghhBBC8q7EzxypqTGZ8zZWqVSKrKys4mhSseDE6ucvhgpZclUxt4YQQgghhBBizvHLzwFAm13+TOkEXuf0S2duTkN6k37m7bi82R0TQgghhJDclPiAuUymnkVeLpfrLc/6P3v3HR5VnbZx/D4zk97pvXew0RURRbCLulYUddV1Xbsui669rG0tr72svXd3bWDDLiKiiKCA9E4IJQnpycyc948wk5nJ1GSSKXw/1+Xl5NRfZibh5D7PPL+amj3qY8eGLUWSZDMcqqkjMAcAAADi2YffrZEkdbftkBS4HYu/PLw5heGpKdam7wwAAID4aMkSjKsVS1FRkXr06OFeXlRUpEGDBsVqWK3OYkuTJKUaDtXU2GM8GgAAAACePCfb9NTNujswt4fXv1zybqUSaVuVob3b6pAR3dS9Y05E+wEAAKBe3FeYDxo0SNnZ2Zo3b5572a5du7RkyRKNHDkyhiNrXa4Kc0mqramK4UgAAAAA+PrHQ980WhZsws9gOXgz5vyUxWLo76eP0MmHDohwTwAAAEgJUGGempqqadOm6d5771WbNm3UtWtX3XPPPerUqZMmT54c6+G1GsPW0MO9tnrP6d0OAAAAJIKVG0oaLWtvKfOY8DPXa12wVuNGcxJzAAAANEvcB+aSdNlll8lut+v6669XdXW1Ro0apWeeeabRRKDJzLDa5JQhi0zV7UGTnQIAAACJqpO1RJJU6MiTGeDDvf5zc4+WLCTmAAAArSruAvO77rqr0TKr1aoZM2ZoxowZMRhR/HAaNlnMOtlrqmM9FAAAAAAhdLSWSpK2OvIi2s+zwrw5E4ACAAAgcnHfwxwNnJb6Pub2WirMAQAAgHjX0bpLUvDA3GJpnIhbvCb9jP64AAAAEBiBeQJxBeZ1BOYAAABA3HNVmBcFCcwPGdFdXdtn65gDezcsJCQHAACImbhryYLATEv9y2XW1cZ4JAAAAACCM92BeWGQwDwjzabHr54ow7Oq3GO9QYk5AABAq6LCPIGY1voKc9NeF+ORAAAAAAgm31KpNMMuh2louzOn0XrTY7pP31CckBwAACB2CMwTiGlNrX9gp8IcAAAAiGcdLfXV5ducuXJG+GeX16Sf0RwUAAAAQiIwTyS7W7LIQYU5AAAAEM8a+pfn+l1vBInCvdaRmAMAALQqAvNEYttdYU5gDgAAAMQ1V2C+NUj/8oC88nIScwAAgNZEYJ5Idvcwl5OWLAAAAEA8CxWYe/YwD4Z25gAAAK2LwDyBGLsDc8Nhj/FIAAAAAATTrApzAAAAxAyBeSLZ3ZLFcFBhDgAAAMSrDKNGuZZqSc0PzA1KzAEAAFoVgXkCMVyBuZMKcwAAACBedbTukiSVODNVo5SI9/eMyMnLAQAAWheBeQKx7A7MLU4m/QQAAADiVRtLuSRpuyOn2cciLwcAAGhdBOYJxEhxBeZUmAMAAADxKt9SIam+wrzZKDEHAABoVQTmCcTiCsxNKswBAACAeJVvqZQUPDA3zcD7e2bkxOUAAACti8A8gbgCcysV5gAAAEDcygsjMA/G8IjJKTAHAABoXQTmCcSaklb/fxGYAwAAAPHKVWFe6swKuA1BOAAAQHwiME8g1tTdFeYmgTkAAAAQr8JpyRIug2QdAACgVRGYJxB3hTmBOQAAABCXLHIq16iS1PQe5gAAAIgdAvMEYkurD8xtcsR4JAAAAAD8yTGqZDFMOUxDZWZ6s49HfTkAAEDrIjBPIK4Kc5vscjgpSQEAAADiTUP/8kyZQf7cCtppxQjwGAAAAC2OwDyBuCrMUwyH7A5njEcDAAAAwFe4/cvDbclCD3MAAIDWRWCeQGypuwNzOWS3E5gDAAAA8cazwjwaiMsBAABaF4F5AkmhwhwAAACIa3mWCklSiRmlwJzEHAAAoFURmCcQS4orMLcTmAMAAABxqKElS1aTj+EdkpOYAwAAtCYC8wRi2FIl1bdkqaMlCwAAABAXTI+G5OH2MA8XFeYAAACti8A8gRgp9YG51TBVW1sb49EAAAAAkKS3v1jhfhz1wDwqRwEAAEC4CMwTiKvCXJLsNTUxHAkAAAAAlxdnLZUkGTKVF+VJPykxBwAAaF0E5gnEMzCvq66O4UgAAAAASNKcRZvdjzONGtmM+taJ0QrMicsBAABaF4F5AjEMQ3WySZLqagjMAQAAgFib+d0a9+NcS5UkqdyZJmcz/tQyPGJyCswBAABaF4F5grETmAMAAABxKduov0YvN9OjdkyDxBwAAKBVEZgnGIdRH5jba5j0EwAAAIgn2Zb6wLzM2czAnIwcAAAgZgjME4wrMHfUUmEOAAAAxJNcI0qBOQAAAGKGwDzBOC2uwLwmxiMBAAAA4MlVYd7cliwUmAMAAMQOgXmCcRopkiRHHYE5AAAAEE+yjfpJP8ucGTEeCQAAAJqKwDzBOC27A/NaepgDAAAA8STH1cM8ipN+AgAAoHURmCcY01ofmDupMAcAAADiiiswL6eHOQAAQMIiME8w7sDcToU5AAAAEE+yDSrMAQAAEh2BeYIxd7dkERXmAAAAQFyJVoW5YTDtJwAAQKwQmCcYw5YqSTKpMAcAAADihlUOpRl2SVKlmdasY7XNo0IdAAAgVmyxHgAiZKuvMDftdTEeCAAAAABXMXi60XB9Xm2mNOuYndpm6eqzRio3K7VZxwEAAEDkCMwTjGHbXa3ioMIcAAAAiDXTrP+/KzCvMW1yhvFBXtO1YwAH7tO12WMDAABA5GjJkmBcLVkMBxXmAAAAQLzIMOoLWqqaWV0OAACA2CIwTzCWlN0fyyQwBwAAAGLOtyVLtUkbFQAAgERGYJ5gXIG5xUlgDgAAAMSLSCvMDVfSDgAAgLhCYJ5grCn1PcwNAnMAAAAgbrgrzJ3hVZiH6mEOAACA2CAwTzCW1PrA3EpgDgAAAMQNepgDAAAkBwLzBGPbHZhbTHuMRwIAAADApaGHOYE5AABAIiMwTzBWV4U5gTkAAAAQN1wV5kz6CQAAkNgIzBNMSlq6JMlq0pIFAAAAiBeuCvMqAnMAAICERmCeYGxp9RXmNtMR45EAAAAAcEl3V5jTkgUAACCREZgnGFeFuU20ZAEAAADiRUaEFeam2ZKjAQAAQFMRmCeYlPTdFeayy+QqGwAAAIgL6RYqzAEAAJIBgXmCSUvPkCSlGg7V2Z0xHg0AAAAAKfIKcwAAAMQnAvMEk5K+uyWL4VR1DRN/AgAAAPHANelnjWkLa3vDaMnRAAAAoKkIzBOMq4e5JK3ZsD2GIwEAAADgkrp7jqGaMFuy0F0RAAAgPhGYJxhLapr78daiktgNBAAAAIBbqlEfmNcqvApzAAAAxCcC8wRjGBY5ZJUkOWprYjwaAAAAABY5ZTPq5xeqNa0xHg0AAACag8A8ATkt9VUrdTXVMR4JAAAAAFd1uSTVhtmSBQAAAPGJwDwBOY36i3B7DRXmAAAAQKy5+pc7TUN/P2N0jEcDAACA5iAwT0BO6+7AnJYsAAAAQMylefQvH9avXYxHAwAAgOYgME9ApqU+MHfUEZgDAAAAseZqyVJjMuEnAABAoiMwT0S7K8ydVJgDAAAAMecKzGsJzAEAABIegXkisqVKosIcAAAAiAeuHua1pk0yYjwYAAAANAuBeQIyXIE5k34CAAAAMUeFOQAAQPIgME9AttQ0SVJdTXWMRwIAAAAg1WPST4MScwAAgIRGYJ6ArGn1gbmdHuYAAABAzDVUmFtjPBIAAAA0F4F5AkpJS5fEpJ8AAABAPEjz6GFuUGAOAACQ0AjME5CrJYvFtMs0zRiPBgAAANizpdDDHAAAIGkQmCcgy+7APEUO2R3OGI8GAAAA2LPZjPprcrtoyQIAAJDoCMwTkKvCPMWwq85OYA4AAADEkk0OSZKdP68AAAASHld0Ccia0lBhTmAOAAAAxI5heFSYm1YZNDEHAABIaATmCciSmipJSjEIzAEAAIBYMk3JqvprckcEf15dcMJeLTUkAAAANAOBeQKy2AjMAQAAgHjhbslihtfDPNVmUd9u+S04IgAAADQVgXkCMlyBueyqsztiPBoAAABgz+XVkkUWhdORxWKhbQsAAEC8IjBPQIarhzkV5gAAAEDMWd0V5uH9eWW25GAAAADQLATmCcjwbMniIDAHAAAAYqmhwtwqascBAAASG4F5AnIF5qmya+PWshiPBgAAANiz2VyTfoZZYQ4AAID4xRVdArKkNFSYf794S4xHAwAAAOyZnE5Tv67YLpuxuyWLrAqriTkAAADiFoF5AvJsyZKVnhLj0QAAAAB7pm9+2ShJslJhDgAAkDS4oktA7sBcdjmcTBkEAAAAxMKazbskyavCnPpyAACAxEZgnoAMj5YstXWOGI8GAAAA2LO5epjbqTAHAABIeFzRJSBLSrokKdVwqLamLsajAQAAAPZsVmN3YC4LLcwBAAASHIF5ArKkZbofO+uqYjgSAAAAADbtbsliWmM8EgAAADQXgXkCMmwpMi22+i9qCcwBAACAWLLtrjB38OcVAABAwrM1Zad169Zp7ty52rhxo8rKylRQUKCuXbvqwAMPVOfOnaM9RviTkiHVlMmgwhwAAGCPxDV5/HBXmIsKcwAAgEQXUWA+e/ZsPfnkk1q8eLFM01Rubq4yMjK0a9cuVVVVyTAM7b333rrgggs0ceLElhozJCl1d2Bur471SAAAANCKuCaPP64Kcyb9BAAASHxhBeabNm3SNddcoz/++EOHH364rrzySu21117Kzs52b1NaWqqffvpJ33zzjWbMmKEBAwbo7rvvVvfu3Vts8HsyIy1TKpMsBOYAAAB7BK7J45d1d4W5QxYZhqHzpgzVM+//HuNRAQAAoCnCCszPOOMM/fnPf9bTTz+t1NRUv9vk5eXp0EMP1aGHHqqrr75ar7zyiqZNm6avv/46qgNGPUtqhpySrI6aWA8FAAAArYBr8nhlKsVdYV7fkuX4Cf30/aItWrp2ZywHBgAAgCYIKzD/73//qzZt2oR90MzMTJ1//vk68cQTmzwwBGdNz5JTks1ZI6fTlMVixHpIAAAAaEFck8cni0z3Y7usCueq3DRDbwMAAIDYCKvJXiQX5tHYD6HZ0jMlSelGrWrtjhiPBgAAAC2Na/L45JrwU6KHOQAAQDKIaNJPl507d+qZZ57R999/r23btunpp5/W7NmzNWjQIE2aNCnaY4QftowsSVK6UafaOqfS/X8qFwAAAEmKa/L44JrwU6rvYR5WiTkAAADiVsQlEBs2bNCUKVP05ptvqmPHjtqxY4ccDofWrFmjyy67TF999VULDBO+rLsrzDOMWtXUUmEOAACwJ+GaPH5YVR+YO03JGfmfVwAAAIgzEVeY//vf/1bbtm310ksvKTMzU8OGDZMk3XfffaqpqdETTzyhgw8+ONrjhA9LmqslS51q6uwxHg0AAABaE9fk8cPiCswJywEAAJJCxFd1c+fO1UUXXaTc3FwZhvfnDU899VStWLEiaoNDYF6BORXmAAAAexSuyeOH1aifwdO5uxeL7+sBAACAxNKkMgir1ep3eW1tLReIrcS7wpzAHAAAYE/DNXl8cFWYO5jwEwAAIClEfFU3cuRIPfnkk6qsrHQvMwxDTqdTr732moYPHx7VAcI/S1qGJHqYAwAA7Im4Jo8f7h7mrgrzWA4GAAAAzRZxD/Pp06dr6tSpOuywwzRmzBgZhqFnnnlGq1at0rp16/Tqq6+2xDjhw7PCvJYKcwAAgD0K1+Txw7K7JYuDHuYAAABJIeKrugEDBujtt9/WmDFjNG/ePFmtVn3//ffq0aOHXn/9dQ0ePLglxgkftGQBAADYc3FNHj98J/2kGw4AAEBii7jCXJJ69+6t++67L9pjQQSM3S1Z0o1aFdGSBQAAYI/DNXl8sGp3hblJUg4AAJAMwgrM58+fH9FBR40a1aTBIHyWtCxJUorhVG1NTYxHAwAAgJbGNXl8shjeFeZ0MQcAAEhsYQXmZ555pgyfzxaapun1tWEYMk1ThmFo6dKl0Rsh/LKkprsf26sqYjgSAAAAtAauyeOTa9JPepgDAAAkh7AC8xdffLGlx4EIGRar7EaqbGatHNWVsR4OAAAAWhjX5PHJ1ZLFubslCz3MAQAAEltYgfno0aNbehxoArstXba6Wv3wyyr96bhxsR4OAAAAWhDX5PHJ1ZIlogpzn08GAAAAIH40adLPhQsX6scff1RdXZ37Y6CmaaqyslI///yz3nzzzagOEv7ZrZlS3S6Z1eUqKatRfk5arIcEAACAVsI1eXxwV5jTuxwAACApRByYv/LKK7rtttsa9UuUJIvFogMPPDAqA0NotbZMSVK2UaOqGjuBOQAAwB6Ca/L4YXH1MDfrK8yJzQEAABJbxDPTvPzyyzrwwAM1b948nXfeeTrllFO0cOFCPfjgg0pLS9OUKVNaYpzwo9ZSH5hnGTWqtTtiPBoAAAC0Fq7J44fFXWHOpJ8AAADJIOKruo0bN2ratGnKy8vTXnvtpZ9//lnp6ek6/PDDdcEFFzAZUSuqsWRIkrIs1aqqscd4NAAAAGgtXJPHD+vuHubulizM+gkAAJDQIg7MU1JSlJ6eLknq1auX1q1bp7q6OknS8OHDtXbt2qgOEIG5A3OjRtUE5gAAAHsMrsnjh29LFgAAACS2iK/qBg8erC+//FKS1LNnTzmdTi1cuFCSVFhYGNXBIbjU7DxJUpalRlU1tGQBAADYU3BNHj+s8q4wp74cAAAgsUU86ec555yjSy65RKWlpbrzzjt16KGH6qqrrtLhhx+uDz74QCNGjGiJccKP/fbureL1UrZRrepaKswBAAD2FFyTxw+LUd/D3EEPcwAAgKQQ8VXdpEmT9MQTT6hfv36SpFtvvVW9e/fW66+/rj59+ujGG2+M+iDhX3peG0lSjqWaliwAAAB7EK7J44erwtwVmNPCHAAAILFFXGEuSQcffLDGjRsnSSooKNAjjzyi2tpa5efnR3NsCMGaXSBJyjWqdOc7izRycCe1L8iI8agAAADQGrgmjw9W1VeYO02ScgAAgGQQcYV5bW2trr/+ep1yyinuZQsXLtSBBx6o22+/XQ4HvbRbizUrX5KUaamVTQ6de9unsR0QAAAAWgXX5PHDYvhWmBOcAwAAJLKIA/OHHnpIs2bN0vHHH+9eNnToUF199dX63//+p6eeeiqa40MQlvQsOQyrJCnXUhXj0QAAAKC1cE0ePyyuCnOm+wQAAEgKEQfmM2fO1NVXX62zzz7bvSwvL09nnnmmrrzySr3zzjtRHSACMwxDVUaWJCnHIDAHAADYU3BNHj9cPcydJpN+AgAAJIOIr+qKi4vVrVs3v+t69+6trVu3NntQCF+5kSlJyqPCHAAAYI/BNXn88G3JAgAAgMQW8VVd37599cknn/hd99lnn6lnz57NHhTC175zJ0lSDoE5AADAHoNr8vhhbUJLFrOlBgMAAIBms0W6w7nnnqvp06erpKREkyZNUtu2bbVz507Nnj1bn376qe68886WGCcCKOjQUbs20sMcAABgT8I1efyw0JIFAAAgqUQcmB999NEqKyvTI488ok8//dS9vKCgQDfccIPXxENoedbsfElSLj3MAQAA9hhck8cPV4W5g0k/AQAAkkLEgbkknXbaaTr11FO1Zs0alZSUKDc3V3369JHFQlVFa7NmF0iiwhwAAGBPwzV5fKCHOQAAQHJp8lWdYRjq06eP+vbtq6qqKlVUVERzXAiTNStfUn1gbrNS1QIAALAn4Zo89qyulixUmAMAACSFsAPzRYsW6W9/+5veffdd97KXXnpJBx10kE455RSNHz9ezzzzTEuMEUF4Vpg7nEwfBAAAkMy4Jo8/FldLFnqYAwAAJIWwruqWLl2qadOmadmyZcrMzJRUf7F+xx13qEePHnr44Yd10UUX6f7779fs2bNbdMDwZtsdmOcYVTJMp5xOU9uKq2SahOcAAADJhGvy+GQ1XBXmBOYAAADJIKwe5k8++aQGDx6s559/XhkZGZLqK1kk6Z577tGgQYMkSdu3b9dLL72kSZMmtdBw4cuaUyBZU2R11KnAUqH/frVSL8xcotMPH6Sphw2M9fAAAAAQJVyTxyd3hTmBOQAAQFII66pu/vz5OvPMM90X5pL03XffqXv37u4Lc0k68MADtWTJkuiPEgEZhkXWvA6SpHaWMr0ws/75f/WTZbEcFgAAAKKMa/L4ZHH1MDfpYQ4AAJAMwgrMS0pK1KlTJ/fXq1atUnFxscaMGeO1XUZGhmpra6M7QoRkK+goSWpnLYvxSAAAANBSuCaPTw0tWQjMAQAAkkFYgXl+fr62b9/u/vqHH36QYRjaf//9vbZbtWqV2rRpE90RIqSU/Po/nNpaCMwBAACSFdfk8ckVk9PDHAAAIDmEdVU3evRovfHGG3I6nbLb7XrnnXeUlpam8ePHu7epra3VK6+8ouHDh7fYYOFfapv6wLw9FeYAAABJi2vy+ORqyWLGeBwAAACIjrAm/bzwwgt16qmnuicO2rx5sy6++GLl5ORIkt555x298sorWrNmje6+++6WGy38Sm3TWVJ9D3MAAAAkJ67J409JeY3a7n5s0sMcAAAgKYQVmPfv319vvvmmnn32We3YsUPnn3++pk6d6l7/wAMPyGaz6dFHH9XgwYNbbLDwz1awuyWLtVz1tS1crAMAACQbrsnjzxc/bVC/7PracnqYAwAAJIewAnNJ6tevn+644w6/695++221b99eFgt9+2IhJb+9nKahNMOuHKNaZWZGrIcEAACAFsA1efwxdjdjMSMIzE36twAAAMStqFxNd+zYkQvzGDKsKSp2ZkqS2u3uY261UOECAACwJ+GaPDYMI/LAHAAAAPGLK+okscNZ37vS1ce8W4fsWA4HAAAA2CNY3BXmAAAASAYE5klimyNXktTBukuSVFvnjOVwAAAAgD2Cq66cHuYAAADJgcA8SZj5XSRJna3FkqSqWnsshwMAAADsEZrSwxwAAADxi8A8SZz4p4MlSd1SSiVJVTUE5gAAAEBLcwfmJoE5AABAMrBFusP8+fMDrjMMQ1lZWerevbuys+mh3ZrSO/aUJBUYZUpVnWpqJYfDqV9XbFeX9lnq1DYrxiMEAABAtHBNHj8sVJgDAAAklYgD8zPPPFOG0XAxaJqm19eSZLFYdPzxx+vWW2+V1Wpt/igRkjUzV9asfDkqStTZWqJ1jvb6Zfk23fL0D5KkD+47LsYjBAAAQLRwTR4/DKM+MGcGIQAAgOQQcUuWxx9/XGlpaTrllFP04osv6qOPPtJLL72kadOmyWaz6ZprrtG1116rTz/9VE8++WRLjBkBpHboIamhLcuPSwrd60zTjMmYAAAAEH2Jek2+detWTZw4MdbDiCrXbQoqzAEAAJJDxBXmTz31lKZOnaqrr77avax3794aOXKkMjMz9dlnn+mll16S0+nUyy+/rAsvvDCqA0ZgKe17qGrNInVNLZWqparqhj7mldV2ZWWkxHB0AAAAiJZEvCafO3eubrnlFm3bti3WQ4kqWrIAAAAkl4grzH///XeNHz/e77oxY8bo119/lSQNHjxYW7Zsad7oEJHUdt0lSZ0sJZKk0vIa97risupYDAkAAAAtIBGvyd955x098MADsR5G1BkRBOaDe7WRJB13UJ8WHRMAAACaLuIK8/bt22vevHk64IADGq2bN2+e2rZtK0kqLi5Wbm5u80eIsKW2rw/M2xvFkqTS8lr3ujo7XRUBAACSRSJek997772xHkKLcAXmTjN0YD79jBHaVlzpDs4BAAAQfyIOzKdOnar77rtPVVVVOvzww9W2bVvt2LFDn332mV5++WVdcsklKiws1OOPP64xY8a0xJgRQGq7bpKkXKNSGUatVm8uda8jMAcAAEge8XpN/v777zeqIp80aZKuvfbaVhtDa2toyRJaqs2iYX3bteyAAAAA0CwRB+bnnXeeqqqq9PTTT+ull16SVD+hZE5Oji699FJdcMEFevfdd1VbW6u///3vUR8wArOkZ8ma01aOsh3qZC3RGnsH9zoCcwAAgOQRr9fkU6ZM0ZQpU1rtfPGAST8BAACSS8SBuSRdcsklOu+887Rw4ULt3LlTHTt21ODBg5WVlSVJOvbYY3X88cdHc5wIU2r77qryE5jbHQTmAAAAyYRr8vhgGEz6CQAAkEwinvTTZfPmzVq7dq02bdqkNWvWaOvWre51Vqs1KoND5FLb95AkdbaWeC0nMAcAAEg+XJPHXiSTfgIAACD+RVxhbpqmbrrpJr311lsyzYZOfYZh6IQTTtDtt98uw+BiMVZcE3928gnMackCAACQPFrjmvyxxx7T3Llz3S1fJMnpdOqRRx7RW2+9pV27dmnEiBG66aab1LNnz7CPu3jx4maNyzRNVVZWNusYkaiqqvL6vy/3pJ+7A3PX2BxOh99jpdmS87q8urra/biurq5VX6NEFOp9hQY8V+HjuQofz1V4eJ7Cx3MVvlg9V6Zphn19HHFg/vTTT+udd97RZZddpilTpqh9+/YqKirSe++9p8cff1z9+/fXOeecE/GgER0puyvMu1qLVT/1UP0b4bVP/1DPTrnq3C4rdoMDAABAVLT0Nfnzzz+vhx56SKNGjfJa/thjj+n111/XnXfeqY4dO+qee+7R+eefrw8//FCpqanN/bbCUldXp6VLl7bKuTytXbvW73KLT4W5a2xVlY3/CFy+YoVyMpKz8n/Tjlr34+KS4pi8Roko0PsKjfFchY/nKnw8V+HheQofz1X4YvFchXu9GnFg/vbbb+svf/mLLrzwQveybt266eKLL1ZdXZ3eeustAvMYSuvQU3bTomxLjdpYyrXTmSNJWr2pVJf/35d6845jYjxCAAAANFdLXZNv3bpV1113nX7++Wf17t3ba11tba2effZZzZgxQxMmTJAk3X///Ro/frw+++wzHX300c37psKUkpKifv36tcq5pPrqp7Vr16pXr17KyMjwWbuxYdLP3YX+gwcPliRlfFcmqdZr6wH9+ys/J61FxxsrKRtLJRVJkgryCzR48IDYDijOBX9fwRPPVfh4rsLHcxUenqfw8VyFL1bP1cqVK8PeNuLAfMuWLRo7dqzfdWPGjNGzzz4b6SERRYYtRZscBepp26Getu3aWZvjXldV0/hjoQAAAEg8LXVN/vvvvysvL0/vv/++Hn30UW3atMm9btmyZaqoqPA6b25uroYMGaL58+e3WmBuGIYyMzNb5VyeMjIy/J7Xt4e5axurpXElef0x0ltwlLGTnl7jfpySkhKT1ygRBXpfoTGeq/DxXIWP5yo8PE/h47kKX2s/V5G0K4x40s+uXbtq2bJlftctWbJEbdq0ifSQiLJ19naSpJ7WHerWITvGowEAAEC0tdQ1+cSJE3Xfffepe/fujdYVFhZKkjp37uy1vEOHDtqyZUuTzpcMfHuYAwAAILFFHJgfc8wxevjhhzVz5kw5nfUT1jidTn344Yd69NFHddRRR0V9kIjM+t2BeQ/bdqWlJmePRAAAgD1ZLK7JXRMz+fZ+TEtLU01Njb9d9giG4aowBwAAQDKIuCXL+eefr59++knTp0/X1Vdfrfz8fJWUlMjhcGj06NG6/PLLW2KciICrwry7bYeqKr37Jq7aWCJTUr9u+a0/MAAAAERFLK7J09PrW4nU1ta6H0tSTU3NHt2rs2HSz4hrkQAAABCHIg7MU1NT9dxzz+nrr7/W/PnzVVpaqry8PI0aNco9+Q9i64KzJ6nyf7OUaanTiE612ryzYd0V938tSXrrzqOVnhrxyw8AAIA4EItrclcrlqKiIvXo0cO9vKioSIMGDWqRcyYC96SfMR0FAAAAoqXJiemECRMaXYxv3bpV69ev16hRo5o9MDTd6GFdtGXxUFWtXqgj+zn0wZLG21TXOAjMAQAAElxrXpMPGjRI2dnZmjdvnjsw37Vrl5YsWaJp06ZF9VyJxN3D3KSHOQAAQDKI6ucGP/74Y5111lnRPCSaKL3rQElSWslav+tNkxoYAACAZNRS1+SpqamaNm2a7r33Xn3++edatmyZrrzySnXq1EmTJ0+O+vkSheFuyUJgDgAAkAwoMU5Sad3qA/Pqjcsk9Wq0vs7hbN0BAQAAIOFddtllstvtuv7661VdXa1Ro0bpmWeeaTQR6J6EwBwAACC5EJgnqfSu/SUZspcUKceoUpnpPRGTncAcAAAAQdx1112NllmtVs2YMUMzZsyIwYjik2V3Ts7nNwEAAJIDU7knKUtaplI7dJck9bZta7T+hie+1+pNpa09LAAAACCpuHuYU2EOAACQFAjMk1ja7j7mvfwE5kXFVbrxye9be0gAAABAUqElCwAAQHIJqyXLu+++G9bBFi1a1JyxIMrSuw5Q2S+fqbetyO/60vLaVh4RAAAAmopr8vhkoRkLAABAUgkrMP/nP/8Z9gENg8qKeJHWbZAkqbtth6xyyCFrjEcEAACApuKaPD65W7KYPOcAAADJIKzA/PPPP2/pcaAFpLTpLEtGjlKqytTVWqz1jnZe61NtdOQBAABIFFyTxzsCcwAAgGQQVmDetWvXlh4HWoBhGErvOkCVK39Wb1tRo8C81u6M0cgAAAAQKa7J45MrJqcxCwAAQHIIq8T4jDPO0NKlSyM68OLFizV16tQmDQrRk9Yt8MSfVgtVMAAAAImCa/L4ZDGY9BMAACCZhFVhftZZZ+kvf/mLhg0bpilTpmjixInKyMhotF15ebm+++47vfHGG1qyZIluuummqA8YkUnvOkCS1NtPYO5wmpqzaLPG7d2ltYcFAACACHFNDgAAALS8sALzww8/XKNGjdJjjz2m66+/Xna7Xf369VO3bt2UkZGhXbt2qbCwUCtWrJDNZtPJJ5+se+65R+3atQt9cLSotC79JMOiAmul8i0VKnFmea1//sPfCcwBAAASANfk8cj08wgAAACJLKzAXJLatGmj66+/XhdffLE++eQTzZs3Txs2bFBZWZkKCgrUt29fnXXWWTrkkENUUFDQkmNGBCypGUrt0FO1W9doSFaxvi/zDsyrax0yTVOGwUdIAQAA4h3X5PHF8wqaliwAAADJIezA3KWgoECnnXaaTjvttJYYD1pAereBqt26Rn/ZP0NLvk9TSXmNe11JWY1e+XiZph05OIYjBAAAQCS4Jo8Phk+F+cSR3WM3GAAAAERFWJN+IrG5Jv6s3bxcZx7VOBh/Y/ZyFe6oaO1hAQAAAEnEUNu89FgPAgAAAM1EYL4HSN8dmNcUrtah+3bQtCMHNdrm/Dtma2NRWWsPDQAAAEhY3hXmtGQBAABIBgTmewBbXgdZc9tJTodqNi1Xr065fre78N9faPn64lYeHQAAAJD4mPQTAAAgORCY7wEMw1BGz2GSpOp1i2W1Bn7Z3/tmlUzTlGlyyQ8AAAAEY/GsMOfyGQAAICkQmO8hMnrvJUmqXP2rqmvtAbdLsVl0zWNzdMN/vic0BwAAAMJGSxYAAIBkYGvKTrW1tXr77bf1/fffa9u2bbrjjjv0448/aujQodp7772jPUZEQUbvfSVJtYWrZa0tD7hdeWWdfl+9Q5JUUVWn7MzU1hgeAAAAIsQ1eex59zAHAABAMoi4wnznzp068cQTdfvtt2vdunVatGiRqqur9fXXX+vMM8/UL7/80hLjRDPZsvOV2qGHJGlo5vaA2837vdD9uM7ubPFxAQAAIHJck8cfJv0EAABIDhEH5nfffbcqKio0a9Ys/e9//3O37XjwwQe111576aGHHor6IBEd6T3q+5jXblii0w8bGHL7979drZc/WtrSwwIAAECEuCaPDwZ15QAAAEkn4sD8yy+/1OWXX66ePXvKMBqqKNLS0nTuuefq999/j+oAET3pPYdIkqrW/y6LNXQFzNtfrNAbs5drzebSlh4aAAAAIsA1eXzweOqpMAcAAEgSEQfmNTU1ys/P97vOarWqrq6uuWNCC8noXh+Y123boFR7Rdj7VVYHniQUAAAArY9r8vjgGZFTaw4AAJAcIg7M99prL7366qt+133wwQcaNmxYsweFlmHNylNK++6SpNyyteHvF0Y1OgAAAFoP1+TxwnPST0MmqTkAAEDCs0W6w+WXX64///nPOu644zRhwgQZhqEPP/xQDz/8sL777js9/fTTLTFORElm731Uum2D8kqWSRoa1j4Wg8AcAAAgnnBNHh/oYQ4AAJB8Iq4wHzlypJ577jllZGTo6aeflmmaev7557Vt2zb95z//0dixY1tinIiSzP4jJUm5xctkyBnWPq9+sqwlhwQAAIAIcU0eH2jJAgAAkHwirjCXpFGjRun1119XdXW1SktLlZ2draysrGiPDS0gvftgWdIyZaupUE/bdq21dwi5z8/LiuR0mvr4h7X67Mf1uum8scrPSWuF0QIAACAQrsnjC5N+AgAAJIeIK8w9paenq2PHjlyYJxDDalNG3/0kSUNTNoW938sfL9Xj7yzSyg0l+t9XK1tqeAAAAIgQ1+Sx492ShcAcAAAgGURcYT5o0CAZIXpaL126tMkDQsvL7DdCFUvmaGjKRs2s2i+sfd76fIX7cbCPm1bX2LWtpErdO+Y0c5QAAAAIhGvy+OD7CpjM+gkAAJDwIg7ML7744kYX5xUVFVqwYIHWr1+vf/zjH1EbHFpGZt/9ZMpQV1uxCizlKnZmR7R/Vkbgt80V93+lTdsqdPuFB2jvfu2bO1QAAAD4wTV5fHBVmDtNqssBAACSRcSB+aWXXhpw3dVXX63ffvtNJ554YrMGhZZlzczVzvSualu9UcNSNurbmkER7Z+dkRpw3aZtFZKkrxdsIjAHAABoIVyTxxdXXXmoqn8AAADEv2b1MPd1/PHHa9asWdE8JFqIs/twSdLw1DUR72u1GPrljyKVVdYG3IaPowIAAMQG1+Sth5YsAAAAySeqgfnatWtlt9ujeUi0kH0PO0qmDPVJ2aapB7SVJF115ki99q8j9dhVE4PuO+v7Nbrxybn6x4PftMZQAQAAEAGuyVuPYdQH5CYTfgIAACSNiFuyPPLII42WOZ1ObdmyRbNmzdLEicHDVsSHtPz2yug5VNXrftORnYp01M1HKz8nTZKUnZmqrIwUVVTV+d13zeZdkqTN2ysCHp/iGgAAgJbDNXl8cPUw59IXAAAgeUQlMJek7OxsTZ48Wddcc02zB4XWkT10vKrX/aaKJd+q27g/ea3bq29b/fBbYZOPbfJnAwAAQIvhmjy+UGEOAACQPCIOzJctW9YS40AMZA0aq+2fPKXaovWq2bpWaR17udedN2VY8wJzP3n55m3l+viHdTphQl8V5KY3+dgAAAB7Oq7J4wMxOQAAQPKJag9zJBZrRrYy+42QJJUv/sprXae2WWEdY86ize7HTqfp9fiqh7/V7c/Ncy/7x0Pf6H9frdT9ry1o+qABAACAONHQkoXoHAAAIFmEVWF+1llnhX1AwzD0wgsvNHlAaF05ex+iyj/mqfy3b9Rm4pkyLNaI9r/rhfn64L7jtKO0Spf/31fu5Wu37NLaLfW9zh0Op6xWi8oq63uiL19fHLXxAwAA7Cm4Jo9fNCMEAABIHmFVmJumGfZ/TqezpceMKMrsu58smblyVJSqas2iJh/nv1+tVGl5rftrh8f7oKbO4bWtzcYHGwAAACLFNXn8MdxRORXmAAAAySKsCvOXXnqppceBGDGsNmUPGaddP32k8sVfK7Pvfk07kE9ZjcPRsOB/X63SGUcMcn9tsxKYAwAARIpr8vjjisn9zd8DAACAxBTV5LKyslLffPNNNA+JVpA97CBJUsUf8+SoKmvaQXyKaqpq7O7Hr3/2h9c6AnMAAICWwzV566GHOQAAQPIJq8Lc06ZNm3TjjTdq/vz5qqur87vN0qVLmz0wtJ60Lv2V2qGHaovWa9eCT1Uw7sSI9l+xoVjvf7Paa1lxWU3A7VNoyQIAANAsXJPHFwrMAQAAkkfEyeWdd96pX375RaeccooGDx6s4cOH69xzz9XAgQNlGIYeeeSRlhgnWpBhGMobe5wkadf8WXLaa0Ps4e3vD0RWwUSFOQAAQPNwTR4fDKJyAACApBNxcjl//nxdccUVuv7663XiiScqNTVVM2bM0DvvvKNRo0bp888/b4lxooVlDzlQ1py2clSUqHxxy36El0k/AQAAmodr8vjiaslCL3MAAIDEF3FyWVFRocGDB0uS+vbt6/6op9Vq1RlnnKEffvghuiNEqzCsNuWNPkaSVDrvPZmmM6rHNz3+ekihwhwAAKBZuCaPDxaDHuYAAADJJuLkskOHDtq2bZskqWfPniotLVVRUZEkKS8vTzt27IjuCNFqcvebJCMtU3U7Nqtyxc9RPbbD6RGYU2EOAADQLFyTAwAAAC0j4uRywoQJevDBB7VgwQJ17txZnTp10rPPPqvy8nK988476tixY0uME63Akpap3OGHSZJKf3gvqseuszdUrNPDHAAAoHm4Jo8PrrpyOrEAAAAkj4iTy8suu0y5ubl66KGHJElXXnmlXnzxRY0aNUoffPCBzjnnnKgPEq0nb9TRksWm6g1LdXR/R9SOW1vXcCwCcwAAgObhmjxe0JIFAAAg2djC2ejUU0/VSSedpKOPPloFBQV666233B/5nDJlirp06aKFCxdq77331ujRo1t0wGhZtpw2yh52kMoXfaHj2q7QEcddqEvv/bLZx62qsTecw2aozu5QZbVdedlpmr+kUJu2lev4Cf2afR4AAIBkxTV5/DHcgTkAAACSRViBeXV1tW644QbdeeedOuqoo3TSSSdp3333da8fOXKkRo4c2VJjRCvLHztF5Yu+UNXy+eo28Ux1KMhQUXFVs4555f1fux/brBZd+O8vtHVnpbp3zNaGreWSpAE9CjSkd9tmnQcAACBZcU0efww/jwAAAJDYwuqN8d577+ndd9/VKaecoq+++kpTp07VMccco+eff147d+5s6TGilaW2767MfiMkmSr94T3dfuE4TTmoj6YdMajRtuccMySsY5ZX1bkfO52mtu6slCR3WC5JxWU17scOR0PP8zp79FrDAAAAJCquyeOXubvEnEpzAACAxBd2M+lBgwbpn//8p7755hs98cQT6tevn+6//35NmDBBl19+uebMmdOS40Qry9v/eElS2a9fqKC2UOcft5faF2S61z80/WA9dtVEZaSnRHzsHaXVfpdbjPrKnDc++0OnXT9L67bs0rMf/K4/Xf2hVm4sifg8AAAAyYZr8vhiEJEDAAAknYhnX7RYLJowYYIeeOABzZkzR9dff722bdumv/zlL5o4caIeeeSRlhgnWllGjyHKGjJOMp3aNusJmU6HLB6fNO3dJU/dO+YoOyPywPyP9cV+l1ut9Sd4+eNlqq516OE3F+p/X62sX/bR0si/CQAAgCTFNXl8cF0eO2nJAgAAkDQiDsw9ZWdn69RTT9Wrr76qF198UampqXr00UejNbaouPfee3X00UfrmGOO0eeffx7r4SSUtpPPkSU9S7WFq1U6f6YMo/EfArlZqREf1+n0X4lj8Tm+Z7Du79wAAABIjGvyZNVQYc61KgAAQLIIa9LPQLZu3aqZM2fqgw8+0LJly9S1a1ddeuml0Rpbs33//fdaunSpPvjgA5WUlOioo47SuHHjlJ6eHuuhJQRbdoHaTDxT22c9oeKvX9eAU/dttE37goyonc9qMbRmc6nfdb5herLYVlylpWt3aNw+XWW1JOf3CAAAWla8X5PvCWjMAgAAkDwiDszLy8v1ySef6IMPPtD8+fNls9k0adIkXXXVVdp///1bYoxNdsABB2j06NGyWCzavn270tLSZLVaYz2shJKz76EqX/y1qjcslfHDy3p0xiXKy05zr+/SLlvTjhiklz9e1uxzWa2GLrvvK7/rflxSqIqqOmU1oQVMPPvrnZ/J7jBVVlmno8f1jvVwAABAgkika/Jk5qrpMHdXmJsm0TkAAECiCyswt9vt+vrrr/X+++/rq6++Uk1NjYYMGaJrr71WU6ZMUU5OTkuPs8lsNpvuvPNOvfLKK/rb3/6mlJTkClxbmmFY1O6ov2njU9NVufJnddhrsbI7jfPa5tTJAwMG5uP26aINW8u0vrAs9LlCfJT1tOtnafLoHrr0lH2TpkWL3VH/R9WvK7YRmAMAgKAS+Zo8WTHpJwAAQPIJKzAfN26cdu3apdzcXJ188sk66aSTNGjQoJYeW9jef/99PfDAA17LJk2apGuvvVaSdM011+jCCy/UtGnTNGrUKI0ZMyYGo0xcqe26Kf+AE1Ty3Vva8emzyui9j6wZ2X63HdyrjZau3en++uKT9lFZRa0uuCt0//hAvc09ffbjen3580a9cNPhTeqfDgAAkKji/Zp8T+QKzInNAQAAkkdYgfnQoUN10kknadKkSUpNjb+QcsqUKZoyZUqj5atWrZLdbtfAgQOVn5+v8ePHa/ny5QTmTZA/7k+qWDpHdTs2a+eXL6v9UX/zu92Jh/RTdmaqXv54qS44YW/lZKbK4QjvTwhHmB9htTuc+uSHtTr50AHhHddpxn1/8CQpmAcAAC0o3q/J92Qmk34CAAAkDUs4Gz377LM66qijEu7CfN26dbrttttkt9tVXl6uOXPmaL/99ov1sBKSxZaqdkfWh+Rlv3ymqvW/e62/5uxROmlif40e2klD+7TVnRcdqF6dcyVJ+TlpuvG8MTp2fJ+g5winwtwlUEuW8so6r96RxbuqNe3Gj/T4O7+GfexYCNWOBgAAIFGvyZOZ6wqOwBwAACB5hBWYJ6qJEydq33331XHHHaepU6fqjDPO0LBhw2I9rISV0XOocvY5VJK07b2H5KiucK87YO8uOvvoIQGD7FFDOmlYn7ZBj3/L0z+EPZYUW+O37oZtNTrvzq/0f68tcC/74LvVKq+q06zv14Z97JjgbywAAICEQw9zAACA5BNWS5bW9Nhjj2nu3Ll66aWX3MucTqceeeQRvfXWW9q1a5dGjBihm266ST179gx5vOnTp2v69OlRGZtpmqqsrIzKscJRVVXl9f94kDHuVFWuXSx7aZEK339Y+UdfGvYEnHV1tVEbh+m0e70WVVVV+m5J/cSiX/28URceP1iSVFtb596mNV+7SDkdjlYbXzy+r+IVz1X4eK7Cx3MVHp6n8PFchS9Wz5VpmkkzYTn8C7OzIAAAABJAXAXmzz//vB566CGNGjXKa/ljjz2m119/XXfeeac6duyoe+65R+eff74+/PDDVv1Ial1dnZYuXdpq53NZu3Ztq58zGOuQo5Tzw4uqWTFfqz97TbXdw2tzs2tn9ALzbUVbtXRpudey8mqH+/HSpUvldJravr3Ua1m8Kivb1erji7f3VTzjuQofz1X4eK7Cw/MUPp6r8MXiuaKNSnJqqDDnhggAAECyiIvAfOvWrbruuuv0888/q3fv3l7ramtr9eyzz2rGjBmaMGGCJOn+++/X+PHj9dlnn+noo49utXGmpKSoX79+rXa+qqoqrV27Vr169VJGRkarnTe0waqwVavs29eVtWy2egw/SCntuoXca4DDqSc//jwqI+jRrasGD+7s/rqyslKbdmx0fz1w4CBd/dgPWr+1IVQfPHhwVM4dXfVjzsvNa7Xxxe/7Kv7wXIWP5yp8PFfh4XkKH89V+GL1XK1cubLVzoXW1dDDHAAAAMkiLgLz33//XXl5eXr//ff16KOPatOmTe51y5YtU0VFhcaOHetelpubqyFDhmj+/PmtGpgbhqHMzMxWO59LRkZGTM4bTMb4E2XftExVqxdq18ePq+s5d8mSkhZyvxdvPlzfLdysDgUZuu25H5t8/rT0NK/nZFuJ90erb3j6J6+wXFLcPYeebDZbq48vHt9X8YrnKnw8V+HjuQoPz1P4eK7C19rPFe1YkperwpxJPwEAAJJHXEz6OXHiRN13333q3r17o3WFhYWSpM6dO3st79Chg7Zs2dIq40NjhmFR+2MvlTUrX3Xb1mvn7BfC2q8gJ13Hju+j0UM7qSAndMAeiNPp9Pp6607vwHzlhpImHzsW+DsaAAAgcVFhDgAAkDziIjAPxjUpk2/fx7S0NNXU1MRiSNjNlp2v9lMulSTtWvCJKpb9EPa+hmHo6esmy2JpWlLscHj/WVJndwbYMkEQmAMAAAAAAAAxF/eBeXp6uqT6Xuaeampq6NMZBzL77Ku8/Y+XJG2b+Zhqi9aHvW9qilUptqa9BR1O78Dc3ozAvKSsRqYZ27og8nIAAIDEYxje15AxvqQEAABAFMR9YO5qxVJUVOS1vKioSJ06dYrFkOCjzYSpSus6UM7qCm195x45a6tC77Sb09m0vyoefftX2R0NIbnn41Bq6xxaX7hLD7+5ULc/N09n3vyxXvpoacDtq2vtuvieL/TEfxc1aazhoLcpAABA4qKHOQAAQPKI+8B80KBBys7O1rx589zLdu3apSVLlmjkyJExHBlcDKtNnU75p6w5bVS3c7O2zXw87IrtQIF5u7x0XX7qfvrr8XsF3HdTUcOknnURBOZXPfKtLr7nS306b51++K2+R/5bn68IuP13CzdrfWGZZs5ZE/Y5AAAAAAAAACSeuA/MU1NTNW3aNN177736/PPPtWzZMl155ZXq1KmTJk+eHOvhYTdrZq46njBdslhVsWSOSn/8MKz9nAGC9eduPFyTRvdQRpo14L6X3PulthXXV7NH0sN81cbSsLeVJIez5fujU2AOAAAAAAAAxF7cB+aSdNlll+mkk07S9ddfr6lTp8pqteqZZ55pNBEoYiu9+yC1nXS2JGnn7BdUtvirkPuEKkTPSEsJuv6tz5dLkuyO0BXtT767WOsKd4XcLpgn/rtIy9cXey2rqXM065iSZCExBwAASDhcwQEAACQfW6wH4Ouuu+5qtMxqtWrGjBmaMWNGDEaESOSOPEq12zao7JfPtG3m40pp00XpXQcE3N5qMRpN4Ol1vOzgN0XWby3Tgj+Kwqow/+Db1fp47tqQ2/nyDPVnzlmjmXPW6IP7jpMkvf3FCr0wc4muP2e0xgzrHPGxAQAAkPjoYQ4AAJA8EqLCHInDMAy1O/Kvyhw4RnLYtX3WEzIddQG3v/WC/YMeb2jvtirISQu4/vfVO3TTk3O1fmt5wG08BQvWt+6sbLSscEdFwN7lZZW1emHmEknS/a8tCHreTdvK9cG3qwOen0k/AQAAEo+hpk1gDwAAgPhFYI6oMwyL2h/1NxlpmaotWqdts/4TcBLQvfu1b7Ssf/d892OLxdAtfw0eqkvSwuXbmzxel5uenKtbnv5Br3/2h3vZZfd9qbVb/LdxmfPrZvfjFFvgXuuS9Le7PteT7y7Wf7/yP7koeTkAAEDiMwnQAQAAEh6BOVpE/SSgf5cMi8oXfamSOe+E3Cct1arX/nWk7rl0vNfy3KzQvepLK2qbPFaXTdvK9dPSrXrl42XuZVU1gfuTp9gafnxstsY/SjtKq1RU7F21vmT1zmaPEwAAAAAAAEDLIDBHi8nsu5/aHX6eJKn469dU/tu3frcbPaSTJGnK+D7KzkyV1er9tszJjM/JXVM9qspTfMbsdJr6862f6rzbPlNVjd1ruT+0ZAEAAEhcris8g17mAAAACS/uJv1EcskdcYTqireqdN77KvrwEVmz8pTRe2+vbWacOULL1xdrSO+2fo+RmhK83UksOJymUlIaQvItOypUXFatgpx0SZLd0dCrvHBHhfuxM0BrmjWbS+VwmrJa+CMLAAAgUfheudGSBQAAIPFRYY4W1+bQM92TgBa+9W9Vb1rutT491aa9+7WXzZo4b8erHv6m0Xhf/aSh97lnJXl5VcOkpwHycv2xrljPf/h70HPOWbRZl//fV9pYVNaEEUu//FGkZWtpCQMAABBtJpXlAAAASSNxEkokLMOwqMPxVyi9xxCZddXa+vbdspcVx3pYzbJ8fUmjZR/PXavq2vr2K56V5OWVDYF5oApzSXr361VBz3nXC/O1elOpHnpjYWSDlVRcVq0bn5yrGQ/7b4sDAACApqCiHAAAINkQmKNVWGyp6nTKtUpp312O8mJt/e+9Mh11oXeMQJ9OabrwhKFROdbildtDbvO/r1Y2WvbeN/Wht2er8gqPCvPfV+8I2Mc8XJ490cNVUlbjfmwGCe0BAAAAAACAPRmBOVqNJS1DnU66Spa0TNVsXKbCN+6Qo6o84uP0757vd3nnghQdPLxLM0dZ79rH54TcZpGfUP31T5dry/YKr1Dct6r8m4Wbgh63rLJWd7/0k35autXveksT+pxbPCYVbW5gDwAAgHqNrsq4zAIAAEh4BOZoVSltuqjDcVdIkqrWLNL2j5+MqOK5Q5tM5WWn+V03Ya/caAzRrSmV2HaHU3+9c7ZXKO17mI1by4Ie/+WPlurbhZt0y9M/+F1vszahR6bHLg4CcwAAgKji6goAACB5EJij1WX2H6EOJ/xdkqGKJXNU8t1bYe/rdJpe1dKeUm3RfTt/+fOGJu/rWVXuW2G+cMU2PfTGL16TgXraUVrtfvzirCWav6RQ0x/82r3Maon8+/R8xsjLAQAAooULKwAAgGRji/UAsGfKHjJO9rKd2jn7eRV/84Zsee2Vs/chIfdzOk0FyMuj7ptfgrdOCcazetzpcHqt+2Ndsf5YVyyr1X/w7dly5a3PVwRdHy4jREsW0zS9tgEAAEAETK6jAAAAkgUV5oiZ/DHHKn/ciZKkbTMfV8UfPwbctlfn+nYrB+7TpdUC85+XFTV5X8+2J44ArVc2b/Pfvz1UIN6Uliyez5lvYF5ZXafz75itR9/+NeLjAgAA7MmIyQEAAJIPgTliqmDCacoedpDkdGjrf+9T5apf/G53298O0N9PH66zjh6ijm2y3Ms7tMkMeY77r5ygY8b1jtqYw+E16WeAHii+k4a6cnJriMC8SS1ZPCvMdwf4dXanTNPU1ws2auvOSn08d63m/Lo54mMDAADs6Uyf/wMAACBxEZgjpgzDovbHXqKswQdITru2vn23qjcua7RdXnaaDhnRXWkpVk09bKAOGdFNN58/Vn26NJ7o84j9e3l93a9bvs4+ekhLfQt+efUwD7NpuKuyPFSFeVNasvgG+JU1Dp1z+5e69Zl5Xtvd9eJ82X1ayAAAAMA/KswBAACSD4E5Ys6wWNXhuMuU0Wc/mfZaFb5xh2q2rg24fVZGiv5++giNGNTR7/qLT9qn0bL0NJsemn5wlEYcmmcXljdmLw9rH7vDDDqpqUuoCnT/4/GehHTJ+irV2Z36aelW+fa4Ka/0PxkpAAAAgiNABwAASHwE5ogLhjVFHU+aobRuA+WsrtCWV25WzZZVUT1Haoo1qscLxrOiu7LaHvZ+j73za8hA3OYxWeiytTv18dy1XoG4P56rS8prvT4u7Hs6hzM+KsztDqdXL3gAAID4Ywb5CgAAAImIwBxxw5KSpk6nXKu0zv3krCrTlldvUU3h6ugdv7VmC1X4bVh8ffLDujAm/Wz4sZ3x8Ld69O1f9cvybcHH45GYX/P4PNXUNYTihs/zYnfE/k89u8Opv9z+mS6/78uQNwMAAABizaS2HAAAIGkQmCOuWDOy1fmMmxsqzV+9JWh7loiO3YRWJk3145LCJu/rG2D7ysywNVq2qag86D6+mfPshbvcjxtVmMdBD/NN28q1o7Ra6wrLRJE5AACIV8TkAAAAyYfAHHHHkpahzqddr7Qu/eWsKq9vz1K4xu+2kRQfN2WyzKZ6cdbSJu8bKthPsUb+Y+sM8kQ1rjCPPDCvrXOots4R8X4BeQzXGSctYgAAAALh/j4AAEDyIDBHXLKkZarT1Bt2h+Zl2vzS9apYPr/Rdt06ZId9zNasMG+OmXP83xxw8dfu5ZVPlrkff/jdan0+f73X+mBtTXwD86se+U6vf/ZHOEOVJDmcps648SOdfuNHUatO9xytIw5axAAAAAAAAGDPQGCOuGVNz1LnqTcovecwmbXVKvrvfapa95vXNqdOHqhjxvXWTeeNDHk83wrzA/fpovOmDGu0XX52WqNl150zWlPG91FGWutNHBrI179s1LK1O72WVVTVSZJ27qrWf/63WA+8/otXeB2sEt/3PkJFVZ1e+XiZ/419VNfYdeG/P1d1rUM1tQ7tqqwN75sIwTPgD1Yd797GaeqGJ77XA68viMr5AQAAIlN/QcXcKwAAAImPwBxxzZKepc6n36jM/iNlOupU+PrtXhOBZqTZdMGf9taQXgUhj+VZYX7+ccP099OH67iD+jQ+p5+fiq7ts3X+8Xvp2RsO9wrUB/dqE+F31HxllXWa8fC3ev+bVY3WebZFqapteBw0dA7RMz3YH36fzlunLdsrGg7l0clz+fpizW9GL3cXRxhNzFdvLtXCFdv0+fwN/KEKAABajUEzFgAAgKRDYI64Z1is6nDcFUrr0l+mvVZbXrlF1RvC6xFuszYEuJ4V5v27FyjFZvU7waa/Za4QNjsjRadMGuCxbdjfRtQ99d5vjZb976uV7seV1XXux2awTilBAuavft6gaTd9rKVrdjZat75wl978fLnXsuXri/XDb1skSdMf/Ea3PjNPm7eV67G3f9V1j89RSVmNZn2/RuUhKtE9hxROSxbPlyGcgB0AACCauPoAAABIHgTmSAiWtAx1nnqD0roOlLO6XFteuUUVf/wYcr+0VFvDMcLsYe5vO88A1zOEj0WFeTCzvl/rfnzXC/P126rt+vLnDdpVURNwn0D5ssNp6r5XF2hXRa3ueP5HLV2zU7c9Wx+AS9LF93yp0nLv4Ptfz87T7c/9qI1FZe5lm7dX6KO5a7Vo5XadefPHevydRbrv1catUzwrwx0eE306gkz6WVpeI9M0ZfOYCDUak4/OX1Koj+aubfZxAABAkkuMKXIAAAAQAQJzJAxLepY6n3GTuz3L1nfu0a5fZgfdJyO1oee41aPXihmkDsjf5KCeW1s8jnPE/r1CD7yV+E64uWJDia55bI7+79UFuvvlnwPu93GAYLim1t5wbKepp99frHm/F+rie74MOZbCHZXux3Y/E4H+tHSr19clZTX6y+2f6aWPlrrP53luf35aulXTbvpYj779q9dNjjp78ycevfWZeXrs7V+1rrAs9MZhcDpNbd5eTrsYAACSHf/UAwAAJDwCcyQUS0qaOp50lXL2mSiZTm2f9bgK37hDpt270nnaEYMkSRefvG/DvuFWmAdpySJ5V5in2Cw6eES3SL6FFlMbJCgOVnW9dG3jdiuSVFPrvc/KjaWS6gNwfyG4J6dHyB1qW6m+lUxRcZXenF3f4sWzDYszQGD+yif1E5N+8sM6r21q65ofmLsUlwWuzI/E0+//pgvu/Fz//XJl6I0BAEDCoIc5AABA8iEwR8IxLFa1O/oi5R/wJ0lS5cqfVTLzUckjND918kC9ecfRGjm4o3tZOHl5xzaZXsF6v+756lCQoe4dc9zLPCvQLRZDf5kyTEce0Ev/OGNEc76tZotGZbWnX1dudz8uq6z1CqXv99NSxZPnJKP2MMblW0Xu3ZLF/x+inq/DJfc2VL3X2ZvfksXFNKXNO2v11PtLVNKM8PyDb+snqn1+5pJoDQ3AbqZp6rG3f9XbX6yI9VD2SJXVdZq/pDDq/wYBicZ09WahRQsAAEDCIzBHQjIMQ20OOUPtjrxAMiyqWb1Aed89KXvxFvc2GWm2RvsEM2ZoJ/3fFRO8AvP7LjtIT14zyatHdrVH5XVGqk152Wm66MR9NLBnQVhjf/7Gw8LaLlK+FeHNtWDZ1oDrvlm4Kei+XhXfTQhR7B4V5r6tZlw8XxNPTTlfYKae/LhIs+dv0sNvLozicQFEy8qNJfpo7lq9wA2pmLjt2R916zPz9OIsnn9AEi1ZAAAAkgCBORJa7vDD1OXMf8lIz5alepeK3/0/2ctLQu6Xn5PWaNmIQR2Um5Xq1ZLFYjFk9Qlmd5RWux+ne4Ty/lq5+NM2LyOs7SJVWVMX1eN9+fPGJu/7yQ/r3I/DmYTTt6e8M4we5v56zfs7n2ma2rqzskn9wz13WbOl1GtdaXmN7nj+x0b92JMRvdcRz6pronuzEJFZvKr+00if/bg+xiMBYoOCcgAAgORDYI6El959kNqdeYcc6blylBRq83NXq3rDMr/b3nz+WF05dT91aZfdaJ0ryA4VfHds4z/wDtQj3bMtTEuqqraH3qiVLPijyP04VJuEyuq6RoGXZ99zz/D8/W9X6dXdvcsDBea+bQH+99VK/eX2z9w9zxu2Cx2y7djV0IbFs6+6JD334e+au3iLbnn6h5DHSWRzFm3WGTd+rF+Xb/O7vrS8Rm/OXq7tHjeSAOyBuLGGPRw/AQAAAMmDwBxJwZpdoPJRp8ua31H2Xdu15dVbVLXut0bbjRjUURNH9vBaduN5Y3T6YQM1akh9sG2xBg/MDx7RXeceO1T3XznBa7lnYN63W577cdu89EbHOPnQ/qG/qQhVVIeuMO/XPV85mSlRP3cwOwIEqZu3lau2zqFTr5ulT+et81rnGU67KsxN09RT7/6m1z79QxuLyhpV/rv4Vpg/92F9m4A3PlvuXvbrim3609Uf6q3PlyuYp99f6n7sO/nojpI9IyC+64X5Kqus1fX/+d69rLK6zv08/9+rC/TSR0t123M/x2qI2MP5fkIFAFoTk34CAAAkHwJzJA1nVhu1Pf1fyug7XKa9VltevkmbX7pBztqqoPuNGtJJUw8f5O5xbg1RYW6zWnTCwf3Ur1u+13LP3U4/bJCmHNRHd1w0zqvo7sWbDpckTTtisK4/Z3T431wYnvjvopDbWAypQ0HLtISJ1JYdFdqyo8LvOs9JP11BtWdefdHdX6iqxn9FfaAWLp4eeWuhJOnFWfWBuGmamrNoszZsLQvYfsRzTIE4HE699sky/b56R8htE5XrJseZN38sqeHTBFt2VMZyWHusyuo6WuYgLvAuBGjOAgAAkCwIzJFULGkZ6njiP5Q5cIwkqXr9EhW9/7AcVeVhH+PgEd0kSd075kR2bo/EPCPdpvOP20t79W3nFWYV5NZXm1sshsYM66yu7bMiOkcwhWEEloZhBGxl0tpq6xwB+5vb/VSYOz0Ca9NUwFDaGWF4+NH3a7R41Xbd9cJ8XXT3FwG7CoQTxM+ev0GvfvqH/vnodxGNIZFs3Vn/PqustqvO7lSYrfsbMU1T9736s7vFDiK3vnCXTr1ulm5/7sdYDwWgIwsAAACApGELvQmQWCwpaep44gzt/PJllc59V5V/zNP6NYvU5azblNaxV8j9jzygt7q0z9aA7vmRndcjiPYMpYOFCGFksFFlMQw5Wjkvz85IUXlV43YxNbUO1dY1rto2TdMrHHdVdvv2EA/EdJpasaFYT/x3kc45ZqjXuh2lVfrwuzXaVtzwqYPH3lmkiSO7e53fH9+WLP5sLCoLa4yReuvz5aqpdWjakYPD3qekrEZFxZUa0KMgqmNJTbG6H1fXNr1v/h/rivXV7ollTz98ULPH1dLqK7mlrIz6lkZVNXZlpMX2n9CZc9ZIkub9XhjTccQaQS2AeMCvIgAAgORBYI6kZBiG2k48Uxk9hqrwrbtk1lZpyys3q91RFyh70P5B97VaDA0f2CHic3pWmHsF5kH+hIq0lUJGmi1gK5Jw1DmcIVvORJvnBJ6eauocfr+X02/4yCtgdwXl4VR4S/UV5v96Zp6Ky2p0zWNzvNb969l5WrWxtNE+FZ7nC3Aez+Xbiqv0+5rGFe6hJowNZOacNfp24SadfGh/2awW7dO/vXtdnd3hbh1z5AG93JPThnL2LR/LaUoXn7SPjti/V5PGtWztzkbLPL/D5rwXawJ8uiAeOZ2mTr1uliTpnbuO0e+rd+jGJ+fqlEkDdGYENzGizWjln+W45fEja5omz0vMEBdiz+T7G4efBAAAgMRHSxYktcx+w9XzimeV0qaLnFVlKnrnXm2b9URELVrC5ZnRWC0NP1rRrDA/cWK/CEflbeWGkia30GiqYIF5hZ/Kc99q9Eff/lVSBIG5Uyqr9D8Bqr+wXPKumvZXDe97/nNv+1R1du/vq7K6Tms2+z9+KE/8d5F+X71DNz/1g65/4nuVlte413m2p/E9Z7AbLq7hPvr2r9peEryPfyAzHv7Wz3EbzlldY296x9YYJAqzf1ynd75YEfF+nv3rt5dW6cl3F0uS3pwdfNLYlkYuXM/zpmRrf2onHjgcTm0sKqeXPRBj/AQCAAAkDwJzJD1rRra6nHWbu6952S+facNjF8tR0bRwMxCvlizW8JKsgpy0iM5hRGFCqRRb6/7Y2wO0UnE6pera0FXGrp7Z4bREkeoD3bRUa+gNvcbScOytAXrBhzr/Ffd/rV+Wb3N/bZqmioor9dHctV7bLfijSH+763N9Nm9dwGPtqqh1P/a84eCZhxXuqNDZt3yiN2b/EXRcUn37k41FZSouq/YK45vCKzAP4/VzWbtll9e5g33yoqU8+MZCPT9ziTZvr79htmN3+O2vlc6O0ir/N0DM1gmq6+xOLVmzQ44AN5wkKsz92RND47tf/knTH56rn1b6n0QZQEvb837vAAAAJDsCc+wRrFl56njiDLWdfI4kyVldrk3P/1NVaxdH7RwBW7IECXCmnz5CA3uG32PaMx/r3jE7sgHu1tS2IdHmdDoDTvrpq7yyVrc9Oy/M45rKiDAw92wPctUjjauqw7Flu3dYVWd36p+PfqfHdlfIu9z05Fxt2lauh95cGPBYdXan/li3Uw6n6VWF7xlWv/PlShWX1ejlj0JPmnnXi/N14b+/0Fk3f6JpN33sFcJWVtfpP/9dpKVrGrdf8cfzxkFVjd3rTTnrpxKv8ZZX1uqmJ+fqhZlLdOm9X+rMmz8O6xx1dkfYN0jC5flzWFlV30rm7pd+0gffrtZVfirp/3zrp7rsvq+0eXu5V9Vy/cOW/xl6+M1fdPUj3+n5mUsCbhMfP8mRK9xRoW9/2RT111jaM/uZf79oS/3/l7bMHAoAwmQm6m9lAAAA+CIwxx7DMAzljT5G3c6/X9asfNlLilT4xh3aNus/qivZGpXju1itHi1ZguzTuV2W7rxonN91Xdtn64SDvVuweJ4jxRZeKPzno4d4fR1ua5OW5nCaqrOHF5i/8sky/bG+OKxtTdP0CpbD8dPS8F7/SFquVNXYvSYXjcTdL83XPx76VsfPeF/n3zHbvdzu0ZIlNaXpv75rPY7zysfL9OGcNWHdKJizaLMu/PcX7q99+5D/uLxc597xlTZvq6/gfmP2ci34o0hv726D4vmyeD/2bvNyxo0fa/pD30T0PYXi+bZ3/Rgt3/2eCtTCR6pv42N67FzfIzuqQ/Pry90Tor779arAVeYJms2cf8ds3f3yT/rml41+1y9bt1MvzloSdo/8QO+lPU2sv/VYnx+IlQT9VQwAAIAgCMyxx0nt0EPdL3pEGX2Hy7TXquyXT7Xp2atlLwuvwjYQm9VQblaqUmwWdSjIbFgRIkSwePQ7v+yUfRt2MxtXSnsUruvg4d0CHjM7I8X92LeCPVBP8dbmcJpht/QoKQu/jciSNTu1c1fz2o4Ectl9X2nD1vCqOMMZs93h1KufNK4Q37TNf2sFzx7muZmp7se+lbqhQsPpD37t7uW9aVv4/fzvemG+19eBgtyrH/1OklRWWet3vS/P8S9du1NVNXat3FAS9rh8LV2zU/OXFAY8h+vGU6CWJp7PX6rN4nUDxuE0tbEo+nMgBPPfr1b6XR6NFk2x9NvqxhPnvvfNKs146Fu99fkKvfJp5P3mm5PZllfV6e0vVqhop/+2TIlo3ZZdYf/OamkrN5Y06+caiHeu3z978o07AACAZEFgjj2SJTVDnU6+Sm0mnS1JclaVacMTl2rnV6/JdIbfl9mTYRh6/sbD9NptR3n1CQ/1d5NnCN6+IKNhPzXuEZ2eZtM/zxqlKQf10ZSD+uq5Gw7TG7cfpbOOGuy1nc3j/BaLoXb5DceNl8C8ts6h1z4N3X9bkoojCMx9e4ZH2x/rGt9Y+eWPokbL1mzZFfJYM+esCfs5kLxfuyyPmyK+lbi+k4P62rC13N3qw2Zt+j8DDqfpN7ItKauRaZr6fP6GgPt6/lh45v2e42lq6HDVI9/q1mfmqai4Ug6Hs77Fi+kZmLv+7z9w9qzAT0mxev0MP/7OohZpJRLMN79s8rs8TrorNZm/5//p935zP16+viSs43i+GmaQ16aouFLPffC7ior9B+KPv/OrXpi5RP+I8qcbXOrsTq0r3KXH3vlVO0qb9umTUHxbJl1y75e66O4vgvbCjxZz9/nnLt6iHaVV+uDb1XrkrYUyTVO1dQ5def/XuvKBr1VdG94nBwAAAAAgVmyxHgAQK4Y1Rfljpiir/0gVvvVv1W3fqJI5b6tu5yZ1mHK5DFtK6IP48NcmJdTkhp6hkWfW069bfqNgYdKoHkpNsWrcPl0kyR2En3zoAL04a6nHOLwD81SPr48b31v3vebdV9vl/isn6Mr7vw463mh56/Pwq0d/91OJGisPvrGw0bIbn5zbaFk4FebrCyOr/KzzCL08J5n1DMhXbizRpggqoJsTmNsdgduTLPBzE0FytTQxvFJO1wSpqzaWynOF3eFs9DO1dssufT5/vU4+dIBys1IVzI6Sat381FxtL6nWU9dO8lpXU+cI2EO/2uMGRIrV4hXcx9N7MVwrNhTr24WbddrkAcpMj/z3WksJlffX1gUPeZ1OU+sKd3mFxMHaMd381Fxt2Fqu+Uu36rGrJjZav2jFdkmR3aAL18sfLdUbs5e7v964tVx3BGjH1VTl1Q6dd+dXOmi/brr4pH1U5jF5cK3dqYxm/KyH68ufN+iB13+RzWq4J3w+aL+u6tkp171NdY1D6anRu/y0O5xavalUfbvle80fArQ+3n8AAADJgsAce7yUNl3U7bx7VTL3XRV/87oqls7Vhs2r1H7KJcroMbTZxx/YoyBghag/j8w4RF/M36ATJ/bXM+//5rUuNSW8vuWeIajVYngF6KOHdNDlUzrpwfe9W1bs3a+d+nXL18SR3fXFT4ErgxGequo62ayWoBX9kVZQewbjnpW0nueI5IbH7B/Xef19X15Zq+zMVJmmqa8X+O8v7clud7hDMV83P/WD/338hOBOp6nzbvus0bY1dY23vfTeLyVJxbtq9I9pI7R41XZZLYaG9G7baH+naWrD1vqbB9Nuaphw9OE3F6pwR+C2G54V+6ZMxbrt/9otu/TyR0t19IG9VZCT7l4e7gS+f3+gvmK6zu7QBSfs3SJjbJIQw68N8UmJVz5ZpjdnL1fntllhnc71XgjYoqQFsy7PsFySVkcwH0K47A7J7rDr47lrdfFJ+3jdVGuNT0WYpvTV7t8bnr8Xqqrtfj/hES2Pvf2rPvtxvU4+tL/OOmpIwO3ufuknlVXW6uoz9onuALDHM5rVDAoAAADxiJYsgCTDlqKC8Serw/FXyprdRvbSIm155RZVLJ8feucQjhrXWxecsJcenXFI6I1NqWenXJ1z7FDlZqU2mlQxmP336ux+7BmkdW2frY5tvAOlguzG98quOXtU0OOfe6z/mwd/8pmYFPXKq+qi3v7mnS9WyOE0dd3jc/TE/xa7l7uC9EjeL1J9tfycXze7vz7z5k+0bN1OfffrZt336oKw9o/Ua5/+0WicgaqCX9zdNsafNVtKVVFVp2sfm6OrH/nO/Vx73oQIdNwVG0qC9lf3bIXkcJhx0Y/2jdnLdc9LP3stizR4jPQTDZ4Wr9yurxZsDPpcmKYZUauR0BXmwd/Pb+4OobfsaOj535xcuDWLk1ujEtozMG+NVly1dQ4tXL7N7zrPt020v/fPflwvqf5TS8vXF+urnxvf8DVNU98u3KSFy7dp83b/c0QAzRX7fykAAAAQLQTmgIfsoQeq+4UPK3PAaMnp0Na37tKau07Tzq9ea/IxbVaLjjmwj3p4fCTd1+BebZSVbtOQ3m28lndqkxlgj8b+PnW4+7HFIr36ryP1wk2HKzM9RRedtLdGDOqgm/4yNuD+2ZnB21vs07+93+Xn+AnSJ4/uEeaok9f7364OuU2kOeyildv1x7qdWrRyu9dyu8Op6lq7/nJ74yrtSNgdTs146FstacG2I299vkL//XKlV6uiQM9DsH701TV2r9DbFQh6BoNN7dvs2ZLFaZpB23z4U7ijQsdOf08PvfGL13LTNLViQ3GjnvPhWrxqe+iNgrA0I6i89vE5uu+Vn/Xdws0Bt3n4zYX6862fet2ECSZQD3mXUBXmfjXr5kbrJeatEZh7nsERw49JGIbRajedpj/4je57dUGj1kme336gT8UATUUjFgAAgORDYA74sKSmq8MJVyp3xBGSJNNRp5I5b6v427eaPCFoKHddfKBeuuVIpad5V36fMmlA2Mfw3NdiGMrJTFWb3Pr2DW3zMnTz+ftr5OCOIY+Tluq/7YvNGv6fhM0J5vYkkQaxgdTZndqyvSKsvulhHa+Fq1FXbijxqh5uyvNQVFzl1bdfpvTM+7/p5Gtmuhc1NRjzDLQdTlPrwpjA1dMFd30uqb7ytbS84TWZs2iz/v7AN7rhyeZ9csXhNFVndzaaFDiUcFq4VFbXeYWrFVV1qqiqc3898/s1Afd1Vfq++ukyv+t9Q9PmVpj7E60K8/Ign0DwVFldp5uemqvP5q2L6FytEZh7PhV2h7PVJ6t1MySnx6+UUMPYWFSmB1//RZu3hz8XQ+NjeO9bZ/f41IgzPia+BgAAABC/CMwBPyy2VLU74nx1Oes2WbPrq76Lv3ld2z95ukVCc4tPn3GXzPQUZWVEPklfoNA7HFMPG+h3ebj90485sHfYvZUROX+TdNodzqgGcC3dvuHHJYW64/mG0LipQd63CxvmBti0rVzvfr3KK+yta0qFsuQ12a7DYeqG/zSe1DUYp09/eYfTVHFZtbsv/AY/k7KWltfohZlLtGlb6JBw+oNf68+3fqKZcwKH1/6E+rEsKq7UqdfN0nWPz3GP/bTrZ+m062e5t9leErrliufvsp+XbdVf75itN2cv1+k3fOTucV0/oODHacr9pGZVMns8Qf5aEv3nv4t027Pz3OeorrVr5pw1WrCsSA+9uVAbi8r0n/8tCqstjedNxabcGAiH53PxwberdfqNH2nlhpKQ+/20dKvWFwa/SbRha5keeuOXsCbBNXzGEuo1uuaxOZo9f33AeRDWbdmljUVlqqyu0/Mf/u7/nB7vrR+XFHrdSHNQYY6oM4N8BQAAgETEpJ9AEOndB6vHZf9R6bwPtfPzF1S24FPV7dyiDsdeKltu40kGW0IkMejlp+6nN2b/octP3a/J5/OcVFCS2ual6/TDByk3K3DLFovRUDVYkJOunbuqm3z+PUm0Jld98t3F2lYcfu9oTxlpVlXVeAd2rR0oRaPS/go/k502Nfj3rNz+5pfQk58G43CYuvnJufp15TZ1KAjcYunhNxdq3u+FevuLFRo+qEPQY67a2HjCSNM0Q7Y4CbXeFei7QtBNfoL9cHje1HGFni99VP9pgPteaejDHmo8oRhG41B9V0V9ZXhedlrEx/O85/TT0q1e60zT1Ie7b1Cs2bxL3Ttm65RrZ3qd/6qHv1VZZZ3WbN6luy4+MOi5rJb652jxqu269rE5Ov3wQQFvVjaVZyH1u1+vkiRd+cDX+uC+4wLus2JDsW55uv41C7bd0+//pgXLijRnUej2O4ZheP2Mh/pxd31SZoufXuOV1XW6ZPfEv0fu3yto2yaXfz0zz+vrWLanQXIzac4CAACQNKgwB0IwDIvyx05R+2MvlZGSpuq1i7Xx6b9r+yfPqHrTilgPz8uk0T301LWTg/ZL92fGtBEB1x08vJsOG9Mz6P6e1ZJ79W3nt9r5byfsFdGY4J+/EPiPdcXNuEnR+LVq7RsewQJ6z5YmkQbgTQnMHU7TqwXJd2H24w6krLJWC1dsk2lKW3dWBtxuyZqd7scLlhUF3C5QNf65//pUP/y2pdHyZWsbjhvqUwi+Ny5cwaSncKJGf5+C8Ke50ZK/T7JcdPcXmnbTx6qtc+iDb1fr/W9W+d33y5836IHXF3i/R4IE+L7tfTZvr2gU/JZV1r9v/li3U6G4fmc+8d9FkqRXP2loY2Oapu54/kfd+swPzaqYD7Tvropa/fh7odfPlsvqTcEry3/8vVDf/rLJXaleWR1eP37P963ZjPrbFR4V8r8FqW73fKv7vqwE5og2YnIAAIDkQ4U5EKacvQ9WWtf+Knr3QdUWrtKun2Zp10+z1PmMm5XRq+XC4NbobnLQft2CnL9+AMFCsPrgqj6EGNy7jb5f3DhkDLelC4Krq4tuuxR/bUt8JxVtadMfbFwd7jLtpo91+NieuuTkffXUu4sjOq49wpYspmnqyvu/0prNkfUsl6RH3lqoT35Y1+jntSbMdhvh/pwHqsbfXlqt25/7UY/OOMR9w2zTtnLNePhbr31dvaw9fx7tDqdsVotXAPx9iMrhyuo6fTpvvcbt3UXtCzK81oU930Ezf7cZHr93fG3dWaknd79fDhnZXTk+kxr/3+6WK2s27dIR+/fUkQf0VrD7CZ5tU25//kfdeN6YEOMKzmoN/Hu1otquuYvrb34Ul9W456KIVKD3ygszl+jTeevUviBDz15/mM/axvuYpql7Xv5ZBTlpYU1m7MswvEPq5nyg5Ponvnc/rqyuC7Jlw2vgez6n0xT/GqElcCsGAAAgeVBhDkQgtW1Xdf3z7co/4AT3si2v3KwdX7wkZ3WFaraubV4PXb9aNjFPD9HvvF1+fRiWmmLVeVOGea275fz9JUmGT9Lkr/Iz0QPzKeP7xHoIkqTr//N96I0i0NL9ysOxc1fwyUo/+aF+QsVZ36+N6LiRfm+mqYjDctfPu2uMvj/+NWFOzBl2YB6iOvbiexqqwucv8W4rMn/JVp1w1Qc6+5ZP3M/Nw28u1GnXz9KO0iqZHse+84UAE5Pu/gb/87/Feub933TVw9802iTFFt7PuiFDTqepsjAn2PQVbHJhz+cpWI/w1ZtL9dg7i1Rdaw8adHseo2hnZdDQ1/co/iavdP2OTPETmHuOPdTbYsPWshBbNPbp7glK/bVx8vd9XXzPF/p24aYmheVS/et8o8fvrWj9G+mq6Pd7zt1PnL9QnQpzRB0l5gAAAEmHwByIkGFNUZtDpqnn9BeVNbg+MC6d+67W3neWNj09XWW/fhHV8w3pXT/paEZay3wgJFR2cPjYhnYsg3oWeK1z9Vr2za38BVnNmYjUZdKoHnr9tqOafZymGDWkY0zOi3pvfb484n3qIgzMm9JLvaKqTm/M/iPg+uowAvNnP/hdpeXhhcbhTJC6cmOJ5izaHDAoLq+qc0/e+em8daqpdWjmnDUhfxd4crWN2V7auH2PzWro21826e8PBP7kgCTNXbxZ1z/xvU6/4SOt2dy4L3soQTu/ePwKCudldTrNoBXmvp8UCBr6egTvm7eV64I7P2+0iavC3OpRjV9da9fKjSXegXmIOynf/LIp4Lpw389eE3L6HsNpasPWpvWydzO83yfRuqcc7EaI62nzdwPMEQc3CZHkuCcDAACQ8GjJAjSRNT1LHf/0D5XOn6mdX70ms7Y+gNo+8zGltu2itG6Dmj2pnSRddup+eu+bVTp0ZPdmH8uvIOnF3v3aebUM6Ni2YdLCS07ex/3Yt6LcX2AejQrzvOxUpdi8U7JTJw3QG7MjD1MjlZme0uLnQGAvzloa8T5Pvftb2NsuW7tTfbrmRXyO//xvsb5aEHhi0EAtWRasqlBKTqne+PyXiFrghFMde+XuCVCHDww8eajNalHhDu9JFcOp/C0qrtJdL85XiZ/+1y7zl2zVD78VhnWsot1Vzp/+sE4X/GnvkPt4Cvb71TX5pxRJQBtehbkU/HWorXPo9Btm6dJT9lVxmf/nydVP3vP36z8f/U6rNpbq/OMaPskT6p+QYOvD+b5XbyrVTU/O1Z8O6acTDu7XaKdoVGP7DjGSwNw0TRXuqFSntpkR/ntav21VTeMe6w6nSbkIosogIQcAAEg6/MkANFPeqKPV/W8PKWvQWPeyzS9erzV3nKSKP35s9vFzs1J15pGD1aV9drOP5emsIwdIkv5+RuAJP30V5KTrkX8comevP0yHj+3lXu4bZORnpzXat0NBRkRV5r7BuFQftPhOXJiS0jq/xrIyCMyT2YyHv9WbTahiDxaWS4Fbsrw/r1jX/efHiPvFB6uq9bVqU0nQ9fe98rP7sWmGX5E8J8REqE0JWVMC3FCrqm2oBjZNU+sKd8nucGrN5lLVBunnf+1jc9yPq2tDT0zpdJpBw2ffc4W6uVBWWac7np/v1ebGk9VS/3vL8/VftbG+yv6zH9c3jCvEeYJFyOF8GuHy//tKJeU1evaD3yX5qTCPeouxyFqyvDBzif5652y9GeFNUddr6TmBrwstWdBSTHqzAAAAJA0CcyAKbDlt1PHEGer21/tlycx1Ly967wHVbGla39eWdvQBPfXWnUdr3N5dItqvZ+fcRpP8+VaUH7F/T43ft6vXspzMVD197WQdPCLwBKOhOE2z0bn89UtvCaF6vftzzjFDWmAkaClvfBb9TyrU1IUOayPx9pcrwj93kHYwH363WsvWFXsta2o2+vR7v2nZ2p1N23m3VD83yCTpo59KVFRcpeJd1fp8/npdcs+XOvGfH+qy+74Ku0f9RXd/oR2ljft1e3KawSvWG7dkCevUAdvcWCyG3wl3peATZJqmqZc/XqrP569XKJH2Cnc6Ta/zLV9fHFboHopvQB3JEd/5cqUk6eWPl+nSe78MsXUD1yvpt4e5g8AcAAAAQHAE5kAUpbbvoV5XPqe2h/9FkmTW1WjzKzepZN77clZXhNi79aWnRqcrk2+InWKz6qozR3ots1otys9J01+P3ytgOBaK6SfUMgxD/77kQL/bTx7dQ8P6ttXoIZ2adD5PwSYYlKSczMYV6EcfGNlEodHuk56RltgTrSYDfy0hmmP+71tDb7RbsGp0VxDpYppmk8PR975ZpRkPf9ukfV0CfVJkVWG1Lv2/73TWLZ/o3a9XSQqvctrXVz8H/ySAvwrz2jqHCndU6KK7v9CH33nf+Ay38vrJdxf7XW61GAFfH8/v77rH56ja4z20YkOJ3vhsuR54/Zf6BUFC/kifJYfT9Eropz/4TVQCc9/v0zfI/+KnDfrq5w0hj7N2SwQT8u5+XvzNIfDMh0tlJzQHAAAAEASBOdAC8kYeqZ5/f17pPYbKrKnUztkvaOOzV6lm69pYDy0i4RZvh8iSvbbJyUzVlacPD7n9nw7u12iZ6ScCMiQN6d3WqxewS8c2mbrzogN1w3ljdOz4yMJrX6Eq2Xt0ym20LCXorITeJo/uoUE920Q8rmDa5GaE3ggtaleYk3mGKzWCFkSRZp0t0X4jXLV1Tr/hckV1QxV2ZTNuPoS6cXHXi/O1eZv35JanXjdTtz4zTxu2luk7nzY0zQ2SDUMBK8w3eYxjY1G5nnrvN139yLeav6TQb4sRf0zTjPgTA3aHs9Fv2Gi0L5k5Z43X157jqqiq0/2vLdB9ry7wujHQkqpqHPppZTMnMgU80IgFAAAg+RCYAy3EmpGjzqffqDaHni0jJU324kJtenq6trx2q2oK18hZF3jSvEQTzmRsoSq0fU09fFCjZf4CINe5nc7G4ZNn1nP+ccP0xu1HRTQGT769031leUwKev7xw/T8jYdF9D3n56SFfYMiXL6tc9D6SgJM+thU/nr7R0sM83K9OXu5Pvg2ePuqnaXVTT5+qImBf1+9o1HVsd1hasPWMr/bN7eth2EEbsni69N567RkzU7d+sw8r98pphm473p9sXhkYyyrqG100yQaFea+ffo9b3zO+r4hTK8Ls8VOOFzjDvQU7KoMfy4AIFz0MAcAAEge0enHAMAvw2pT/tgpyho0Vju/fFkVS75X1epftWn1r5KkTlNvVGaffWI8ysDCbdly6uSBeuztX3Xw8MD9yV2T3ElSh4LMkMf0F1D7m0DPdVi/uY5HWmIYhjLTUzRqSEfNXxJ+WwtJOm58r5Dhd2ZGw3PVqW2W2uZFFlbnZqWpzh7dECc/p/Hkq2hdxVEOzP3cF4qKrxZsVL9u+S1z8CiJp8kar318TuiNgrAYRpN+3j1/L55+w0ca2qet3+3enL1cvTrnRHTs827/rNGylpn0s37i2Nnz1+unpVu9lkca8gcSKugvr2qhHyTskQyfz2bEz28qAAAANBWBOdAKUvI7qOMJf1fN2OO188uXVLVmkSSp8LVblTf2OLU5eKoMa+Me2LFy2Sn76t1vVun84/cKa/sjxvbUXn3bqnO77IDbeAbOA3oUqE1umnbuiixM9BfeuCrM87JTVerT/sLfH61XTRupk6+dGfY5jxiRp9MP6x8yMPesMPfXvmXk4I5e4VDj/W0qrYhuiNOpTVZUj4fIlZQ3vSran4z0lvlne0dptXaUFrbIsdHYjtKqRhOJhsPz91B5VZ3m/e7/NXv1k2VNHpunaFSY+zJNU3e9OL/R8o1FZbr9uR+jcg7XvxWBKvAXra2MynkAAAAAJCdasgCtKK1zH3U+/SZ1nnarrFl5kqTSH97TmrtOU/nSuVGrrmuuyWN66tEZE9WxTehKcKk+tO7WISdo2xLfVY/MmKjuHbN16uQBYY/LX59wVyBy8/n7N1rnL2BPT4sscDR2f8Q6VEuWvfu1cz/2F5iH6oGenmqLeruNSaN7RLzPpafsG9UxJKruHQPf/InEhq3R7ZXcgTY7SWFdYZnuf21BxPtF2tqquVqiqt/3xqbLU+8u1q6K6PT8r/Ez2ScAAAAAhIvAHIiBjJ5D1fOKZ9Xh+Cvdy4r+e6+2f/Sktn/6jArfuiupepxLjfuc52Sm6rGrDtW0IwaHtf9ZRw3WEWN7Nj7u7kC7X7d8HTqqu/fKKGQ9rpYvwQLv/Jw07Tugvftru5++GZYQv23TUq1eVerRkOlRjXyCn0lU/UltwR7ZiSTFao31EPz68ueNsR4ComTN5l2xHkJILVFh/s9Hv/O73LeHfHM8+e5ivfbpHzHty489D283AACA5EEyAsRQ9tAD1fXce5TRdz9JUtkvn2rX/FmqXD5fOz55Ws7a6LZzaG1jh3WK2rFOPnSArFbL7sf93cuDFVxG449X1+GDVXaOHNRRaR793v1N5mcNkZinpVqVnRk4MN9/r87BB+rh2PF9dOz4PsrJTNWMaSO034D2OvGQ8ALzaIZWiSzYawHEyqoNJa16vpboYR5IU1rUBPPqJ8v03Ie/R/WYgD++PcwBAACQ+AjMgRhL69xHnU+7Xp1OuVaWzIaWI2W/fqGid++P4cia79BRkbcECcexB/ZxPzaCBNnhtri5/pzR7sfpqValpTZUF7sKf1jTiwAAXl9JREFUy30r5H15tmyp8xP8hGqlkJ5qVV5W4Ek6h/mZ3K9zO/89yv96/F766+7+8wft1023XnCA8rLDmwDU7givj3o0b4bEo4tPit/JeLHneuJ/i1v1fH+76/NWO9eW7RWtdi6gJZi7b7HHS3s9AAAANB2BORAnMvuPUI+LHlPHU65R3uhjJEmVK37S6ttPVPnv38q+a7tK5n0gpz06PV5bQ3P/ZhwztD6UPXpcb6/lnuFzsBg60PkzfPqYjxnWUL2dlZGiJ64+tOH4IYJySTJ9qsv8VZiH6mGek5WqAT0LNGJQhwDn8Na9Y7auOnNkyLFFKlSvdpcTJ/YPvVEC69I+Oj3MIzVqSMeYnBcAAAAAANSLbPY7AC3KkpahrP4jldV/pCwZOSr++jVJUtG7D7i3MQzDHajHO5s1vPD1uRsO07biKt3w5Pdek7X944wR+m31Du3Tv53X9uGE2FLgKq/Hr56o6x6fo03b/Fc0egXyTZhjz9/kncE6svTslKMu7eoD2pvP31+Pvv2rPp671msbz29l+hkjtG//9iopb9znvk1ueJXkvs44YpAWLt+mg4Z300NvLgy5fbhPy74D2mvh8m1NGtOeKCczNdZDAAA0xe5/p8O9RgEAAED8osIciFMFB56k7hc+0mj5zq9eU83mlTEYUeT2G9hBg3oW6KgDegXdrl1+hgb3btNoeXqaTSMHd1SKzXsCRs9AO1gRe6AK87Z5GRq/bze/6wx5h+Sef/Zefuq+6tEpJ+D5zj9umMYM7aRx+3RttM5iMQKG7w9NP8Rn3MFL8w8e3k35OWl+t7vn0oMC7nf/FRMC9jI/bfJA3XXxgUpLCW+yy3ADgb5d84Ku93dzYU8xdXLj16Jjm0y/256U5BX9AJCofP81pCULAABA4ttzkwogAaS06aw+172jLn++U/kHniRJMuuqtenF67T1v/fKWR3fPV9tVovuuewgXXhieP2grzxtuCTpnGOGBN3Os2tIsMk0A/X4lhr3FHdNqnnchH4B26dMGt1TUw8bGPCYUw7qq+vPHeO/wtww/IbMR4/rHbK/uST169Y4eK6uadwrvUOAwFWS+nXP1+mHDwp5rnD9+eghSg0RsIf63p65bnLUxpNo9h/WuP1KlwDv2XDeIwCA2CEmBwAASB4E5kACSO86QG0mTFXXc+9RWreBksOuiqVztfa+s1S+bG6shxc14/bporfuOFp/OiR4Na1noJ2eGjiwnTw68KSjvjn7jGkj9cCVE3TcQX0iDieDFZP16lw/keshI7urIKdxuxR/Z/J3vGF92+m6c0brsasmupf1654f0Tgl78rw3Kymt/+wGIZOnNhfb95+lNfyUYMberCnpVp13EF91TVIP3BnK1TiZaY37j7WuW3gmyktzTCkK4/v5LeavG+3fL/7hOqBDwCIFaJyAACAZENgDiSQtM591OWs29X28PPdy4reuVdF7z2oypULkuJjwOlpoadW8Ay0gwXmVmvgX3H9uxd4fZ1is6hvt3wZhhHVcPLeyw/SE/88VHv1baeb/jJWg3p6n9dfaB3odRw7rLO6d2xoCZNis+jMIwdHNB7PewFpQZ67kMfZfSDf5/iyk4fprosP1Lt3H6u37zxGedlpuvn8sQGPE61er+P37aq7Lj5Qfbo0rsQ/+2jvTyx075itw8f2bNb5rj9ndJP37do+S3mZ/t/n/irMpx0xaI+sMPd3gymeHT62Z6OfbwB7kj3v9zQAAECyIjAHEoxhGMobeYR6Xvm8ckcfI8lQ+W/fqPCN27X1rbtU9usXclSUxnqYLcorMA8jYPdn+MAOuurMkXpkxiGN1tk8Wqq0yfE+vhHhH8RpKVZ3hXXvLnm65zLvHuPHH+y/p3i4Is2bPQNqz/Dd0ymTBgQ9xt792rkr532lplg1tE9bryC9U9ssHTKicc/4ow7opTa56X6P49mCxvemiL/K+uEDO2hon7Ya2ret1/Jb/7q/jjqgt572aP1itVh03IS+umraSPeyQ0d19zuOQNLTbHr86omhN/QnyH0ti8VwtwdyOWL/XrLukYF5YkyA2qdLnk4+tL/OPXZorIcSkWDtm/YEyXCDGfFhz/vtDAAAkPwIzIEEZc3MUbvJ56jL2bcpa8g4SVLlip+07cNHteE/l6ts8ddyxHmP86YywmjJEir0leqrknt2ahz8pqVYdeXU/XTB8UPUo33LVbkeuE8XZTQx8G8qz4D9sDE9dfKh/XXbBQd4bROqav32C8dFpeI5WG/7y08brqeunaT3752i125raPsyaVQP3X/FBK9tDx/bUxNH1gfenp8O+OC+47TfwPoWMZ7tTwb2LJDNatH4/bpqxKAOSku16rwpwyIau9NpqluHhhsOe/VtF/a+wWI6fxX3KTZLTCrMB/dqPBGvi+fNhmDuvGhck8/fnE9AtKbeXXN11lFDlJmeEuuhRCQ7I7HGG21OJ4E5oot3FAAAQPJo3aQGQNSldxuk9G6DVD54f+36+RPVbdsgR0WJtr3/kCQpe++Jan/UBTKsyfPj7pkdtsnN8LtNpK1KfE0c2UOVlZVaunRXs47jz20XHKCPflirv56wl9/1LVn46BnIptgsOuuo4BOsRks431KqzaJau1OSZLUY6rS7z7jN2jBmfz3PD9iriztQDlSxLkkP/+MQfbtwk048pKGq/6a/jJXdYfqdqDUYRwuGbb5BXorNGpMe5t075mjp2p1+17Uv8P9z52tYgBsJWek2VVTbg+7b2jeTmsrzUycJFZiZ0kUn7aPH3v411iOJusG92gR877qQlyNaqDAHAABIPlSYA0kie9D+6nLGzepxyRPKG3uce3n5oi+06flrVL1xmUxHXQxHGD2GYejaP4/WlVOHhx3cxZN9BrTXP88apYIc/+Gu6RO77TugfcBjTRjeuNVJuIKFsPdfMSHilg05GYH/SWmfH/p1yvSoeA00NIfDT8rlse0xB/bWISO66eqzGldA9+qcqzOPHOxVCWwYhjss9wzm/Rk7rJP7sW9w7/uaSdI+/f2HxaFuiPiut1mNiCvMTwjQ6ufEQ/op3EPZHU6lpnhXeedlp+rKqfs1u+L93DAq+ju33bNbhrQ0U6aO3L+XzjmmdW6atSbPn9VAqDAHAAAAEAiBOZBkDFuK2h56lnr/83Vl9h8lSaotXK3NL1ynDU9cLntxYYxHGB3779XZ3YajVbVCKVn7/IagsENBhv551qiA23YoyAwaqAcTLPTs1z1fFxzvvwLe161/3V99uuTo9AmB25KcfGjgFjmZ6fWVxGOHNfTuDhTm+6sw9/w2UlOs+vvpI3TgPl1DDbsR3yrzKeP7uB+/eNPhuvbPDRN9+oZtmWkp6tnJuyf8P89u2sSgtXaH19eGYUTcw3zEoA5+l//5mKHKzWpoM/TPswO/t+x2p9dNhBvOG6OXbj5CE0f2aFbF++Wn7qt2eaFvoLTNC/xpgXgSg+L/qOjSrn5uhUSeULZDgBum4XxP/n6XAE3DewkAACDZEJgDScqwpqjjyVer/XGXy5ZfH57ZS7Zq+/MzlLXwf7Lv3BLjESam/n4mnIy2Ew/pp8mje+jqs0bqmesPU1aIXsOuwDlSoULYcAOl/QZ20J0XjlXnNoEnacxIs2nk4I5+1z08/RBdOXU/r4DaXy9vSerTNa/RskgnYg3EZvWuph63Txf345ysVK8x+atOfXD6IV7PaXZGih67aqKf1yf481pT62i0LNJQM9UWuP/3DeeNUb/u+brjonEat3eXgNvZnU5ZLQ2XCaOHdHI/B80JiR1OM6z+5DZreJco500Zqs7tspo+ID8CTYgbK57vxWj525/2lhT8kybx7q8BbuqF8z1RYY5oM2nOAgAAkDQIzIEkZhiGcoYdpB4XP652R/1NlvT6UCm1cKm2v3CV1vx7qipX/hzjUSaWDgWZevzqiXr5liPcy8woVyqmp9l02an7NalKOhKhQsFof1+BAqoObTI1cWQPrwpv37zroekH6+yjh+i4g/qopQTrY+4b4Lfz02LGajEaPWfdO+bo1X8d5bUs1NNaXdu4t3ekmWb/HvmNKt5dBvQo0P1XTAg5UalhGAHb1HjeTLnslH0jGltmekpYN3m6d8wO63hOp6nHrpoY0cSroUTyfDe1QvuVW48Me9srTtuvSecIJj+n/pMGwcZ/0sT+UTnX+cdHNqluuALdeKHCHK2JmBwAACD5EJgDe4jc/Sar1/QXlT/lSvcy016rwjfu0PaPn1JdErRqOeqA3pKC9/yOhm4dcpSXnRZ6w1YyYb/6PuZd24dXZfvMdZP18D8OCTpBphT9SfHsDmfQ9Z5Vob4Vor275Omkif2Vsrty+gKPCVONKP1Llu/zmnpWrrvyt9suOEAXn7SPBvQo8NrW1cPc33MWaTuVaj8V5pFmezarRQ9NPySynXY7ZdIAdWiTqT8fPUQ9O+f63aa6pmGMOVmBP1ng69BR3XXA3l1CTuh53TmjlRrmRKwOpymb1eIOgKPB970QTN9u+Q1fRPA65WQG/+SIp3Cr7ZsiaGumbvk6/fBBAdf/8+xRXjcPA5kyvm+TxhZKoE+iBFruyRn81xEAAACAPRiBObCHSe87XMWHXaXMfSe7l+36+WNteOJylXz/X5X+9LG2ffiY7OUlsRtkEw3oUaCXbj5CN5+/f6yH0qr236uz/u+Kg/R/V0wIa/sObTLVK0AQ6inaFeYOjzTZXwjnGXKFCryOOdCjfUuU6vv698gPuM41nn0GtNcR+/dqtD6aT1WNnwrzNI/JN6cd0fDcHXmA91gOHt5Nd118oCTvILRdXroemn5wWOc/7qC+eua6yerUNktXnLafJo7srvsuP8hrG88Ad8Sgjhrcq03I4/bslKMrThsuq8UIGZibZvCKf0/RbK0x/YwROnxsz4gm0z1sTM+g6wPNteD7Hnfd+PKnJdumBDu2YUhTDxsYcP2A7gWtevPw7kvGe30d6LWnwhytyuftxlsLAAAg8TWt8S2AxGaxKfeQs9Rh0tkq/fEDVa5coJrNK7Tzy1fcm5T/9o26XfCAUgo6xXCgkYtmlWmiMAxD/bsXhN4wQmaUKzA9K8z9BV0e7bK9Hgdis1pkdzjVu0vo8D8cfz56iAp3VOjg3WFp2/zoTTrZvWOONmwtC2vbEYM66rMf13sty8tuqOI+7qC+6tU5V/2652tXRa0++n6te91+AztoaJ+2jY559tFD1LtL4/7v/nhmjW3zMnTl1OGNthnUq0BTxvdR9445SrFZdPel43XL0z/op6VbwzpHqMBcMsMPzKMYTh08vJsOHt5Nn89fH3rj3SL9BIE/w/q2Vae2mQHXt+TEnMHGH+y8T14zSe13T7p572Xj9Y+Hvm3S+Qf1LNCydcUht7v7kvEa3Nv7xowjwIufnx34Uw9WqyGHw6SHOaKOHuYAAADJgwpzYA9mSctQwfhT1OXs29XmkGlK69ZQuWo66rThsYtV+tPHqtm6Vs7aqhiOFLHg8FMmd/KhTe9p7PAIzNP99B62RFBhLkmv/etIvXzLEcrODL8lSDDZmam67W/jNGl0fcVwh4JM3XDuGHfFdnM8+PeD3Y89K/fvuHBco23/ctywRhOkelbxpqRYNWZYZ7XNa9xHPaKeIAGEE84ahqHzj9/Lq9o+kk8khBOGh9uSxRV8RuvGidS8SU2bItVmDfie79o+vF7uTRXsZy1Q9fnAHgVeE60O7Nkm7E8w+DpqXO+wtvN3E80RoM3TmKGdAx9n9/dEhTmixYjC710AAADEFwJzADIsVuUfcIK6nn27el39mvL2P969bscnT2nT09O14fHLVLt9o8xolx0jbnkGoO3y0tWrc67OOmpIk49ndzQcz19I5tWSJYzjpafZWrwdxOihnfxWbPtyPVeu3urnHjvUa32ggHivfu00YlAHr2WZ6Sm68bwxuukvY/XM9fWtk/p0zVO3Dtka3KuNguXZ0ejL3FLtP3xb7jx/42H6zzWH+t3WNCVb2BXm9c/98RNapk+2p0tO3ififcJ9OgO9rjf+ZUzE54xEsPEFWufvpkrvLnlNeg3CuTnmuZ1nRby9CS1ZXOuoMEe08Y4CAABIHrRkAeDFYktV24lnqs2E07Tz69dVtvBzOavK5CjfqY3/uVy2vA4qGH+ysocdJMPKr5Bk1r97vvvx09dNDjvYCiTbY5JDfy05vA6fYJ9sz0qv/96OObCPxu/bNaIg31/IYhiGV5W5zWrRIzMmypB3wNitQ46yM1JUXlUnSerRKadJ4/fUku0/PPmvkK9nKvwKc9cEna4JYSUpK92mvfq10w+/BZ/M2GIxIgpOu7Tzrvb+08H9vL42mxiZmaYZMJ0ONTlvS3K9F+66+EB9+fMGffLDOkmBg/STDx2gd79e5bXsXxcEn1PC6udgndtmacuOCu+x7N6uf/d8LVtXLJvVIqcj8ufb9famwhwAAABAIKRdAPwyrCn1wfkh01S3faO2ffioajavkL20SNs+fFQlc99V+2MuUlrXgc0OUhNdssYu3Trk6P4rJig/J01Wa/M/kHTpKfvq/lcX6KSJodu6RGsiz5Y2/fThmjlnjc6d0lBRHiosb/R+CfMN5K/XdIrNohdvPkIbtpapqLhSA3pE3su+T5c8rd5c6v66qT/PUf05ML0D8GCO2L/xpJsDe7XRdeeM0bHT3wu6r8Uw5Nw98meum+yxpvFzMG6fLhrW1/vTBuf4fJLAn3Dfy0U7K/0uj0aPdF/h9od3vReG9mmroX3augPzHp38t7/xfev886xR2ndAB7/buvfxGcrFJ+2jeb8XNgrMXa46c5Re+WSppozvq03bysP4Lry5gnc+LAUAAAAgEFqyAAjKMAyltu+uLn++Qx2Ou0IZfepbEtTt2KTNL1ynNXecpC2v36bKlQvkrPYfcCS7zJATGCauft3z1S4/cCVwJLq0y9Y9lx2kMcMC9xdONAeP6K57LjsoaLV0Iz7J8n4D20uSUlPCC4h9pdgs6tM1T2Ob+LzeesH+OmDvhn2jXWHesU39ZJaRjM+UKZs19DgO3KeL32Ddtef5xw8Lur/n99qhTcOkm/7uGVgthgzD0JSD+oQcl9dYwnw6a+scAfZv+uvRuW1Wo2WnHz5Iz15/mPvrYIXWvtXf9142XseO76M/H+2/NZPnWI+f0NfrfRWI7/d3+NjGN0Ckhur99gUZuuK04bvf8500oEe+jjso/FYw7pYsVJgjSuhhDgAAkHwIzAGExTAsyh42Xp2n3qhuFzyo9B4NgUnVql9U+MbtWnvfWSqZ977sZcUxHGnrufSUfTW4Vxudfvig0BsjIk1tbZGIjj2wj/5++nA9cbX/ft4tLS87TYeM6O7+OtoFzfdedpCumjZSp0waENF+hmHo0mM66q4LA/fwDpV5ThnfV8MHNlQ4P3/jYV7rI/ngRDg3Egb1bBNiPP7DdlNSTQsE5v7C/amHDVR+TuBPQVxx2n4N5/Z5fgb2bKO/Hr+XsjJS5I/nSA8b0zOssftOABxoH3+vdYrNqvsun6C/HBf8xoj3Plz6omWYu38CIpkAGQAAAPEpecsiAbSY1Hbd1OXMf6muZKvKf/tWdTs2qfy3byRJO2e/oJ2zX1De2CnKGjxORf+9V/bSbep5xbOyZuXFeOTRddiYnjpsjP9qSETOs+1LS008GQ98oxSr1eIVWMeCZxgc7RZL+TlpGr9f14j2ceVNbXNT1LtLrl68+XDNXbxFj7+zyHu7MG6sePbOb/xJAP/fq7+l+4VoLSJJ044crLzsNP20dKuWrt3ZaH2w0dbZG3qEnHHEIL3y8TJJwW9gHHlAL330/VodOba7PvphQ+PzNSG38wzDI30veG4e7q7+Wrb4CxybUxHer3u+BvUs0JihnbRxa6kW/L5OXdo1rr4HAAAAAInAHEAzpOR3VMGBJ0mS2hx8urZ/+owql8+XJJX+8L5Kf3jfve2GJy5Vz7+/sMf3O0dg2RkpOubA3jLN0H3AE1ksqg8LQkwcGZUbFNH8tnyOVZCTrqMO6N04MA9wzvRUj8ubIOMK+G37WXHw8G6BD7RbRppNp0waoMIdFf4D8yATjNbaGyrM+3XL9xhK4NfmguP30qEju6tLm1S/gXlTeN48ifR94TnWcPe1WgydOnmA3vhsefANm/H+SrFadMEJe0uSBnTLVufMXa02uS0AAADij8PhUF1dXVSPWVNT4/6/xcKnGoNpiecqJSVFVmvT2pz6Q2AOICpsee3V6eR/yllbrdL5s1T8zeuSsyEAclZXaM0dJ6nt5HOUO+pognP45Qq1EB3XnTNa6wvLtHe/dkG3a4mK/hvODdxKJZSmtuS57JR99cF3q70mYQ1WmRzud92rc26TA9Y/HdJPn/24XpNH9wg8FjNwD/NgrFaLBvZso8pK/xOGmjJVkJOm4rKasI9paULo7eK5dSS/448+oLfe/2aVxu1d/0kEf89ScyrM6VeOlsTVDAAAicM0TRUWFqqkpCTqx3Y6nbLZbNq8eTOBeQgt9Vzl5+erU6dOUcmbCMwBRJUlNV0F4/6kvFFHyXTUSaapdfef416/47PntOOz52RYU9Ttbw8qJb+jJKl223pZs9vImpEdq6EDLerwsT31yQ/rdOqkfpJKWuWcY4d1Dmuyzbzs1Kife/TQTk3eN9x80+lTsT15TE9N9mmTFPRQYV5IeX4q4NgD++j9b1Zrwn6BK849L9C6dcjRO3cdo9QUqx57+9eA+9TWNbRkieYNjCevnaQ/3/KJKqrtYW3veW7fHuaRCPYtdOuQrY1F5e6vC3LT9eq/jpLN1ZYpyvn2xSftE90DAn5wWwYAgPjnCss7dOigzMzMqBbyORwO1dTUKC0tLaqVzsko2s+VaZqqrKxUUVGRJKlz59B/A4dCYA6gRVhS0yXVt4Hofc2bKlv0pbbPfNy93nTUacsrtyh3+GGqXLVA1et+V0affdV56g0xGjHQsi4+aR9NPWygMlJMLV1aEuvheOnbLV9nHTVY7fJ9e3yHr3/3fC34oygq4wkUPGWkWVVVE2Elts/Bjjqgl2Z9v1ZS4P7gwS6bO7XN0jt3HRN08kjf6+7UlPqLQN9K50mjemj2/PU6ZfIALV65XWu37FLX9lnRK1k169vT9Omar8Wrtoe1i1cf8kjP57FDoOdn+KAOGtanrV6ctdRruc1jDgN/bYsG9igIfmoj8I2W3l2Sa/4MxBeDqBwAgITgcDjcYXnbtm1b5PiSlJ6eTmAeQks8VxkZ9X/LFhUVqUOHDs0+Lp8RANDiDItVuftOUveLHlXemGPdy+0lW7Xzi5dUve53SVLV6oXa8vrtqlq7WPZdO2I1XKBFGIbhZ9LJ+HHyoQOaNfnoyZMGqG+3lg0mn77uMN13+UHur8OpRPcNqb0nmQww6WeIpDg1xRq0GsUVkDcai09F/GWn7qvXbjtKe/Vtp5MPHaCrzhypf18y3u+opp8xQu0LAr9/hvetn8Ty2PF93MtcZ0tJCT/cb06RTXqqTadOHqATD+kX8L1uMQz3+6xnpxy/2/jOYfDX4/fymhTYH9piIPZ4FwIAEM9cPcszMzNjPBK0FNdrG43+9FSYA2g1KQWd1HbSn9Xm0LNUs3mlSua+q8o/5nltU7VqgapWLVBal/7qcvbtMizcmQUSQVqKVSdN7K9/v/hT8w8WIAnPzUpVblZD+5hwep3nZHq3m/GqoG6hfOvUSfUV44eO6uG13PfbMgxD2RkpkuorssfvW9/D219LloOHd9PBw7vp2Onv+T3n0aPydcLEIdqrfyd98O1q9zEl6W8n7K1bnp6rEw7uF3LsnjcCmvIR1WlHDA5xfKldfoZevOlwpaX6//1+7rFDtXNXtRatrK+Kb2pPewAAAMAX86klr2i+tgTmAFqdYViU3nWAOp10lSTJWVst0+nQuvvOlqsmsmbzCq258xRJUmb/kUpp21VmbbXaHnauDCu/uoBk5gwzHw2nwvzMIwercEeFDtvd29zzEuqkif317Ae/a9zeXbz2MZpZKZqXnaaH/3FIo+VHjeul2fPXa/igDn72ajCkT1v16JSjbh3Cn9PBajE0oEe+rFaLzjhikBYsK9LEkfWV3J3bZemJf04K6zhNndw0lMG92mjp2p06YmwvSfV9ywMpyE3X7ReOC3hzAIhHzC0LAAASmWma3EzwQOoEIObq+51Lfa57W3WlRdr01HQ5ayrd6ytX/CStqK9adVTtUtbAMdo26z/KHT5ZbQ89OyZjBhBdFovRqGVJKP56XfvKz0nT7ReOc39teATCx0/oq737tVPPzrneO7XQdWL/7gV6+ZYjGlW9+0qxWfTIPw5p8gXraZMH6rTJA5u0r+cZo3m9fPuFB2jrzkp16+C/DUtQBJGIY/xZCQAAYmH58uV6/PHH9eOPP6q0tFT5+fkaOXKk/vrXv2rIkCFhH6ewsFA33XSTbrjhBnXr1q0FR5xY6GEOIK6k5HVQr3+8pB6XPaXckUc2Wl+xdK6K3n1AZm2VSn94X9s+fFSOitIYjBSAr/7dg0/M6I8rPPae1DG8hLQpOWpWeor7sWEY6tst32vCSX/CaWUSrrzstLCquAOF5X/7095e/2+u9DTv2gmvlixRjAJTbNamheUiL0dicL1PDxvbM6bjAAAAyW/FihU69dRTtXPnTl133XV69tlnddVVV2nz/7d33+FRlen/x99nJplJryQ06b1EQKlSBcuK6MKKFbDgun4tLCoCIv7su6tYQGFRUdZVXNeOim1FWUQUERBEadI7CWmkTZ/z+yObgTGBTDA9n9d1cTFzznOe85w7Z2bu3DnznEOHuPLKK9mwYUPIfX377bcsX768ysZaV+kKcxGplcJik2h04R9pdOEfAXAe3E7mpy/gTt8d1C7/x2Xk/7iMqA69sTVuQ1h8CtEd+2CNiiurWxGpQo2Tovj71HPLvYL6RE/+eTCfrtrD6KHtuP7hz4EKTG1wGpXUrm2SuGRwW5qnnHy6k5J5xQFeuGcEzU7RtrpdPLANw846g+gTxvhb9O/WhN5dGrN2S3ql9FdjDENzYkgNOX7eXXV+Jzq3SqrBsYiIiEhD8PLLL5OQkMBLL71EePjx3wvOO+88LrroIubPn8+CBQtqcIR1nwrmIlInRDTvwBl/fBLT56Fw63fkb1yOY9eGwPqi7WuLp24BMj+GlN9PxtaoBfkbl2G6XXgLcgiLSSBh0FjCExrX0FGI1H8tm1Tsj1XNUmK48dLuAHRsmcAv+3Lp3aUx4Ct321CmZPk1wzD40+i0U7bp2TGFUQPb0KppXK0qlpeorGI5gNVqYeYNfRkzbQnwqxuO1pK5Jmxhp/+FSE3DKNXHIDHOXtODEBERkQYgMzMTKP37UFRUFDNmzMDhcASWffHFF8yfP5/t27cTFxfHRRddxF133UVUVBTvvfceM2bMAGDEiBGMGTOGxx57rPoOpBZTwVxE6hTDGk5Mt8HEdBuM3+0k/8dl+F1FFO38AdeBbYF2Rz94psztnQe2ccbNz+hmFiK10BOThuD2+Iiwh1FUVFRu+/iYqilOGYbBzZU05UldcOL7oeWE2nRNv0teO7ILP2zLYHifluW2PdlYa/oYpP7TOSYiIiLVbdiwYXz11VdcddVVXHbZZfTv35+2bdtiGAa/+93vAu2WLFnC3XffzSWXXMIdd9zBwYMHmT17Njt27ODll19m2LBh3HLLLTz33HPMmzePTp1O7z5I9ZEK5iJSZ1lsEcT3GQlA4qCxmKYfb14m2V++SuGWVWVu48k6SNZnL5J07jgMe5QK5yK1iMVilJpTuywzb+jLp6v2MPGSbtUwqvrvxCnVg+Ywr+H3x8tHdOTyER1/Wyd6j5dqogmBREREpLpcc801HD16lIULF/Lwww8DkJiYyKBBg5gwYQI9evTANE2efPJJBg8ezJNPPhnYtnXr1lx//fV89dVXDBs2jJYtiy9O6dKli276eQIVzEWk3jAMC+HxqaSOmYLvvGwskTEYYTa8uelYoxPIWfk2x1a9T94P/yHvh/8AYImIxu8sBCC660BSfz8Zw2I93qnmxBWpdfp3b0r/7k1rehj1RnCRnDIf1zUXD2zDx9/s5rqRXWp6KCIiIiIilW7y5Mlcf/31fP3116xatYrVq1ezZMkSPvroI2bMmMGgQYM4cuQIN998M16vN7Bdnz59iImJ4ZtvvmHYsGE1dwC1nArmIlLvGIZBWFxy4Hl4YhMAks4dT3hSUzI/fi6wrqRYDlC4+Rv2H9pBeHIzks4dT97az0jcsJSsH9sSce2jWMI1N6mI1G81fVX56fr1sP80Oo1LB7elaaPomhmQNEB187UjIiIidVd8fDyjRo1i1KhRAGzevJlp06bx5JNP0r178X2iHnroIR566KFS22ZkZFTrWOsaFcxFpMEwDIO4nucR03UQeeuXUvDTV/gKcvAV5gbaeHPT8eamc3Dn+sAyz5FdHFw4laRh44hs11OFcxGpdwakNSU7z0mbZvE1PZRKYbEYtfKGrVL/GJqMRURERKpReno6l112GZMnT+byyy8PWte1a1fuuOMObrvtNnw+HwDTpk2jb9++pfqJj68feX9VUcFcRBociy2ChH6XkNDvksAyv9dN7jfvkrvynTK38WQdJP3dWUR16EOTK+7BNM3AlZimz4Pf5cAaFVct4xcRqWz3Xt836H0NICUhsgZHJFK3qGwuIiIi1aFRo0aEhYXx+uuvc+mll2K3B1/Qt2vXLux2Ox06dCA5OZkDBw5w4403BtYfPXqUqVOnctVVV9GyZUssFkt1H0KdoIK5iAhgCbORNPRqkoZejfvofo6t/QQjoSl7LY1I/OKpQLui7WvY9ZfLALCltiIssQlF21aDNYzU0XfgLzxG9oo3aXr1/dibtKmpwxERqbCSYvmL956H2+MjJspWwyMKXZjVgvd/V9GIVCdNxCIiIiLVyWq18uCDD3Lbbbdx2WWXMW7cONq1a4fD4eCbb77hX//6F5MnTyYxMZE777yT+++/H6vVyrnnnkteXh7z588nPT2dbt26ARAXV3zh39KlSxkyZAjt2rWrycOrNVQwFxH5FVtKC1IuupmioiLYsoXGd7wCBzZTsHklhZu/CbRzZ+zFnbG3+InPS8a7x+88fXDh3bS4ZS6u9L1EtOhCWExCNR+FiMjpaZJc9+b9fvCmAcxatIY/jTmzpociIiIiIlKlhg0bxltvvcXChQt5/vnnyc7Oxmaz0bVrV2bPns0FF1wAwOWXX050dDQvvfQSb775JlFRUZx11lk8+eSTtGjRAoB+/fpxzjnn8NRTT7Fq1SoWLFhQk4dWa6hgLiJSDsOwENWpL9Gd+lLUcwTu9L1487MwXUXk/7jspNvtf24SALbUlsSdfRF+ZyEJ54wBwPT7wLDU2RvsiYjUJt3aJvPP+y/Ue6pUP0OTsYiIiEj169atG08//XS57UaOHMnIkSNPuj46OpqXX365ModWL6hgLiJSAVFtehDVpkfgedLwCRTtXI8n+zCO3T/iztiH6XEGbePO2Efmpy8AYHo9xHQfzOE3HsUaHU+za/+iAo+ISCXQe6nUJFOTs4iIiIjUGyqYi4j8BtaoOGLThhY/GXoVAH6Pi8yPn6Ng09el2ud8/SY5X78JgDfnCLv/OpaWk14ADCwRxdMgGOF2FX5ERETqAH1ai4iIiNQ/KpiLiFQyS7id1NF3kDr6DnyOAnK/fY9j331w0vb75t4c9Nx+RiciW3Yluss52FJbYVisVT1kERER+Q00MYuIiIhI/aGCuYhIFbJGxpA84lqShk/AMAych3bgOrCVqI59yP7yVQq3fldqG9eBbbgObCP328XH+4lJwmKzkzrmLhXRRURERERERESqiArmIiLVoGSKlYhm7Ylo1h6AxpdNxfR58WQdpGjneo6t+QRrdDzuI7tKbe8ryMYHHFw4FQBbamsajbwZe7MOOPdvASCyZdfqORgRERERERERkXpKBXMRkRpkWMOwpbbCltqKhAGjATD9PnwFuRTtWIcrfTf5P3xeajt3xh4O/XNG0LKEc8aQMOhyDIsVx+6N2FJbERaXjK8oH19BNrbUVtVxSCIiIg2GoclYREREROodFcxFRGoZw2IlLC6ZuLMuACDlopsxTRNvbjpH3ngUT/bhMrfL/XZx0DQuAPEDRpO39jNMr5sz/vgUttSWeHLTyVr6T8KTmpLQ//cc+34J0Z0HYG/arsqPTUREpD4ydftPERERkXpDBXMRkTrAMAzCE5twxs3P4CvKx2KPxF+Ux9FPnsN1aAem1wOmienzBG13bNX7gccHXryzVL8lNyPN/W4JbWe8WaXHICIiIiIiIiJS26lgLiJShxgWK2ExCQBY4lNoevX9gXV+t5Osz/9B0Y51+ApzK9ax34u3IJei7WsJS0jBYovE78gnqv3ZQc18jgLw+7BGx//GIxERERERERERqX1UMBcRqScstghSRt0aeO49dhSfsxBLuB3XkV1kLX0Zv6OAiFbdiWjZhZzlrwdtv++ZG8vsN7zRGUR37Is1Op6spS8DkDjsGhIHXobp92FYrFV3UCIiIrWYJmIRERERqX9UMBcRqafC4lMIi08BIDypKTFdBwattzduTeHW7/C7nRRu/Q5Mf5n9eDIPkJt5IGhZzvLXAwX3yDZnEn3udcVTwvh9VXAkIiIitVPJTT81h7mIiIhUl3vuuYfFixefss22bdsq3O+ECRNo3rw5jz32WEjthw8fzpgxY5g0aVKF91XbqWAuItJARbU/OzDlijc/G3f6HnyOfKyRMbiP7sex92ccO9eX249j90Ycu6eQCGQsiyBp+HiKtq0m8dzxRDRrX9x/XhaWyBgs4faqPCQRERERERGRem3mzJlMmTIl8HzQoEHce++9jBw58jf1O3fuXKzW0L9B/s4772C318/f8VUwFxERwmKTCItNCjyPan82CQNGB5670vdgCbdhjU0m472nKNqxrsx+TI+TrP+8BIDj5elYY5LwFWQH1sf2PI+4sy/ECLdTtH0tMd2GEBabGFhftOtHbKmtAvO0i4iIiIiIiMhxsbGxxMbGllqWkpLym/pNSEioUPukpKTyG9VRKpiLiEi57I1bBx43HjsVX1EBYbGJeI5l4Ni5gcxPXyhzuxOL5QD5G74gf8MXgefZX76KvWl74npfBBYLRz94hohW3Wg2/uEqOQ4REZGqYNb0AERERET+57333mPevHmMGDGCxYsX07t3b55//nmWLVvGggUL2LZtG16vl06dOnHXXXdxzjnnAMFTspT0MWnSJObPn8/hw4fp1KkT9913H7169QKCp2SZO3cu33//PUOGDGHRokXk5OTQq1cvHnzwQdq2bQtAdnY2jzzyCF9//TVWq5XLLruMn3/+mT59+tS6aV1UMBcRkQoxrOGBq8LD41MJP+sCwjoPYsuWLXQ8ozF2i0nO12/iProfz6/mPi+L6/AOji6ZG3ju3LuJXX+5DIDIdr1IHnEdtpQWmKaf9Lcew5uXSeqYuwiLT8H0erBGxlTNgYqIiJTDUKlcRESkTjNNE5f7t9+Ly+f34XT7wOLFagk9P7DbrBhG5d8L5eDBg6Snp7N48WKcTic///wzt912G1OnTuWJJ56gsLCQ2bNnc/fdd7N8+XJsNlupPjIyMnjjjTd44oknCA8P58EHH2T69On85z//KXPM69evJzIykgULFlBYWMj06dN56KGHeOWVV/D7/dx88834fD5eeOEFTNNkzpw5rF27lj59+lT68f9WKpiLiEilscYmYYuKovEf7i5zvSc3naLt63Ds2oC3IAf3kV2n7M+xcz0Hdq4nomU3nPs2BZYfeGFy4HGTq/8fUW17YpomhmEE/vcW5GIJt2OxR1bOwYmIiJyMqZt+ioiI1DWmaTJ93kq27Mkuv3EV6dI6icdvH1QlRfNbb72VFi1aALBlyxbuu+8+xo0bF1h/7bXXMnHiRLKysmjatGmp7T0eDw8++CBdunQB4Oabb+a2227j6NGjpKamlmrv9XqZNWtWYGqXCRMm8MQTTwDw/fffs3HjRj799FNatWqF0+nk6aef5rzzzqvsw64UKpiLiEi1CU9oTHyfkcT3Kb4Zien34Tq8E/w+PDlHsNij8TnyKdzyDabXg/dYJt5jGUHF8l878u9HMMLtmD4v4YlN8GQdPL6/Rmdwxo1PYoSFY5p+/EX5FGz+Buf+LUS07EZcrxEY1nB8Rfl4j2Vgb9quymMgIiIiIiIiUtVat24deNylSxfi4+N58cUX2b17N3v27GHLli0A+Hwnv8K+XbvjvyOXzJvu8XjKbNuoUaOgedBjY2MDbTdv3kx8fDxt27YN7C85OZk2bdqc1rFVNRXMRUSkxhgWKxHNOwIQ0aJLYHlczxEA+D0u8n9chifzAK703SQOHEvRrg3krfsM/Mc/1E2PCyCoWA7gyTzA7sevKnPfhVu+Jfu/rxF75rnkrf2keAytuuMrzCWu1/nEnHkuFlsEhiX0u4SLiEjDpIlZRERE6h7DMHj89kGVNyWL00VEhB1rBX6HrKopWQAiIiICj9esWcPEiRMZOnQovXv35uKLL8bhcHDbbbedso+ypmoxzbIzn7LalrBarfj9/hBHXvNUMBcRkVrLEm4nvvdFQcui2p9FowsmAuDYt5n8H7/EfWQPps9NTLch+J0FeI4dxe8oOOWV6QCm2xEolgM49/4MQNbSl8la+jIAyRfehMUeQXTHvoBBwZZvMaxhxKYNDWznKzyG3+0gPLFJZRy2iIjUEZqIRUREpG4zDIMI+28vj/p8Bvi9RNjCsFpr30VXCxcupF+/fsybNy+wbNGiRcDJC+CVqXPnzuTn57Nz587Ale+5ubns3bu3yvd9OlQwFxGROiuyZVciW3Ytc51pmhRuWknehqVEtumJN+cIps9DVPuzie46kIz3nqRw63fl7iPrPy8CcPRXy/3OQqI798cSbufQq/fhyTlCTNpQCjb+l8g2PWh8+XQwTVyHthPRqnuVXTUgIiI1T1eYi4iISG3WtGlTvvjiC9auXUuTJk1YvXo1zzzzDABut7vK99+vXz969uzJtGnTmDlzJoZhMG/ePBwOR638XVkFcxERqZcMwyCm+2Biug8uc33qmLswvW6MMBuuI7ux2Iq/rmb6vLgObCPzswWn7D/r84Vkfb4waFnBxv8C4Nj9I3tmXRO0zt68E66D2whv3BZr++F4cxLxhzcn6z8LcezbREy3wSQM/AOWsONfY8v55j18eZkknXcdlnA7AN68LBx7NhLTbRCGNbxiQRERkUqmUrmIiIjUfn/+85/JzMzk//7v/wBo3749f/3rX5k6dSobN24Mmqu8qjz77LM8/PDDTJw4EbvdztVXX82uXbsID699v9eqYC4iIg2SYbFi2CIBiGjWPmidvXFrYnudh99ZhOn3ggk5K94ofuzzUbDp6wrvz3VwGwCe9F3Epe8i8xvIPGF97sq3yV35Ni0nLcCTfQhvXiY5y/9VPL7W3Ynpcg4Ah//9MJ7MA/iK8kjo//vTOHIREal8te/KKBEREWkYtm3bFvT8D3/4A3/4wx+CliUmJjJ37txS215wwQWBxyVTtJysj379+gXta9myZYHHkyZNYtKkSScdR3Z2Nps3b2bOnDlYLBacTicWi4VXXnmFxo0bh3qo1UYFcxERkTIYFivWqNjA85SLbwk8Tv7dTbgO/oIRbsex+0ei2vbC7yzEW5hDTJdzyP7qDfLWfHxa+90390+llmW89xQ5yW8Q1f4sPJkHACjc/A0J/X9fPN+cz4uvKI+MD58tnhqmyznEpA3F3rj1aY1BRERCozK5iIiISPnCwsK48847ueqqq7jiiisoLCzktddew2azMWTIkJoeXikqmIuIiFSQNSKaqHa9AMqcQ73RBRNJPv8GTLcT0+fBm5dFWGwS3vxsCg7uIH3bBuItblw7f8CwhhM/YDSO3T/iOvjLSffpyTrIsayDgeeuwzvJWDKXgo3LS7U9tvpDjq3+kEYjbyGu13k49m2icPO35K37jOguAwhv1ALXoR0kDPg99qbtyV62CNProdHF/4dhWMrcvzc/B2t0HEYF7vguItJQaGIWERERkZOLi4vj+eefZ86cObz55psYhsFZZ53Fq6++SlJSUk0PrxQVzEVERKqAYRgY9kggEmtUHADW6Hh8cY1xkETrLl2IjIw8foOToVfh3L+Vgq2r8BzdR8QZXbBExeJ3Ocjf8AXhyc1x7FzPiWWZsorlJ8r85DkyP3kuaFnhllXAKgAcO38IWmeJiCa660Dw+7BERBOe3BxfQQ5H3ngUd8Zeojr0odHIm8lY/DSxPUcQmzYMX1E+loiocgvppmnidxZgjYw9ZTsRkbpEV5iLiIiIhKZ///688cYb+Hw+nE4nERERWK2184IsFcxFRERqyK/vBh7RojMRLTqXapc4sHjeN5+jgKId64hs1Z1jaz7m2HcfBLVres0D+L1uCrd+F7gBaUWUXJl+MkXb17DvmTUAOPdtxmKLJP2dWUDxTU39rkLCk5qSMuq2UoXxvLWfkvX5QlL/MCUwH7uIiIiIiIhIbaOCuYiISB1hjYwhNm0oAMkjriVp+AQw/WD6Mf1+LOF2AKI79Cah3yXkrv4Qa1Qc9sZtCU9qSlh8CsfWfYYvL5P4fpdQsGUV3pwjuA7twJN9qMLjKSmWwwk3Nc08wN6nrycsPoXItr2wRkYTltCErM8XAsXzsbsH7iW2x7lYbJGYpknemo/xFeWROPhyLPZoLPZIfEXHwAye5CDn67co3LqapuMeDJpfvqq5Du0ge/nrJJ93PbbUltW2XxGpAwxNxiIiIiJS36hgLiIiUkcZhgGGFbAW/3cCW2orUi+ZVGqbpCFXHn+ccrz4a5p+3Bn78OZm4Nj7M/bGrcn55l38riIaj76TyDZn4sk+RPZ//0Xh1u/KHZv32FHy139e5rrcb94h95t3Si3P3/DF/8beEnfGPhKBIvMmbJ36ULhtNTkr3gTg0CszsMYk0uiimwlPbh6IhWmaZLw/G29eFimjbsWXn01Eq274HYVBBXZ3xj7c2QeJ6Tyg3OMASH/vKbzHMjjw0hTa3vv2Kdu6Du3ACLdhS1FhXaQhMTU5i4iIiEi9oYK5iIiIYBgW7I1bY2/cmuhOfQGI7TE8qE14UjMaXzYVv8dFwcb/YoTZiO5yDr7CXHK/+4DY7kNwZ+ylaNePFP3yPQCWiBj8zoIKjcWdsS/wOO/zF8n7/MWg9Z7sw3iyD3PghcmBZXG9R5K39pPA8wPP/7lUv5aoOBIGjCb7y1cBOBpmI77vKBKHXYNz/2ZsKa2wRsYEbeMrPIb3WEbxE9NP4S9riGrXC29+Fq5DO4hq2xNLRHSg7cGXpwPQZsZb9f4Gqa70PWR/8U8Sh11DRPOONT0ckRqhMrmIiIhI/aOCuYiIiFSIJdxO3Nm/O/7c1oSUi24GIKJFl8A60+/DsFgxS6ZW8XvJ/M8/8BXm4Hc78WQfxnQ7SBx8BbbGrTn870fA5z2tMZ1YLD8Zf1FeoFgOYHrd5H77HrnfvhdYFtvrfPLXL8WwhhPZ/iyKtq0O6iP97cdCGo83N4PwpKbH9+11YwmzhbRtWUy/D19RPmExCfjdDpz7thDZrieGYQHAcBbgObIL2nY/7X1U1OHXHsDvLMD97pO0+vOCatuviIiIiIhIVVLBXERERKpEyRXWgZubWsNJGXnzSdu3mf4G3tx0wuKSKXK62fnf92jdoz9hrjz8bifRnfpR8PMKfIW5RLbthfPAVgp++gpf0TG8xzLB9BPZtgdJw6/Fk32Y7C9fPX51+P/Ym7bDdXjnSceQv34pAKbPE1QsD09pgefo/pCPff9ztxOTNhTT76Nw00qg+I8JGBai2vUk/+cVJJ9/AwBHXn+Y6M4DSB19B4a1ODUzTRPT7aBgy7eYHhdZn/8DgIRBl+PJ3E/h1u9IGj6BhAGjMU2TuJUvkOV1EXnzM9ganVFqPH6vm4If/0tUp36ExSSUOeaCTSsxbBGExSZT8PMKEodeFZgXvywl3xzw5Wfh97go2vkDUW16YrFHhhwnERERERGR2kYFcxEREakVDMMgPLHJ/x578DTtSlhyc6KiOgTanDhNjL1JG+J7XwQUX4Ftet1YbMXFWnvj1sR0KZ6j3PR5wWINFO5Nvw/n3k1YImMo+PlrPLnp+F1FWCNjKNyyqtS4YtKGkTLqVgq3rOLY9x/hOrQdS1QcptuJ6XWf9HgKfvoq6Llz/5bi//dtAooL5SUKt65i92OrSBx6NZFtepD930U4924q1WfuyuNzqGcvW1Q853z6HixeFwB565eSdO64oKvZ/c5C9syeCH4vtvVLOeOPTwLFU8j4vS6sUfH4i/LIeH928M4sFpKGXQMc/+OHryiPvPVfEHvmsEAzwxbJ0Y/+TuHmb4hJG0rqpaWnwxGZNWsWK1aswDRNLr/8cq6//vqaHlKl0hzmIiIiIvWHCuYiIiJS5xkWK4at7CubS67aPrFtZJszAbA3aRu0zufIx/R6CItNwvT7wDAC057EdBtETLdBgbZ+txPnvk3Ym3fCYo8snn7G5yV72SKOff8xYFb4OHK++jc5X/075PZ5P/wn+Pn3H5H3/UfYm3XA3rwDMV3OIWvZa+AvnurGnb6b7OWvE9GiC0feeLQ4HuH2wNXuJ3Id/IUDL03Bc3Q/LW79O978bHJXvoNj94+BG7QCmG4HhZu/AYr/SOB3FhIWn0KjC/9YoWM3/T4cuzcSntyc8ITUCm17Ir+rCMMWefybDVLjli1bxi+//MIHH3yAy+Vi7NixnHPOOXTsWPfnvjdO43UuIiIicromTJhAXl4eH3zwQZnr77//flauXMmXX3550nz4vffeY8aMGWzbtg2A4cOHM2bMGCZNmlRm+7lz57J48WKWLVsW0hhN0+T9999nyJAhJCcnl9pfXaCCuYiIiMj/WCNjA4/Lu2mnxRZBVPuzg5YZ1jCSz7+B5PNvwDRNXId2YI2KxfT78OZlEhaTSO63i/E7C/HkHMaTdYjkC26k4KfluA7vxBIVh78or3gssUmEJzXDltKC2DOH48k9Qv6GZfjdRUS06MKxVe9jiYzF78gvNTbXoe24Dm0nb03pud1zv3k36LnpcZH5yfOl2jn3bQ483j//tqB13tz0k8alaPtaAOzNO5L9xT+J6z2SmO6D8eZlYmvUAtPr4diajwiLTyW+90WYpkn++qXkrfs0cMPXM/5vLqbbgSUyNlA8N02z3CK488A2Dr0yk9he559y+h+pXs2aNePOO+/EarUSFRVFy5YtOXLkSL0omJdQ2VxERESqw9ixY5k2bRrbt2+nQ4cOQevcbjefffYZ1157bYUuHnnnnXew208+FWNFrVmzhnvuuYcvv/wSgJEjRzJ48OBK6786qGAuIiIiUgUMwyCi+fEk1pbcHIDU308u1Ta+z8jAY0/2YcLiG2FYw4Pa2Ju2JabLOYHnSeeOAwyKiorYunUrnVqfQdG3b1Ow8b9ljidh4Fhyv3nnVCMmttd5+F1FuI/sxpN9KISjPLmjHzwDnPqq+bx1n+HJPFBq+YHnj1/dEhafAqaJNy8TIzyCsIQUPEf3E9/vUkyfB+eBX3Af2Ulk6zQsUXGASf76zwPb5qx4gyZjpxPV4WxMrwf8/t90XFJxnTt3Djz+8ccf2bx5M2eddVYNjkhERESkbrrwwgt55JFHWLJkCXfddVfQui+//JL8/Hwuu+yyCvWZlJRUmUPENIMvJYiIiCAiIqJS91HVVDAXERERqUXCk5qG1K5kqpiSq0cskbGkXnI7KaNuw5eXiYmJ31GI31WIvWk7LLZIEodcQdH2dbgz9xPVpge2pu1wp+/Gnb4HW2or7E3bAWB6PeRt+BJfQTYRzTuRv2kFEWd0wX10L57Mg/gc+XhzjmBLbYU7Yy/2Zu2JbJWGK2MPRdu+J9Trbcsqlv+a99jRwGPT4wzcfPXY6g+D2jn2/BT0vKRoDnDkrb8S2eZMXBn7iPe48befA1FRIY1RQvfhhx8yZ86coGXnnXce9957LwAbNmxg0qRJPPbYY8TExNTACEVERESCmaaJ6XH95n78Ph+mx4XfAob11N9UPZERbq/Q1eARERGMGjWKjz76iDvvvDNo2w8++ICBAwdiGAZ333033377LceOHaNRo0aMHj2ayZMnY7FYSvX56ylZ3nzzTV566SXS09MZNGgQzZo1C2q/fft2Zs+ezbp16ygsLKRp06aMHz+e6667jtWrV3PttdcCMGLECP72t78BBE3Jkpuby+zZs/n666/JycmhW7duTJkyhd69ewPFU8B8//33DBkyhEWLFpGTk0OvXr148MEHads2eErNqqKCuYiIiEg9YhhG8VXZAPG/WmexEt2pL9Gd+gaW2Zu0LTWXuxEWTnzv3wWeR3UInnoGTj1FijvzAJZwO5boeHz52WCaFP6yBmtMAmGxSbjT91C0awOGJQxbSguiu5yDLz8bS1Qs1uh4jn40H0/2YXwFOVijYrFGJ+LO2HN6Afkfx+6N/zvgmFLz2kvluPTSS7n00kvLXLdy5UqmT5/Ok08+yYABA6p5ZCIiIiKlmabJoVdn4jpQc3Nr28/oTLNrH61Q0Xzs2LH8+9//Zt26dYEic1ZWFl9//TVPP/00N998M8nJySxcuJCYmBiWL1/Oo48+SlpaGuedd94p+/744495+OGHuffeeznnnHNYunQps2fPpmnT4ot6HA4HN9xwA/379+f1118nLCyMd999l7/+9a/07duXXr16MXfuXCZNmsTbb79Nx44d+eST49NE+nw+brrpJlwuF3/7299o3Lgxr732Gtdffz3//ve/SUtLA2D9+vVERkayYMECCgsLmT59Og899BCvvPJKRUN8WvTbgoiIiIhU2KmSelujMwKPLYlNAEjof7yQGtmqO/F9RwVv1KRN4GGz8Q8BxTcCPXEuedPv+99V6Qae3HQiWnbFGhGNtyAH596fyfvhcyJbn0l05/449m7CeyyD2B7DKdz8Le6j+/DbIjncKI1mYcHT3UjV2rt3L9OmTeOFF14I/BIkIiIiUjvUvRvFd+/enc6dO7NkyZJAwXzJkiXExcUxcOBADh48yIUXXkjz5sVTQk6YMIEFCxawbdu2cgvmr776KiNHjmTcuHEA/OlPf2LDhg1s3boVKC6YX3vttVxzzTWBbwzefvvtvPDCC2zbto0uXboQH1981U5SUlKpqVhWrlzJpk2beOutt+jevTtWq5X777+fH3/8kYULFwa+rej1epk1axYJCQmBY3jiiSd+e/BCpIK5iIiIiNRKv77xqmGxYkttBYAttWVgeVhMIjHdBhPT7fjNhGwpLY4/HnIFAEVFRRzasqUqhyxlWLhwIR6Ph/vuuy+w7O67765zN38SERGR+sUwDJpd+2ilTMni8/lwuVzY7XasVTglS4mxY8cyb9487rvvPsLDw3n//fcZPXo0MTExjB8/ns8++4xXXnmFvXv3snXrVjIyMvCHcC+fX375hYsvvjhoWa9evQIF86SkJK655ho++eQTtm7dyt69e9nyv/w61P5jY2Np3759YJlhGPTu3Zuvv/46sKxRo0aBYjlAbGwsHo+n3P4riwrmIiIiIiISZP78+axatYpFixYFlvn9fubNm8fbb79NXl4eZ599Ng888ACtWrU6ZV8PP/wwDz/8cFUPWURERKTCDMPAsP32G1KaPh+GHyy2CCwVKJifrksuuYRZs2axYsUKWrRowZYtW3jqqadwOByMGzcOh8PBRRddxO9//3v+3//7f4ErxkPx65t2hocf/3ZmZmYmV1xxBYmJiYwYMYIBAwaQlpbG0KFDQ+67rD8Q+P1+wsKOl6ltNlvI460KKpiLiIiIiEjAP//5T5599ln69OkTtHz+/Pm88cYbgfkmn3jiCW666SY++uijav2lxjRNioqKqm1/Docj6P+TcbvdQePy+X1l9mUPK//qq7rI6XQGHns8nmr9GdVFoZ5XolhVhGIVOsUqNPUpTi6XC7/fj8/nw+cr/Rn9W5UUmU3TrJL+fy02NpbzzjuPTz/9lMaNG3PWWWfRunVrli5dyqZNm1ixYgWNGjUCim+ymZmZGTj2kivBS8ZpmmYgNp07d2bt2rWMHz8+sK+NGzcGjuuDDz4gNzeXTz75JFBI/+WXXwL9+Xy+QCxK+jxxf+3btycvL48dO3bQrVu3wBjWrl1Lu3btAu1/Hcdfj7ksJds6HI4yr3Y/1T2Yfk0FcxERERERIT09nZkzZ7Ju3TratGkTtM7tdvOPf/yDqVOnBq4gmj17NoMHD2bp0qWlvrpblTweT+Crv9Vpz549p1x/5MgRtmwpCDx3FJUuLvyyfTuxkVV/1VlNOJjlDjzOyc2pkZ9RXVTeeSXHKVahU6xCp1iFpr7EKSwsDJfrt0+/cipV3f+JLrnkEqZNm0Z8fDw33ngjTqeTxMREABYvXsyIESNIT09n7ty5eL1eHA4HTqczMLVJyR+7TdPE6/XidDq57rrruPPOO3nhhRc499xz+fbbb/n8889p1KgRTqeT5ORkHA4HS5YsoVevXuzZs4ennnoKKJ7+0Ol0Bqak2bhxI5GRkUH7O/vss+nQoQP33nsv06ZNIykpiTfffJPt27czffp0nE4nXq8X0zRL/TH+xDGXxeVy4fV62bVr10nbhHqRhwrmIiIiIiLCpk2biI+P58MPP+Tvf/87Bw8eDKzbunUrhYWF9O/fP7AsLi6Orl27smbNmmotmIeHhwfNe1nVHA4He/bsoXXr1kRGRgat2/j+8cdNmjShS5fjc+dHrswH3EHtO3boQEKsvQpHW3PCDxwDMgBITEikS5eONTugWu5U55UEU6xCp1iFTrEKTX2Kk8vl4tChQ9jt9lI3oqwMpmkG5jA/nTnJT8eQIUOIi4sjJyeHSy65hIiICHr37s306dN55ZVXmD9/PqmpqYwcOZLmzZuzefNmIiIiAleGl8TBMAzCwsKIiIjg/PPP54knnuDvf/87zz33HD179uT666/n448/JiIiglGjRrF9+3Zmz55NYWEhzZo1Y+zYsSxbtizQf/fu3RkyZAj33HMPd9xxR2Au8pL9LVy4kMcff5y7774bj8dD165d+cc//hG4gWlYWBiGYQT9nH495pMJCwujZcuW2O2l860dO3aEHFsVzEVEREREhOHDhzN8+PAy1x05cgSApk2bBi1PTU3l8OHDVT62ExmGQVRUVLXuEyAyMvKU+7XZbEHrrZbSV5IX91H5v6TXBhERx6+oCw8Pr5GfUV1U3nklxylWoVOsQqdYhaY+xMlisWCxWLBarRW6KWeoSqYKMQyjSvo/mWXLlpVaNnHiRCZOnHjSbcaOHcvYsWMDz//73/8GrR81ahSjRo0KWnb33XcHHk+dOpWpU6cGrb/xxhsDjyMjI3nxxRdL7bNEo0aNeOSRR4iIiCgzVpMnT2by5MmnHHNZrFYrFouFyMjIMgvrFflDhiXkliIiIiIi0iCVzF3666+x2u32av3qsYiIiIhIVVPBXERERERETqnkKh23O3iKEZfLVee/oi0iIiIiciIVzEVERERE5JRKpmLJyMgIWp6RkUGTJk1qYkgiIiIiIlVCBXMRERERETmlzp07ExMTw+rVqwPL8vLy2Lx5c+AGTSIiIiIi9YFu+ikiIiIiIqdks9kYP348Tz75JElJSTRv3pwnnniCJk2acP7559f08EREREREKo0K5iIiIiIiUq4///nPeL1e7rvvPpxOJ3369GHhwoWlbgQqIiIiUluZplnTQ5AqUpk/WxXMRUREREQkyGOPPVZqmdVqZerUqUydOrUGRiQiIiJy+sLDwwEoKirSDcvrqaKiIuD4z/q3UMFcRERERERERERE6i2r1UpCQkLgBuZRUVEYhlFp/ft8PlwuV2BfcnKVHSvTNCkqKiIjI4OEhIRK6VMFcxEREREREREREanXmjRpAhAomlcmv9+P1+slLCwMi8VS6f3XJ1UVq4SEhMDP+LdSwVxERERERERERETqNcMwaNq0KampqXg8nkrt2+FwsGvXLlq2bKkpX8pRFbEKDw+v1Cv7VTAXERERERERERGRBsFqtVb6tCl+vx8Au91OREREpfZd39SFWOk7AiIiIiIiIiIiIiIiqGAuIiIiIiIiIiIiIgKoYC4iIiIiIiIiIiIiAoBhmqZZ04OoC3744QdM08Rms1XbPk3TxOPxEB4ejmEY1bbfukixCp1iFTrFKnSKVegUq9AoTqFTrEJXU7Fyu90YhsFZZ51Vbfusr2pbTp6fk0Ok4abItBEWFUuU/fgtorLznHi8/qD2KQmRWCz183Xq8frJznMCEB0ZTkxkeA2PqHbTe3foFKvQKVahU6xCoziFTrEKXV3IyXXTzxDVxMluGEa1/jJQlylWoVOsQqdYhU6xCp1iFRrFKXSKVehqKlaGYegXp0pS23Ly2MREAOLKWJcUVztvYlVVwsMsNE6Kqulh1Bl67w6dYhU6xSp0ilVoFKfQKVahqws5ua4wFxERERERERERERFBc5iLiIiIiIiIiIiIiAAqmIuIiIiIiIiIiIiIACqYi4iIiIiIiIiIiIgAKpiLiIiIiIiIiIiIiAAqmIuIiIiIiIiIiIiIACqYi4iIiIiIiIiIiIgAKpiLiIiIiIiIiIiIiAAqmIuIiIiIiIiIiIiIACqYi4iIiIiIiIiIiIgAKpiLiIiIiIiIiIiIiAAqmIuIiIiIiIiIiIiIACqY11p+v59nn32WwYMH06NHDyZOnMjevXtreljVLjc3l/vvv58hQ4Zw1llncfXVV7N27drA+hkzZtCpU6egf0OGDAmsb0hxPHjwYKlYdOrUibfffhuALVu2MH78eHr27MmwYcNYuHBh0PYNJVarV68uM06dOnVixIgRgM4rgPnz5zNhwoSgZZVxDpXXR11UVqyWLVvGZZddRq9evRg+fDiPP/44TqczsL681ys0nFhVxuutIcRqwoQJJ33vev/994GGdV6Vlx/o/UoqS0P4zA+FcvLQKScPjXLy0CgnD51y8tApJw+dcvKTq/f5uCm10ty5c80BAwaYy5cvN7ds2WJOnDjRPP/8802Xy1XTQ6tWN9xwg3nppZeaa9asMXfu3Gk+8sgj5plnnmnu2LHDNE3THDNmjPn000+bGRkZgX9ZWVmB7RtSHL/88kszLS3NTE9PD4qHw+Ews7OzzX79+pkzZ840d+zYYb7zzjtmWlqa+c477wS2byixcrlcQfHJyMgwV65caXbt2tV86623TNPUefXyyy+bnTp1MsePHx9YVhnnUCh91DVlxWrNmjVmly5dzBdeeMHcs2eP+dVXX5lDhw4177nnnkCbU71eTbPhxMo0f/vrraHEKicnp9R715/+9Cfzd7/7nZmfn2+aZsM6r06VH+j9SipTff/MD5Vy8tApJw+NcvLyKScPnXLy0CknD51y8lOr7/m4Cua1kMvlMnv16mW+/vrrgWXHjh0zzzzzTPOjjz6qwZFVrz179pgdO3Y0161bF1jm9/vN888/35wzZ47p9XrNtLQ0c+nSpWVu39Di+Nxzz5mXXnppmeuef/55c/DgwabH4wkse+qpp8wLL7zQNM2GF6sTud1u8+KLLzbvuOMO0zTNBn1eHTlyxLzxxhvNnj17mr/73e+CEoPKOIfK66MuOVWspkyZYt5www1B7d9//32za9eugQ//U71eTbPhxKoyXm8NJVa/tmTJErNr167m1q1bA8saynlVXn6g9yupLPX5M78ilJNXjHLy06Oc/Djl5KFTTh465eShU05evoaQj2tKllpo69atFBYW0r9//8CyuLg4unbtypo1a2pwZNUrMTGRBQsW0L1798AywzAwTZNjx46xZ88eXC4X7dq1K3P7hhbHbdu20b59+zLXrV27lj59+hAWFhZY1r9/f3bv3k1WVlaDi9WJ/vWvf3H48GFmzJgB0KDPq02bNhEfH8+HH35Ijx49gtZVxjlUXh91yaliNXHiRKZNm1ZqG6/XS0FBAXDq1ys0nFhVxuutocTqREVFRcyaNYvrrruOTp06BZY3lPOqvPxA71dSWerzZ35FKCevGOXkp0c5+XHKyUOnnDx0yslDp5y8fA0hHw8rv4lUtyNHjgDQtGnToOWpqakcPny4JoZUI+Li4hg6dGjQsk8//ZR9+/YxaNAgfvnlFwzD4JVXXmHFihVYLBaGDh3KHXfcQWxsbIOL4y+//EJKSgrXXHMNe/bsoVWrVtx6660MHjyYI0eO0LFjx6D2qampABw6dKjBxaqEy+Xi+eef57rrrgvEoyGfV8OHD2f48OFlrquMc6i8PpKTk3/7QVSTU8Wqa9euQc/dbjcvv/wy3bp1IykpCTj16xUaTqwq4/XWUGJ1ojfeeIPCwkJuueWWoOUN5bwqLz+YPXu23q+kUtTnz/yKUE5eMcrJK045eTDl5KFTTh465eShU05evoaQj+sK81rI4XAAYLPZgpbb7XZcLldNDKlWWLduHffeey8jRoxg+PDhbN++HYvFQvPmzXn++eeZPn06X331Fbfeeit+v79BxdHtdrNnzx4KCgq44447WLBgAWlpadx0002sWrUKp9NZZhygOEFtSLE60QcffIDL5Qq6iYfOq7JVxjlUXh/1kdfrZdq0aezYsYMHHngAKP/1Cg0nVpXxemsosSrh8/lYtGgR11xzDbGxsYHlDfm8+nV+oPcrqSwN9TO/PMrJT045+elRTh46fcadHuXkp6acvOKUkwerj/m4rjCvhSIiIoDiF1rJYyg+ISIjI2tqWDXqiy++4O6776ZHjx48/fTTAEyaNInrr7+euLg4ADp27EhKSgpXXnklP/30U4OKo81mY82aNYSFhQXeULp3787OnTtZuHAhERERuN3uoG1K3mCioqIaVKxO9P7773PBBReQmJgYWKbzqmyVcQ6V10d9U5IkrV69mmeffTbwdb7yXq8DBgxoMLGqjNdbQ4lVie+//55Dhw5xxRVXBC1vqOdVWfmB3q+ksjTUz/xTUU5+asrJT49y8tDpM67ilJOXTzl5xSknP66+5uO6wrwWKvlKQkZGRtDyjIwMmjRpUhNDqlGvvfYakyZNYsiQIbz44ouBF5NhGIE39BIlX9c4cuRIg4tjVFRUqb++dezYkfT0dJo0aVJmHAAaN27c4GIFkJ2dzfr16xk5cmTQcp1XZauMc6i8PuqTjIwMxo0bx/r163nxxRdLfaXvVK9XaDixqozXW0OJVYkvvviCM888kxYtWpRa19DOq5PlB3q/ksrSUD/zT0Y5eWiUk1eMcvKK0WdcxSgnD41y8opTTl6sPufjKpjXQp07dyYmJobVq1cHluXl5bF582Z69+5dgyOrfq+//jqPPPII48aNY86cOUFvOlOmTOHGG28Mav/TTz8B0L59+wYVx61bt9KrVy/Wrl0btPznn3+mffv29OnTh3Xr1uHz+QLrVq1aRZs2bUhOTm5QsSrxww8/YBgGffv2DVqu86pslXEOlddHfXHs2DGuu+46srOzef3114NuZALlv16h4cSqMl5vDSVWJdatW1fqnIKGd16dKj/Q+5VUlob6mV8W5eShUU5eccrJK0afcaFTTh465eQVp5y8AeTjptRKTz/9tNm3b1/ziy++MLds2WJOnDjRvOCCC0yXy1XTQ6s2u3btMrt162bedtttZkZGRtC/vLw8c9myZWanTp3M+fPnm3v37jWXL19uDh8+3LzrrrsCfTSUOPp8PvPyyy83R40aZa5Zs8bcsWOH+de//tXs3r27uXXrVjMzM9Ps06ePOX36dHP79u3mu+++a6alpZnvvfdeoI+GEqsSc+fONS+44IJSy3VeFZs+fbo5fvz4wPPKOIdC6aMu+nWspk+fbnbr1s1ctWpVqfcur9db7uvVNBtOrCrj9dZQYmWapun1es1u3bqZH374Yan2Dem8Ki8/0PuVVKaG8JlfHuXkoVNOXnHKyU9NOXnolJOHTjl56JSTl60h5OMqmNdSXq/XnDVrltm/f3+zZ8+e5k033WTu37+/podVrZ577jmzY8eOZf6bPn26aZqm+dlnn5mjR482zzzzTHPgwIHmY489ZjqdzkAfDSmOWVlZ5owZM8yBAweaaWlp5pVXXmmuWbMmsP7HH380r7jiCrN79+7mueeeay5atCho+4YUK9M0zQceeMC84oorylyn86rsxKAyzqHy+qiLToyVz+cz09LSTvreVRKP8l6vpln/Y1WiMl5vDSVWmZmZZseOHc0VK1aUuU1DOa9CyQ/0fiWVpSF85pdHOXnFKCevGOXkp6acPHTKyUOnnDx0ysnL1hDyccM0TbPqr2MXEREREREREREREandNIe5iIiIiIiIiIiIiAgqmIuIiIiIiIiIiIiIACqYi4iIiIiIiIiIiIgAKpiLiIiIiIiIiIiIiAAqmIuIiIiIiIiIiIiIACqYi4iIiIiIiIiIiIgAKpiLiIiIiIiIiIiIiAAqmIuIiIiIiIiIiIiIABBW0wMQEZHqdc8997B48eKTrk9ISGD16tXVOCLo1KkTt99+O5MmTarW/YqIiIiI1ATl5CIitZcK5iIiDVBKSgrz5s0rc11YmD4aRERERESqmnJyEZHaSe/AIiINkM1mo2fPnjU9DBERERGRBks5uYhI7aSCuYiIlGnChAk0b96cNm3a8Oqrr+JwOOjXrx/33nsvLVq0CLT76aefmDNnDj///DMej4e+ffsyZcoUOnToEGiTlZXFU089xfLly3E4HHTt2pW77rqLs88+O9CmoKCAmTNnsnTpUjweD4MHD+aBBx4gOTm5Wo9bRERERKS2UE4uIlL9dNNPEZEGyuv1lvnPNM1Amy+//JJ3332XmTNn8vDDD7N161auvfZaioqKAPjuu++4+uqr8fv9/OUvf+HRRx/l8OHDXHXVVezcuROAoqIirrrqKr799lumTJnCvHnziI6O5o9//GOgDcCrr76Kx+PhmWee4c4772TZsmU89NBD1RsUEREREZFqpJxcRKT20RXmIiIN0MGDB+nWrVuZ6yZPnsytt94KFCfW7777Li1btgSgbdu2jBkzhsWLFzNu3DieeuopWrRowUsvvYTVagVg0KBBnH/++cydO5c5c+awePFi9u/fz/vvv0/nzp0B6N27N6NHj2bNmjW0a9cOgLS0NGbNmgXAgAED2LhxIytWrKjSOIiIiIiI1BTl5CIitZMK5iIiDVBKSgrPPfdcmesaN24ceNyrV69AYg7QtWtXWrRowdq1axkzZgw//fQTt912WyAxB4iLi+Pcc8/lq6++AmDt2rWcccYZgcQcwG638+mnnwbt98SvggK0aNGCvLy80z9IEREREZFaTDm5iEjtpIK5iEgDZLPZSEtLK7ddampqqWXJycnk5eWRn5+PaZo0atSoVJtGjRqRn58PQG5ubkhzHkZFRQU9t1gsQV9FFRERERGpT5STi4jUTprDXERETio3N7fUsszMTJKSkoiNjcUwDDIzM0u1OXr0KAkJCQDExsaSnZ1dqs369evZvn17ZQ9ZRERERKReUU4uIlK9VDAXEZGTWr9+fVBivWnTJg4cOMCAAQOIioqie/fufPLJJ/h8vkCb/Px8li9fHvg6Z+/evdm/fz/btm0LtHG73UyaNIm33nqr+g5GRERERKQOUk4uIlK9NCWLiEgD5Ha72bBhw0nXd+zYEQCHw8FNN93ELbfcQmFhIbNnz6Zjx46MGjUKgClTpnDjjTfyxz/+kfHjx+PxeFiwYAFut5vbb78dgD/84Q8sWrSIW265hcmTJ5OUlMS//vUvnE4nEyZMqPJjFRERERGpjZSTi4jUTiqYi4g0QEePHuXKK6886fp33nkHKL4SpX///sycOROA4cOHM23aNGw2GwADBgzg5Zdf5tlnn+Wuu+7CZrPRu3dvHn/8cTp06ABATEwMr732GrNmzeIvf/kLXq+XHj16sGjRoqCbF4mIiIiINCTKyUVEaifD1N0bRESkDCVXmixatKiGRyIiIiIi0jApJxcRqX6aw1xEREREREREREREBBXMRUREREREREREREQATckiIiIiIiIiIiIiIgLoCnMREREREREREREREUAFcxERERERERERERERQAVzERERERERERERERFABXMREREREREREREREUAFcxERERERERERERERQAVzERERERERERERERFABXMREREREREREREREUAFcxERERERERERERERQAVzEREREREREREREREA/j8ZFbbKbLA0xgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(metrics, k_fold=False, log_scale=True, save_path=f'Result/history/{reservoirname}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amn4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
